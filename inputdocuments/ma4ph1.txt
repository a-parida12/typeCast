Mathematik fuĚr Physiker I
Michael Dreher
Fachbereich fuĚr Mathematik und Statistik
UniversitaĚt Konstanz
Studienjahr 2011/12

2

Etwas Juristisches:
Dieses
Werk
ist
unter
einem
Creative Commons AttributionâNonCommercialâNoDerivs
3.0 Unported Lizenzvertrag lizenziert. Um die Lizenz anzusehen, gehen Sie bitte zu
http://creativecommons.org/licenses/by-nc-nd/3.0/de/ oder schicken Sie einen Brief an Creative Commons, 171 Second Street, Suite 300, San Francisco, California 94105, USA.

Wer aus den BuĚchern nicht mehr lernt, als was in den BuĚchern steht,
der hat die BuĚcher nicht halb genutzt.
Gotthold Ephraim Lessing, 1729â1781

4

Inhaltsverzeichnis
1 Grundlagen
1.1

1.2

1.3

1.4

1.5

1.6

11

Wiederholung aus der Schulanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.1.1

Reelle Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.1.2

Funktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

Die komplexen Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

1.2.1

Zur Einstimmung: Die Cardanische Formel . . . . . . . . . . . . . . . . . . . . . . .

13

1.2.2

Die komplexen Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

1.2.3

Funktionen komplexer Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

1.2.4

Ausblick: Elektrotechnik . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

1.2.5

Ausblick: Mechanik

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

1.3.1

Allgemeine Eigenschaften . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

1.3.2

Drehungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

Gruppentheorie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

1.4.1

EinfuĚhrung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

1.4.2

Ausblick: Gruppen in der Physik . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

1.4.3

Ausblick: Mobilfunk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

Der Raum und der R3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

1.5.1

Allgemeines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

1.5.2

Vektorprodukt und Spatprodukt . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

Die Ebene und der R

2

3

1.5.3

Drehungen im R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

1.5.4

Der affine Raum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

SchluĚsselbegriffe

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 VektorraĚume

46
47

2.1

Allgemeine Eigenschaften . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

47

2.2

Linearkombinationen, Erzeugendensysteme usw. . . . . . . . . . . . . . . . . . . . . . . . .

49

2.2.1

Linearkombinationen und UnterraĚume . . . . . . . . . . . . . . . . . . . . . . . . . .

49

2.2.2

Lineare UnabhaĚngigkeit und Basen . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

2.2.3

Dimension eines Vektorraumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

VektorraĚume mit Skalarprodukt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

57

2.3.1

Skalarprodukte, Normen, Orthogonalsysteme . . . . . . . . . . . . . . . . . . . . . .

57

2.3.2

Approximationsprobleme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

60

2.4

Ausblick: VektorraĚume in der Physik 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

62

2.5

Ausblick: die HelmholtzâProjektion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

2.6

SchluĚsselbegriffe

64

2.3

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5

6

INHALTSVERZEICHNIS

3 Matrizen

65

3.1

Operationen mit Matrizen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

3.2

Gleichungssysteme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

3.3

SchluĚsselbegriffe

72

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Homomorphismen

73

4.1

Allgemeine Eigenschaften . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

4.2

Geometrische Aspekte . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

76

4.3

Lineare Gleichungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

4.4

Basistransformationen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

4.5

Differentialgleichungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

84

4.6

Ausblick: Lineare Abbildungen in der Physik . . . . . . . . . . . . . . . . . . . . . . . . . .

87

4.7

Ausblick: VektorraĚume in der Physik 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

90

4.8

SchluĚsselbegriffe

92

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Normierte RaĚume, Reelle Zahlen, Folgen, Reihen
d

d

5.1

Folgen im R und C

5.2

Folgen und Reihen in normierten RaĚumen

5.3

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

93
93

. . . . . . . . . . . . . . . . . . . . . . . . . . .

96

5.2.1

VollstaĚndigkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

97

5.2.2

Reihen in normierten RaĚumen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

99

5.2.3

Konvergenzkriterien . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

Folgen und Reihen reeller Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.3.1

Schranken und Grenzen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

5.3.2

Beispiele fuĚr konvergente Folgen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

5.3.3

Nichtabsolute Konvergenz und Umordnungen . . . . . . . . . . . . . . . . . . . . . . 107

5.4

Potenzreihen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

5.5

Beispiel: Die Exponentialfunktion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

5.6

SchluĚsselbegriffe

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

6 Funktionen

115

6.1

Grenzwerte von Funktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

6.2

Stetigkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

6.3

Differenzierbarkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

6.4

Mittelwertsatz und Taylorscher Satz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

6.5

Elementare Funktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

6.6

6.7

6.5.1

Der geometrische Zugang zu den Winkelfunktionen . . . . . . . . . . . . . . . . . . . 137

6.5.2

Der analytische Zugang zu den Winkelfunktionen . . . . . . . . . . . . . . . . . . . . 141

6.5.3

Die Hyperbelfunktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

6.5.4

Wurzeln aus komplexen Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

Verfahren zur numerischen LoĚsung nichtlinearer Gleichungen . . . . . . . . . . . . . . . . . 149
6.6.1

Das Halbierungsverfahren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

6.6.2

Funktionaliteration und der Banachsche Fixpunktsatz . . . . . . . . . . . . . . . . 150

6.6.3

Das Newtonverfahren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

SchluĚsselbegriffe

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154

A Algebraische Strukturen

155

Einleitung
Herzlich willkommen im Physikstudium !
Zu jedem Physikstudium gehoĚrt ein Mathematikkurs, und dieser Versuch eines Vorworts soll beschreiben,
welche Ziele wir mit diesem Mathematikkurs anpeilen. Im Schnelldurchlauf: es sind etwas andere Ziele als
im gymnasialen Mathematikunterricht (wobei hier nicht eroĚrtert werden soll, ob der schulische Mathematikunterricht sinnvoll konzipiert ist und ob er seine Ziele uĚberhaupt erreicht).
Bekanntlich ist die Physik eine Wissenschaft. Das Gegenteil davon ist die Unwissenschaft, wie sie vertreten wird von WuĚnschelrutengaĚngern, Schamanen, Esoterikern und sonstigen verwirrten PersoĚnlichkeiten.
Zwischen Wissenschaftlern und Nichtwissenschaftlern besteht der ausschlaggebende Unterschied, daĂ die
Wissenschaftler ihre Arbeitsmethoden praĚzise begruĚnden und ihre erzielten Ergebnisse rechtfertigen koĚnnen,
und zwar (z.B.) auf folgenden Wegen:
Experiment: die Voraussagen einer Theorie werden gepruĚft (Arbeitsweise in der Experimentalphysik),
Theorie: eine Aussagen wird logisch praĚzise aus elementaren Prinzipien hergeleitet, die man als wahr
voraussetzt (Arbeitsweise in der theoretischen Physik und der Mathematik),
numerische Simulationen: das sind Rechenexperimente auf dem Computer, z.B. um eine Theorie zu
testen.
Da wir nun als Wissenschaftler wahrgenommen werden wollen, kommen wir nicht drumherum, diese AnspruĚche an wissenschaftliches Arbeiten auch gegen uns gelten zu lassen, woraus sich ein erster Unterschied
zum Schulunterricht ergibt:
Die schultypische Frage nach dem Wie (Wie verlaĚuft der Rechenweg ?) wird ersetzt durch die Frage nach
dem Warum: Warum glaube ich eigentlich, daĂ mein Rechenweg mich uĚberhaupt zum Rechenziel fuĚhrt ?
Gelegentlich artet die Abiturvorbereitung aus in ein Antrainieren halbverstandener Rechenrituale; diese
Phase soll im Physikstudium nicht wiederkehren.

Was ist eigentlich Mathematik ?
Wie eben schon angedeutet, geht es in der Mathematik nicht um das Abspulen von immergleichen Rechenschemata, ganz im Gegenteil:
(a) Mathematik ist die Kunst, stumpfsinniges Rechnen zu vermeiden. Es gibt keinen Platz fuĚr haĚĂliche
Mathematik, genauso wie es keinen Platz gibt fuĚr haĚĂliche Physik.
(b) Mathematik ist eine Wissenschaft, die Strukturen erforscht und durchschaubar macht.
Dabei strebt die Mathematik zuallererst nach Erkenntnis und versucht zu erklaĚren, warum die (von ihr
betrachtete Facette der hoĚchst vielfaĚltigen) Wirklichkeit so ist, wie sie ist; und genau dasselbe gilt natuĚrlich
fuĚr jede andere Wissenschaft. Ein besonderes Merkmal einer jeden Naturwissenschaft ist es, daĂ ein Forscher1 sich zwar ein bestimmtes Forschungsziel stellen kann, aber es ist zunaĚchst nicht klar, ob das Problem
uĚberhaupt2 loĚsbar ist, ob man es selber schafft (oder ob jemand anders schneller ist), oder ob der gewaĚhlte
Weg nicht vielleicht eine Sackgasse ist und man am Ende der muĚhseligen Plackerei mit leeren HaĚnden
1 aus

GruĚnden der sprachlichen UĚbersichtlichkeit sind nur die maskulinen Personenbezeichnungen angefuĚhrt . . .
Spielzeugprobleme, bei denen jeder ahnt, was herauskommt, und die nur Wenige interessieren

2 Ausnahme:

7

8

EINLEITUNG

dasteht. In der Schule ist das bekanntlich anders, denn dort bekommt man nur Aufgaben, die garantiert
mit dem vorher vermittelten Wissen loĚsbar sind.
Kreative und phantasievolle Menschen sind klar im Vorteil, und es lohnt sich, um die Ecke zu denkenâ!
â
Ein weiteres wichtiges Kennzeichen mathematischer Arbeit ist die Abstraktion. Das bedeutet, daĂ man
bei einer Situation die Eigenschaften von allen beteiligten Objekten entsprechend der Kategorien wichtig
/ unwichtig sortiert, die unwichtigen wegwirft, damit die wichtigen klarer zu sehen sind (und damit das
Problem uĚberhaupt erst einmal handhabbar wird). Ein Physiker macht genau das gleiche, wenn er sagt
hierbei betrachten wir nur Punktmassenâ. Bei Vektoren kann es zum Beispiel sein, daĂ ihre Pfeil-Gestalt
â
wichtig ist (wenn man z.B. die KraĚfte auf einen KoĚrper untersucht, dann ist die Veranschaulichung der
KraĚfte als Pfeile voĚllig natuĚrlich). Man kann die Eigenschaft eines Vektors, ein Pfeilâ zu sein, aber auch
â
wegwerfen, und lediglich die beiden Eigenschaften uĚbrigbehalten, daĂ man Vektoren addieren kann, und
daĂ man Vektoren mit einer Zahl multiplizieren kann (sowie einiger Rechenregeln fuĚr diese Operationen).
Genau dasselbe trifft aber auch auf Funktionen zu, denn diese lassen sich auch zueinander addieren bzw.
mit einer Zahl multiplizieren.
Wir duĚrfen also Funktionen als Vektoren ansehen !
Das ist ein Beispiel fuĚr einen Abstraktionsschritt. Und weil wir spaĚtestens in der Quantenmechanik sowieso
dazu gezwungen sein werden, diesen Abstraktionsschritt (Funktionen als Vektoren anzusehen) zu vollziehen,
wollen wir den Gedanken Vektor = Pfeil gar nicht weiter verfestigen (dann muĚĂten wir spaĚter umlernen)
und arbeiten in diesem Kurs fast von Anfang an mit abstrakten Vektoren (ab Kapitel 2).
Wir schauen uns noch ein Beispiel an fuĚr Funktionen als Vektorenâ:
â
â˘ Wir betrachten eine Funktion, die aus einer reellen Zahl eine reelle Zahl macht. Bekanntlich kann
man von einer solchen Funktion Minima suchen, indem man die Ableitung gleich Null setzt.
â˘ Jetzt betrachten wir eine Funktion, die aus einer Funktion eine reelle Zahl macht. Aus GruĚnden der
sprachlichen Klarheit redet man besser von einem Funktional, und das wichtigste Beispiel ist vielleicht
das Wirkungsfunktional aus der Theoretischen Mechanik. Viele Bewegungsgesetze der Mechanik (z.B.
alles vom Typ F = ma) folgen aus dem Prinzip der kleinsten Wirkung. Und wie sucht man nun Minima
des Wirkungsfunktionals ? Durch Nullsetzen der Ableitung (was auch immer das ist), wobei die
eingesetzten Argumente des Wirkungsfunktionals jetzt keine Zahlen x â R sind, sondern Funktionen
aus einem unendlichdimensionalen Funktionenvektorraum.
Ein wenig hochtrabend formuliert, benutzt die Theoretische Mechanik also die Differentialrechnung in
unendlichdimensionalen FunktionenvektorraĚumen, und dafuĚr wollen wir mit diesem Mathematikkurs die
Grundlagen zu legen versuchen.
Mit etwas GluĚck kann es dann auf dem Wege der Abstraktion gelingen, Gemeinsamkeiten zwischen Dingen
zu entdecken, die weit entfernt voneinander scheinen, und schon hat man ein wenig besser erkannt, wie die
Welt aussieht und was deren ZusammenhaĚnge sind.

Methodische Konsequenzen
Es ist praktisch nicht durchfuĚhrbar, sich im Studium auf jede EventualitaĚt des spaĚteren Berufslebens
vorzubereiten und sich fuĚr jede auftretbare Situation ein paĂgenau zugeschnittenes Rechenschema zurechtzulegen. DafuĚr ist die uns umgebende Welt einfach zu vielgestaltig. AuĂerdem ist das GedaĚchtnis gar nicht
in der Lage, mehrere hundert isolierte Einzelinformationen abzuspeichern, wenn diese nicht untereinander
vernetzt sind. Viel wichtiger ist es, inhaltliche ZusammenhaĚnge zu erkennen, auch damit der Lernstoff ein
assoziatives Netz bildet, das sich erheblich einfacher einpraĚgen wird. Die im Skript aufgefuĚhrten Beweise
sind anhand des Kriteriums ausgewaĚhlt worden, ob sie inhaltliche ZusammenhaĚnge beleuchten und somit
ein tieferes VerstaĚndnis ermoĚglichen (und eine akzeptable LaĚnge aufweisen).
Bei den Hausaufgaben treten also Rechenaufgaben mit Zahlen in den Hintergrund. KreativitaĚt und Phantasie sind gewuĚnscht, weshalb bei einigen Hausaufgaben nicht sofort ersichtlich ist, wie der LoĚsungsweg
aussieht; noch dazu kann es mehrere Wege zur LoĚsung geben. Wenn man sich ungluĚcklicherweise verirrt,
kann die LoĚsung womoĚglich lang und etwas haĚĂlich sein (aber das merkt man natuĚrlich).

EINLEITUNG

9

Garantie: Die 4 Aufgaben eines Hausaufgabenblattes sind auf etwa 6 Seiten loĚsbar.
Das haĚngt natuĚrlich von der SchriftgroĚĂe ab, aber wenn Sie mehr als 8 Seiten brauchen, machen Sie sicherlich
etwas falsch, und wenn Sie weniger als 4 Seiten benoĚtigen, machen Sie eventuell etwas falsch.
Am Ende eines jeden Kapitels sind SchluĚsselbegriffe aufgelistet. Diese heiĂen deshalb so, weil sie eine
SchluĚsselbedeutung haben fuĚr das VerstaĚndnis des Skriptinhalts. Demnach sollten Sie diese Begriffe verstanden haben, also: Definition wiedergeben koĚnnen, Eigenschaften beschreiben und Querverbindungen zu
anderen Begriffen benennen. Mit diesen Begriffen sollten Sie so vertraut sein, daĂ Sie die kurzen Beweise verstehen und bei den laĚngeren Beweisen (es sind eher wenige) auf jeden Fall die zentralen Gedanken
angeben koĚnnen.

Wie loĚst man Hausaufgaben ?
Weil das Wichtigste am Studium nicht etwa die Vorlesungen sind, sondern die eigenstaĚndige Auseinandersetzung mit dem Lernstoff, kommen hier noch einige Anmerkungen zu den Hausaufgaben.
Lassen Sie sich von Ihrem UnterbewuĂtsein helfen. Dieses braucht allerdings seine AufwaĚrmzeit, und es
muĂ wissen, wobei es eigentlich helfen soll, und es ist auch nicht zuverlaĚssiger als die Eisenbahn.
Sie sollten also so schnell wie moĚglich die Aufgabe verstehen. Dazu besorgen Sie sich von allen vorkommenden mathematischen Fachbegriffen die exakte Bedeutung, also die Definition. Diese finden Sie im Skript
und in BuĚchern (meistens mit einem Index ausgestattet); nur eingeschraĚnkt empfehlenswert ist Wikipedia. Tragen Sie alle Eigenschaften dieser Begriffe zusammen, die Sie finden. Stellen Sie sicher, daĂ Sie die
zusammengetragenen Dinge auch verstanden haben (hierbei sind ggf. Skizzen oder evtl. Zahlenbeispiele
brauchbar).
Nun muĚĂten Sie ein solides VerstaĚndnis dessen haben, worum es in der Aufgabe eigentlich geht. Wenn es sich
um eine Rechenaufgabe handelt, ist jetzt nicht mehr allzuviel zu tun (die abiturtypischen Rechentechniken
setzen wir als gefestigt voraus).
Falls es um einen Beweis geht: druĚcken Sie Voraussetzungen und Behauptung mit Ihren herausgeschriebenen
Definitionen aus (formulieren Sie also um). Da Sie die Aufgabenstellung jetzt verinnerlicht haben, koĚnnen
Sie also beim Schlangestehen im Supermarkt druĚber nachdenken oder bei jeder anderen Gelegenheit. Echte
Zahlenbeispiele koĚnnen hilfreich sein zur Ideenfindung, Taschenrechnerexperimente ebenfalls. Werden Sie
kreativ (ab dieser Stelle gibt es kein Kochrezept mehr, wie auch ? Es geht ja schlieĂlich um Phantasie, und
wenn es dafuĚr ein Schema zum Abspulen gaĚbe, dann wuĚrden wir wahrscheinlich in einer langweiligen Welt
leben).
Diese Suche nach einer Stelle, wo man ansetzen kann, ist vielleicht muĚhselig, aber fuĚr den Lerneffekt
unverzichtbar. Eine abgeschriebene LoĚsung haben Sie binnen 10 Tagen vergessen, aber Sie werden sich
monatelang an den Moment erinnern, als nach laĚngerer Anstrengung endlich der Groschen fiel. Der Schwung
dieses Erfolgs wird Sie durch das ganze Semester tragen. Es lohnt sich ! Eine selbstgefundene LoĚsung ist
soviel wert wie 10 abgeschriebene. Wer abpinselt, tut sich selbst keinen Gefallen und studiert einfach nur
ineffizient.
Nach einiger Zeit haben Sie also eine mutmaĂliche LoĚsung erhalten. WomoĚglich ist der Gedankengang
etwas zickzackig (das geht den professionellen Mathematikern und Physikern genauso), eventuell findet Ihr
UnterbewuĂtsein dann am naĚchsten Tag3 eine Idee, wie man ihn begradigen kann.
Wenn Sie schon im Team arbeiten, koĚnnen Sie ja so vorgehen, daĂ jeder einzeln fuĚr sich die LoĚsung sucht
und danach beide Varianten verglichen werden. Vielleicht kann man beide Wege zu einem zusammenfuĚgen,
der schoĚner ist. Oder Sie lesen bei Ihrem Kommilitonen Korrektur (es hinterlaĚĂt einen schiefen Eindruck,
wenn in einer Woche auf einmal 7 Physikstudenten der Meinung sind, daĂ 14 + 14 = 18 !). Falls Sie im
Team auf gemeinsamem Blatt abgeben, seien Sie nicht uĚberrascht, wenn die UĚbungsleiter dann strengere
BewertungsmaĂstaĚbe anlegen, als wenn Sie alleine abgegeben haĚtten.
Achten Sie auf die aĚuĂere Form Ihrer LoĚsung. Die LoĚsung soll so klar dargestellt werden, daĂ auch ein Leser, der weder Aufgabe noch einen korrekten LoĚsungsweg kennt, erkennen kann, was eigentlich hier los ist.
Korrektoren sind dankbar, wenn sie nicht gedraĚngt sind, irgendwelche hingekritzelten Gedankenfragmente muĚhsam zusammenzupuzzlen, sondern wenn die LoĚsung ein richtiger Text ist mit syntaktisch korrekten
3 im

UĚbrigen werden die meisten Texte besser, wenn man sie mit einigem zeitlichen Abstand noch mal neu schreibt

10

EINLEITUNG

SaĚtzen der deutschen Sprache. Die Kunst, sich im ganzen Satz gut auszudruĚcken, ist bemerkenswert schwierig, sodaĂ man fruĚhestmoĚglich mit dem UĚben anfangen sollte. Es ist durchaus wahrscheinlich, daĂ Sie Ihre
Hausaufgabenzettel spaĚter noch ein mal benoĚtigen werden, und dafuĚr waĚre es dann nutzvoll, die LoĚsungen
so klar und einleuchtend aufgeschrieben zu haben, daĂ Sie diese auch nach einem halben Jahr noch zuĚgig
verstehen.

Rechentechnische Hinweise
Jede Rechnung dient einem bestimmten Zweck.
Oft moĚchte man mit der Rechnung irgendwelche Ergebnisse ermitteln und diese dann woanders weiterverwerten. In einem solchen Fall ist es ein natuĚrlicher Wunsch, die Rechnerei nicht laĚnger zu haben als
angemessen.
Es kann aber auch sein, daĂ der Rechenweg selbst aufschluĂreich ist und einige ZusammenhaĚnge beleuchten
kann. Das funktioniert natuĚrlich nur, wenn man beim Rechnen kein unentwirrbares Gleichungsspaghetti
fabriziert hat.
In beiden FaĚllen lohnt es sich, oĚkonomisch zu rechnen.
Generell gilt dabei: wenn eine Gleichungsumformung keinen benennbaren Nutzen hat â weglassen oder
zumindest so weit wie moĚglich hinauszoĚgern. Spezieller heiĂt das zum Beispiel: UnterdruĚcken Sie den Reflex
Ich multipliziere erst mal alles ausâ.
â
3x+7
â˛â˛
Als instruktives Beispiel suchen wir Wendepunkte von f = f (x) = (x+2)
2 durch Nullsetzen von f . FleiĂiges
Ausmultiplizieren saĚmtlicher Klammern und die Regel ( uv )â˛ =

uâ˛ vâuv â˛
v2

fuĚhren auf

â3x2 â 14x â 16
,
+ 8x3 + 24x2 + 32x + 16
6x5 + 66x4 + 288x3 + 624x2 + 672x + 288
f â˛â˛ (x) = 8
,
x + 16x7 + 112x6 + 448x5 + 1120x4 + 1792x3 + 1792x2 + 1024x + 256
f â˛ (x) =

x4

und nun stecken wir fest, denn die Gleichung f â˛â˛ (x) = 0 bekommen wir nicht geloĚst.
Das ist einfach keine Mathematik mehr, denn der VerstoĂ gegen Prinzip (a) ist offensichtlich (kein Mathematiker oder Physiker mit aĚsthetischem GespuĚr rechnet freudvoll einen Monsterterm wie (â3x2 â 14x â
16)â˛ Âˇ (x4 + 8x3 + 24x2 + 32x + 16) â (â3x2 â 14x â 16) Âˇ (x4 + 8x3 + 24x2 + 32x + 16)â˛ aus. Abgesehen davon
ist die Wahrscheinlichkeit von Rechenfehlern viel zu hoch.).
Es liegt aber auch ein VerstoĂ gegen Prinzip (b) vor, denn der Bruch fuĚr f â˛â˛ (x) enthaĚlt eine verborgene
Struktur, die jedoch aufgrund der miĂlungenen Darstellung unsichtbar ist. Oder ist Ihnen etwa aufgefallen,
daĂ man diesen Bruch kuĚrzen kann durch x4 + 8x3 + 24x2 + 32x + 16 ?!
Wenn man stattdessen das komplett uĚberfluĚssige Ausmultiplizieren des Nenners sein laĚĂt und jede Gelegenheit zum KuĚrzen sofort nutzt, bekommt man auf wesentlich schnellerem Wege
f â˛ (x) =

â3x â 8
,
(x + 2)3

f â˛â˛ (x) =

6x + 18
.
(x + 2)4

Weitere Tips:
Nutzen Sie bei Zwischenergebnissen, wann immer es sich anbietet, die MoĚglichkeit zur Zwischenprobe, und
zwar auf einem anderen Wege. Wenn Sie solche anderen ZugaĚnge gezielt suchen, trainieren Sie gleichzeitig
Ihren Durchblick durch die Mathematik, und auf den kommt es ja an.
GewoĚhnen Sie sich UĚberschlagsrechnungen im Kopf an; Sie werden schneller sein als der Taschenrechner.
Formulieren Sie mit Worten, was Sie tun wollen, und schreiben Sie das dann auch hin. Es sollen keine
Romane sein, eine Anmerkung der Form Setze Gleichung (â) in (ââ) ein und loĚse nach y aufâ reicht voĚllig,
â
tut Wunder und begluĚckt den Leser, weil die Struktur Ihrer Arbeit sichtbar wird.

Viel Erfolg !

Kapitel 1

Grundlagen
1.1
1.1.1

Wiederholung aus der Schulanalysis
Reelle Zahlen

Wir listen einige bekannte Eigenschaften der reellen Zahlen auf:
Seien a, b, c, d â R beliebig. Dann haben wir:
KommutativitaĚt der Addition: a + b = b + a
AssoziativitaĚt der Addition: (a + b) + c = a + (b + c)
0 ist neutrales Element fuĚr die Addition: 0 + a = a + 0 = a
Subtraktion: Jede Gleichung a + x = b (mit gegebenem a, b) ist loĚsbar, die (eindeutige) LoĚsung x wird
geschrieben als x = b â a.
KommutativitaĚt der Multiplikation: a Âˇ b = b Âˇ a
AssoziativitaĚt der Multiplikation: (a Âˇ b) Âˇ c = a Âˇ (b Âˇ c)
1 ist neutrales Element fuĚr die Multiplikation: 1 Âˇ a = a Âˇ 1 = a
Division: Jede Gleichung a Âˇ x = b (mit gegebenem a, b; jedoch a 6= 0) ist loĚsbar, die (eindeutige) LoĚsung
x wird geschrieben als x = b/a.
Distributivgesetz: Addition und Multiplikation sind verzahnt gemaĚĂ der Regel (a + b) Âˇ c = a Âˇ c + b Âˇ c.
Bemerkung 1.1. Man sagt auch, daĂ die Menge der reellen Zahlen, gemeinsam mit den Operationen
(+, Âˇ), einen KoĚrper bildet. Weitere Beispiele fuĚr KoĚrper sind die rationalen Zahlen oder die gebrochenrationalen Funktionen (also Funktionen der Form f = f (x) = p(x)
q(x) , wobei p und q Polynome sind).
AuĂerdem haben wir noch eine Ordnungsrelation â¤, mit folgenden Eigenschaften:
ReflexivitaĚt: Es ist a â¤ a.
Antisymmetrie: Wenn a â¤ b und b â¤ a, dann ist a = b.
TransitivitaĚt: Wenn a â¤ b und b â¤ c, dann auch a â¤ c.
Und diese Ordnungsstruktur ist verzahnt mit der KoĚrperstruktur auf folgende Weise:
Addition von Ungleichungen: Wenn a â¤ b und c â¤ d, dann ist auch a + c â¤ b + d.
Multiplikation von Ungleichungen mit nichtnegativen Zahlen: Wenn a â¤ b und 0 â¤ c, dann ist
ac â¤ bc.
11

12

KAPITEL 1. GRUNDLAGEN

Warnung 1.2. Man darf zwar 2 Ungleichungen addieren, aber nicht voneinander abziehen oder durcheinander dividieren, oder mit negativen Zahlen multiplizieren.
Bekanntlich kann man die reellen Zahlen als Punkte auf der Zahlengeraden darstellen, und in dieser Form
wird der Abstand zweier reeller Zahlen a, b auf der Zahlengeraden gegeben durch |a â b|. Insbesondere stellt
|a| den Abstand des Punktes a vom Ursprung dar. Der Betrag hat folgende 3 Eigenschaften:
â˘ Es ist stets |a| âĽ 0; und |a| = 0 genau dann, wenn a = 0.
â˘ Es ist |a Âˇ b| = |a| Âˇ |b|.
â˘ Wir haben die Dreiecksungleichung: |a + b| â¤ |a| + |b|.
Bemerkung 1.3. Diese 3 Eigenschaften werden uns spaĚter in anderen ZusammenhaĚngen noch oĚfters
begegnen; wir werden dann sagen, daĂ eine Norm vorliegt.

1.1.2

Funktionen

Zu einer Funktion f gehoĚren zwei Dinge:
â˘ ein Definitionsbereich Df â R. Wenn man den Definitionsbereich veraĚndert (zum Beispiel verkleinert),
dann erhaĚlt man eine andere Funktion.
â˘ eine Vorschrift, wie die x aus dem Definitionsbereich in den Wertebereich abgebildet werden.
Beispiele fuĚr Funktionen sind:
â˘ Polynome; der Definitionsbereich ist ganz R,
â˘ gebrochen rationale Funktionen f = f (x) = g(x)/h(x) mit Polynomen g und h; der Definitionsbereich
ist R \ {Nullstellen von h},
â˘ die Winkelfunktionen sin und cos,
â˘ die Logarithmusfunktionen (definiert auf R+ ).
Eine wichtige Eigenschaft von Funktionen ist die Stetigkeit. Wenn wir sagen, daĂ eine Funktion f im Punkte
x0 stetig ist, dann meinen wir im wesentlichen folgendes:
Wir koĚnnen den Abstand von f (x) und f (x0 ), das heiĂt den Wert |f (x) â f (x0 )|, beliebig klein bekommen,
wenn wir dafuĚr sorgen, daĂ der Abstand von x und x0 , also |x â x0 |, nur klein genug ist (eine genauere
Definition kommt spaĚter).
Die oben genannten Funktionen sind stetig uĚberall dort, wo sie definiert sind.
Beispiele fuĚr Unstetigkeiten sind Sprungstellen oder Polstellen. Es gibt aber noch weitere Typen von Unstetigkeiten.
Eine weitere wichtige Eigenschaft einer Funktion ist die Differenzierbarkeit. Anschaulich gesprochen, ist eine
Funktion f im Punkte x0 â Df differenzierbar, wenn der Graph der Funktion in (x0 |f (x0 )) eine Tangente
hat, die nicht vertikal verlaĚuft. Dann kann diese Tangente als Graph einer linearen Funktion
t = t(x) = f (x0 ) + A(x â x0 ),

x â R,

interpretiert werden. Der Anstieg A dieser linearen Funktion heiĂt Ableitung von f in x0 und wird geschrieben als
A=

df
(x0 )
dx

oder

A = f â˛ (x0 ).

Etwas exakter formuliert, ist die Funktion f in x0 differenzierbar genau dann, wenn der Grenzwert des
Differenzenquotienten vorhanden ist, das heiĂt
lim

xâx0

f (x) â f (x0 )
x â x0

existiert.

1.2. DIE KOMPLEXEN ZAHLEN

13

Der Wert dieses Grenzwertes ist dann gleich A.
Jede differenzierbare Funktion ist stetig.
Die meisten aus der Schule bekannten Funktionen (Polynome, gebrochen rationale Funktionen, Winkelfunktionen, Exponentialfunktionen, Logarithmusfunktionen) sind uĚberall dort, wo man sie sinnvoll definieren
kann, differenzierbar. Die Wurzelfunktionen und die Betragsfunktion f = f (x) = |x| sind im Nullpunkt
nicht differenzierbar.
Die Rechenregeln fuĚr die Ableitung (Produktregel, Kettenregel usw.) sowie die Formeln fuĚr die Ableitungen
der elementaren Funktionen wollen wir als bekannt voraussetzen.
Wenn die Ableitung f â˛ einer Funktion f stetig ist, dann heiĂt f stetig differenzierbar. Wenn diese stetige
Ableitung f â˛ wiederum differenzierbar ist, dann nennt man f zweimal differenzierbar, und die Ableitung
von f â˛ wird mit f â˛â˛ bezeichnet. Wenn diese zweite Ableitung f â˛â˛ stetig sein sollte, dann nennen wir f zweimal
stetig differenzierbar, und so weiter. Wenn saĚmtliche Ableitungen existieren und stetig sind, dann heiĂt f
unendlich oft differenzierbar.
Frage: Was sind die Unterschiede (in grammatischer und in mathematischer Hinsicht) zwischen den beiden
folgenden Formulierungen:
â˘ Sei f eine stetig differenzierbare Funktion.
â˘ Sei f eine stetige, differenzierbare Funktion.
Jede stetige Funktion ist auch integrierbar, das heiĂt, man kann bestimmte und unbestimmte Integrale
bilden. Darunter verstehen wir das folgende:
Sei das Intervall [a, b] im Definitionsbereich der stetigen Funktion f enthalten. Dann wird der FlaĚcheninhalt
derjenigen FlaĚche, die von der xâAchse, dem Graphen der Funktion f und den Geraden x = a und x = b
eingegrenzt wird, als bestimmtes Integral von f uĚber dem Intervall [a, b] bezeichnet:
Z b
A=
f (x)dx.
x=a

Dies ist lediglich eine anschauliche Beschreibung, die fuĚr die ersten Zwecke jedoch ausreicht. SpaĚter werden
wir eine exakte Definition nachreichen; insbesondere werden wir dann umgekehrt vorgehen: der FlaĚcheninhalt wird definiert mit Hilfe des bestimmten Integrales.
R
Das unbestimmte Integral
f (x)dx bezeichnet die Menge aller derjenigen Funktionen F = F (x), deren
Ableitung F â˛ (x) gleich f (x) ist. Solche Funktionen F heiĂen Stammfunktionen.
Die Beziehung zwischen Differenzieren und Integrieren wird durch den folgenden Satz hergestellt, der besagt,
daĂ jedes bestimmte Integral mit variabler oberer Grenze eine Stammfunktion ist.
Satz 1.4 (Hauptsatz der Differentialâ und Integralrechnung). Sei f eine stetige Funktion und
[a, b] â Df . Sei F = F (x) die Funktion, die auf [a, b] definiert ist und durch
Z x
F (x) =
f (t) dt
t=a

gegeben wird. Dann ist F differenzierbar, und es ist F â˛ (x) = f (x) fuĚr jedes x â [a, b].

1.2

Die komplexen Zahlen

1.2.1

Zur Einstimmung: Die Cardanische Formel

Wir schauen uns die Gleichung
x3 + px + q = 0

(1.1)

an. Hierbei sind p und q gegebene reelle Zahlen, und wir suchen reelle LoĚsungen x. FuĚr solche kubischen
Gleichungen gibt es ein LoĚsungsverfahren, das nach Geronimo Cardano (1501â1576) benannt ist:
q
â
Sei D = ( p3 )3 + ( 2q )2 und w = 3 â 2q + D. Dann wird eine LoĚsung gegeben durch
x1 = w â

p
.
3w

14

KAPITEL 1. GRUNDLAGEN

Durch naivâunbekuĚmmertes Einsetzen kann man nachrechnen, daĂ diese Zahl x1 tatsaĚchlich eine LoĚsung
von (1.1) ist.
Wenn man den zu dieser LoĚsung gehoĚrenden Linearfaktor (x â x1 ) abdividiert, also die Polynomdivision
(x3 + px + q) : (x â x1 ) durchfuĚhrt, kommt man zu einem quadratischen Polynom mit bis zu zwei Nullstellen
x2 und x3 . Insgesamt hat die Gleichung (1.1) dann maximal 3 LoĚsungen x1 , x2 und x3 .
Wir probieren dieses Verfahren mal an einem Beispiel aus:
2
20
x3 + x â
= 0.
3
27
p
â
1 3
Wir kommen auf D = 108
10 + 108. Dies ist eine irrationale Zahl, aber wenn wir daraus
729 und w = 3
dann x1 berechnen, dann heben sich die IrrationalitaĚten mysterioĚserweise heraus und wir finden
x1 =

2
,
3

was auch tatsaĚchlich eine LoĚsung der kubischen Gleichung ist, wie man durch eine Probe feststellt. Abdividieren des Linearfaktors (x â 32 ) fuĚhrt uns zum quadratischen Polynom x2 + 23 x + 10
9 , welches keine reellen
Nullstellen hat.
Als weiteres Beispiel betrachten wir
x3 â 63x + 162 = 0.

(1.2)

Dann ist p = â63, q = 162, und fuĚr w erhalten wir
q
â
3
w = â81 + â2700.
Und hier ergeben sich mindestens zwei Probleme:
â˘ was soll

â
â2700 sein,

â˘ wenn wir
â eine Deutung von
â81 + â2700 ?

â
â2700 gefunden haben sollten, was ist dann die dritte Wurzel aus

Wir koĚnnten uns jetzt auf den Standpunkt stellen, daĂ diese Formeln keinen Sinn haben, und die Gleichung
(1.2) eben keine einzige LoĚsung hat. Aber verwirrenderweise gibt es LoĚsungen, und zwar gleich drei, naĚmlich
die reellen Zahlen 3, 6 und â9, wie man leicht nachrechnet.

Cardano und viele seiner Kollegen in den nachfolgenden Jahrhunderten gewannen den Eindruck, daĂ es
nuĚtzlich
sein kann, mit solchen AusdruĚcken zu rechnen. Allerdings gelang es lange nicht, Terme der Form
â
â2700 zu interpretieren. Man meinte, daĂ Quadratwurzeln aus negativen Zahlen nur in der Einbildungâ
â
existieren, und bezeichnete solche Zahlen demzufolge als imaginaĚre Zahlen, im Gegensatz zu reellen Zahlen,
die eine Entsprechung in der Wirklichkeit haben.
Drei Jahrhunderte spaĚter, 1831 durch Carl Friedrich GauĂ (1777â1855) und 1837 durch William R.
Hamilton (1805â1865), gelang es endlich, diese imaginaĚren Zahlen exakt und logisch sauber einzufuĚhren.
Das sollten wir jetzt auch tun.

1.2.2

Die komplexen Zahlen

Definition 1.5. 1 Unter einer komplexen Zahl2 z verstehen wir ein geordnetes Paar (x, y) â R2 . Wir
bezeichnen x als Realteil3 und y als ImaginaĚrteil4 der komplexen Zahl z = (x, y); dafuĚr schreiben wir:
x = âz,

y = âz.

Die Menge aller komplexen Zahlen wird mit C bezeichnet.
1 Einige Worte zur Typographie: Definitionen und SaĚtze sind in italics gesetzt. Innerhalb einer Definition steht der zu
definierende Begriff in aufrechten Lettern. Definitionen und SaĚtze sind dort zu Ende, wo der Textsatz in italics aufhoĚrt. Das
Ende eines Beweises markieren wir mit .
2 auf Englisch: complex number
3 real part
4 imaginary part

15

1.2. DIE KOMPLEXEN ZAHLEN

Wir sagen, daĂ zwei komplexe Zahlen z = (x, y) und w = (u, v) gleich sind, wenn sie in ihren Realteilen
bzw. ImaginaĚrteilen uĚbereinstimmen, das heiĂt x = u und y = v.
Komplexe Zahlen kann man addieren und multiplizieren, und zwar wie folgt:
Definition 1.6. Seien z = (x, y) und w = (u, v) komplexe Zahlen. Dann definieren wir die Summe z + w
und das Produkt z Âˇ w als
z + w := (x + u, y + v),

z Âˇ w := (xu â yv, xv + yu).

Wir vermerken kurz, daĂ (0, 1) Âˇ (0, 1) = (â1, 0).
Lemma 1.7. 5 Addition und Multiplikation komplexer Zahlen sind kommutativ und assoziativ. AuĂerdem
gilt das Distributivgesetz. Die komplexe Zahl (0, 0) ist neutrales Element fuĚr die Addition, und die komplexe
Zahl (1, 0) ist neutrales Element fuĚr die Multiplikation.
Das bedeutet in Formeln: fuĚr beliebige (p, q), (x, y), (u, v) â C gilt:


(p, q) + (x, y) = (x, y) + (p, q),



(p, q) + (x, y) + (u, v) = (p, q) + (x, y) + (u, v) ,

(q, p) Âˇ (x, y) = (x, y) Âˇ (p, q),



(p, q) Âˇ (x, y) Âˇ (u, v) = (p, q) Âˇ (x, y) Âˇ (u, v) ,


(p, q) Âˇ (x, y) + (u, v) = (p, q) Âˇ (x, y) + (p, q) Âˇ (u, v),


(0, 0) + (x, y) = (x, y),

(1, 0) Âˇ (x, y) = (x, y).
Beweis als UĚbungsaufgabe.
Es ist klar, wie man die Subtraktion als Umkehrung der Addition einfuĚhrt: naĚmlich komponentenweise.
Es fehlt uns bloĂ noch die Division, um zu zeigen, daĂ die komplexen Zahlen ebenfalls einen KoĚrper bilden,
siehe auch Bemerkung 1.1. Wir verschieben die EinfuĚhrung der Division fuĚr einen Moment. Als Vorbereitung
schauen wir uns vorher erst diejenigen komplexen Zahlen genauer an, deren zweite Komponente Null ist:
Lemma 1.8. Es ist
(x, 0) + (u, 0) = (x + u, 0)

und

(x, 0) Âˇ (u, 0) = (x Âˇ u, 0).

Beweis als UĚbungsaufgabe.
Wir beobachten, daĂ die komplexen Zahlen mit verschwindender6 zweiter Komponente sich wie reelle Zahlen
verhalten. Es ist z.B. 5 + 7 = 12 und (5, 0) + (7, 0) = (12, 0). Anstelle von (x, 0) koĚnnten wir in Zukunft
einfach x schreiben und somit Tinte sparen.
Jede reelle Zahl kann als komplexe Zahl interpretiert werden.
Wir definieren:
Definition 1.9. Die komplexe Zahl (0, 1) wird als imaginaĚre Einheit7 i bezeichnet,
i := (0, 1).
Weiterhin rechnet man nach, daĂ fuĚr z = (x, y) gilt:
z = (x, y) = (x, 0) + (0, y) = (x, 0) + (0, 1) Âˇ (y, 0).
In unserer neuen Schreibweise lautet das z = x + i Âˇ y.
5 Ein Lemma ist ein kleiner Satz oder oft auch ein Hilfssatz. Die nach Bedeutung abgestufte Folge der beweisbaren
mathematischen Aussagen lautet Theorem, Satz, Lemma.
6 Wir sagen, daĂ eine Zahl verschwindet, wenn sie Null wird.
7 imaginary unit

16

KAPITEL 1. GRUNDLAGEN

Die Formel (0, 1) Âˇ (0, 1) = (â1, 0) kann man also auch schreiben als
i Âˇ i = â1.
Und, allgemeiner, die Formeln fuĚr die Addition und Multiplikation lauten in der neuen Schreibweise:
z + w = (x + iy) + (u + iv) = (x + u) + i(y + v),
z Âˇ w = (x + iy) Âˇ (u + iv) = (xu â yv) + i(xv + yu).
Wir rechnen also wie gewohnt und beachten dabei lediglich die Sonderregel i2 = â1.
Bevor wir zur Division kommen, brauchen wir noch 2 Begriffe:

Definition 1.10. Sei z = (x, y) â C eine komplexe Zahl. Dann heiĂt
z := (x, ây) = x â iy
die komplex konjugierte Zahl8 zu z, und
|z| :=

p
x2 + y 2

heiĂt Betrag9 von z.
Man zeigt schnell, daĂ die folgenden Rechenregeln gelten:
Satz 1.11. Seien z = (x, y) und w = (u, v) komplexe Zahlen. Dann gilt:
z + w = z + w,
z Âˇ w = z Âˇ w,
z = z,

z Âˇ z = (|z|2 , 0).
In Worten zusammengefaĂt: es ist egal, ob wir zuerst konjugieren und dann addieren (multiplizieren), oder
umgekehrt. Diese beiden Regeln koĚnnen wir auch als kommutative Diagramme veranschaulichen:

z, w
ďŁŚ
ďŁŚ+
y

?

âââââ

z + w âââââ
?

z, w
ďŁŚ
ďŁŚ+
y

z+w
=z+w

?

z, w âââââ
ďŁŚ
ďŁŚÂˇ
y

z Âˇ w âââââ
?

z, w
ďŁŚ
ďŁŚÂˇ
y

zÂˇw
=zÂˇw

Das Fragezeichen ist dabei als PlatzâFreihalter zum Einsetzen zu lesen. Wir bezeichnen ein Diagramm als
kommutativ , wenn jeder Pfad von der Ecke links oben zur Ecke rechts unten das gleiche Ergebnis liefert.
Die letzte Regel in Satz 1.11 bedeutet, daĂ das Produkt aus einer komplexen Zahl und ihrer Konjugierten
stets reell ist.
Satz 1.12. Die Betragsfunktion erfuĚllt die Eigenschaften einer Norm (siehe Bemerkung 1.3). Das heiĂt:
|z| âĽ 0;

|z| = 0 genau dann, wenn z = (0, 0),

|z Âˇ w| = |z| Âˇ |w|,
|z + w| â¤ |z| + |w|.

Beweis. UĚbungsaufgabe.
8 complex
9 absolute

conjugate
value

17

1.2. DIE KOMPLEXEN ZAHLEN

Die erste und die letzte dieser Eigenschaften sind geometrisch unmittelbar einsichtig, wenn man sich die
komplexen Zahlen (die ja geordnete Paare reeller Zahlen sind) als Vektoren in der Ebene vorstellt. Der
Betrag einer komplexen Zahl ist nichts anderes als die geometrische LaĚnge des zugeordneten Vektors.
Nun zur Division. Gegeben sind zwei komplexe Zahlen w = u + iv und r = p + iq, und gesucht ist deren
Quotient z = x + iy, also soll gelten
r Âˇ z = w.
Wir multiplizieren mit r:
r Âˇ w = r Âˇ (r Âˇ z) = (r Âˇ r) Âˇ z = |r|2 Âˇ z = |r|2 Âˇ (x + iy) = |r|2 x + i(|r|2 y).
Jetzt nutzen wir aus, daĂ wir auf der ganz linken Seite eine bekannte komplexe Zahl stehen haben:
r Âˇ w = (p + iq) Âˇ (u + iv) = (p â iq) Âˇ (u + iv) = (pu + qv) + i(pv â qu).
Wir vergleichen Realâ und ImaginaĚrteil in den beiden Formelzeilen:
pu + qv
pv â qu
x=
, y=
.
|r|2
|r|2

Das Ganze wird vielleicht etwas einsichtiger in folgender Schreibweise:
z=

pu + qv
pv â qu
u + iv
(u + iv)(p â iq)
(pu + qv) + i(pv â qu)
w
= 2
+i 2
.
=
=
=
2
2
2
r
p + iq
(p + iq)(p â iq)
p +q
p +q
p + q2

Damit haben wir gezeigt, daĂ die Menge der komplexen Zahlen, zusammen mit der Addition und der
Multiplikation, einen KoĚrper bildet, was nichts anderes heiĂt, daĂ wir mit den 4 Grundrechenarten umgehen
koĚnnen, wie wir es gewohnt sind.
Leider ist es nicht moĚglich, auf C eine Ordnungsrelation zu definieren, die sich mit den Rechenoperationen
vertraĚgt (warum ?).
Wir wenden uns nun nochmal der geometrischen Veranschaulichung der komplexen Zahlen als Vektoren
(bzw. Punkte) der zweidimensionalen Ebene zu. Wir bezeichnen die horizontale Achse als reelle Achse, und
die vertikale Achse als imaginaĚre Achse. Durch Hinschauen stellen wir dann fest, daĂ fuĚr jede komplexe
Zahl z 6= 0 gilt:
âz = |z| cos Ď,

âz = |z| sin Ď.

Hierbei bezeichnet Ď den Winkel zwischen dem positiven Teil der reellen Achse und dem Vektor, der zu z
gehoĚrt. (Ab jetzt wollen wir (aus sprachlichen GruĚnden) meistens nicht mehr unterscheiden zwischen einer
komplexen Zahl und dem Vektor, der diese komplexe Zahl geometrisch veranschaulicht.) Dieser Winkel Ď
wird auch als Argument von z bezeichnet.
Was passiert bei Addition und Multiplikation ?
Man sieht schnell, daĂ die Addition komplexer Zahlen der gewoĚhnlichen Vektoraddition entspricht.
FuĚr die Multiplikation betrachten wir zwei komplexe Zahlen
z = x + iy = |z|(cos Ď + i sin Ď),

w = u + iv = |w|(cos Ď + i sin Ď),

und erhalten aus den schulbekannten Additionstheoremen, daĂ
z Âˇ w = |z|(cos Ď + i sin Ď) Âˇ |w|(cos Ď + i sin Ď)
= |z| Âˇ |w|((cos Ď cos Ď â sin Ď sin Ď) + i(cos Ď sin Ď + sin Ď cos Ď))
= |z| Âˇ |w|(cos(Ď + Ď) + i sin(Ď + Ď)).

Das Produkt von z und w ist also eine komplexe Zahl, deren Betrag gleich dem Produkt der BetraĚge von
z und w ist; und das Argument von z Âˇ w ist gleich der Summe der Argumente von z und w.
Die Multiplikation komplexer Zahlen entspricht einer Drehstreckung.
Interessant ist noch der Fall z = w, dann haben wir
z 2 = |z|2 (cos(2Ď) + i sin(2Ď)),

wenn z = |z|(cos Ď + i sin Ď).

Mittels vollstaĚndiger Induktion bekommen wir die sogenannte Formel von Moivre (Abraham de Moivre,
1667â1754):
z n = |z|n (cos(nĎ) + i sin(nĎ)),

wenn z = |z|(cos Ď + i sin Ď) und n â N.

18

KAPITEL 1. GRUNDLAGEN

1.2.3

Funktionen komplexer Zahlen

Genauso wie man Funktionen von reellen Variablen definieren und untersuchen kann, kann man dies auch
mit Funktionen von komplexen Zahlen tun.
Einfache Beispiele fuĚr solche Funktionen sind Polynome,
P (z) = an z n + anâ1 z nâ1 + Âˇ Âˇ Âˇ + a1 z + a0 ,

n â N,

mit Koeffizienten aj â C und Definitionsbereich C.

Ein anderes â und sehr wichtiges â Beispiel ist die komplexe Exponentialfunktion. Bekanntlich erfuĚllt die
herkoĚmmliche Exponentialfunktion die Beziehungen
ex1 +x2 = ex1 Âˇ ex2 ,

eâx1 =

1
,
ex1

e0 = 1

fuĚr beliebige reelle x1 , x2 â R.
Definition 1.13. Sei z = x + iy â C eine komplexe Zahl. Dann definieren wir
ez = ex+iy := ex (cos y + i sin y).
Diese komplexwertige Funktion mit Definitionsbereich C heiĂt komplexe Exponentialfunktion.
SpaĚter werden wir eine andere Definition bevorzugen, vgl. Definition 5.70. Weiterhin ist ex+iy im Moment
einfach nur eine Schreibweise. Wir denken ausdruĚcklich nicht daran, eine Zahl e â 2.71828 . . . in die
(x + iy)-te Potenz zu erheben, denn eine solche Potenz ist anschaulich kaum vorstellbar.
Satz 1.14. Die komplexe Exponentialfunktion hat die folgenden Eigenschaften:

10

e0 = 1,
ez+w = ez Âˇ ew , z, w â C,
1
eâz = z , z â C.
e
Beweis. Wir haben e0 = e0+i0 = e0 (cos 0 + i sin 0) = 1(1 + 0) = 1.
Seien nun z = x + iy und w = u + iv. Dann ist einerseits
ez+w = e(x+u)+i(y+v) = ex+u (cos(y + v) + i sin(y + v)),
andererseits, wegen der Additionstheoreme fuĚr die Sinus- und Kosinusfunktionen,
ez Âˇ ew = ex (cos y + i sin y)eu (cos v + i sin v)

= ex+u ((cos y cos v â sin y sin v) + i(cos y sin v + sin y cos v))

= ex+u (cos(y + v) + i sin(y + v)).
Also ist ez+w = ez Âˇ ew .

Und die dritte Beziehung beweist sich jetzt fast von selbst:
1 = e0 = ez+(âz) = ez Âˇ eâz .

Wenn wir annehmen, daĂ z den Realteil Null hat (also eine sogenannte rein imaginaĚre Zahl ist), dann folgt
die Beziehung
eiy = cos y + i sin y,

y â R.

Falls wir hier y = Ď setzen, entsteht die beruĚhmte Gleichung
eiĎ + 1 = 0,
10 die Schreibweise . . .
, z, w â Câ am Ende der zweiten Formelzeile soll ausdruĚcken, daĂ die vorher geschriebene Aussage
â
fuĚr alle z, w â C gelten moĚge. In Zukunft werden wir sehr oft diese abgekuĚrzte Ausdrucksweise gebrauchen.

19

1.2. DIE KOMPLEXEN ZAHLEN
die saĚmtliche fuĚnf fuĚr den Physiker unverzichtbaren Zahlen in einer Zeile vereinigt.

Und wenn z den ImaginaĚrteil Null hat (also reell ist), dann erhalten wir aus cos 0 = 1 und sin 0 = 0, daĂ
ez = ex .
Also stimmt die neu definierte komplexe Exponentialfunktion mit der herkoĚmmlichen reellen Exponentialfunktion uĚberein, wenn das Argument der komplexen Exponentialfunktion zufaĚlligerweise reell ist.
FuĚr eine komplexe Zahl z = x + iy mit Betrag |z| und Argument Ď haben wir jetzt drei aĚquivalente
Schreibweisen
z = x + iy,
z = |z|(cos Ď + i sin Ď),

z = |z|eiĎ ,

die wir im Folgenden gleichberechtigt verwenden werden.
Nun zu den Winkelfunktionen. Wir koĚnnen die Winkelfunktionen im Reellen mittels der Exponentialfunktion ausdruĚcken:
Satz 1.15. Sei Ď â R. Dann gilt
cos Ď =

1 iĎ
(e + eâiĎ ),
2

sin Ď =

1 iĎ
(e â eâiĎ ).
2i

Beweis. Wir haben
eiĎ = cos Ď + i sin Ď,

eâiĎ = cos(âĎ) + i sin(âĎ) = cos Ď â i sin Ď,

denn die Kosinusfunktion ist gerade11 , und die Sinusfunktion ist ungerade12 . Jetzt brauchen wir diese
beiden Gleichungen bloĂ zueinander addieren bzw. voneinander abziehen, und der Beweis ist vollendet.
Damit koĚnnen wir jetzt auch Winkelfunktionen fuĚr komplexe Zahlen erklaĚren:
Definition 1.16. FuĚr z â C definieren wir sin z und cos z durch
sin z :=

1 iz
(e â eâiz ),
2i

cos z :=

1 iz
(e + eâiz ).
2

Frage: FuĚr x â R gilt bekanntlich | sin x| â¤ 1 sowie | cos x| â¤ 1 und cos2 (x) + sin2 (x) = 1. Welche dieser
Beziehungen bleiben guĚltig, wenn man x â R ersetzt durch z â C ?

Falls z in dieser Definition reell sein sollte, erhalten wir genau die herkoĚmmlichen Winkelfunktionen. Wir
haben also diese Winkelfunktionen von der Menge der reellen Zahlen in die Menge der komplexen Zahlen
fortgesetzt, im Sinne einer VergroĚĂerung des Definitionsbereiches. Wir koĚnnten noch weitere Funktionen
von den reellen Zahlen in die komplexen Zahlen fortsetzen, verschieben das aber auf ein spaĚteres Kapitel.
Stattdessen wollen wir uns uĚberlegen, welche Eigenschaften Funktionen haben koĚnnten.
Zum Beispiel koĚnnte eine Funktion w : C â C stetig13 sein. Was heiĂt das ? Wir koĚnnen die Stetigkeit
genauso definieren wie bei Funktionen w : R â R. In Worten ausgedruĚckt, ist die Stetigkeit der Funktion
w : C â C im Punkte z0 â C folgendermaĂen definiert:

Wir koĚnnen erzwingen, daĂ der Wert |w(z) â w(z0 )| beliebig klein wird, wenn wir nur dafuĚr sorgen, daĂ
|z â z0 | klein genug ist.

Hierbei bedeuten die Betragsstriche natuĚrlich den komplexen Betrag.

Die bisher betrachteten Funktionen (Polynome, komplexe Exponentialfunktion, komplexe Winkelfunktionen) sind allesamt in ganz C stetig.
Eine weitere interessante Eigenschaft einer Funktion ist die Differenzierbarkeit. Hierbei muĚssen wir allerdings etwas aufpassen und zwei FaĚlle unterscheiden.
11 in

der Schule sagt man gelegentlich achsensymmetrischâ
â
punktsymmetrischâ
13 âEs lohnt sich, fuĚr die Schreibweise w : C â Câ eine Lesevorschrift mitzuteilen. Der Doppelpunkt bedeutet bewirkt,
â
â
daĂâ. Das erste C gibt den Definitionsbereich der Funktion w an, und die Passage â Câ lesen wir als abgebildet wird nach
â
â
Câ. Insgesamt also w bewirkt, daĂ C abgebildet wird nach Câ. Das zweite C enthaĚlt (als Teilmenge) die Menge der von w
â
tatsaĚchlich angenommenen Werte, wobei es erlaubt ist, daĂ nicht ganz C von den tatsaĚchlich angenommenen Funktionswerten
von w ausgeschoĚpft wird.
12

20

KAPITEL 1. GRUNDLAGEN
â˘ Funktionen w : R â C,
â˘ Funktionen w : C â C.

Im ersten Fall koĚnnen wir uns vorstellen, daĂ die Funktion w auf einem Intervall [a, b] â R definiert ist, wir
koĚnnen also schreiben
t â [a, b].

w = w(t) = u(t) + iv(t),

Naheliegenderweise wird man die Ableitung wâ˛ (t) komponentenweise definieren wollen:
wâ˛ (t) := uâ˛ (t) + iv â˛ (t),

t â [a, b].

Hierbei haben wir stillschweigend angenommen, daĂ die reellwertigen Funktion u, v : [a, b] â R im
herkoĚmmlichen Sinne differenzierbar sind.
Im zweiten Fall haben wir es mit einer Funktion w = w(z) zu tun, wobei jetzt z aus ganz C stammen darf.
Wenn wir w = u + iv und z = x + iy schreiben, haben wir
w(z) = u(x + iy) + iv(x + iy),

x + iy â C,

und wir definieren die Ableitung als Grenzwert der Differenzenquotientenâ. Und zwar:
â
Wenn der Grenzwert
w(z) â w(z0 )
, z â C,
lim
zâz0
z â z0

existiert, dann sagen wir, daĂ die Funktion w : C â C im Punkte z0 â C differenzierbar ist, und setzen
wâ˛ (z0 ) gleich diesem Grenzwert. Wir verlangen dabei, daĂ z auf beliebigem Wege gegen z0 laufen darf (z.B.
entlang einer heftig verknoteten Kurve, oder auch punktweise wild herumhopsend); und jedesmal soll der
Grenzwert des Differenzenquotienten denselben Wert haben. Eine saubere Definition kommt spaĚter, siehe
Definition 6.39.

Warnung 1.17. Die Ableitung ist jetzt eine komplexe Zahl, und es ist nur schwer moĚglich, den Wert der
Ableitung in gewohnter Form als Anstieg der Tangenteâ zu interpretieren.
â
Diese Ableitung hat genau die gleichen Eigenschaften wie die herkoĚmmliche Ableitung reellwertiger Funktionen:
Satz 1.18. Seien w = w(z) und r = r(z) differenzierbare Funktionen von C nach C. Dann gilt:
(w + r)â˛ (z) = wâ˛ (z) + râ˛ (z), z â C,
(cw)â˛ (z) = c Âˇ wâ˛ (z), z â C, c â C,

(w Âˇ r)â˛ (z) = wâ˛ (z) Âˇ r(z) + w(z) Âˇ râ˛ (z), z â C,
 w â˛
wâ˛ (z) Âˇ r(z) â w(z) Âˇ râ˛ (z)
, z â C,
(z) =
r
r(z)2
w(r(z))â˛ = wâ˛ (r(z)) Âˇ râ˛ (z), z â C.

falls

r(z) 6= 0,

Die Beweise dieser Formeln verlaufen genauso wie im reellen Fall. Dies ist deshalb moĚglich, weil die reellen
Zahlen genauso wie die komplexen Zahlen einen KoĚrper bilden; mit anderen Worten, daĂ die Grundrechenarten denselben Regeln genuĚgen.
Genauso wie im reellen Fall kann man nachweisen, daĂ jede differenzierbare Funktion w : C â C stetig ist.
AuĂerdem sind die oben vorgestellten Funktionen differenzierbar, und es ist
(z n )â˛ = nz nâ1 , z â C,
(ez )â˛ = ez , z â C,

n â N = {1, 2, . . . },

sinâ˛ (z) = cos(z), z â C,
cosâ˛ (z) = â sin(z), z â C.
Warnung 1.19. Leider gibt es Funktionen, die wunderbar harmlos ausschauen, aber nicht differenzierbar
sind. Ein Beispiel einer solchen Funktion ist w = w(z) = |z|2 . Diese Funktion ist lediglich im Ursprung
differenzierbar, und sonst nirgendwo. Sei z.B. z0 = 1 + 2i. Wenn wir z einmal horizontal gegen z0 schicken
und einmal vertikal, bekommen wir fuĚr den Grenzwert des Differenzenquotienten zwei verschiedene Werte,
wie man schnell nachrechnet. Deshalb ist w im Punkte z0 = 1 + 2i nicht differenzierbar. Wir muĚssen eine
genauere Betrachtung der komplexen Differentialrechnung auf spaĚter verschieben.

21

1.2. DIE KOMPLEXEN ZAHLEN

1.2.4

Ausblick: Elektrotechnik

Hier ergibt sich ein kleines Problem mit den Schreibweisen: in der Elektrotechnik bezeichnet man die
StromstaĚrke mit i bzw. I, weshalb wir fuĚr die imaginaĚre Einheit in diesem Abschnitt j schreiben, j2 = â1.
Einen elektrischen Wechselstrom mit Amplitude im , Kreisfrequenz Ď = 2Ďf und Phasenverschiebung Ďi
koĚnnen wir ausdruĚcken als
i(t) = im cos(Ďt + Ďi ),

im > 0,

Ď > 0,

Ďi â R,

t â R.

â
Wenn wir fuĚr den sogenannten Effektivwert der StromstaĚrke I schreiben, I := im / 2, dann erhalten wir

â
2IejĎi ejĎt .
i(t) = â

Der Ausdruck IejĎi ist eine komplexe Zahl, die nicht von der Zeit t abhaĚngt. Um die Schreibweise zu
vereinfachen, fuĚhren wir den komplexen Effektivwert ein:
I := IejĎi
und es folgt
i(t) =

â

2â IejĎt .

Die Unterstriche sollen ausdruĚcken, daĂ die betreffende GroĚĂe sich wie ein Zeigerâ benimmt. Genauso
â
verfahren wir mit der Spannung:
u(t) = um cos(Ďt + Ďu ), um > 0,
â
U := um / 2, U := U ejĎu ,
â

u(t) = 2â U ejĎt .

Ď > 0,

Ďu â R,

t â R,

Die komplexen Zahlen IejĎt und U ejĎt drehen sich in der komplexen Ebene im Gegenuhrzeigersinn, wenn
t waĚchst; und die Radien der Kreisbahnen sind I bzw. U . FuĚr einen Umlauf brauchen diese Vektoren die
Zeit 1/f , so wie man es erwartet.
Der Winkel zwischen den Vektoren IejĎt und U ejĎt ist immer gleich, naĚmlich Ďi â Ďu . Dies ist genau die
Phasenverschiebung zwischen StromstaĚrke und Spannung.
Spannung und StromstaĚrke stehen zueinander in Beziehung uĚber den Widerstand. Wir haben die folgenden
3 FaĚlle:
Ohmscher Widerstand: U = RI, R â R,
Spule (induktiver Widerstand): U = jĎLI, L â R,
1
Kondensator (kapazitiver Widerstand): U = âj ĎC
I, C â R.

Nun fuĚhren wir den komplexen Scheinwiderstand ein:
Z=

U
.
I

Mit diesem Scheinwiderstand koĚnnen wir genauso rechnen wie mit ohmschen WiderstaĚnden: bei einer Reihenschaltung addieren sich die ScheinwiderstaĚnde, bei einer Parallelschaltung addieren sich die Reziproken
der ScheinwiderstaĚnde.
FuĚr die Schaltung in Abbildung 1.1 haben wir zum Beispiel
Z=

1
1
R1 âj/(ĎC1 )

+

1
R2 +jĎL

âj

1
.
ĎC2

Wenn man den Scheinwiderstand Z in der Form
Z = R + jX,

R, X â R,

schreibt, dann heiĂt R = âZ der Wirkwiderstand und X = âZ der Blindwiderstand der Schaltung. Es ist
eine interessante Aufgabe, mathematisch praĚzise zu beweisen, daĂ bei allen typischenâ Schaltungen der
â
Wirkwiderstand niemals negativ werden kann. AĚquivalent dazu waĚre zu zeigen, daĂ Strom und Spannung
nicht zueinander um mehr als Ď/2 phasenverschoben sein koĚnnen.

22

KAPITEL 1. GRUNDLAGEN

R_1

C_1
C_2

R_2

L

Abbildung 1.1: Elektrische Schaltung

1.2.5

Ausblick: Mechanik

Eine ebene Welle, die sich im Ortsraum R3 ausbreitet, wird haĚufig modelliert mittels Funktionen der Form
(t, x) 7â ei(kxâĎt) .
Der Pfeil 7â besagt, daĂ das Paar (t, x) abgebildet wird auf ei(kxâĎt) â C. Hierbei benennen t â R und
x â R3 die Variablen fuĚr Zeit und Ort, ihre Einheiten sind natuĚrlich Sekunde und Meter. Weiterhin ist
k = (k1 , k2 , k3 ) ein Vektor, der senkrecht auf den Wellenfronten steht und in der Physik Wellenzahlvektor
1
genannt wird. Seine Einheit ist Meter
. Das Produkt kx ist zu verstehen als k1 x1 +k2 x2 +k3 x3 . Und Ď (omega)
1
. SchlieĂlich ist natuĚrlich i2 = â1. Der Zusammenhang
heiĂt oft Kreisfrequenz mit der Einheit Sekunde
Ď
Ď
â
, mit c als Ausbreitungsgeschwindigkeit der Welle.
zwischen diesen Parametern ist c = |k| =
2
2
2
k1 +k2 +k3

Frage: Warum liefert dies tatsaĚchlich eine Beschreibung von ebenen Wellen im R3 ?
Frage: Sei k â R3 gegeben. Wie sieht die Menge aller x â R3 geometrisch aus, fuĚr die kx = const. ?

1.3

Die Ebene und der R2

1.3.1

Allgemeine Eigenschaften

Wir starten mit einigen Begriffen, die bekannt sein sollten:
Ebene: genauer gesagt, die Menge der Ortsvektoren der (zweidimensionalen) Ebene. Achtung: dies ist
nicht dasselbe wie der R2 !
R2 : die Menge aller geordneten Paare14 (x, y) mit x, y â R.
Ortsvektor: ein Vektor ( Pfeilâ), der im Ursprung anfaĚngt.
â
Addition von Ortsvektoren: ergibt wieder einen Ortsvektor.
Multiplikation eines Ortsvektors mit einer reellen Zahl: ergibt wieder einen Ortsvektor.
Kartesisches Koordinatensystem: besteht aus zwei Koordinatenachsen, die einander im Ursprung
schneiden (und zwar rechtwinklig), und je einem Einheitsvektor pro Achse. Diese beiden Einheitsvektoren sind gleichlang.
Bemerkung 1.20. In manchen BuĚchern (auch in der Schule) werden Vektoren definiert als AĚquivalenzklassen von Pfeilen, wobei zwei Pfeile dann als aĚquivalent gelten, wenn sie durch Parallelverschiebung
auseinander hervorgehen.
Von einer solchen Definition haĚlt der Autor sehr wenig. Zum einen gehen diese AĚquivalenzklassenbetrachtungen am eigentlichen Wesensgehalt von Vektoren vorbei, und zum zweiten sind physikalisch relevante
Vektoren eben keine AĚquivalenzklassen (man denke an die Kraftvektoren an einer Wippe).
14 Die Formulierung geordnetes Paarâ soll lediglich ausdruĚcken, daĂ die Reihenfolge der beiden EintraĚge im Paar wichtig
â
ist. Es ist also (2, 3) 6= (3, 2), wie bereits aus dem schulischen Umgang mit Koordinatensystemen bekannt.

1.3. DIE EBENE UND DER R2

23

Wichtig ist, sich folgendes klar zu machen:
Die Addition zweier Vektoren und die Multiplikation eines Vektors mit einer reellen Zahl kann man rein
geometrisch definieren, voĚllig ohne den Begriff eines Koordinatensystems ! Es ergibt sich z.B. ~a + ~b als
der Diagonalenortsvektor des von den Ortsvektoren ~a und ~b aufgespannten Parallelogramms. Und die
Multiplikation eines Ortsvektors mit einer Zahl kann man sich so vorstellen, daĂ an der Pfeilspitzeâ in
â
Pfeilrichtungâ gezogen wird.
â
Der folgende Satz kann dann bewiesen werden. Beim Beweis ist zu beachten, daĂ das Konzept der Koordinaten eines Vektors nicht zur VerfuĚgung steht (man hat also nur die Elementargeometrie der Mittelstufe
zur Hand), was fuĚr die AssoziativitaĚt der Addition eine gewisse Erschwernis darstellt.
Satz 1.21. Sei V die Menge der Ortsvektoren der Ebene. Weiterhin seien15
+ : V Ă V â V,
Âˇ : RĂV âV

die geometrisch definierten Operationen Addition zweier Vektorenâ und Multiplikation eines Vektors mit
â
â
einer reellen Zahlâ. Dann haben wir die folgenden Eigenschaften fuĚr alle Vektoren x, y, z â V und alle
reellen Zahlen Îť, Âľ â R:
KommutativitaĚt der Addition: x + y = y + x
AssoziativitaĚt der Addition: (x + y) + z = x + (y + z)
~0 ist neutrales Element der Addition: ~0 + x = x + ~0 = x
Subtraktion: Jede Gleichung a + x = b (mit gegebenem a, b â V ) ist loĚsbar, die (eindeutige) LoĚsung x
wird geschrieben als x = b â a.
AssoziativitaĚt der Multiplikation: (Îť Âˇ Âľ) Âˇ x = Îť Âˇ (Âľ Âˇ x)
1 ist neutrales Element der Multiplikation: 1 Âˇ x = x
Zwei Distributivgesetze: Addition und Multiplikation sind verzahnt gemaĚĂ der Regeln (Îť + Âľ) Âˇ x =
Îť Âˇ x + Âľ Âˇ x und Îť Âˇ (x + y) = Îť Âˇ x + Îť Âˇ y.
Frage: KoĚnnen Sie an einem Beispiel zeigen, daĂ die Regel 1 Âˇ x = x nicht aus den uĚbrigen Regeln geschluĂfolgert werden kann ? Bei diesem Beispiel waĚren die Operationen + und Âˇ anders definiert, und die
genannten Eigenschaften wuĚrden immer noch gelten, abgesehen von 1 Âˇ x = x.
Bemerkung 1.22. Wir sagen auch, daĂ (V, +, Âˇ) einen Vektorraum16 uĚber R bildet.
Es gibt einen Zusammenhang zwischen der zweidimensionalen Ebene und dem R2 , der durch ein (kartesisches) Koordinatensystem vermittelt wird: Bekanntlich kann man jeden Ortsvektor der Ebene als Linearkombination der Basisvektoren des Koordinatensystems darstellen. Die Koeffizienten dieser Linearkombination sind reelle Zahlen, bilden also gerade ein geordnetes Paar, also ein Element aus dem R2 .
Allerdings haben wir einige Freiheiten bei der Wahl des Koordinatensystems. Es ist zwar uĚblich, daĂ die
eine Koordinatenachse nach rechtsâ zeigt und die andere nach obenâ, aber das ist keineswegs zwingend.
â
â
Die Koordinatenachsen koĚnnten auch gespiegelt sein oder schief liegenâ. Und wenn das Koordinatensystem
â
(oder, praĚziser, das geordnete Paar der Basisvektoren) anders gewaĚhlt werden, dann aĚndern sich auch die
Koeffizienten eines Vektors bezuĚglich des jeweiligen Koordinatensystems.
Die Koeffizienten eines Vektors haĚngen von der gewaĚhlten Basis ab !
Im Folgenden haben wir uns fuĚr ein Koordinatensystem (also eine Orthonormalbasis) entschieden, das wir
vorerst nicht mehr aĚndern, und dessen Basisvektoren e1 â V und e2 â V seien. Dann koĚnnen wir jeden
Vektor x â V schreiben als
x = Îž1 e1 + Îž2 e2 ,
15

Îži â R.

FuĚr eine Lesehilfe zu den beiden Formelzeilen verweisen wir auf FuĂnote 13.
space

16 vector

24

KAPITEL 1. GRUNDLAGEN

Auf diesem Wege
wir eine Abbildung von V in den R2 , die jedes x â V abbildet auf seine
 bekommen
Îž1
2
Koordinaten Îž2 â R . (Wenn wir eine andere Basis (e1 , e2 ) gewaĚhlt haĚtten, dann saĚhe diese Abbildung

anders aus.) Die Koordinaten von e1 bezuĚglich der Basis (e1 , e2 ) sind zum Beispiel 10 , die Koordinaten

von e2 sind 01 .

Wenn wir die geometrisch definierten Rechenoperationen auf den R2 uĚbertragen, kommen wir zur folgenden
Definition:
Definition 1.23. Der R2 ist definiert als Menge geordneter Paare reeller Zahlen:

 
Îž1
: Îži â R .
R2 =
Îž2



Das Paar 00 wird mit 0 bezeichnet; auĂerdem setzen wir e1 = 10 und e2 = 01 . Dann definieren wir 2
Operationen:
   


Îž1
Îˇ1
Îž1 + Îˇ1
+ : R2 Ă R2 â R2 ,
+
:=
,
Îž2
Îˇ2
Îž2 + Îˇ2
 
 
ÎťÎž1
Îž1
.
:=
Âˇ : R Ă R2 â R2 , Îť
ÎťÎž2
Îž2
Wir stellen schnell fest:
Satz 1.24. Der R2 hat zusammen mit den Operationen + und Âˇ die Eigenschaften aus Satz 1.21, ist also
ein Vektorraum uĚber R.

Ab jetzt werden wir den geometrisch definierten Vektorraum der Ortsvektoren in der Ebene einerseits
und den R2 andererseits synonym gebrauchen, wobei wir im Hinterkopf behalten, daĂ diejenige Abbildung
zwischen V und R2 , die jedem Vektor seine Koordinaten zuordnet, von der gewaĚhlten (Orthonormal-)Basis
in V abhaĚngt.
Geometrisch ist glaubhaft, daĂ es zwischen 2 Ortsvektoren einen Winkel gibt (es sei denn, einer der Vektoren
waĚre der Nullvektor), und daĂ jeder Ortsvektor eine LaĚnge hat. Wenn ein Ortsvektor x gegeben wird durch
x = Îž1 e1 + Îž2 e2 , dann betraĚgt seine LaĚnge
q
|x| = Îž12 + Îž22 .

Diese Rechnung ist aber nur guĚltig, weil e1 und e2 eine Orthonormalbasis bilden. Wenn wir den Winkel
zwischen dem Vektor x und der e1 âAchse Ď taufen, dann ist
Îž1 = |x| cos Ď,

Îž2 = |x| sin Ď.


Nun betrachten wir noch einen weiteren Vektor y mit Koordinaten ÎˇÎˇ12 und Winkel Ď zur e1 âAchse. Der
Winkel â (x, y) ist dann gerade Îą = Ď â Ď, und aus dem Additionstheorem fuĚr den Cosinus erhalten wir
cos Îą = cos(Ď â Ď) = cos Ď cos Ď + sin Ď sin Ď =

Îž2 Îˇ2
Îž1 Îˇ1
Âˇ
+
Âˇ
.
|x| |y| |x| |y|

In den ZaĚhlern der BruĚche rechts entdecken wir das bekannte Skalarprodukt im R2 :


Definition 1.25. FuĚr x = ÎžÎž12 â R2 und y = ÎˇÎˇ12 â R2 definieren wir das Skalarprodukt x Âˇ y â R als
x Âˇ y := Îž1 Âˇ Îˇ1 + Îž2 Âˇ Îˇ2 .

DafuĚr schreiben wir in Zukunft auch hx, yi.
Damit haben wir dann
cos â (x, y) =

hx, yi
,
|x| Âˇ |y|

falls keiner der Vektoren x, y gleich dem Nullvektor ist (ansonsten wuĚrden wir durch Null dividieren). Wenn
aber einer der Vektoren x, y gleich 0 waĚre, dann gaĚbe es auch keinen Winkel zwischen x und y.

1.3. DIE EBENE UND DER R2

25

Bemerkung 1.26. In die Berechnung des Skalarprodukts hx, yi und der LaĚngen |x|, |y| gehen die Koordinaten Îž1 , Îž2 , Îˇ1 , Îˇ2 ein. Diese Koordinaten haĚngen vom gewaĚhlten Koordinatensystem ab, also von der
gewaĚhlten Basis (e1 , e2 ). Wenn wir eine andere Basis gewaĚhlt haĚtten, zum Beispiel (eâ˛1 , eâ˛2 ), dann haĚtten
wir andere Koordinaten Îž1â˛ , Îž2â˛ , Îˇ1â˛ , Îˇ2â˛ bekommen. Obwohl dies andere Koordinaten sind, bekommen wir fuĚr
cos â (x, y) am Ende denselben Wert ! Das ist geometrisch auch glaubhaft. Hierbei haben wir stillschweigend
vorausgesetzt, daĂ die Vektoren e1 , e2 aufeinander senkrecht stehen und jeweils die LaĚnge 1 haben; und daĂ
entsprechendes auch fuĚr eâ˛1 , eâ˛2 gilt. Das bedeutet gerade, daĂ (e1 , e2 ) eine Orthonormalbasis (ONB) sein
soll, und (eâ˛1 , eâ˛2 ) ebenfalls.
Satz 1.27. Das Skalarprodukt
hÂˇ, Âˇi : R2 Ă R2 â R
besitzt die folgenden Eigenschaften fuĚr alle Vektoren x, y, z â R2 und Skalare Îť â R:
hx, yi = hy, xi ,
hx, y + zi = hx, yi + hx, zi ,
hÎť Âˇ x, yi = Îť hx, yi ,
hx, xi âĽ 0

hx, xi = 0 genau dann, wenn x = 0.

Beweis. Beweis durch Nachrechnen aus Definition 1.25.
Weil das Skalarprodukt hx, xi eines Vektors x mit sich selbst nie negativ ist, laĚĂt sich die Wurzel ziehen:
Definition 1.28. FuĚr x â R2 erklaĚren wir die euklidische LaĚnge (Betrag, Norm) vermoĚge
p
|x| := hx, xi.

Satz 1.29 (Ungleichung von CauchyâSchwarz17 ). Seien x, y â R2 zwei Vektoren. Dann gilt
| hx, yi | â¤ |x| Âˇ |y|.
Das Gleichheitszeichen gilt genau dann, wenn die Vektoren x und y parallel sind.

Beweis. Geometrisch ist die Sache plausibel: wir haben â1 â¤ cos â (x, y) â¤ 1, und deshalb â|x| Âˇ |y| â¤
hx, yi â¤ |x| Âˇ |y|. Aber fuĚr spaĚtere Zwecke ist es nuĚtzlich, einen Beweis zu haben, der ohne Appelle an das
geometrische VorstellungsvermoĚgen auskommt und stattdessen nur Satz 1.27 ausnutzt.
Seien nun also x, y â R2 fest gewaĚhlt, und sei weiterhin Îť â R beliebig. Dann haben wir
0 â¤ hx + Îťy, x + Îťyi ,
was sich ausmultiplizieren laĚĂt wie folgt:
0 â¤ hx + Îťy, xi + hx + Îťy, Îťyi
= hx, x + Îťyi + hÎťy, x + Îťyi = hx, xi + hx, Îťyi + hÎťy, xi + hÎťy, Îťyi

= hx, xi + hÎťy, xi + Îť hy, xi + Îť hy, Îťyi = hx, xi + Îť hy, xi + Îť hy, xi + Îť hÎťy, yi
= hx, xi + 2Îť hy, xi + Îť2 hy, yi = |x|2 + 2Îť hx, yi + Îť2 |y|2 .

Wir koĚnnen annehmen, daĂ y 6= ~0 ist, weil ansonsten die Behauptung zu der BanalitaĚt 0 â¤ 0 zusammenschrumpft. Dann ist aber |y|2 6= 0, und wir duĚrfen dividieren:
0â¤

2 hx, yi
|x|2
+
Îť + Îť2 .
2
|y|
|y|2

Nun sind x und y fest, aber Îť variabel. Die rechte Seite ist also eine quadratische Funktion in Îť:
P (Îť) =
17

2 hx, yi
|x|2
+
Îť + Îť2 ,
2
|y|
|y|2

Augustin Louis Cauchy (1789â1857), Hermann Amandus Schwarz (1843â1921)

26

KAPITEL 1. GRUNDLAGEN

von der wir wissen, daĂ sie niemals negative Werte annehmen kann. Also ist die Diskriminante der zugehoĚrigen quadratischen Gleichung â¤ 0:


hx, yi
|y|2

2

â

|x|2
â¤ 0,
|y|2

woraus die gewuĚnschte Ungleichung sofort folgt.
Wenn in der CauchyâSchwarzâUngleichung das Gleichheitszeichen eintritt, dann ist die Diskriminante gleich
0, also gibt es ein Îť â R mit hx + Îťy, x + Îťyi = 0, also gilt fuĚr dieses Îť die Beziehung x + Îťy = ~0. Dann
sind aber x und y parallel. Die Umkehrung dieser Aussage gilt auch, wie man schnell sieht.
Man beachte, daĂ wir nirgends benutzt hatten, daĂ x und y Vektoren aus dem R2 sind. Derselbe Beweis
laĚuft auch im R3 oder Rn .
Satz 1.30. Die Norm aus Definition 1.28 erfuĚllt die folgenden drei Eigenschaften, fuĚr alle Vektoren x, y â
R2 und alle Skalare Îť â R:
|x| âĽ 0

|x| = 0 genau dann, wenn x = 0,
|Îťx| = |Îť| Âˇ |x|,

|x + y| â¤ |x| + |y|.

Beweis. Wir benutzen nur die 4 Eigenschaften aus Satz 1.27, aber keinerlei geometrische Interpretation:
â
Wegen Âˇ âĽ 0 ist |x| âĽ 0. Wenn |x| = 0 sein sollte, dann muĂ hx, xi = 0 sein, und das geht nur fuĚr x = 0.

Weiterhin ist
|Îťx| =

p
p
p
p
p
hÎťx, Îťxi = Îť hx, Îťxi = Îť hÎťx, xi = ÎťÎť hx, xi = |Îť| hx, xi = |Îť| Âˇ |x|.

Und schlieĂlich haben wir, wenn wir die Ungleichung von CauchyâSchwarz zitieren,
|x + y|2 = hx + y, x + yi = hx + y, xi + hx + y, yi = hx, x + yi + hy, x + yi
= hx, xi + hx, yi + hy, xi + hy, yi = |x|2 + 2 hx, yi + |y|2
â¤ |x|2 + 2|x| Âˇ |y| + |y|2 = (|x| + |y|)2 .

Nun ist |x + y| nie negativ, also duĚrfen wir die Wurzel ziehen und erhalten |x + y| â¤ |x| + |y|.
Die folgenden Eigenschaften koĚnnen auf aĚhnlichem Wege leicht gezeigt werden:
Satz 1.31. Seien x, y â R2 beliebige Vektoren. Dann gilt fuĚr die Norm aus Definition 1.28, daĂ
|x â y|2 = |x|2 + |y|2 â 2 hx, yi ,


|x + y|2 + |x â y|2 = 2 |x|2 + |y|2 ,

hx, yi = 0 ââ |x + y|2 = |x|2 + |y|2 .

Geometrische Interpretationen davon zu suchen ist eine lehrreiche UĚbungsaufgabe.
Bemerkung 1.32. Seien a â R2 und Î˛ â R gegeben. Wir suchen einen Vektor x â R2 mit ha, xi = Î˛. Man
stellt schnell fest, daĂ es keine eindeutige LoĚsung geben kann: Denn ausgeschrieben haben wir a1 x1 +a2 x2 =
Î˛ und somit zwei Unbekannte, aber lediglich eine Gleichung. Und tatsaĚchlich koĚnnen wir zu einer LoĚsung
x einen beliebigen Vektor addieren, der auf a senkrecht steht und bekommen so eine weitere LoĚsung.
Es gibt keine Division als Umkehroperation des Skalarprodukts.

1.3. DIE EBENE UND DER R2

1.3.2

27

Drehungen

Literatur: Greiner und MuĚller: Quantenmechanik. Symmetrien. Kapitel I.8: Rotationen und ihre Gruppeneigenschaften
Wir stellen uns folgendes Problem:
In einer Ebene sei durch die Einheitsvektoren e1 und e2 ein kartesisches Koordinatensystem
gegeben. Ein

Vektor x habe bezuĚglich dieses Koordinatensystems die (gegebenen) Koordinaten ÎžÎž12 , also
x = Îž1 e1 + Îž2 e2 .

Wir wollen den Vektor x um den Ursprung drehen, und zwar um den Winkel Ď im Gegenuhrzeigersinn.
Das Bild von x sei xâ˛ ; was sind die Koordinaten von xâ˛ ?
Offensichtlich koĚnnen wir mit gesuchten Îžjâ˛ schreiben:
xâ˛ = Îž1â˛ e1 + Îž2â˛ e2 .
Um diese Îžjâ˛ zu bestimmen, gehen wir einen Umweg: Wir drehen die Vektoren e1 und e2 um den Ursprung,
mit dem Winkel Ď. Die Bildvektoren seien eâ˛1 und eâ˛2 . Dann ist geometrisch glaubbar, daĂ
xâ˛ = Îž1 eâ˛1 + Îž2 eâ˛2 ,
das heiĂt, der Bildvektor xâ˛ hat in der gedrehten Basis (eâ˛1 , eâ˛2 ) gerade die alten Koordinaten
Nun ist anhand einer Skizze plausibel, daĂ
eâ˛1 = cos Ď e1 + sin Ď e2 ,

Îž1
Îž2


.

eâ˛2 = â sin Ď e1 + cos Ď e2 .

Insgesamt ergibt sich damit
xâ˛ = Îž1â˛ e1 + Îž2â˛ e2
= Îž1 (cos Ď e1 + sin Ď e2 ) + Îž2 (â sin Ď e1 + cos Ď e2 )
= (Îž1 cos Ď â Îž2 sin Ď)e1 + (Îž1 sin Ď + Îž2 cos Ď)e2 ,
nach einem Koeffizientenvergleich (den wir spaĚter rechtfertigen werden) also
Îž1â˛ = cos Ď Îž1 â sin Ď Îž2 ,

Îž2â˛ = sin Ď Îž1 + sin Ď Îž2 .

Wir schreiben diese zwei Gleichungen als eine Vektorgleichung:




 â˛
â sin Ď
cos Ď
Îž1
.
+ Îž2
= Îž1
cos Ď
sin Ď
Îž2â˛
Um dies noch etwas anders zu schreiben, fuĚhren wir eine neue Notation ein: Ein Schema der Form


a11
a12
A=
, aij â R,
a21
a22
heiĂt 2 Ă 2âMatrix reeller Zahlen, fuĚr die wir eine Operation
Matrix mal Spaltenvektor ergibt Spaltenvektor
gemaĚĂ

a11
a21

a12
a22

 
 


 
a
a
a11 Îž1 + a12 Îž2
Îž1
= Îž1 11 + Îž2 12
:=
a22
a21
a21 Îž1 + a22 Îž2
Îž2

definieren. Damit koĚnnen wir insgesamt schreiben:
 â˛ 
 
Îž1
cos Ď
â sin Ď
Îž1
â˛
x = â˛ =
= R(Ď)x.
Îž2
sin Ď
cos Ď
Îž2

28

KAPITEL 1. GRUNDLAGEN

Definition 1.33. Die Matrix


cos Ď
â sin Ď
R(Ď) =
,
sin Ď
cos Ď

Ď â R,

heiĂt Drehmatrix zum Winkel Ď.
Was passiert, wenn wir fuĚr x einen der Basisvektoren e1 =
erhalten wir dann

  

cos Ď
â sin Ď
1
cos Ď
=
= eâ˛1
sin Ď
cos Ď
0
sin Ď

1
0


, e2 =

0
1



einsetzen ? Als Bildvektoren

beziehungsweise

  

cos Ď
â sin Ď
0
â sin Ď
=
= eâ˛2 ,
sin Ď
cos Ď
1
cos Ď
und das sind gerade die Spalten der Drehmatrix R(Ď).
Die Spalten der Drehmatrix sind die Koordinaten der Bilder der Basisvektoren.
Als NaĚchstes betrachten wir 2 Drehungen: zuerst drehen wir x um den Winkel Ď und erhalten xâ˛ . Dann
drehen wir xâ˛ um den Winkel Ď und erhalten xâ˛â˛ . Wie koĚnnen wir xâ˛â˛ direkt aus x bestimmen ?
Wir haben also
xâ˛ = R(Ď)x,

xâ˛â˛ = R(Ď)xâ˛ ,

(1.3)

und geometrisch ist auch klar, daĂ
xâ˛â˛ = R(Ď + Ď)x.
Die Additionstheoreme der Winkelfunktionen liefern uns


cos Ď cos Ď â sin Ď sin Ď
â sin Ď cos Ď â cos Ď sin Ď
R(Ď + Ď) =
.
sin Ď cos Ď + cos Ď sin Ď
cos Ď cos Ď â sin Ď sin Ď
Andererseits koĚnnen wir die beiden Gleichungen (1.3) auch ineinander einsetzen:


xâ˛â˛ = R(Ď)xâ˛ = R(Ď) R(Ď)x .

Man beachte die Reihenfolge der Multiplikationen auf der rechten Seite: erst wird die Matrix R(Ď) mit dem
Spaltenvektor x multipliziert, was wieder einen Spaltenvektor liefert. Dieser wird dann in einem zweiten
Schritt von links mit R(Ď) multipliziert, und wir erhalten xâ˛â˛ .
Im Sinne eines Assoziativgesetzes wollen wir nun die Klammern umsetzen:


???
xâ˛â˛ = R(Ď)R(Ď) x.

Hierbei muĚĂten wir aber noch erklaĚren, was das Produkt zweier Matrizen R(Ď) und R(Ď) sein soll.
Definition 1.34. Seien A und B zwei Matrizen des Formats 2 Ă 2,




b11
b12
a11
a12
.
,
B=
A=
b21
b22
a21
a22
Dann definieren wir das Matrixprodukt AB gemaĚĂ


a11 b11 + a12 b21
a11 b12 + a12 b22
.
AB :=
a21 b11 + a22 b21
a21 b12 + a22 b22
Das heiĂt: vom linken Faktor A nehmen wir jeweils eine Zeile, und vom rechten Faktor B jeweils eine
Spalte. Das Skalarprodukt dieser 2 Vektoren schreiben wir dort hin, wo Zeile (vom linken Faktor) und
Spalte (vom rechten Faktor) einander kreuzen.

29

1.4. GRUPPENTHEORIE
Bemerkung 1.35. Die Multiplikation ist nicht kommutativ. Das heiĂt, meistens ist AB 6= BA.

Satz 1.36. FuĚr die Drehmatrizen gilt allerdings
R(Ď)R(Ď) = R(Ď + Ď) = R(Ď)R(Ď),

Ď, Ď â R.

Von besonderer Bedeutung ist der Drehwinkel 0. Dann haben wir als Drehmatrix


1
0
R(0) =
,
0
1

und man rechnet schnell nach, daĂ xâ˛ = R(0)x gleich dem Ausgangsvektor x ist.
Definition 1.37.

1
I2 :=
0

Die 2 Ă 2 Einheitsmatrix18 ist

0
.
1

Der Name erklaĚrt sich daraus, daĂ Multiplikationen mit dieser Matrix den Ausgangsvektor unveraĚndert
lassen; genauso wie Multiplikationen mit der reellen Zahl 1 eine reelle Zahl nicht aĚndern.
Klar ist auch: wenn wir einen Vektor x erst um Ď nach links drehen, und anschlieĂend um âĎ nach links
drehen, erhalten wir wieder den Ausgangsvektor x. Es ist also
R(âĎ)R(Ď) = I2 .
Definition 1.38. Sei A eine Matrix vom Format 2 Ă 2. Wenn es eine Matrix B gibt mit
BA = I2 ,
dann heiĂt die Matrix A invertierbar19, und die Matrix B heiĂt inverse Matrix20 zu A. Wir schreiben auch
B = Aâ1 .
Bemerkung 1.39. Die Multiplikation ist zwar nicht kommutativ. Aber wenn BA = I2 ist, dann ist auch
AB = I2 , wie wir im naĚchsten Abschnitt beweisen werden. Wenn dem nicht so waĚre, dann muĚĂte man
zwischen linksinversen und rechtsinversen Matrizen unterscheiden, was uns zum GluĚck erspart bleibt.
Offensichtlich ist dann:
Satz 1.40. Jede Drehmatrix R(Ď) ist invertierbar, und ihre Inverse lautet

 

cos(âĎ)
â sin(âĎ)
cos Ď
sin Ď
â1
R(Ď) = R(âĎ) =
=
.
sin(âĎ)
cos(âĎ)
â sin Ď
cos Ď

1.4

Gruppentheorie

Literatur: Greiner und MuĚller: Quantenmechanik. Symmetrien. Kapitel I.7: Definition einer Gruppe

1.4.1

EinfuĚhrung

Wenn wir mit einigem Abstand auf die reellen Zahlen, komplexen Zahlen, Vektoren und Matrizen schauen,
dann entdecken wir einige Gemeinsamkeiten, die uns zu den folgenden Definitionen fuĚhren:
Definition 1.41. Sei G eine beliebige (nichtleere) Menge. Sei weiterhin âŚ eine Operation mit 2 Argumenten
auf G:21
âŚ : G Ă G â G,
âŚ : (x, y) 7â x âŚ y.
Wenn diese Operation auf ganz G Ă G definiert ist und die folgenden Bedingungen erfuĚllt, dann heiĂt (G, âŚ)
eine Halbgruppe22 .
18 identity

matrix

19 invertible
20 inverse

matrix
Aus FuĂnote 13 wissen wir schon, wie die erste Formelzeile zu lesen ist. Und die zweite Formelzeile, in der der Pfeil 7â
jetzt einen FuĂâ bekommen hat, druĚckt das Verhalten der Abbildung âŚ ein weiteres Mal aus, diesmal allerdings auf der Ebene
â
von konkreten Elementen x und y anstatt auf der Ebene der Menge G Ă G.
22 semi-group
21

30

KAPITEL 1. GRUNDLAGEN

âŚ ist assoziativ: (x âŚ y) âŚ z = x âŚ (y âŚ z) fuĚr jegliche x, y, z â G,
neutrales Element: Es gibt genau ein e â G, sodaĂ fuĚr jedes x â G gilt: e âŚ x = x âŚ e = x.
Definition 1.42. Sei (G, âŚ) eine Halbgruppe, die auĂerdem noch folgende Bedingung erfuĚllt:
inverse Elemente: Zu jedem x â G gibt es genau ein y â G mit x âŚ y = y âŚ x = e.
Dann nennen wir (G, âŚ) eine Gruppe23 .
Definition 1.43. Sei (G, âŚ) eine Halbgruppe bzw. Gruppe. Wenn die Operation âŚ kommutativ ist, also
x âŚ y = y âŚ x fuĚr jegliche x, y â G gilt, dann nennen wir die Halbgruppe/Gruppe abelsch24 , nach Niels
Henrik Abel (1802â1829).
FuĚr Halbgruppen haben wir die folgenden Beispiele:
1. (N0 , +)
2. (N0 , Âˇ)
3. (Z, Âˇ)
4. (M2 , Âˇ), wobei M2 die Menge der 2 Ă 2âMatrizen bezeichnet.
Die ersten drei Halbgruppen sind abelsch. FuĚr abelsche Gruppen kennen wir unter anderem die folgenden
Beispiele:
1. (Z, +)
2. (Q, +)
3. (C, +)
4. (R2 , +)
5. (R \ {0}, Âˇ)
6. (C \ {0}, Âˇ)
7. ({R(Ď) : Ď â R}, Âˇ) â die Gruppe der Drehmatrizen.
Und nichtabelsche Gruppe sind zum Beispiel
1. ({A â M2 : A invertierbar}, Âˇ)
2. die Menge aller Abbildungen, die ein Quadrat auf sich abbilden (Ecke auf Ecke), mit der NacheinanderausfuĚhrung als Operation.
Wir klassifizieren die Gruppen:
Die Operation âŚ kann zum Beispiel sein die
Addition: von Zahlen, Vektoren, Pfeilen, Matrizen, . . . ,
Multiplikation: von Zahlen oder Matrizen,
NacheinanderausfuĚhrung: von umkehrbaren Abbildungen einer Menge auf sich.
Das neutrale Element e ist dann in diesen drei FaĚllen die
Null: Null als Zahl, Null als Nullvektor, Null als Nullmatrix, . . .
23 group
24 abelian

1.4. GRUPPENTHEORIE

31

Eins: Eins als Zahl oder Einheitsmatrix,
identische Abbildung: bildet jedes Objekt auf sich selbst ab.
Das inverse Element aâ1 zu einem Element a der Gruppe ist dann in diesen drei FaĚllen
das entgegengesetzte Element: beispielsweise zu 5 â R also â5,
das reziprokeâ Element: beispielsweise zu 5 â R also 0.2; oder die inverse Matrix,
â
die Umkehrabbildung: diejenige Abbildung, die die Abbildung a ungeschehen macht.
Frage: Eine Gruppe G habe genau 1492 Elemente. Wieviele neutrale Elemente hat sie ? Wieviele inverse
Elemente ?
Definition 1.42 ist uĚbermaĚĂig streng. Man kann einiges weglassen und hat trotzdem den selben Inhalt:
Satz 1.44. Sei G eine nichtleere Menge und âŚ eine assoziative Operation mit 2 Argumenten, die auf ganz
G Ă G definiert ist, mit Werten in G. Wir setzen weiterhin voraus:
Es gibt mindestens ein linksneutrales Element e â G, das heiĂt e âŚ x = x fuĚr jedes x â G.

Zu jedem x â G gibt es mindestens ein linksinverses Element x â G, das heiĂt x âŚ x = e.
Dann ist (G, âŚ) eine Gruppe.

Beweis. UĚbungsaufgabe.
Eine Folgerung aus diesem Satz ist: Die Matrix I2 ist das einzige neutrale Element fuĚr die Matrizenmultiplikation. Sei die Matrix A invertierbar. Dann gibt es also (mindestens) eine Matrix B mit BA = I2 .
Laut dem vorigen Satz ist dieses B dann die einzige linksinverse Matrix. Und obendrein ist dieses B auch
noch rechtsinvers, d.h. AB = I2 . Und dies, obwohl die Multiplikation von Matrizen im Allgemeinen nicht
kommutativ ist.
Man kann Gruppen auch anders definieren: anstatt zu fordern, daĂ zu jedem Element ein Inverses existiert,
kann man auch verlangen, daĂ jede Gleichung loĚsbar ist:
Satz 1.45. Sei (G, âŚ) eine Gruppe und a, b â G beliebig. Dann gibt es genau ein x â G bzw. y â G mit
a âŚ x = b,

y âŚ a = b.

Beweis. Wir probieren unser GluĚck mit x = aâ1 âŚ b:
a âŚ x = a âŚ (aâ1 âŚ b) = (a âŚ aâ1 ) âŚ b = e âŚ b = b.
Also ist dieses x eine LoĚsung. Um zu zeigen, daĂ es keine weitere LoĚsung gibt, nehmen wir das Gegenteil
an. Sei also
a âŚ x = b,

a âŚ z = b.

Dann haben wir a âŚ x = a âŚ z. Wir setzen von links ein aâ1 dran und erhalten aâ1 âŚ a âŚ x = a1 âŚ a âŚ z, woraus
x = z folgt. Die Aussagen uĚber y lassen sich genauso beweisen.
Bemerkung 1.46. Es gilt auch die Umkehrung: Sei G eine Menge mit einer zwei-argumentigen Abbildung
âŚ darauf, die aus G nicht herausfuĚhrt. Wenn dann noch âŚ assoziativ ist, und wenn jede Gleichung a âŚ x = b
und jede Gleichung y âŚ a = b jeweils mindestens eine LoĚsung x bzw. y haben, dann ist (G, âŚ) eine Gruppe.25
Warnung 1.47. Es ist zwar jedes linksinverse Element auch rechtsinvers, aber die LoĚsungen x und y zu
a âŚ x = b und y âŚ a = b sind im Allgemeinen verschieden.
Dieser Satz wirkt sich auf die VerknuĚpfungstafeln aus wie folgt:
In jeder Zeile einer Gruppentafel taucht jedes Element genau einmal auf. Analog fuĚr jede Spalte.
25 Ein Beweis kann im schuĚlerfreundlich geschriebenen Buch Herbert KaĚstner, Peter GoĚthner, Algebra â Aller Anfang
ist leicht, (Mathematische SchuĚlerbuĚcherei 107, Teubner Leipzig 1989) nachgelesen werden.

32

KAPITEL 1. GRUNDLAGEN

Wenn dies einmal nicht der Fall sein sollte, hat man sich entweder verrechnet, oder es ist keine Gruppe.
Jetzt, wo wir uĚber den Begriff der Gruppe verfuĚgen, koĚnnen wir die Definition des KoĚrpers kuĚrzer formulieren. Am Beispiel von (R, +, Âˇ) wuĚrde die aĚquivalente Definition lauten:
â˘ (R, +) ist eine abelsche Gruppe mit neutralem Element 0,
â˘ (R, Âˇ) ist eine abelsche Halbgruppe mit neutralem Element 1,
â˘ (R \ {0}, Âˇ) ist eine Gruppe,
â˘ Addition und Multiplikation sind verzahnt uĚber das Distributivgesetz.
Frage: KoĚnnen Sie an einem Beispiel zeigen, daĂ ohne den zweiten â˘ tatsaĚchlich die AĚquivalenz nicht gilt ?
Lemma 1.48. Seien a und b Elemente einer Gruppe, und aâ1 , bâ1 ihre inversen Elemente. Dann wird
das inverse Element zu a âŚ b gegeben durch
(a âŚ b)â1 = bâ1 âŚ aâ1 .
Beweis. Beweis durch Einsetzen.
Man beachte die geaĚnderte Reihenfolge ! Andererseits ist diese neue Reihenfolge auch einleuchtend, wenn
man sich z.B. a vorstellt als Drehung um den Ursprung der Ebene um einen Winkel Ď nach links, und b
als Spiegelung an einer gegebenen Geraden in derselben Ebene. Dann waĚre natuĚrlich a âŚ b die Kombination
beider Bewegungen, und (a âŚ b)â1 kann sich jeder selbst uĚberlegen.

Wenn zum Beispiel die Matrizen A und B invertierbar sind, dann ist auch ihr Produkt AB invertierbar,
und die inverse Matrix des Produkts ist (AB)â1 = B â1 Aâ1 .
Die Formel (a âŚ b)â1 = bâ1 âŚ aâ1 kann man sich auch als kommutatives Diagramm veranschaulichen:
âŚ

(a, b)
âââââ
ďŁŚ
ďŁŚ
Tauschyund(Âˇ)â1

(bâ1 , aâ1 ) âââââ
âŚ

aâŚb
ďŁŚ
ďŁŚ â1
y(Âˇ)

(a âŚ b)â1
= bâ1 âŚ aâ1

Der Punkt Âˇ ist jeweils als PlatzâFreihalter zum Einsetzen zu verstehen.
Das Assoziativgesetz hat auch ein kommutatives Diagramm:
b
ďŁŚ
ďŁŚ
ÂˇâŚcy

âââââ
aâŚÂˇ

aâŚb
ďŁŚ
ďŁŚÂˇâŚc
y

b âŚ c âââââ a âŚ b âŚ c
aâŚÂˇ

Entsprechendes gilt fuĚr Distributivgesetze bei KoĚrpern und VektorraĚumen.

Wir tragen einige uns bekannte Strukturen zusammen:
Halbgruppen besitzen eine Operation âŚ, und ein Beispiel ist (N0 , +).
Gruppen besitzen eine Operation âŚ, und gemaĚĂ Satz 1.45 ist jede Gleichung mit dieser Operation loĚsbar.
In diesem Sinne besitzt eine Gruppe zwei Operationen, wobei die eine die Umkehrung der anderen
ist. Ein Beispiel ist (Z, +) mit der Subtraktion als zweiter Operation.
KoĚrper besitzen zwei Operationen + und Âˇ, die beide umgekehrt werden koĚnnen. In diesem Sinne besitzen
KoĚrper vier Operationen. Beispiele sind R, Q, C.
Es faĚllt auf, daĂ wir keine algebraische Struktur angegeben haben mit genau drei Operationen. TatsaĚchlich
kennt die Mathematik aber solche Strukturen:

33

1.4. GRUPPENTHEORIE

Ringe besitzen drei Operationen. NaĚmlich eine Addition mit deren Umkehrung (Subtraktion), und eine
Multiplikation. UĚber die Umkehrbarkeit der Multiplikation wird nichts ausgesagt. Die Addition ist
kommutativ und assoziativ, die Multiplikation ist assoziativ (aber womoĚglich nicht kommutativ), und
das Distributivgesetz gilt. Ein Beispiel ist (Z, +, Âˇ). Bekanntlich kann die Division aus Z herausfuĚhren.
Ein weiteres Beispiel fuĚr Ringe ist (R2Ă2 , +, Âˇ), also die Menge der Matrizen vom Format 2 Ă 2, fuĚr die man
die Addition komponentenweise definiert, und Âˇ ist die Multiplikation der Matrizen gemaĚĂ Definition 1.34.

{ Es reicht, wenn die folgenden Betrachtungen dieses Abschnitts zum passiven Wissen der Leserschaft
gehoĚren. Diese theoretischen Untersuchungen brauchen wir lediglich fuĚr das Unterkapitel uĚber die Signalkodierung beim UMTSâVerfahren. }

Ein weiterer Ring wird von den durch 5 teilbaren Zahlen gebildet. Diesen Ring schreiben wir naheliegenderweise als (5Z, +, Âˇ). Und ein weiterer Ring besteht aus den Resten von ganzen Zahlen bei Division durch
5. Offenkundig kann eine ganze Zahl bei Division durch 5 nur die Reste 0,1,2,3 oder 4 hinterlassen. Damit
wir die Reste besser unterscheiden koĚnnen von den Zahlen, setzen wir Klammern. Es gibt also die Reste [0],
[1], [2], [3], [4], und wir vereinbaren die Schreibweise {[0], [1], [2], [3], [4]} =: Z/5Z. FuĚr die Addition erhalten
wir dann die VerknuĚpfungstafel 1.1:
+
[0]
[1]
[2]
[3]
[4]

[0]
[0]
[1]
[2]
[3]
[4]

[1]
[1]
[2]
[3]
[4]
[0]

[2]
[2]
[3]
[4]
[0]
[1]

[3]
[3]
[4]
[0]
[1]
[2]

[4]
[4]
[0]
[1]
[2]
[3]

Tabelle 1.1: VerknuĚpfungstafel von (Z/5Z, +)
Es ist lohnenswert, auch noch die VerknuĚpfungstafel fuĚr die Multiplikation aufzustellen. Dabei wollen wir
den Rest [0] ignorieren, da sein Verhalten bei der Multiplikation aĚuĂerst vorhersehbar ist. Als Lesehilfe
verweisen wir auf 2 Âˇ 4 = 8, was den Rest 3 laĚĂt, also [2] Âˇ [4] = [3].
Âˇ
[1]
[2]
[3]
[4]

[1]
[1]
[2]
[3]
[4]

[2]
[2]
[4]
[1]
[3]

[3]
[3]
[1]
[4]
[2]

[4]
[4]
[3]
[2]
[1]

Wir vermerken, daĂ kein KoĚrper entsteht, wenn die die Zahl 5 durch die Zahl 6 ersetzen (man schaue
sich die entsprechende VerknuĚpfungstafel fuĚr die Multiplikation an); und der tiefere Grund ist, daĂ 6 keine
Primzahl ist. Man kann beweisen, daĂ Z/qZ ein KoĚrper ist fuĚr jede Primzahl q. FuĚr diesen KoĚrper schreibt
man auch Fq .

1.4.2

Ausblick: Gruppen in der Physik

Wir listen einige Gruppen auf, die in der Physik oĚfters auftreten:
Symmetriegruppen eines Kristalls: gegeben sei ein Kristallgitter. Diese gibt es in verschiedenen AuspraĚgungen (kubisch, tetragonal, rhombisch, hexagonal, trigonal, monoklin, triklin, mit noch zusaĚtzlichen Unterscheidungen, ob weitere Atome raumzentriert oder flaĚchenzentriert oder auf den Mittelpunkten der BasisflaĚchen angebracht sind). Die Elemente der Symmetriegruppe dieses Gesamtgitters
bestehen aus allen Bewegungen, die das Gitter auf sich abbilden. Die VerknuĚpfung ist natuĚrlich die
NacheinanderausfuĚhrung. Dazu zaĚhlen alle Verschiebungen (sofern sie GitterplaĚtze aufeinander abbilden) und ggf. noch diverse Spiegelungen an Symmetrieebenen oder einige Drehungen. Die das
Verhalten dieses Kristalls beschreibenden Differentialgleichungen koĚnnen sehr kompliziert sein, aber
wenn man die Symmetrien herausfaktorisiertâ, wird es ein wenig einfacher.
â
Verschiebungsgruppen im Rn : das sind alle Verschiebungen, die den Rn auf sich abbilden. Die VerknuĚpfung ist wieder die NacheinanderausfuĚhrung (das schreiben wir in Zukunft nicht mehr mit).

34

KAPITEL 1. GRUNDLAGEN

Orthogonale Gruppe im R3 : das sind alle laĚngentreuen Abbildungen im R3 , die den Ursprung unveraĚndert lassen, also Drehungen, Spiegelungen, Drehspiegelungen. Diese Gruppe hat unendlich viele
Elemente, man nennt sie O(3).
Spezielle orthogonale Gruppe im R3 : das sind alle laĚngentreuen Abbildungen im R3 , die den Ursprung
und die Orientierung unveraĚndert lassen, also alle Drehungen. Schreibweise: SO(3).
Lineare Gruppe des R3 : das sind alle linearen Abbildungen des R3 auf sich, die invertiert werden
koĚnnen. Spate werden auf Spate abgebildet. Schreibweise: GL(3).
Spezielle lineare Gruppe des R3 : das sind alle Elemente der GL(3), die die Orientierung des R3 erhalten und Spate auf Spate mit gleichem Volumen abbilden.
Heisenberggruppe: die obigen Gruppen bestanden aus Abbildungen (und die VerknuĚpfung war jeweils
die NacheinanderausfuĚhrung), die Punkte im R3 auf Punkte im R3 abbildeten. Im Gegensatz dazu
besteht die Heisenberggruppe26 aus Abbildungen, die Elemente eines gewissen Zustandsraums auf
Elemente dieses Zustandsraums abbilden. Dieser Zustandsraum wiederum besteht aus allen Funktionen von R3 nach C, deren Betragsquadrat noch dazu integrierbar ist auf dem R3 . Sei Ď ein solches
Element des Zustandsraums, also eine Funktion
Ď = Ď(x) : R3 â C.
Elemente der Heisenberggruppe wirken auf Ď wie folgt:
Verschiebung um p â R3 : Ď = Ď(x) wird abgebildet auf Ďp = Ďp (x) = Ď(x + p)
Phasenfaktor: Ď = Ď(x) wird abgebildet auf Ďq = Ďq (x) = eiqx Ď(x)

Drehung in C: Ď = Ď(x) wird abgebildet auf Ďt = Ďt (x) = eit Ď(x).
AuĂerdem natuĚrlich noch alle Kompositionen dieser Operationen.
Lorentzgruppe: 27 diese ist in der speziellen RelativitaĚtstheorie ganz wichtig. Sie bildet den R1+3 in sich
ab, und zwar so, daĂ EigenzeitabstaĚnde gleich bleiben. Der Raum R1+3 verwaltet die Variablen fuĚr
Zeit und Ort. Er heiĂt Minkowski28 âRaum und hat anstelle des gewoĚhnlichen Skalarprodukts ein
Pseudoskalarprodukt:
(t, x1 , x2 , x3 ) Âˇ (s, y1 , y2 , y3 ) = âts + x1 y1 + x2 y2 + x3 y3 .
Man beachte das Minus vor dem Produkt der Zeiten. Das Pseudoskalarprodukt eines Zeit-RaumVektors mit sich selbst kann also negativ werden, und ein Zeit-Raum-Vektor gehoĚrt zum Lichtkegel,
wenn dieses Pseudoskalarprodukt dieses Vektors mit sich selbst gleich null ist. Die EigenzeitabstaĚnde
werden uĚber dieses Pseudoskalarprodukt bestimmt.
Die LorentzâGruppe hat unendliche viele Elemente, man schreibt (fuĚr die Gruppe) auch O(3, 1).
Von diesen Gruppen gibt es noch einen weiteren Zusammenhang zur klassischen Mechanik: typischerweise
wird die zeitliche Entwicklung eines physikalischen Systems beschrieben durch ein System von Differentialgleichungen. Diese Differentialgleichungen enthalten Ableitungen bezuĚglich der Ortsâ und Zeitvariablen
(und eventuell noch bezuĚglich weiterer Variablen). Nun haben wir:
Wenn dieses Gleichungssystem sich nicht aĚndert unter einer Verschiebung aus der Verschiebungsgruppe des
Ortsraumes R3 , dann gilt fuĚr dieses physikalische System der Impulserhaltungssatz.
Wenn dieses Gleichungssystem sich nicht aĚndert unter einer Drehung aus der SO(3) des Ortsraumes R3 ,
dann gilt fuĚr dieses physikalische System der Drehimpulserhaltungssatz.
Wenn dieses Gleichungssystem sich nicht aĚndert unter einer Verschiebung aus der Verschiebungsgruppe des
Zeitraumes R1 , dann gilt fuĚr dieses physikalische System der Energieerhaltungssatz.
Es versteht sich also von selbst, daĂ es nuĚtzlich ist, nach weiteren Gruppen zu suchen, deren Elemente das
Gleichungssystem nicht aĚndern !
Literatur: Greiner und MuĚller: Quantenmechanik. Symmetrien. Kapitel VII: Die SU(3)-Symmetrie. Kapitel VIII: Quarks und die Gruppe SU(3). Kapitel XI: Charm und SU(4).
26

Werner Karl Heisenberg (1901â1976), Nobelpreis fuĚr Physik 1932
Hendrik Antoon Lorentz (1853â1928), Namensgeber fuĚr die LorentzâKraft und die LorentzâTransformation, Nobelpreis
fuĚr Physik 1902
28 Hermann Minkowski (1864â1909)
27

1.4. GRUPPENTHEORIE

1.4.3

35

Ausblick: Mobilfunk

Beim Mobiltelefonieren uĚberlagern sich die Funkwellen der einzelnen Teilnehmer â eben weil die Kommunikation nicht entlang von DraĚhten erfolgt. Da nun auf einer Antenne (am Mobiltelefon oder Funkmast)
verschiedene Signale auftreffen, stellt sich die Frage, wie man diese Signale voneinander trennt.
Die naheliegendste MoĚglichkeit waĚre, jedem Teilnehmer eine eigene Frequenz (bzw. ein Frequenzband) zu
geben. Allerdings scheitert dies daran, daĂ es nicht genug FrequenzbaĚnder gibt.
Im GSM-Verfahren wird daher das Zeitschlitzverfahren eingesetzt: jedes Frequenzband mit einer Breite von
200kHz wird zeitlich auf 8 Teilnehmer aufgeteilt. Das heiĂt, jedes Mobiltelefon auf diesem Band innerhalb
einer Funkzelle bekommt einen von 8 Zeitschlitzen. WaĚhrend eines solchen Zeitschlitzes kommunizieren
Mobiltelefon und Basisstation miteinander, waĚhrend der anderen 7 Zeitschlitze ist die Sendeâ und Empfangselektronik des Mobiltelefons abgeschaltet und die anderen Teilnehmer sind an der Reihe.
Aus verschiedenen GruĚnden ist man bei UMTS vom Zeitschlitzverfahren abgegangen. Stattdessen funken
jetzt mehrere Telefone innerhalb einer Funkzelle gleichzeitig auf demselben Frequenzband ihre Signale zur
Basisstation. Das Frequenzband ist jetzt breiter, naĚmlich 5MHz, und die Funksignale eines Telefons sind
uĚber die gesamte Breite von 5MHz verschmiert, was eine hoĚhere Sicherheit gegenuĚber schmalbandigen
StoĚrungen erlaubt. Andererseits funken jetzt viel mehr Teilnehmer gleichzeitig auf diesem Band, sodaĂ
diese Signale irgendwie getrennt werden muĚssen.
Es gibt noch ein weiteres Problem: die Funkwellen, die z.B. ein Mobiltelefon an seine Basisstation sendet,
hoĚren ja nicht an der Grenze der Funkzelle ploĚtzlich auf. Stattdessen strahlen sie auch in benachbarte
Funkzellen hinein. Deshalb braucht man noch einen Mechanismus, um die Signale von/zu verschiedenen
Basisstationen zu unterscheiden.
Aus diesem Grunde werden 2 Verfahren gleichzeitig verwendet:
orthogonale variable Spreizfaktoren: diese OVSF haben die Aufgabe, das schmalbandige Signal auf
die gesamte Breite des Frequenzbandes zu spreizen; und zwar auf eine solche Art und Weise, daĂ man
die Teilnehmer einer Funkzelle unterscheiden kann.
scrambling codes: diese trennen verschiedene Basisstationen (Funkmasten) voneinander.
Der Ablauf vom Mikrofon bis zur Sendeantenne ist in etwa wie folgt:
â˘ Die Sprachsignale werden digitalisiert (also abgetastet) und liegen danach als Folge von Nullen und
Einsen vor.
â˘ AnschlieĂend werden uĚberfluĚssige Informationen weggeschnitten, das Signal wird komprimiert. Die
Bandbreite des Signales ist jetzt etwa 12kHz.
â˘ Das komprimierte Signal wird jetzt kanalkodiert, es wird also Redundanz hinzugefuĚgt. Die sich ergebende Bandbreite ist (3.84/Spreizfaktor)MHz.
â˘ Das Signal wird gespreizt. Dabei hat jeder Teilnehmer innerhalb einer Basisstation seinen eigenen
Spreizkode. Die Bandbreite ist jetzt 3.84MHz.
â˘ Der Scrambling Code wird angewandt, um Signale unterscheiden zu koĚnnen, die zu verschiedenen
Basisstationen gehoĚren. (Bandbreite bleibt gleich).
â˘ Das Signal wird auf eine TraĚgerfrequenz von 2GHz aufmoduliert (Phasenmodulation). Die Bandbreite
ist jetzt 5MHz, da man an den Bandgrenzen nicht exakt abschneiden kann.
Zu den Spreizfaktoren/Spreizkodes:
Ein Spreizfaktor F ist eine der Zahlen 4, 8, . . . , 128, 256. Ein Spreizkode ist eine Folge von F Zahlen,
jede Zahl ist 0 oder 1. Der Spreizfaktor F haĚngt von der Anzahl der Teilnehmer in einer Zelle ab: je mehr
Teilnehmer, umso hoĚher muĂ F sein. Jeder Teilnehmer bekommt von seiner Basisstation einen Spreizkode zugewiesen. Spreizkodes von verschiedenen Teilnehmern sind orthogonalâ zueinander29 . Bei einem
â
Spreizfaktor F gibt es genau F moĚgliche Spreizkodes, die paarweise aufeinander orthogonalâ stehen.
â
P
Zwei Spreizkodes ~a und ~b aus {0, 1}F stehen orthogonalâ aufeinander, wenn das Produkt F
j=1 (2aj â 1)(2bj â 1) gleich
â
null ist. Dieses Produkt verhaĚlt sich nicht wie ein Skalarprodukt, deshalb jedesmal die GaĚnsefuĚĂchen.
29

36

KAPITEL 1. GRUNDLAGEN

Die Spreizung verlaĚuft so: sei z.B. F = 4, und der Spreizkode eines Teilnehmers sei z.B. (0, 1, 1, 0). Die
Sprachsignale dieses Teilnehmers seien z.B.
0, 1, 0, 1, 1, . . . .
In einem ersten Schritt werden diese Signale mit dem Faktor F verlaĚngert:
0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, . . .
Da diese viermal so lange Bitfolge in derselben Zeit gesendet wird, hat sich die Bandbreite vervierfacht.
AnschlieĂend wird diese Folge mit dem Spreizkode (0, 1, 1, 0) geXORt:
0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, . . .
Wenn man jetzt noch mal mit dem Spreizkode XORen wuĚrde, kaĚme man wieder zum Ausgangssignal.
Auf der Antenne der Basisstation kommt nun ein ganzes Gemisch von Signalen an. Wenn die Basisstation
das Signal eines bestimmten Teilnehmers herausfiltern soll, dann wird dieses Signalgemisch mit dem Spreizkode geXORt. Weil Spreizkodes verschiedener Teilnehmer aber orthogonalâ aufeinander stehen, fuĚhrt dies
â
zu einer AbschwaĚchung der Signale der anderen Teilnehmer, und das gewuĚnschte Signal ist also das staĚrkste.
Nun kommen die Scrambling Codes ins Spiel:
Da es nur F paarweise orthogonaleâ Spreizkodes gibt mit der LaĚnge F , braucht man ein neues Verfahâ
ren, um Signale verschiedener Basisstationen voneinander abzugrenzen. Man nimmt dazu weitere Kodes
(scrambling codes), die beinahe orthogonalâ zueinander sind und auf folgendem Wege konstruiert werden:
â
Sei F2 = Z/2Z der KoĚrper der Reste bei Division durch 2. Dieser KoĚrper besteht nur aus den Elementen 0
und 1, und es gelten die Regeln 0 + 0 = 0, 0 + 1 = 1, 1 + 1 = 0 sowie 0 Âˇ 0 = 0 Âˇ 1 = 0, 1 Âˇ 1 = 1.

Sei F2 [X] die Menge aller Polynome in der Variablen X mit Koeffizienten aus F2 . Man kann solche Polynome
addieren, subtrahieren und miteinander multiplizieren (aber nicht durcheinander dividieren !) und erhaĚlt
jedesmal wieder ein Ergebnis in F2 [X]. Es gelten die uĚblichen Gesetze (KommutativitaĚt, AssoziativitaĚt,
DistributivitaĚt, 2 neutrale Elemente). Wir sagen auch, daĂ (F2 [X], +, Âˇ) einen Ring bildet.
Nun sei p â F2 [X] ein festes Polynom aus diesem Ring mit dem Grade N . Dieses Polynom ist fuĚr die
Festlegung der scrambling codes entscheidend. Genauer gesagt, gibt es zwei solche Polynome p: eines fuĚr
die Kommunikation vom Mobiltelefon zur Basisstation, und ein anderes fuĚr die andere Richtung. Die
Grade diese Polynome sind 24 und 18; und diese beiden Polynome sind im UMTS-Standard festgelegt, also
europaweit einheitlich (in Amerika und Asien gibt es verschiedene Abweichungen).

Dieses Polynom p ist ein ganz besonderes Polynom: denn es muĂ irreduzibel sein. Das bedeutet, daĂ man
es nicht in Faktoren aus F2 [X] zerlegen kann. Nun nimmt man sich alle diejenigen Polynome aus F2 [X]
her, die p als Faktor enthalten. Diese bilden ein sogenanntes Ideal, man schreibt fuĚr dieses Ideal auch (p).
Dann faktorisiertâ man F2 [X] nach diesem Ideal und bekommt F2 [X]/(p). Dies ist definiert als die Menge
â
aller Restpolynome, die bei Division mit Rest eines Polynoms aus F2 [X] durch p entstehen koĚnnen, also
diejenigen Polynome aus F2 [X] mit Grad â¤ N â 1. Jedes Element g von F2 [X]/(p) hat die Form
g(X) âĄ aN â1 X N â1 + Âˇ Âˇ Âˇ + a1 X + a0

mod p,

aj â {0, 1}.

Diese Restklassen bilden einen Ring, wie man leicht nachpruĚft. Weil jedoch das Polynom p irreduzibel ist,
bilden diese Restklassen nicht bloĂ einen Ring, sondern etwas viel Besseres: naĚmlich einen KoĚrper. (Zur
Erinnerung: es ist Fq ein KoĚrper genau dann, wenn q eine Primzahl ist). Mit anderen Worten, man hat
sogar noch die Division. Das heiĂt, zu jedem Polynom g = g(X) aus F2 [X]/(p) gibt es genau ein Polynom
h = h(X) aus F2 [X]/(p) mit
g(X) Âˇ h(X) âĄ 1 mod p.
Nun definiert man sich eine Folge g0 (X), g1 (X),. . . von Polynomen aus F2 [X]/(p) gemaĚĂ der Vorschrift
gk (X) âĄ X k

mod p,

k = 0, 1, 2, . . . .

Es ist also gk nichts anderes als der Rest des Polynomes X k bei Division durch p(X). Jedes dieser Polynome
wird gegeben durch seine Koeffizienten aN â1 , . . . , a1 , a0 , wobei aN â1 â {0, 1} der Koeffizient der hoĚchsten
XâPotenz ist und a0 das Absolutglied. Wir schauen uns jetzt die Folge der Absolutglieder der gk an:
(a0 (g0 ), a0 (g1 ), a0 (g2 ), . . . ).

1.5. DER RAUM UND DER R3

37

Dies ist eine unendlich lange Folge von Nullen und Einsen; sie ist periodisch mit der PeriodenlaĚnge 2N â 1.
Die PeriodenlaĚnge ist deshalb so hoch, weil p irreduzibel ist.
Und in dieser Folge stecken die scrambling codes: jede Basisstation in Europa bekommt einen Startindex zugewiesen, und die Folgenelemente ab diesem Startindex bilden gerade den scrambling code dieser
Basisstation. Verschiedene scrambling codes sind dabei im wesentlichen orthogonalâ.
â
Das mit dem Spreizkode geXORte Signal wird jetzt mit dem passenden scrambling code ein weiteres Mal
geXORt. AnschlieĂend wird das Signal auf die TraĚgerfreqenz aufmoduliert und gesendet.
An der Empfangsantenne kommt dann ein Wellensalat an, aus dem durch XORen mit dem scrambling code
und Spreizkode das gewuĚnschte Signal herausgefiltert wird.

Der Raum und der R3

1.5
1.5.1

Allgemeines

Die Begriffe Ortsvektorâ, Addition von Vektorenâ, Vervielfachung von Vektorenâ, Kartesisches Koorâ
â
â
â
dinatensystemâ definieren wir im Falle des dreidimensionalen Raumes analog wie im Falle der Ebene, bloĂ
daĂ jetzt noch eine weitere Dimension hinzukommt.
Die Menge der Ortsvektoren im dreidimensionalen Raum bildet, gemeinsam mit geometrisch definierter
Addition und Multiplikation, einen Vektorraum.
Definition 1.49. Der R3 ist definiert als Menge geordneter Tripel reeller Zahlen:
ďŁź
ďŁąďŁŤ ďŁś
ďŁ˝
ďŁ˛ Îž1
R3 := ďŁ­Îž2 ďŁ¸ : Îži â R .
ďŁž
ďŁł
Îž3

Um Vertikalplatz zu sparen, schreiben wir diese Tripel auch als (Îž1 , Îž2 , Îž3 )â¤ . Wir definieren 2 Operationen:
ďŁŤ ďŁś ďŁŤ ďŁś
ďŁŤ
ďŁś
Îž1
Îˇ1
Îž1 + Îˇ1
ďŁ­Îž2 ďŁ¸ + ďŁ­Îˇ2 ďŁ¸ := ďŁ­Îž2 + Îˇ2 ďŁ¸ ,
+ : R3 Ă R3 â R3 ,
Îž3
Îˇ3
Îž3 + Îˇ3
ďŁŤ ďŁś
ďŁŤ ďŁś
Îž1
ÎťÎž1
Âˇ : R Ă R3 â R3 ,
Îť ďŁ­Îž2 ďŁ¸ := ďŁ­ÎťÎž2 ďŁ¸
Îž3
ÎťÎž3

Weiterhin schreiben wir 0 = (0, 0, 0)â¤ , e1 = (1, 0, 0)â¤ , e2 = (0, 1, 0)â¤ , e3 = (0, 0, 1)â¤.
Wir definieren das Skalarprodukt zweier Vektoren x und y als
hx, yi := Îž1 Âˇ Îˇ1 + Îž2 Âˇ Îˇ2 + Îž3 Âˇ Îˇ3 ,

und die euklidische Norm (den Betrag, die LaĚnge) eines Vektors x â R3 als
p
|x| := hx, xi.

Dann folgt schnell, daĂ dieser R3 mit diesen beiden Rechenoperationen einen Vektorraum bildet, und daĂ
das Skalarprodukt die Eigenschaften aus Satz 1.27 besitzt. Dann gilt automatisch auch die Ungleichung
von CauchyâSchwarz, und die Betragsfunktion hat die Eigenschaften, die man von ihr erwartet.

1.5.2

Vektorprodukt und Spatprodukt

Der dreidimensionale Raum ist ein Sonderfall. Denn dort (und ausschlieĂlich dort) koĚnnen wir ein weiteres
Produkt von Vektoren definieren:
Definition 1.50 (Vektorprodukt oder Kreuzprodukt im R3 ). Seien a = (Îą1 , Îą2 , Îą3 )â¤ und b =
(Î˛1 , Î˛2 , Î˛3 )â¤ zwei Vektoren. Dann wird das Vektorprodukt bzw. Kreuzprodukt30 gegeben durch
ďŁś
ďŁŤ
Îą2 Î˛3 â Î˛2 Îą3
a Ă b := ďŁ­Îą3 Î˛1 â Î˛3 Îą1 ďŁ¸ .
Îą1 Î˛2 â Î˛1 Îą2
30

cross product, vector product, outer product

38

KAPITEL 1. GRUNDLAGEN

Das sieht zunaĚchst erst mal kompliziert aus; aber spaĚter werden wir einfache geometrische Interpretationen
und schoĚnere Formeln zur Berechnung finden.
Satz 1.51. Seien a, b, c, d â R3 und Îť â R. Dann gelten die folgenden Eigenschaften:
Ă ist bilinear: Es ist (Îťa) Ă b = Îť(a Ă b) und (a + b) Ă c = (a Ă c) + (b Ă c); analog fuĚr den zweiten Faktor.
Ă ist antikommutativ: a Ă b = âb Ă a
Entwicklungssatz: a Ă (b Ă c) = ha, ci b â ha, bi c
Ă ist nicht assoziativ: aber es gilt a Ă (b Ă c) + b Ă (c Ă a) + c Ă (a Ă b) = ~0.
IdentitaĚt von Lagrange:

31

ha Ă b, c Ă di = ha, ci hb, di â ha, di hb, ci

a Ă b steht senkrecht auf a und b: ha, a Ă bi = hb, a Ă bi = 0.
LaĚnge des Produktvektors: Wenn Ď einen der beiden Winkel zwischen a und b bezeichnet, ist
2

|a Ă b|2 = |a|2 Âˇ |b|2 â ha, bi = |a|2 Âˇ |b|2 sin2 (Ď).
Multiplikationstafel: Wir haben folgende Tabelle:
Ă
e1
e2
e3

e1
0
âe3
e2

e2
e3
0
âe1

e3
âe2
e1
0

(ErlaĚuterung: e1 Ă e2 = e3 usw.)

Linear abhaĚngige Vektoren: Es ist a Ă b = 0 genau dann, wenn a und b linear abhaĚngig sind.
Beweis. Die meisten Eigenschaften lassen sich simples Rechnen zeigen. Zum Beispiel ist
|a Ă b|2 = (Îą2 Î˛3 â Î˛2 Îą3 )2 + (Îą3 Î˛1 â Î˛3 Îą1 )2 + (Îą1 Î˛2 â Î˛1 Îą2 )2

= (Îą2 Î˛3 )2 + (Î˛2 Îą3 )2 + (Îą3 Î˛1 )2 + (Î˛3 Îą1 )2 + (Îą1 Î˛2 )2 + (Î˛1 Îą2 )2
=

(Îą21

+

Îą22

â 2(Îą2 Î˛3 Î˛2 Îą3 + Îą3 Î˛1 Î˛3 Îą1 + Îą1 Î˛2 Î˛1 Îą2 )

+ Îą23 )(Î˛12 + Î˛22 + Î˛32 ) â (Îą1 Î˛1 + Îą2 Î˛2 + Îą3 Î˛3 )2
2

= |a|2 Âˇ |b|2 â ha, bi = |a|2 Âˇ |b|2 (1 â (cos â (a, b))2 ).

Daraus folgt sofort die letzte Eigenschaft: Denn wenn a Ă b = ~0 ist, dann hat entweder einer der Vektoren
a, b die LaĚnge 0, oder die beiden Vektoren sind parallel oder antiparallel; also sind a und b linear abhaĚngig.
Und umgekehrt.
Frage: Wie kann man die Formel fuĚr die ProduktvektorlaĚnge aus der LagrangeâIdentitaĚt herleiten ?
Das Kreuzprodukt gibt es nur im R3 .
FuĚr die geometrische Interpretation nehmen wir an, daĂ e1 , e2 , e3 ein Rechtssystem sind, also im Raum
liegen wie Daumen, Zeigefinger, Mittelfinger einer gesunden rechten Hand. Dann gilt:
Satz 1.52. Seien a, b â R3 , und c = a Ă b. Dann gilt:
1. c steht senkrecht auf a und auf b.
2. Die LaĚnge |c| ist gleich der FlaĚche des Parallelogramms, das von a und b aufgespannt wird.
3. a, b und c bilden in dieser Reihenfolge ein Rechtssystem.
Die ersten beiden Aussagen sind schon bewiesen, den Beweis der letzten Aussage muĚssen wir aufschieben
(siehe Satz 1.62).
31 Joseph

Louis Lagrange, 1736â1813

1.5. DER RAUM UND DER R3

39

Bemerkung 1.53. Gegeben seien zwei Vektoren a und b. Gesucht sei ein Vektor x mit a Ă x = b. Aus
dem vorigen Satz ergibt sich, daĂ a und b aufeinander senkrecht stehen muĚssen. Ansonsten kann es eine
LoĚsung x gar nicht geben.
Aber selbst wenn a und b aufeinander senkrecht stehen sollten, ist die LoĚsung x immer noch nicht eindeutig
bestimmt (auch wenn wir jetzt drei Gleichungen fuĚr drei Unbekannte haben).
Seien zum Beispiel a = (1, 0, 0)â¤ und b = (Î˛1 , Î˛2 , Î˛3 )â¤ gegeben, und x = (Îž1 , Îž2 , Îž3 )â¤ gesucht. Dann ist
ďŁŤ ďŁś
ďŁŤ
ďŁś
Î˛1
0
ďŁ­Î˛2 ďŁ¸ = b = a Ă x = ďŁ­âÎž3 ďŁ¸ .
Î˛3
Îž2

Wir bekommen Î˛1 = 0 (was nichts anderes als a âĽ b bedeutet) sowie Îž2 = Î˛3 und Îž3 = âÎ˛2 . Und
offensichtlich koĚnnen wir Îž1 nicht ermitteln. Es gibt mehrere LoĚsungen: wenn x eine LoĚsung ist, dann ist
auch x + Îťa eine LoĚsung, fuĚr jedes Îť â R (natuĚrlich nur wenn a âĽ b).
Es gibt keine Division als Umkehroperation des Kreuzproduktes.
Als Kombination von Skalarprodukt und Vektorprodukt bekommen wir (nur im R3 !) das Spatprodukt:
Definition 1.54 (Spatprodukt bzw. Determinante). Seien x, y, z â R3 . Dann heiĂt der Wert hx, y Ă zi
Spatprodukt32 oder Determinante33 der Vektoren x, y, z. Eine andere Schreibweise ist det(x, y, z).
Der Namen ergibt sich daraus, daĂ drei Vektoren im R3 einen sogenannten Spat aufspannen; und das
Spatprodukt ist gerade das (orientierte) Volumen des Spats. Wenn die Vektoren x, y, z ein Rechtssystem
bilden, dann ist das Spatprodukt positiv, ansonsten negativ.
Seien nun x = (Îž1 , Îž2 , Îž3 )â¤ , y = (Îˇ1 , Îˇ2 , Îˇ3 )â¤ und z = (Îś1 , Îś2 , Îś3 )â¤ . Dann erhalten wir nach Einsetzen und
Ausrechnen die Formel
det(x, y, z) = Îž1 Îˇ2 Îś3 + Îˇ1 Îś2 Îž3 + Îś1 Îž2 Îˇ3 â Îś1 Îˇ2 Îž3 â Îž1 Îś2 Îˇ3 â Îˇ1 Îž2 Îś3 ,
die man sich wie folgt merken kann: Man schreibt die Vektoren x, y und z spaltenweise nebeneinander, und
anschlieĂend die Vektoren x und y noch einmal rechts daneben:
Îž1

Îˇ1

Îś1

Îž1

Îˇ1

Îž2
Îž3

Îˇ2
Îˇ3

Îś2
Îś3

Îž2
Îž3

Îˇ2
Îˇ3

AnschlieĂend bildet man drei diagonale Produkte von links oben nach rechts unten; diese Produkte werden
positiv gezaĚhlt. Und entsprechend bildet man drei diagonale Produkte von rechts oben nach links unten,
die negativ gezaĚhlt werden.
Satz 1.55. Das Spatprodukt hat die folgenden Eigenschaften:
â˘ Es ist linear in jedem der drei Argumente (trilinear), das heiĂt zum Beispiel fuĚr das erste Argument
det(x + y, z, w) = det(x, z, w) + det(y, z, w),

det(Îťx, y, z) = Îť det(x, y, z).

â˘ Wenn zwei der Vektoren x, y, z gleich sind, dann verschwindet das Spatprodukt: det(x, x, z) = 0 usw.
â˘ FuĚr die kanonischen Basisvektoren gilt det(e1 , e2 , e3 ) = 1.
Beweis. Wir wissen von fruĚher, daĂ sowohl das Skalarprodukt als auch das Vektorprodukt bilinear sind.
Daraus ergibt sich sofort, daĂ das Spatprodukt linear in jedem seiner Faktoren ist (Distributivgesetze).
Die zweite Aussage folgt daraus, daĂ einerseits das Vektorprodukt senkrecht steht auf jedem seiner beiden
Faktoren, andererseits das Vektorprodukt zweier gleicher Vektoren gleich dem Nullvektor ist.
Und die dritte Aussage ergibt sich aus elementarem Rechnen.
32 parallelepipedial
33 determinant

product, triple product

40

KAPITEL 1. GRUNDLAGEN

Satz 1.56. FuĚr das Spatprodukt gelten die folgenden Rechenregeln:
â˘ Wenn man ein Vielfaches eines Vektors zu einem anderen Vektor addiert, bleibt das Spatprodukt
gleich: det(x + Îťy, y, z) = det(x, y, z).
â˘ Wenn man zwei Vektoren im Spatprodukt tauscht, aĚndert sich das Vorzeichen: det(y, x, z) =
â det(x, y, z).
â˘ Die drei Vektoren im Spatprodukt kann man zyklisch tauschen: det(x, y, z) = det(y, z, x) = det(z, x, y).
Beweis. Mit den Eigenschaften des vorigen Satzes haben wir
det(x + Îťy, y, z) = det(x, y, z) + det(Îťy, y, z) = det(x, y, z) + Îť det(y, y, z) = det(x, y, z) + 0,
womit wir die zweite Aussage beweisen koĚnnen:
det(y, x, z) = det(y, x + y, z) = det(y â (x + y), x + y, z)

= det(âx, x + y, z) = det(âx, y, z) = â det(x, y, z).

Und wenn wir dies zweimal anwenden, erhalten wir den dritten Teil:
det(x, y, z) = â det(x, z, y) = + det(y, z, x).
Definition 1.57. Seien x1 , x2 , . . . , xn â R3 beliebige Vektoren.

Wenn es reelle Zahlen Îť1 , . . . , Îťn gibt, von denen wenigstens eine nicht 0 ist, sodaĂ Îť1 x1 + Âˇ Âˇ Âˇ + Îťn xn = ~0
ist, dann heiĂen die Vektoren x1 , . . . , xn linear abhaĚngig34 .
Ansonsten (wenn es also nur eine einzige MoĚglichkeit gibt, den Nullvektor mit den Vektoren xj darzustellen:
naĚmlich alle Îťj gleich 0 zu waĚhlen) heiĂen die Vektoren x1 , . . . , xn linear unabhaĚngig35 .
Anders formuliert: Seien die Vektoren x1 , . . . , xn linear unabhaĚngig. Wenn dann eine Gleichung der Form
Îť1 x1 + Âˇ Âˇ Âˇ + Îťn xn = ~0 wahr ist, dann muĚssen saĚmtliche Îťj = 0 sein.

Wenn die Vektoren x1 , . . . , xn hingegen linear abhaĚngig sind, dann kann man reelle Zahlen Îť1 , . . . , Îťn
finden mit Îť1 x1 + Âˇ Âˇ Âˇ + Îťn xn = ~0, sodaĂ wenigstens ein Îťj 6= 0 ist. Zum Beispiel sei Îť1 6= 0. Dann duĚrfen
wir durch Îť1 dividieren, und bekommen


Îť2
Îťn
Îťj
x1 = â
x2 + Âˇ Âˇ Âˇ +
xn = Îą2 x2 + Âˇ Âˇ Âˇ + Îąn xn ,
Îąj := â .
Îť1
Îť1
Îť1
Wir koĚnnen also (weil Îť1 6= 0 ist) x1 als Linearkombination der anderen Vektoren schreiben. (Aber es kann
sein, daĂ wir z.B. x2 nicht mittels x1 , x3 , x4 , . . . , xn darstellen koĚnnen.)

Frage: Finden Sie ein Beispiel fuĚr Vektoren x1 , x2 , x3 in der Ebene, sodaĂ man x1 als Linearkombination
von x2 und x3 darstellen kann, aber x2 nicht als Linearkombination von x1 und x3 .
Wenn eine Familie36 von Vektoren linear abhaĚngig ist, dann gibt es einen Vektor aus dieser Familie,
den man als Linearkombination der anderen ausdruĚcken kann.
Sobald eine Familie von Vektoren den Nullvektor enthaĚlt, ist sie linear abhaĚngig.
Satz 1.58. Seien x, y, z â R3 beliebige Vektoren.
â˘ Folgende drei Aussagen sind aĚquivalent:37
1. x Ă y = 0,
34 linearly

dependent
independent
36 Wir reden von einer Familie von Vektoren, weil in einer Familie ein Vektor auch mehrfach vorkommen darf. Im Gegensatz
dazu darf eine Menge kein Element doppelt enthalten.
37 Das bedeutet nur, daĂ jede der drei Aussagen aus jeder der beiden anderen geschluĂfolgert werden kann. Die drei Aussagen
sind entweder alle gleichzeitig wahr, oder sie sind alle gleichzeitig falsch.
35 linearly

1.5. DER RAUM UND DER R3

41

2. x und y sind linear abhaĚngig,
3. x und y sind parallel (oder antiparallel).
â˘ Folgende drei Aussagen sind aĚquivalent:
1. det(x, y, z) 6= 0,

2. x, y und z sind linear unabhaĚngig,
3. jedes w â R3 kann auf eindeutige Weise geschrieben werden als w = Îąx + Î˛y + Îłz.
â˘ Je vier Vektoren im R3 sind linear abhaĚngig.
Beweisskizze. Der erste Teil wurde in Satz 1.51 bewiesen.
FuĚr den zweiten Teil beschraĚnken wir uns auf einige anschauliche geometrische UĚberlegungen: Wenn die
Vektoren x, y, z linear unabhaĚngig sind, dann liegen sie nicht in einer gemeinsamen Ebene. Also spannen
sie einen Spat auf, der ein Volumen hat, das nicht 0 ist. Und umgekehrt, was die AĚquivalenz 1 ââ 2 zeigt.
Wenn die Vektoren x, y, z linear unabhaĚngig sind und w ein beliebiger Vektor, dann existiert genau ein
Spat, der die Strecke zwischen dem Ursprung 0 und w als Raumdiagonale hat und dessen Kanten parallel
zu den Vektoren x, y, z sind. Die Koordinaten Îą, Î˛, Îł kann man dann von den KantenlaĚngen ablesen.
FuĚr den Beweis des dritten Teils fehlen uns im Moment die Mittel, sodaĂ wir ihn auf spaĚter verschieben.
Definition 1.59. Wenn die Vektoren x1 , x2 , . . . , xn â R3 paarweise orthogonal aufeinander stehen,
hxi , xj i = 0,

i 6= j,

dann bilden diese Vektoren ein Orthogonalsystem38.
Wenn diese Vektoren auĂerdem noch jeweils die LaĚnge 1 haben, also
(
0 : i 6= j,
hxi , xj i = Î´ij =
1 : i = j,
dann reden wir von einem Orthonormalsystem39. Der Ausdruck Î´ij heiĂt Kroneckersymbol (Leopold
Kronecker, 1823â1891).
Satz 1.60.
â˘ Wenn die Vektoren a1 , . . . , an â R3 ein OGS bilden und jeweils nicht ~0 sind, dann sind
sie linear unabhaĚngig.
â˘ Wenn die Vektoren a1 , a2 , a3 â R3 ein ONS bilden, dann ist ihre Determinante det(a1 , a2 , a3 )
entweder +1 oder â1.
Beweis. Im Beweis des ersten Teils gehen wir indirekt vor: wir setzen voraus, daĂ die Vektoren a1 , . . . ,
an jeweils nicht ~0 sind, ein Orthogonalsystem bilden, aber linear abhaĚngig sind. Dann koĚnnen wir zumindest einen Vektor als Linearkombination der anderen darstellen. Ohne BeschraĚnkung der Allgemeinheit
(o.B.d.A.) sei dieser Vektor a1 (ansonsten numerieren wir die Vektoren um). Dann ist also
a1 = Îť2 a2 + Âˇ Âˇ Âˇ + Îťn an .
Wir bilden das Skalarprodukt mit a1 :
ha1 , a1 i = Îť2 ha2 , a1 i + Âˇ Âˇ Âˇ + Îťn han , a1 i .
Die linke Seite ist ungleich 0, denn a1 6= ~0. Aber die rechte Seite verschwindet, denn es ist haj , a1 i = 0 fuĚr
j âĽ 2. Das ist ein Widerspruch. Also muĚssen die Vektoren linear unabhaĚngig sein.

Zum zweiten Teil: wir argumentieren geometrisch. Es ist det(a1 , a2 , a3 ) = ha1 , a2 Ă a3 i, und die Vektoren
a2 , a3 spannen ein Quadrat der SeitenlaĚnge 1 auf, also ist a2 Ă a3 ein Vektor mit der LaĚnge 1, der auf a2
und a3 senkrecht steht. Damit ist a2 Ă a3 = +a1 oder = âa1 , und somit folgt det(a1 , a2 , a3 ) = ha1 , Âąa1 i =
Âą|a1 |2 = Âą1.
38 orthogonal

system
system

39 orthonormal

42

KAPITEL 1. GRUNDLAGEN

Definition 1.61. Ein Orthonormalsystem mit Determinante +1 bzw. â1 heiĂt positiv orientiert40 bzw.
negativ orientiert41 .
Ein geordnetes Tripel (x1 , x2 , x3 ) von Vektoren heiĂt Rechtssystem bzw. Linkssystem, wenn seine Determinante det(x1 , x2 , x3 ) positiv bzw. negativ ist.
Satz 1.62. Die Tripel (e1 , e2 , e3 ) und (a, b, a Ă b) sind Rechtssysteme (wenn a Ă b 6= ~0).
Beweis. Man rechnet schnell nach, daĂ det(e1 , e2 , e3 ) = 1 ist. Weiterhin ist
det(a, b, a Ă b) = det(b, a Ă b, a) = det(a Ă b, a, b) = ha Ă b, a Ă bi = |a Ă b|2 > 0.

Damit ist der Beweis von Satz 1.52 vervollstaĚndigt.
Bemerkung 1.63. Ein anderer Zugang zum Kreuzprodukt fuĚhrt uĚber den LeviâCivita42 âTensor Îľ.
Dieser ist ein Tensor dritter Stufe (also ein wuĚrfelfoĚrmiges Zahlenschema, analog zu einer Matrix als
einem quadratischen Zahlenschema und einem Vektor als einem eindimensionalen Zahlenschema). Er ist
definiert als
ďŁą
ďŁ´
: wenn (j, k, l) eine gerade Permutation von (1, 2, 3) ist,
ďŁ˛1
Îľjkl := â1 : wenn (j, k, l) eine ungerade Permutation von (1, 2, 3) ist,
ďŁ´
ďŁł
0
: sonst.
Dann ist ~a Ă ~b = â(Îľ Âˇ ~a) Âˇ ~b. Hierbei ist das Produkt Îľ Âˇ ~a (ergibt eine Matrix) definiert analog zum Produkt
von Matrix mal Vektor uĚber das Summieren benachbarter identischer Indizes:
(Îľ Âˇ ~a)jk :=

3
X

Îľjkl al .

l=1

Wir haben auch (a Ă b)j =

1.5.3

P3

k,l=1 Îľjkl ak bl .

Drehungen im R3

Genauso wie im Falle des R2 stellen wir uns folgende Frage:
Gegeben ist ein Vektor x â R3 und eine Drehachse durch den Ursprung. Wir wollen den Vektor x um diese
Achse drehen mit einem Drehwinkel Ď. Wie koĚnnen wir den Bildvektor xâ˛ bestimmen ?
Der einfache Fall: die Drehachse ist eine Koordinatenachse
Die Drehachse sei also die Achse entlang des Vektors e1 , und der Drehwinkel sei Ď. Es bietet sich an, den
Originalvektor x in zwei Teile zu zerlegen:
x = Îž1 e1 + Îž2 e2 + Îž3 e3 = xk + xâĽ ,
wobei xk = Îž1 e1 parallel zur Drehachse ist, und xâĽ = Îž2 e2 + Îž3 e3 in einer Ebene lebt, die senkrecht auf der
Drehachse steht.
Man uĚberlegt sich leicht, daĂ der Bildvektor xâ˛ sich zerlegt als
xâ˛ = xâ˛k + xâ˛âĽ ,
wobei xâ˛k = xk gilt, denn dieser Anteil liegt genau auf der Drehachse. Und der senkrechte Anteil xâ˛âĽ kann
wie bei einer Drehung in der Ebene ausgerechnet werden:
 â˛ 
 
Îž2
cos Ď
â sin Ď
Îž2
=
.
Îž3â˛
sin Ď
cos Ď
Îž3
40 positively

oriented
oriented
Tullio LeviâCivita (1873â1941)

41 negatively
42

1.5. DER RAUM UND DER R3

43

Um die beiden Teilvektoren xk und xâĽ einheitlich zu behandeln, fuĚhren wir 3 Ă 3âMatrizen ein:
ďŁś
ďŁŤ
a11
a12
a13
a22
a23 ďŁ¸ ,
A = ďŁ­a21
aij â R.
a31
a32
a33

AuĂerdem definieren wir eine Operation

Matrix mal Spaltenvektor ergibt Spaltenvektor
gemaĚĂ
ďŁŤ

a11
ďŁ­a21
a31

ďŁŤ ďŁś
ďŁŤ ďŁś
ďŁŤ ďŁś
ďŁś
ďŁŤ
ďŁśďŁŤ ďŁś
a13
a12
a11
a11 Îž1 + a12 Îž2 + a13 Îž3
Îž1
a13
a23 ďŁ¸ ďŁ­Îž2 ďŁ¸ := ďŁ­a21 Îž1 + a22 Îž2 + a23 Îž3 ďŁ¸ = Îž1 ďŁ­a21 ďŁ¸ + Îž2 ďŁ­a22 ďŁ¸ + Îž3 ďŁ­a23 ďŁ¸ ,
a33
a32
a31
a31 Îž1 + a32 Îž2 + a33 Îž3
Îž3
a33

a12
a22
a32

mit der wir die Abbildungsgleichung schreiben koĚnnen als
ďŁŤ â˛ďŁś ďŁŤ
ďŁśďŁŤ ďŁś
Îž1
1
0
0
Îž1
ďŁ­Îž2â˛ ďŁ¸ = ďŁ­0
cos Ď
â sin ĎďŁ¸ ďŁ­Îž2 ďŁ¸ .
Îž3â˛
0
sin Ď
cos Ď
Îž3

Damit wissen wir jetzt, wie wir einen Vektor um die erste Koordinatenachse drehen koĚnnen.

Der schwierige Fall: die Drehachse ist verschieden von allen Koordinatenachsen
Also ist die Drehachse entlang des Vektors r eine beliebige Gerade durch den Ursprung. Die Strategie ist:
1. wir waĚhlen ein neues Koordinatensystem (also neue Basisvektoren r1 , r2 , r3 ), sodaĂ der Drehachsenvektor r gleich einem Koordinatensystemsachsenvektor wird,
2. wir rechnen die Koordinaten von x auf das neue Koordinatensystem um,
3. wir fuĚhren die Drehung im neuen Koordinatensystem aus,
4. wir rechnen die Koordinaten des Bildvektors xâ˛ wieder auf das alte Koordinatensystem zuruĚck.
Es ist also ein Vektor x gegeben und eine Drehachse, die durch einen Vektor r1 â R3 beschrieben wird. Wir
wollen x um die Achse durch r1 mit einem Winkel Ď drehen.
Zu Schritt 1:
Wir duĚrfen annehmen, daĂ der Vektor r1 die LaĚnge 1 hat. Wir suchen uns zwei weitere Vektoren r2 und r3 ,
die in der zu r1 senkrechten Ebene liegen und gemeinsam mit r1 ein ONS bilden (spaĚter werden wir ein Verfahren angeben, solche Vektoren r2 , r3 zu bestimmen (Orthogonalisierungsverfahren von GramâSchmidt)).
Dabei ist zu beachten, daĂ r1 , r2 und r3 in dieser Reihenfolge ein Rechts-System bilden; anderenfalls muĂ
das Vorzeichen des Drehwinkels geaĚndert werden. Diese 3 Spaltenvektoren stellen wir nebeneinander und
bekommen eine Matrix A:
ďŁś
ďŁŤ
A = ďŁ­r1

Zu Schritt 2:

r2

r3 ďŁ¸ .

Wir koĚnnen diese 3 Vektoren als Basis des R3 ansehen und haben dann
x = Ěş1 r1 + Ěş2 r2 + Ěş3 r3 ,
wobei die Ěşj zunaĚchst unbekannt sind. AuĂerdem verfuĚgen wir uĚber die Darstellung
x = Îž1 e1 + Îž2 e2 + Îž3 e3 ,
wobei die Zahlen Îžj gegeben sind, und die ej sind die kanonischen Basisvektoren: e1 = (1, 0, 0)â¤ usw.
Die Zahlen Ěşj koĚnnen wir folgendermaĂen berechnen:
hx, rj i = hĚş1 r1 + Ěş2 r2 + Ěş3 r3 , rj i = hĚşj rj , rj i = Ěşj |rj |2 = Ěşj ,

j = 1, 2, 3,

denn die Vektoren r1 , r2 und r3 bilden ein ONS, und beim Bilden des Skalarproduktes fallen immer alle
Summanden bis auf einen heraus.
AuĂerdem benoĚtigen wir noch den Begriff der transponierten Matrix:

44

KAPITEL 1. GRUNDLAGEN

Definition 1.64. Sei A eine 3 Ă 3âMatrix, und Aâ¤ diejenige Matrix, die durch Spiegelung der MatrixeintraĚge an der Hauptdiagonalen entsteht. Dann heiĂt Aâ¤ transponierte Matrix43 zu A. Also:
ďŁŤ
ďŁś
ďŁŤ
ďŁś
a11
a12
a13
a11
a21
a31
a22
a23 ďŁ¸ ,
a22
a32 ďŁ¸ .
A = ďŁ­a21
Aâ¤ = ďŁ­a12
a31
a32
a33
a13
a23
a33

Satz 1.65. Seien A und B 3 Ă 3âMatrizen. Dann ist
(AB)â¤ = B â¤ Aâ¤ .
Beweis. UĚbungsaufgabe.

Man beachte die geaĚnderte Reihenfolge der Faktoren !
Etwas geschickter hingeschrieben, lauten die Gleichungen hx, rj i = Ěşj :
ďŁśďŁŤ ďŁś
ďŁŤ ďŁś ďŁŤ
ââ r1â¤
ââ
Ěş1
ďŁ­Ěş2 ďŁ¸ = ďŁ­ââ r2â¤
ââďŁ¸ ďŁ­xďŁ¸ ,
â¤
Ěş3
ââ r3
ââ
also

ďŁŤ ďŁś
Ěş1
ďŁ­Ěş2 ďŁ¸ = Aâ¤ x.
Ěş3

(1.4)

Zu Schritt 3:
Im Koordinatensystem der Vektoren (r1 , r2 , r3 ) laĚĂt sich die Drehung muĚhelos durchfuĚhren:
xâ˛ = Ěşâ˛1 r1 + Ěşâ˛2 r2 + Ěşâ˛3 r3
mit unbekannten Koordinaten Ěşâ˛j , die sich bestimmen gemaĚĂ
ďŁŤ â˛ďŁś ďŁŤ
ďŁśďŁŤ ďŁś
Ěş1
1
0
0
Ěş1
ďŁ­Ěşâ˛2 ďŁ¸ = ďŁ­0
cos Ď
â sin ĎďŁ¸ ďŁ­Ěş2 ďŁ¸ .
Ěşâ˛3
0
sin Ď
cos Ď
Ěş3

(1.5)

Zu Schritt 4:
Aus dem vorigen Schritt kennen wir bereits die Zahlenspalte (Ěşâ˛1 , Ěşâ˛2 , Ěşâ˛3 )â¤ , und jetzt suchen wir die Zahlenspalte (Îž1â˛ , Îž2â˛ , Îž3â˛ )â¤ zum Bildvektor xâ˛ . Analog zu (1.4) haben wir auch jetzt die Beziehung
ďŁŤ â˛ďŁś
Ěş1
ďŁ­Ěşâ˛2 ďŁ¸ = Aâ¤ xâ˛ ,
Ěşâ˛3

und wenn wir von links die inverse Matrix (Aâ¤ )â1 dranmultiplizieren, folgt
ďŁŤ â˛ďŁś
Ěş1
(Aâ¤ )â1 ďŁ­Ěşâ˛2 ďŁ¸ = (Aâ¤ )â1 Aâ¤ xâ˛ = I3 xâ˛ = xâ˛ ,
Ěşâ˛3

und somit sind wir fertig, denn insgesamt haben wir jetzt (aus (1.4) und (1.5))
ďŁŤ â˛ďŁś
ďŁŤ â˛ďŁś
ďŁŤ
ďŁśďŁŤ ďŁś
Îž1
Ěş1
1
0
0
Ěş1
ďŁ­Îž2â˛ ďŁ¸ = (Aâ¤ )â1 ďŁ­Ěşâ˛2 ďŁ¸ = (Aâ¤ )â1 ďŁ­0
cos Ď
â sin ĎďŁ¸ ďŁ­Ěş2 ďŁ¸
Îž3â˛
Ěşâ˛3
0
sin Ď
cos Ď
Ěş3
ďŁŤ
ďŁś
ďŁŤ ďŁś
1
0
0
Îž1
cos Ď
â sin ĎďŁ¸ Aâ¤ ďŁ­Îž2 ďŁ¸ .
= (Aâ¤ )â1 ďŁ­0
0
sin Ď
cos Ď
Îž3
Leider sind wir noch nicht ganz fertig, denn wir haben zwei offene Fragen:
43 transposed

matrix

1.5. DER RAUM UND DER R3

45

â˘ gibt es (Aâ¤ )â1 uĚberhaupt ?
â˘ wenn ja, wie finden wir (Aâ¤ )â1 ? Was waĚre der Rechenweg ?
Zur Antwort brauchen wir ein neues Produkt, naĚmlich
Matrix mal Matrix ergibt Matrix.
Wir beobachten, daĂ man sich eine Matrix als nebeneinandergestellte Spaltenvektoren vorstellen kann:
ďŁś
ďŁŤ ďŁś
ďŁŤ
a1j
a2
a3 ďŁ¸ ,
aj = ďŁ­a2j ďŁ¸ , j = 1, 2, 3.
A = ďŁ­ a1
a3j

Wenn nun eine 3 Ă 3âMatrix B entsteht aus 3 Spaltenvektoren b1 , b2 und b3 , dann definieren wir
ďŁŤ
ďŁś
ďŁś
ďŁŤ
AB := A ďŁ­b1

b2

b3 ďŁ¸ := ďŁ­Ab1

Ab2

Ab3 ďŁ¸ .

Oder anders formuliert: Man nimmt eine Zeile von A und eine Spalte von B, bildet das Skalarprodukt
dieser beiden Vektoren, und das Ergebnis schreibt man dorthin, wo diese Zeile/Spalte einander kreuzen.
In unserem konkreten Fall sind die Spalten von A aber gerade die Vektoren r1 , r2 , r3 , die ein Orthonormalsystem bilden. Das heiĂt aber gerade Aâ¤ A = I. Jetzt schauen wir uns die Definition der inversen Matrix
an und erkennen daraus sofort:
â˘ Die Matrix A ist invertierbar.
â˘ Die Inverse zu A ist in diesem Falle gleich Aâ¤ .
Wie wir im Abschnitt 1.4 gelernt haben, ist nicht nur Aâ¤ A = I, sondern auch AAâ¤ = I. Das heiĂt:
Wenn die Spalten von A ein Orthonormalsystem bilden, dann bilden die Zeilen von A ebenfalls ein Orthonormalsystem !44 Es folgt dann auch: (Aâ¤ )â1 = A. Wir koĚnnen also (Aâ¤ )â1 mit einem Rechenaufwand
nahe Null bestimmen !
Damit haben wir die Drehmatrix gefunden:
Satz 1.66. Die Drehung um eine Drehachse durch den Vektor r1 mit dem Winkel Ď wird beschrieben durch
eine Drehmatrix R(r1 , Ď), die gegeben wird durch das Produkt
ďŁŤ
ďŁś
1
0
0
cos Ď
â sin ĎďŁ¸ Aâ¤ .
R(r1 , Ď) = A ďŁ­0
0
sin Ď
cos Ď

Hierbei ist A eine 3 Ă 3âMatrix, deren Spalten ein ONS bilden, und die erste Spalte ist gleich dem Vektor r1
(ggf. normiert auf LaĚnge 1). Die beiden weiteren Spalten r2 und r3 von A sind beliebig waĚhlbar, solange sie
gemeinsam mit der ersten Spalte ein ONS bilden. Es ergibt sich bei anderer Wahl dieser Spalten jedesmal
dieselbe Matrix R(r1 , Ď).
Es ist am besten, dieses Dreierprodukt von Matrizen stehenzulassen und nicht auszumultiplizieren.
Satz 1.67. Die Spalten von R bilden ein Orthonormalsystem.
Beweis. Wir muĚssen nur nachpruĚfen, daĂ Râ¤ R = I gilt. Zur AbkuĚrzung sei die mittlere Matrix R0 getauft,
also R = AR0 Aâ¤ . Dann haben wir wegen Satz 1.65
Râ¤ R = (AR0 Aâ¤ )â¤ (AR0 Aâ¤ ) = ((Aâ¤ )â¤ R0â¤ Aâ¤ )(AR0 Aâ¤ )
= AR0â¤ (Aâ¤ A)R0 Aâ¤ = AR0â¤ R0 Aâ¤ = A(R0â¤ R0 )Aâ¤ = AAâ¤
= I.

44 Wer nicht an die Kraft der Gruppentheorie (insbesondere Satz 1.44) glauben will, moĚge versuchen, dieses Ergebnis zu
FuĂ zu beweisen, d.h. zum Beispiel mit den Methoden der Vektorrechnung . . .

46

KAPITEL 1. GRUNDLAGEN

Solche Matrizen tauchen in Mathematik und Physik so oft auf, daĂ sie eines eigenen Namens wuĚrdig sind:
Definition 1.68. Eine Matrix A mit Aâ¤ A = I heiĂt orthogonale Matrix45 .
Die Bedeutung der orthogonalen Matrizen liegt darin, daĂ sie die LaĚngen von Vektoren nicht aĚndern: wenn x
irgendein Vektor ist und A eine orthogonale Matrix, dann haben x und Ax dieselbe LaĚnge. Die Umkehrung
gilt auch: wenn eine Matrix die LaĚnge von Vektoren nicht aĚndert, dann ist sie eine orthogonale Matrix.
Jede Bewegung des Raumes, die den Ursprung festhaĚlt (also Drehungen, Spiegelungen und jede Komposition
davon, aber keine Verschiebung) kann durch eine orthogonale Matrix beschrieben werden.

1.5.4

Der affine Raum

Wir sollten unbedingt zwei Typen von Vektoren unterscheiden:
â˘ Ortsvektoren,
â˘ Vektoren im engeren Sinne.
Ortsvektoren bezeichnen einen Ort, also einen Punkt im Raum. Ein Ortsvektor startet stets im Ursprung.
Ein Beispiel waĚre ein Vektor, der zum aktuellen Aufenthaltsort eines Teilchens zeigt.
Die Vektoren im engeren Sinne beschreiben alles weitere: Geschwindigkeiten, Beschleunigungen, KraĚfte,
StroĚmungen, elektrische Felder, magnetische Felder usw.
Wenn Ortsvektoren benutzt werden, um einen geographischen Ortâ anzuzeigen, dann kann man sie nicht
â
addieren: die Summe zweier geographischen Punkteâ ist in einer solchen Situation sinnlos.
â
Aber man kann einen Vektor (i.e.S.) an einen Ortsvektor anhaĚngen, um zum Beispiel auszudruĚcken, wo
das Teilchen nach dem Verstreichen eines Zeitintervalles âłt waĚre, wenn es seine jetzige Geschwindigkeit
beibehielte. In diesem Sinne haĚtte man einen Ortsvektor und einen Vektor i.e.S. addiert.
Mathematisch druĚckt man dies durch den Begriff des affinen Raums aus. Grob gesprochen besteht der
affine Raum aus zwei Dingen:
â˘ einer Menge von Punkten,
â˘ und einer Menge von Vektoren.
Die Vektoren bilden einen Vektorraum wie schon fruĚher definiert.
Zwischen beiden Mengen gelten dabei die folgenden Regeln:
â˘ wenn man an einen Punkt einen Vektor anhaĚngt, landet man wieder bei einem Punkt,
â˘ die gerichtete Verbindungsstrecke zwischen zwei Punkten ergibt einen Vektor.

1.6

SchluĚsselbegriffe

Die hier aufgefuĚhrten Begriffe sind von hoĚchster Bedeutung. Ohne eine sichere Beherrschung dieser Begriffe
und Konzepte ist ein VerstaĚndnis spaĚterer Vorlesungskapitel nicht moĚglich.
Sie sollten auch in der Lage sein, die in das Skript eingestreuten Fragen uĚberzeugend zu beantworten.
â˘ algebraische Grundkonzepte: Gruppe, KoĚrper, Vektorraum,
â˘ R und C als KoĚrper, Definition von C, geometrische Interpretation der Rechenoperationen in C,
â˘ zwei Sichtweisen auf den Raum der Ortsvektoren in der Ebene: geometrisch und analytisch,
â˘ Produkte Matrix Âˇ Vektor und Matrix Âˇ Matrix,

â˘ Drehungen im R2 und R3 ,

â˘ Vektorprodukt, Spatprodukt, Determinante,
â˘ affiner Raum im Gegensatz zum Vektorraum.
45 orthogonal

matrix

Kapitel 2

VektorraĚume
Wir haben bisher 4 VektorraĚume naĚher untersucht:
â˘ den geometrisch definierten Raum der Ortsvektoren in der Ebene,
â˘ den R2 ,
â˘ den dreidimensionalen Raum der Ortsvektoren,
â˘ und den R3 .
Diese RaĚume waren geometrisch bzw. analytisch definiert. Es gibt noch viele andere wichtige VektorraĚume,
wie z.B. FunktionenvektorraĚume, die nicht nur in der Quantenmechanik unverzichtbar sind. Da wir aber
nicht jeden einzeln behandeln koĚnnen, untersuchen wir in diesem Kapitel Eigenschaften, die jeder Vektorraum besitzt.
Es gibt verschiedene Typen von VektorraĚumen. Eine Unterscheidung beruht darauf, ob der Vektorraum ein
Skalarprodukt besitzt oder nicht. Eine zweite Unterscheidung bezieht sich auf die Dimension: sie koĚnnte
endlich sein oder auch unendlich. FunktionenvektorraĚume sind praktisch immer unendlichdimensional, und
sie treten auf bei der Modellierung von vielen Prozessen aus der Natur und Technik, z.B. bei der Untersuchung der Ausbreitung von elektrischen Wellen, oder bei der Wettervorhersage, oder bei der Simulation
des elastischen Verhaltens von Bauteilen unter mechanischer Belastung. Eine praĚzise Untersuchung solcher
unendlichdimensionaler VektorraĚume ist sehr anspruchsvoll (sie findet z.B. im vierten Semester des Mathematikstudiums in der Funktionalanalysisvorlesung statt), und deshalb verzichten wir darauf weitgehend.
In der numerischen Simulation zu Anwendungsaufgaben verwendet man stattdessen endlichdimensionale
VektorraĚume mit sehr hoher Dimension als NaĚherungsverfahren; z.B. sind bei der Wettervorhersage Gleichungssysteme mit vielen Millionen Unbekannten regelmaĚĂig zu loĚsen.

2.1

Allgemeine Eigenschaften

Sei K ab jetzt ein KoĚrper. In praktisch saĚmtlichen FaĚllen ist K = R oder K = C.
Definition 2.1. Wir sagen, daĂ V einen Vektorraum uĚber K bildet, wenn zwei Operationen
+ : V Ă V â V,
Âˇ : K ĂV âV
gegeben sind mit folgenden Eigenschaften:
â˘ (V, +) ist eine abelsche Gruppe mit neutralem Element 0 = ~0 â V,
â˘ Wir haben die beiden Distributivgesetze (Îą + Î˛) Âˇ u = Îą Âˇ u + Î˛ Âˇ u und Îą Âˇ (u + v) = Îą Âˇ u + Îą Âˇ v, fuĚr
Îą, Î˛ â K und u, v â V,
â˘ Es gilt das Assoziativgesetzâ (Îą Âˇ Î˛) Âˇ u = Îą Âˇ (Î˛ Âˇ u) fuĚr Îą, Î˛ â K und u â V,
â
â˘ Die Zahl 1 ist auch neutrales Element fuĚr die zweite Multiplikation: 1 Âˇ u = u fuĚr u â V.
47

48

KAPITEL 2. VEKTORRAĚUME

Ab jetzt werden wir mit 0 sowohl die reelle/komplexe Zahl 0 bezeichnen als auch den Nullvektor in V . Die
Schreibweise ~0 werden wir nur noch in NotfaĚllen anwenden, wenn MiĂverstaĚndnisse drohen.
Die Multiplikationspunkte (auf die wir im Folgenden auch verzichten wollen) im Assoziativgesetzâ beâ
zeichnen zwei verschiedene Multiplikationen: einerseits die Multiplikation zweier Zahlen (KoĚrperelemente),
andererseits die Multiplikation einer Zahl mit einem Vektor (und das spaĚter betrachtete Skalarprodukt
zweier Vektoren wird ein dritter Typ von Multiplikation sein). Deshalb die â.
â
Entsprechend bezeichnet auch das Pluszeichen zwei verschiedene Additionen: von KoĚrperelementen (also
Zahlen) bzw. Vektoren.
AuĂerdem sei darauf hingewiesen, daĂ auch das Wort Vektorâ mindestens zwei verschiedene Bedeutungen
â
besitzt: einerseits ein konkretes geometrisches Objekt, welches man sich uĚblicherweise als Pfeil in der Ebene
oder im 3D vorstellt; andererseits ein abstraktes mathematisches Objekt, uĚber dessen Beschaffenheit nichts
gesagt wird (solange die Menge der Vektoren nur die Bedingungen aus Definition 2.1 erfuĚllt).
Zur Notation: Ab jetzt bezeichnen lateinische GroĂbuchstaben (meistens) VektorraĚume, lateinische Kleinbuchstaben e, f, g, h, u, v, w, . . . Vektoren, und griechische Buchstaben Elemente des KoĚrpers K.
Beispiel 2.2.
â˘ Rn als Vektorraum uĚber dem KoĚrper R,
â˘ Cn als Vektorraum uĚber C,
â˘ Cn als Vektorraum uĚber R,
â˘ M nĂm (R) â der Vektorraum der Matrizen des Formats n Ă m mit reellen EintraĚgen (uĚber dem
KoĚrper R),
â˘ R[X] â der Raum der Polynome in der Variablen X mit Koeffizienten aus R,
â˘ C([a, b] â R) â der Raum der auf dem Intervall [a, b] stetigen und reellwertigen Funktionen,
â˘ C k ([a, b] â R) â der Raum der auf dem Intervall [a, b] kâmal stetig differenzierbaren reellwertigen
Funktionen.
Hierbei definieren wir die Addition zweier Funktionen auf naheliegende Weise: seien f und g zwei Funktionen, dann wird die Summe (f + g) = (f + g)(x) definiert als f (x) + g(x). Analog definiert man die
Vervielfachung einer Funktion gemaĚĂ (Îąf ) = (Îąf )(x) = Îą Âˇ (f (x)). Der Nullvektorâ 0 â V ist diejenige
â
Funktion, die auf dem gesamten Intervall uĚberall den Wert 0 â R hat.
Frage: Warum kann man nicht Rn als Vektorraum uĚber dem KoĚrper C betrachten ?
Wie man sieht, kann alles moĚgliche einen Vektor darstellen. Deshalb ist es gar nicht so selbstverstaĚndlich,
daĂ die Aussagen des naĚchsten Satzes stimmen, auch wenn sie offensichtlich erscheinen:
Satz 2.3. FuĚr beliebige Vektoren u, v â V und eine beliebige Zahl Îą â K gelten die Regeln
1. 0 Âˇ u = ~0,

2. Îą Âˇ ~0 = ~0,
3. (â1) Âˇ u = âu.

Hierbei bezeichnet (âu) denjenigen Vektor aus V , der zu u addiert den Nullvektor ergibt: (âu) + u = ~0.
Beweis. Wie benutzen nichts weiter als die Eigenschaften aus Definition 2.1:
1. Es ist u = 1 Âˇ u = (1 + 0) Âˇ u = 1 Âˇ u + 0 Âˇ u = u + 0 Âˇ u. Weil (V, +) eine additive Gruppe ist und
in additiven Gruppen subtrahiert werden darf, muĂ 0 Âˇ u gleich dem neutralen Element der Addition
sein, also gleich dem Nullvektor.
2. Nach demselben Muster wie eben ist Îą Âˇ u = Îą(u + ~0) = Îą Âˇ u + Îą Âˇ ~0, und Îą Âˇ ~0 muĂ gleich dem
Nullvektor sein.
3. Wir benutzen das erste Ergebnis: ~0 = 0 Âˇ u = (1 + (â1)) Âˇ u = 1 Âˇ u + (â1) Âˇ u = u + (â1) Âˇ u. Andererseits
ist ~0 = u + (âu), und es muĂ also (â1) Âˇ u = âu sein.

2.2. LINEARKOMBINATIONEN, ERZEUGENDENSYSTEME USW.

2.2

49

Linearkombinationen, Erzeugendensysteme,
Lineare UnabhaĚngigkeit, Basen und Dimensionen

2.2.1

Linearkombinationen und UnterraĚume

Definition 2.4 (Linearkombination, Span, endlich erzeugt). Seien u1 , . . . , un â V Vektoren und
Îą1 , . . . , Îąn â K Zahlen. Dann heiĂt der Vektor
Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un â V
Linearkombination1 der Vektoren u1 ,. . . ,un . Wenn wir die uj festhalten und die Îąj durch ganz K laufen
lassen, erhalten wir den Span der Vektoren u1 , . . . , un :
ďŁź
ďŁą
n
ďŁ˝
ďŁ˛X
Îąj uj : Îą1 , . . . , Îąn â K .
span(u1 , . . . , un ) :=
ďŁž
ďŁł
j=1

Diese Menge wird auch der von den uj aufgespannte Unterraum oder lineare HuĚlle2 der uj genannt. Wir
sagen auch, daĂ die Vektoren u1 , . . . , un die Menge span(u1 , . . . , un ) erzeugen. Diese Vektoren bilden ein
sogenanntes Erzeugendensystem.
Als Sonderfall legen wir fest, daĂ die leere Menge denjenigen Raum aufspannt, der nur aus dem Nullvektor
besteht:
span(â) := {~0}.
Sei U â V eine Menge unendlich vieler Vektoren aus V . Dann definieren wir span(U) als
span(U) = {alle Linearkombinationen aus endlich vielen Elementen aus U} .
Man beachte, daĂ U auch uĚberabzaĚhlbar3 viele Elemente enthalten darf.
Sei V ein Vektorraum. Wenn es endlich viele Elemente u1 , . . . , un â V gibt mit V = span(u1 , . . . , un ),
dann heiĂt V endlich erzeugt.
Definition 2.5 (Unterraum). Sei V ein Vektorraum uĚber dem KoĚrper K, und sei U eine Teilmenge von
V . Wenn U den Nullvektor von V enthaĚlt und wenn U gemeinsam mit den von V auf U eingeschraĚnkten4
Operationen +â und Âˇâ wieder einen Vektorraum uĚber dem KoĚrper K bildet, dann heiĂt U Untervektorâ
â
raum von V 5 . Man sagt auch Unterraum.
Wenn jetzt eine solche Teilmenge U â V gegeben ist und wir wissen wollen, ob ein Unterraum vorliegt,
dann sind saĚmtliche Eigenschaften aus Definition 2.1 nachzupruĚfen. Zum GluĚck laĚĂt sich diese Rechnerei
abkuĚrzen, denn wir haben den folgenden Satz:
Satz 2.6 (Untervektorraumkriterium). Sei V ein Vektorraum uĚber dem KoĚrper K und U â V mit
folgenden Eigenschaften:
1. U enthaĚlt wenigstens ein Element,
2. wenn u â U und v â U , dann ist auch u + v â U ,
3. wenn u â U und Îą â K, dann ist auch Îąu â U .
1 linear

combination
hull, span
3 Eine Menge M heiĂt abzaĚhlbar, wenn ihre Elemente durchnumeriert werden koĚnnen, wobei man zum Numerieren nur
natuĚrliche Zahlen verwenden darf. In mathematischer Formulierung: es ist M abzaĚhlbar, wenn es eine bijektive Abbildung
von N nach M gibt.
Wenn M soviele Elemente hat, daĂ eine solche Numerierung niemals moĚglich ist, dann heiĂt M uĚberabzaĚhlbar. Es ist Q
abzaĚhlbar, aber R uĚberabzaĚhlbar.
Ein Skalarproduktvektorraum, der eine abzaĚhlbare Teilmenge besitzt, die jedes Element des Vektorraums beliebig genau
annaĚhert, wird in der Quantenmechanik (also im IK4) als separabel bezeichnet. Solche VektorraĚume sind schoĚnâ.
â
4 im Sinne eines verkleinerten Definitionsbereiches
5 linear subspace of V
2 linear

50

KAPITEL 2. VEKTORRAĚUME

Dann ist U ein Unterraum von V .
Wir sagen auch, daĂ U abgeschlossen unter den Operationen + und Âˇ ist.
Beweis. ZunaĚchst ist zu pruĚfen, ob fuĚr die Operationen + und Âˇ die Definitionsbereiche und Wertebereiche
passen: weil V ein Vektorraum ist, haben wir + und Âˇ als Abbildungen + : V Ă V â V und Âˇ : K Ă V â V .
Wenn wir diese Operationen einschraĚnken, bekommen wir + : U Ă U â V und Âˇ : K Ă U â V . Die
Voraussetzungen 2. und 3. besagen gerade, daĂ sogar + : U Ă U â U und Âˇ : K Ă U â U gelten.

Die uĚblichen Gesetze (KommutativitaĚt, AssoziativitaĚt und DistributivitaĚt) vererben sich von V auf U . Es
gilt auch 1 Âˇ u = u fuĚr jedes u â U , weil diese Beziehung sogar fuĚr jedes u â V gilt, und U â V .
Wir muĚssen jetzt bloĂ noch zeigen, daĂ wir auch in U ein neutrales Element zur Addition haben, und daĂ
es in U zu jedem Element auch ein additiv inverses Element gibt. Nach Voraussetzung hat U wenigstens
ein Element, nennen wir es uâ . Nach Voraussetzung 3. ist dann auch 0K Âˇ uâ â U . Aber nach Satz 2.3 ist
0K Âˇ uâ = 0V , also ist 0V ein Element von U .

Sei nun u â U gegeben und ein additiv inverses Element dazu in U gesucht. Dieses ist âu â V , aber nach
Satz 2.3 ist âu = (â1) Âˇ u, was jedoch nach Voraussetzung 3. ein Element von U ist und nicht bloĂ ein
Element von V .

Frage: Seien x1 , x2 â R3 beliebige Vektoren. Ist dann span(x1 , x2 ) eine Ebene im R3 durch den Ursprung ?
Satz 2.7. Sei V ein Vektorraum, u1 , . . . , un â V , und sei U = span(u1 , . . . , un ). Dann haben wir folgende
Regeln:
1. U ist ein Untervektorraum von V .
2. Wenn u â V ein weiterer Vektor ist, dann ist U â span(u1 , . . . , un , u).
3. Wenn u sogar in U liegt, dann ist U = span(u1 , . . . , un , u).
Beweis.

1. Man benutze das Untervektorraumkriterium.

2. Fast trivial: wenn man einen weiteren Vektor (naĚmlich u) beim Bilden von Linearkombinationen
erlaubt, dann kann man mindestens die gleichen Vektoren erzeugen wie zuvor (indem man z.B. diesen
Vektor u mit dem Koeffizienten 0 â K wichtet).
P
3. Wenn u â span(u1 , . . . , un ), dann haben wir Koeffizienten Î˛j â K mit u = nj=1 Î˛j uj . Dann koĚnnen
wir jede Linearkombination von u1 , . . . , un , u als Linearkombination von u1 , . . . , un schreiben:
Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un + Îąu = (Îą1 + ÎąÎ˛1 )u1 + Âˇ Âˇ Âˇ + (Îąn + ÎąÎ˛n )un .
Das beweist span(u1 , . . . , un , u) â span(u1 , . . . , un ) = U . Andererseits haben wir aus 2., daĂ
span(u1 , . . . , un , u) â span(u1 , . . . , un ) = U . Damit ist der Beweis beendet.

2.2.2

Lineare UnabhaĚngigkeit und Basen

Definition 2.8 (Linear abhaĚngig, linear unabhaĚngig, frei, Basis). Sei V ein Vektorraum uĚber K,
und sei (u1 , . . . , un ) eine Familie von Vektoren aus V .
1. Die Familie (u1 , . . . , un ) heiĂt linear abhaĚngig6 , wenn es Zahlen Îą1 , . . . , Îąn â K gibt, von denen
wenigstens eine nicht 0 ist, mit
Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un = ~0.
2. Wenn die Familie (u1 , . . . , un ) nicht linear abhaĚngig ist, dann heiĂt sie linear unabhaĚngig7 oder frei.
Das heiĂt, die einzige MoĚglichkeit, Zahlen Îą1 , . . . , Îąn â K zu waĚhlen, sodaĂ Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un = ~0,
ist Îą1 = Âˇ Âˇ Âˇ = Îąn = 0.
6 linearly
7 linearly

dependent
independent

2.2. LINEARKOMBINATIONEN, ERZEUGENDENSYSTEME USW.

51

3. Sei U eine Familie von unendlich vielen Vektoren aus V . Diese Familie heiĂt linear unabhaĚngig, wenn
jede endliche Teilmenge von U linear unabhaĚngig ist. Wenn U nicht linear unabhaĚngig ist, dann heiĂt
U linear abhaĚngig.
4. Sei U eine Familie von Vektoren aus V (endlich viele oder unendlich viele). Wenn U ein Erzeugendensystem von V ist und linear unabhaĚngig ist, dann heiĂt U Basis8 von V .
Bemerkung 2.9. Wir vermerken zwei SpezialfaĚlle:
â˘ Die Familie (~0), die nur den Nullvektor aus V enthaĚlt, ist linear abhaĚngig.
â˘ Die leere Familie, die nichts enthaĚlt (nicht einmal den Nullvektor), ist linear unabhaĚngig und spannt
den Nullvektorraum {~0} auf. Die leere Familie ist Basis des Nullvektorraums.
Diese Sprechweisen werden uns spaĚter erlauben, Aussagen einfacher zu formulieren.
Frage: Was bedeutet die lineare UnabhaĚngigkeit in FunktionenraĚumen ?
Frage: Sei V = R[X] und U = {1, x, x2 , x3 , . . . }. Ist diese Familie linear abhaĚngig ?

Damit eine Familie U eine Basis von V sein kann, muĂ sie zwei Eigenschaften erfuĚllen:
â˘ es muĂ U den ganzen Raum V aufspannen,
â˘ U muĂ linear unabhaĚngig sein.

Die erste Forderung besagt anschaulich, daĂ U nicht zu wenigeâ Elemente hat; und die zweite Forderung
â
besagt anschaulich, daĂ U nicht zu vieleâ Elemente besitzt.
â
Als schnell beweisbare Merkregeln haben wir dann:
Eine Familie U ist Erzeugendensystem eines Vektorraums V genau dann,
wenn jeder Vektor aus V darstellbar ist als Linearkombination von Elementen aus U.
Eine Familie U ist linear unabhaĚngig in V genau dann,
wenn der Nullvektor 0V genau auf eine einzige Weise dargestellt werden kann
als Linearkombination von Elementen aus U.
Satz 2.10. Eine Familie (u1 , . . . , un ) ist eine Basis von V genau dann, wenn jeder Vektor u â V auf genau
eine Art und Weise als Linearkombination der u1 , . . . , un dargestellt werden kann.
Beweis. Wir haben zwei Richtungen zu beweisen.
=ââ:
â
Sei (u1 , . . . , un ) eine Basis, und sei u â V ein beliebiger Vektor. Wegen der ersten Merkregel koĚnnen wir
den Vektor u auf mindestens eine Weise als Linearkombination der uj darstellen:
u = Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un .
Wir muĚssen noch zeigen, daĂ dies die einzige MoĚglichkeit ist. Sei also auĂerdem noch u = Î˛1 u1 + Âˇ Âˇ Âˇ+ Î˛n un .
Wenn wir beide Darstellungen abziehen, bekommen wir
~0 = (Îą1 â Î˛1 )u1 + Âˇ Âˇ Âˇ + (Îąn â Î˛n )un .
Weil nun allerdings die Vektoren u1 , . . . , un eine linear unabhaĚngige Familie bilden, laĚĂt sich der Nullvektor
nur darstellen, wenn (Îąj â Î˛j ) = 0 ist fuĚr jedes j. Es ist also Î˛j = Îąj .

â=â:
â
Wenn jeder Vektor genau eine Darstellung hat, dann hat er mindestens eine Darstellung. Wegen der ersten
Merkregel ist dann die Familie (u1 , . . . , un ) also ein Erzeugendensystem. Und wegen der zweiten Merkregel
bilden die uj eine linear unabhaĚngige Familie, und somit auch eine Basis.
Dieser Satz gilt auch dann, wenn diese Basis unendlich viele Elemente enthalten sollte; und auch der Beweis
ist im wesentlichen unveraĚndert.
8 basis

52

KAPITEL 2. VEKTORRAĚUME

Lemma 2.11. Seien u1 , . . . , un â V linear unabhaĚngig, und sei v 6â span(u1 , . . . , un ). Dann ist auch
(u1 , . . . , un , v) eine linear unabhaĚngige Familie.
Frage: Was bedeutet dies geometrisch ?
Beweis. Um die lineare UnabhaĚngigkeit zu zeigen, setzen wir
Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un + Îąv = 0
und muĚĂten als naĚchstes zeigen, daĂ Îą1 = Âˇ Âˇ Âˇ = Îąn = Îą = 0 gilt.

Angenommen, Îą 6= 0. Dann haĚtten wir
 Îą 
 Îą 
n
1
u1 + Âˇ Âˇ Âˇ + â
un ,
v= â
Îą
Îą

also v â span(u1 , . . . , un ). Wir hatten aber gerade das Gegenteil vorausgesetzt. Also muĂ Îą = 0 sein. Damit
bekommen wir Îą1 u1 + Âˇ Âˇ Âˇ + Îąn un = 0. Weil die Vektoren uj aber eine linear unabhaĚngige Familie bilden,
muĚssen die Îąj saĚmtlich gleich Null sein.

Satz 2.12 (BasisergaĚnzungssatz). Sei (v1 , . . . , vn ) ein Erzeugendensystem des Vektorraumes V , und sei
(u1 , . . . , um ) eine linear unabhaĚngige Familie in V .
Dann kann man die Familie (u1 , . . . , um ) mit passend gewaĚhlten Vektoren vj zu einer Basis von V
ergaĚnzen. Insbesondere existiert ein Index p â N, sodaĂ (bei geschickter Numerierung der vj ) die Familie
(u1 , . . . , um , vp+1 , . . . , vn ) eine Basis von V ist.
Beweis. Wenn die Familie (u1 , . . . , um ) schon den Vektorraum V aufspannt, dann sind wir fertig. Wir
brauchen nichts ergaĚnzen und setzen p = n. Anderenfalls existiert ein Vektor v â V , der nicht durch die
uj dargestellt werden kann. Dann muĂ ein vk aus dem Erzeugendensystem existieren, das nicht durch die
uj dargestellt werden kann (BegruĚndung bitte selbst finden). Mit diesem vk benutzen wir Lemma 2.11 und
erhalten eine linear unabhaĚngige Familie (u1 , . . . , um , vk ). Wir numerieren die vj so um, daĂ vk jetzt vn
heiĂt. Mit dieser neuen Familie beginnen wir wieder von vorn. Nach n Schritten ist das Erzeugendensystem
(v1 , . . . , vn ) in die linear unabhaĚngige Familie der uj eingefuĚgt worden, und der Beweis ist spaĚtestens dann
fertig.
Eine Familie (u1 ) ist linear unabhaĚngig, wenn u1 6= 0. Auf diesem Wege erhalten wir:
Satz 2.13 (BasisâSatz). Jeder endlich erzeugte Vektorraum besitzt eine Basis.
Die Voraussetzung endlich erzeugtâ ist uĚbrigens nicht noĚtig: Jeder Vektorraum hat eine Basis. Auf einen
â
Beweis muĚssen wir verzichten, da die erforderlichen Hilfsmittel aus der Mengenlehre hier nicht bereitgestellt
werden koĚnnen.
Weiterhin gilt:
Satz 2.14. Jedes Erzeugendensystem enthaĚlt eine Basis.
Beweis. Man wende den BasisergaĚnzungssatz geschickt an.
Frage: Im R1077 seien drei linear unabhaĚngige Vektoren gegeben. Man entscheide, ob diese drei Vektoren
den R3 aufspannen.
{ Es folgen fakultative Betrachtungen, was alles in unendlichdimensionalen RaĚumen schiefgehen kann. }

Ein Vektorraum heiĂt unendlichdimensional, wenn es in ihm beliebig groĂe linear unabhaĚngige Familien
gibt. Unser Basisbegriff fuĚr solche VektorraĚume verlangt, daĂ jeder Vektor dargestellt werden kann durch
eine endliche Linearkombination der Basisvektoren. Der Grund fuĚr diese BeschraĚnkung auf endliche Linearkombinationen liegt darin, daĂ wir gar nicht wissen, wie man unendlich viele Summanden addieren
wuĚrde. Und ohne eine Norm ist eine solche Summation auch kaum definierbar.
Wir schauen uns mal fuĚr ein Beispiel, bei dem eine Summation von unendlich vielen Summanden moĚglich zu
sein scheint, an, was dann trotzdem noch passieren kann. Es sei V der Vektorraum aller stetigen Funktionen
von R nach R, die im Unendlichen abklingen. Diesen Vektorraum statten wir mit der Norm |f |V :=
maxxâR |f (x)| aus, was dann eine Norm gemaĚĂ Satz 2.26 ergibt.

2.2. LINEARKOMBINATIONEN, ERZEUGENDENSYSTEME USW.

53

FuĚr k â Nâ1 := {â1, 0, 1, 2, 3, 4, . . . } definieren wir Funktionen vk gemaĚĂ
vk (x) := max(1 â 2k+1 |x|, 0),

x â R.

Diese stetigen Funktionen sind identisch Null fuĚr |x| âĽ 2âkâ1 , und auf dem Intervall [â2âkâ1 , 2âkâ1 ]
sind sie jeweils stuĚckweise linear. FuĚr groĂe k erhalten wir also Funktionen, die nur auf einem sehr kurzen
Intervall ungleich Null sind. AnschlieĂend schieben wir solche Funktionen auf der reellen Achse hin und
her. Das heiĂt, fuĚr m â Z setzen wir
vâ1,m (x) := vâ1 (x â m),

x â R,

was eine verschobene Kopie von vâ1 ist. Wir beobachten, daĂ die Dreiecksteile der Graphen von zwei
benachbarten Funktionen vâ1,m einander uĚberlappen.
Weiterhin setzen wir, fuĚr k â N0 := {0, 1, 2, 3, . . . } und m â Z,
vk,m (x) := vk (x â 2âkâ1 â 2âk m),

x â R,

also eine verschobene Kopie von vk (jetzt uĚberlappen die Graphen einander nicht mehr).
Das sollen unsere BasisââFunktionen sein. Wir verwenden deshalb AnfuĚhrungszeichen, weil wir absichtâ
lich jetzt unendliche Linearkombinationen erlauben wollen, die bisher ja unzulaĚssig waren. Diese Basisââ
â
Funktionen werden fuĚr groĂe k sehr schmalâ, sodaĂ die Hoffnung besteht, daĂ man wirklich jede Funktion
â
aus dem Vektorraum V damit zusammenbauen kann. Das funktioniert tatsaĚchlich. Denn sei f eine Funktion
aus V . FuĚr K â N0 setzen wir
X
f (m)vâ1,m (x)
fK (x) :=
|m|â¤K

+

X

0â¤kâ¤K;|m|â¤K



1
1
f (2âk m + 2âkâ1 ) â f (2âk m) â f (2âk m + 2âk ) vk,m (x),
2
2

was tatsaĚchlich eine endliche Linearkombination von Funktionen vl,n ist. Man ahnt oder glaubt, daĂ fK
eine AnnaĚherung von f ist, wenn K sehr groĂ ist. Damit ist gemeint, daĂ die Normdifferenz |fK â f |V
beliebig klein wird, wenn man nur K genuĚgend groĂ nimmt. TatsaĚchlich laĚĂt sich sogar beweisen, daĂ
f (x) =

â 
X

K=0


fK+1 (x) â fK (x) + f0 (x),

(2.1)

und die Konvergenz der Reihe geschieht sogar im Sinne von Definition 5.25. Das ist eine sehr schoĚne
Konvergenz.
Eigentlich koĚnnten wir uns jetzt freuen, es gibt nur einen Haken. Durch die Klammersetzung in (2.1) wird
erreicht, daĂ die BasisââElemente vk,m in einer ganz bestimmten Reihenfolge aufsummiert werden. Es
â
stellt sich die Frage, ob man denn nicht die Anordnung dieser Summanden aĚndern koĚnnte (im Sinne eines
Assoziativgesetzes bzw. Kommutativgesetzes der Addition, bloĂ jetzt mit unendlich vielen Summanden).
Eigentlich sollten solche Gesetze ja gelten. Die schlechte Nachricht ist aber: wenn man die BasisââElemente
â
clever anders anordnet, dann kann man erzielen, daĂ die Reihensumme nicht mehr f ist, sondern etwas
anderes. Ein Summenwert kann von der Reihenfolge der Summanden abhaĚngen ! Das ist seltsam, aber
wahr. Man sagt, daĂ die vk,m eine bedingte Basis des Vektorraums V bilden.
Nun gut, das legt die Vermutung nahe, daĂ wir uns bei der Wahl einer Basisâ fuĚr V einfach ungeschickt
â
angestellt haben; und wenn man besser waĚhlt, dann kommt bei beliebiger Anordnung der Summanden der
Linearkombinationâ immer derselbe Reihenwert heraus.
â
Aber dem ist eben nicht so. Egal, wie man in diesem Raum V eine Basisâ waĚhlt: es wird stets so sein, daĂ
â
verschiedene Anordnungen der Summanden der Linearkombinationâ auf unterschiedliche Summenwerte
â
fuĚhren werden, obwohl jede Reihe konvergiert. Dieser Vektorraum V ist ein schrecklicher Raum, denn er
besitzt keine unbedingte Basis. Leider.
{ Ende der fakultativen Betrachtungen }

2.2.3

Dimension eines Vektorraumes

Frage: Man gebe jeweils mehrere Basen an fuĚr

54

KAPITEL 2. VEKTORRAĚUME
â˘ R3 ,

â˘ C3 als Vektorraum uĚber C,

â˘ C3 als Vektorraum uĚber R,
â˘ R[X].

Wir haben also fuĚr einen Vektorraum verschiedene Basen gefunden, die gleichviel Vektoren enthalten. Es
stellt sich die Frage, ob dies immer so ist. Denn nur wenn jede Basis eines Vektorraums gleichviel Elemente
enthaĚlt, hat es uĚberhaupt Sinn, von einer Dimension (als Anzahl der Basisvektoren) zu reden.
Der Beweis dieser Aussage verteilt sich uĚber mehrere SaĚtze und Lemmata. Wir beginnen:
Lemma 2.15 (Austauschlemma). Sei (v1 , . . . , vn ) eine Basis fuĚr V , und sei u ein Vektor mit der
Darstellung u = Îą1 v1 + Âˇ Âˇ Âˇ + Îąn vn . Wenn fuĚr ein k der Koeffizient Îąk 6= 0 ist, dann koĚnnen wir den
Basisvektor vk gegen u austauschen und erhalten wieder eine Basis (v1 , . . . , vkâ1 , u, vk+1 , . . . , vn ).
Es gibt immer ein solches k, 1 â¤ k â¤ n, mit Îąk 6= 0; es sei denn u ist der Nullvektor.
Beweis. Wir nehmen an, daĂ die Basisvektoren so numeriert sind, daĂ k = 1. Zu zeigen sind zwei Dinge:
(u, v2 , . . . , vn ) erzeugt V : Wir haben u = Îą1 v1 + Âˇ Âˇ Âˇ + Îąn vn mit Îą1 6= 0, also




1
Îąn
Îą2
v1 =
v2 + Âˇ Âˇ Âˇ + â
vn â span(u, v2 , . . . , vn ).
u+ â
Îą1
Îą1
Îą1
Nun benutzen wir Satz 2.7 mehrfach:

V â span(u, v2 , . . . , vn ) = span(u, v1 , v2 , . . . , vn ) â span(v1 , v2 , . . . , vn ) = V.
Nach dem Sandwichprinzip ist dann V = span(u, v2 , . . . , vn ).
(u, v2 , . . . , vn ) ist linear unabhaĚngig: Wegen Îą1 6= 0 ist u 6â span(v2 , . . . , vn ) (BegruĚndung bitte selbst
suchen !). Nun brauchen wir bloĂ Lemma 2.11 anwenden, und der Beweis ist komplett.

Als naĚchsten Schritt auf dem Weg zur Definition der Dimension wollen wir nicht nur einen Vektor austauschen, sondern mehrere:
Satz 2.16 (Austauschsatz von Steinitz (Ernst Steinitz, 1871â1928)). Sei (v1 , . . . , vn ) eine Basis,
und sei (u1 , . . . , uk ) eine linear unabhaĚngige Familie von V . Dann gilt:
â˘ k â¤ n,
â˘ bei geeigneter Numerierung der vj ist (u1 , . . . , uk , vk+1 , . . . , vn ) eine Basis von V .
Wir entfernen also aus der Familie der vj k Elemente und fuĚgen stattdessen die uj ein. FuĚr k = 1 erhalten
wir gerade das Austauschlemma.
Beweis. Wir fuĚhren den Beweis mittels vollstaĚndiger Induktion9 uĚber k.
9 Die vollstaĚndige Induktion ist ein Beweisverfahren der Mathematik.
Das Ziel ist stets, eine unendliche Kette A(1), A(2), A(3), . . . von Aussagen zu beweisen. Der Weg zum Beweis fuĚhrt uĚber
die Methode der vollstaĚndigen Induktion, die aus zwei Schritten besteht.
In einem ersten Schritt (Induktionsanfang) beweist man die Aussage A(1).
Im zweiten Schritt (Induktionsschritt) zeigt man (fuĚr jedes k): Wenn A(k) wahr ist (Induktionsvoraussetzung), dann ist
auch A(k + 1) wahr (Induktionsbehauptung). Beim Beweis der I.B. darf dabei die I.V. benutzt werden.
Zum Beispiel soll gezeigt werden: es seien n Geraden (mit n âĽ 1) in der Ebene gegeben, wobei keine zwei Geraden identisch
sein sollen (davon abgesehen ist uĚber die Lage der Geraden nichts bekannt). Die Geraden zerlegen die Ebene in viele geradlinig
berandete StuĚcke. Zu zeigen ist, daĂ diese StuĚcke so mit den Farben schwarz und weiĂ eingefaĚrbt werden koĚnnen, daĂ entlang
einer Kante benachbarte StuĚcke immer unterschiedliche Farben besitzen. Diese Behauptung (formuliert fuĚr n Geraden) nennen
wir A(n).
Der Beweis von A(1) ist einfach: es gibt nur eine Gerade, und diese zerlegt die Ebene in zwei HaĚlften, von denen wir die
eine weiĂ faĚrben und die andere schwarz.
Sei nun n âĽ 1, und seien n+1 Geraden gegeben. Zu zeigen ist, daĂ eine FaĚrbung moĚglich ist. Wir setzen also die Aussage A(n)
voraus und wollen die Aussage A(n + 1) zeigen. Dazu waĚhlen wir eine Gerade aus und entfernen diese (merken uns aber, wo sie
lag). UĚbrig bleiben n Geraden, fuĚr die eine zulaĚssige FaĚrbung moĚglich ist (denn das ist gerade die Induktionsvoraussetzung).
Diese FaĚrbung nehmen wir. Jetzt fuĚgen wir die (n + 1)âte Gerade wieder hinzu. Diese zerlegt die Ebene in zwei HaĚlften, und
in einer von diesen HaĚlften tauschen wir uĚberall schwarz und weiĂ gegeneinander aus. Man sieht, daĂ auf diese Weise eine
zulaĚssige FaĚrbung fuĚr den Fall von n + 1 Geraden entsteht.
Damit ist die Aussage A(n) fuĚr jedes n â N bewiesen.

2.2. LINEARKOMBINATIONEN, ERZEUGENDENSYSTEME USW.

55

FuĚr den Induktionsanfang sei k = 0. Dann gibt es die Familie der uj gar nicht, und es ist auch nichts
auszutauschen.
Sei nun k > 0. Wir setzen voraus, daĂ der Austauschsatz von Steinitz fuĚr k â 1 schon bewiesen ist, und
jetzt wollen wir ihn fuĚr k zeigen. Wir wissen damit, daĂ
â˘ k â 1 â¤ n,
â˘ und daĂ bei geeigneter Numerierung der vj die Familie (u1 , . . . , ukâ1 , vk , . . . , vn ) eine Basis von V ist.
Wir zeigen als erstes, daĂ k â 1 = n nicht sein kann. Denn dann gaĚbe es in der Familie
(u1 , . . . , ukâ1 , vk , . . . , vn ) die vâVektoren gar nicht, und die Vektoren (u1 , . . . , ukâ1 ) wuĚrden eine Basis
von V bilden. Dann koĚnnte man uk als Linearkombination der Vektoren u1 , . . . , ukâ1 schreiben. Wir hatten
aber vorausgesetzt, daĂ die Familie (u1 , . . . , uk ) linear unabhaĚngig ist. Das ist ein Widerspruch, also ist
k â 1 6= n. Zusammen mit k â 1 â¤ n ergibt das k â¤ n.
Um den zweiten â˘ zu beweisen, stellen wir uk in der neuen Basis dar:
uk = Îą1 u1 + Âˇ Âˇ Âˇ + Îąkâ1 ukâ1 + Î˛k vk + Âˇ Âˇ Âˇ + Î˛n vn .
Wenn jedes Î˛j = 0 waĚre, dann haĚtten wir uk als Linearkombination von u1 , . . . , ukâ1 dargestellt, was
nicht moĚglich ist, wie wir eben sahen. Also muĂ mindestens ein Î˛j 6= 0 sein. Wir numerieren die vi
so um, daĂ Î˛k 6= 0 ist (wir tauschen also j und k). Dann gibt uns das Austauschlemma die MoĚglichkeit, in der Basis (u1 , . . . , ukâ1 , vk , . . . , vn ) den Vektor vk gegen uk auszutauschen, was beweist, daĂ
(u1 , . . . , ukâ1 , uk , vk+1 , . . . , vn ) eine Basis ist.
Nach dem Prinzip der vollstaĚndigen Induktion ist somit der Beweis vollendet.
Daraus folgt fast sofort, daĂ die Basen eines Vektorraumes gleichlang sind:
Satz 2.17.

1. Jeder endlich erzeugte Vektorraum hat eine Basis endlicher LaĚnge.

2. Wenn ein Vektorraum eine endliche Basis besitzt, dann hat jede Basis dieses Vektorraumes dieselbe
LaĚnge.
3. Wenn ein Vektorraum zu jedem n â N eine linear unabhaĚngige Familie mit n Elementen besitzt, dann
hat dieser Vektorraum keine endliche Basis.
Beweis.

1. Folgt aus dem Basissatz.

2. Seien (u1 , . . . , uk ) und (v1 , . . . , vn ) zwei Basen eines Vektorraumes V . Aus dem Austauschsatz von
Steinitz folgt dann k â¤ n. Wenn wir die Rollen der uj und vi vertauschen und den Satz von Steinitz
ein weiteres Mal anwenden, erhalten wir n â¤ k. Das ergibt k = n.
3. Folgt ebenfalls aus dem Satz von Steinitz.

Definition 2.18 (Dimension). FuĚr einen Vektorraum V definieren wir seine Dimension10 als
ďŁą
ďŁ´
: V = {~0},
ďŁ˛0
dim V = n
: V hat eine Basis (u1 , . . . , un ),
ďŁ´
ďŁł
â : V enthaĚlt beliebig groĂe linear unabhaĚngige Familien.

Wegen Bemerkung 2.9 kann der erste Fall der Klammer auf der rechten Seite auch dem zweiten Fall
zugeschlagen werden, worauf wir aus didaktischen GruĚnden aber verzichtet haben.
Um die Begriffe einzuuĚben, empfiehlt es sich folgenden Satz zu beweisen, und denkbare Hilfsmittel waĚren
der Austauschsatz und der BasisergaĚnzungssatz:
Satz 2.19. Sei U Unterraum eines nâdimensionalen Vektorraumes V .
1. Dann ist U endlich erzeugt und hat eine Dimension â¤ n.
10 dimension

56

KAPITEL 2. VEKTORRAĚUME
2. Jede Basis (u1 , . . . , um ) von U kann zu einer Basis (u1 , . . . , um , um+1 , . . . , un ) von V ergaĚnzt werden.

Beweis. UĚbungsaufgabe.
Wir brauchen noch einige Operationen mit UnterraĚumen.
Seien U und V UnterraĚume eines Vektorraumes W , so definieren wir
â˘ U âŠ V als Schnittmenge von U und V ,
â˘ U + V := {w = u + v : u â U, v â V }.
Dies sind wieder UnterraĚume von W , genannt Durchschnittâ und Summeâ.
â
â
Frage: Wie kann man U + V beschreiben mithilfe von Basen fuĚr U und fuĚr V ?
Frage: Seien A und B Mengen mit jeweils endlich vielen Elementen (a und b StuĚck). Wieviele Elemente
hat A âŞ B ?
Satz 2.20 (Dimensionsformel). Seien U und V UntervektorraĚume eines endlichdimensionalen Raumes
W , dann gilt
dim U + dim V = dim(U âŠ V ) + dim(U + V ).
Beweis. Sei dim U = k, dim V = m, dim(U âŠV ) = r. Dann muĚĂten wir zeigen, daĂ dim(U + V ) = k + mâ r.

Wir starten von einer Basis (d1 , . . . , dr ) von U âŠ V . GemaĚĂ Satz 2.19 ist r â¤ k sowie r â¤ m, und wir koĚnnen
diese Basis ergaĚnzen zu einer Basis von U ,
(d1 , . . . , dr , ur+1 , . . . , uk ),
und zu einer Basis von V ,
(d1 , . . . , dr , vr+1 , . . . , vm ).
Denn U âŠ V ist Unterraum von U , und Unterraum von V .

Wenn wir diese beiden Basen zusammenfuĚgen, erhalten wir die Familie von k + m â r Vektoren
B = (d1 , . . . , dr , ur+1 , . . . , uk , vr+1 , . . . , vm ).

Es ist klar, daĂ die Familie B die Summe U + V erzeugt.
Zu beweisen bleibt noch, daĂ die Familie B linear unabhaĚngig ist. Wenn wir das gezeigt haĚtten, dann waĚre
B eine Basis von U + V , und die Dimensionsformel waĚre bewiesen.
Sei nun also
r
X

Îąj dj +

k
X

Î˛j u j +

Îłj vj = ~0,

(2.2)

j=r+1

j=r+1

j=1

m
X

und zu zeigen ist Îąj = 0, Î˛j = 0, Îłj = 0 fuĚr jedes j.
Zu diesem Zwecke setzen wir
u :=

k
X

(âÎ˛j uj ) =

j=r+1

r
X
j=1

Îąj dj +

m
X

Îłj vj .

j=r+1

Die linke Summe enthaĚlt nur Summanden aus U , also ist u â U . Andererseits enthalten die rechten Summen
nur Summanden aus V , also ist u â V , und somit muĂ u â U âŠV sein. Weil U âŠV als Basis gerade (d1 , . . . , dr )
hat, haben wir eine Darstellung
u=

r
X

Î´j dj .

j=1

Dieser Vektor gehoĚrt zuPU âŠ V , insbesondere also
P den Vektor u im Raum V haben wir also zwei
P zu V . FuĚr
Darstellungen: einmal j Î´j dj , andererseits j Îąj dj + j Îłj vj . GemaĚĂ Satz 2.10 darf es aber nur eine

2.3. VEKTORRAĚUME MIT SKALARPRODUKT

57

solche Darstellung geben. Also muĂ zwangslaĚufig Îąj = Î´j sein und Îłj = 0 fuĚr jedes j. Mit Îłj = 0 gehen wir
in (2.2) und erhalten
r
X

Îąj dj +

k
X

Î˛j uj = ~0.

j=r+1

j=1

Nun ist aber die Familie (d1 , . . . , dr , ur+1 , . . . , uk ) eine Basis fuĚr U , weshalb die Îąj = 0 und Î˛j = 0 sein
muĚssen.
Damit ist bewiesen, daĂ B eine linear unabhaĚngige Familie ist.
Von besonderer Bedeutung ist der Fall, daĂ U âŠ V = {0} ist. In diesem Fall kann man jeden Vektor
w â U + V auf genau eine einzige Weise als Summe w = u + v mit u â U und v â V schreiben, was die
folgende Definition motiviert:
Definition 2.21. Seien U1 , . . . , Uk UntervektorraĚume eines Vektorraumes W mit der Eigenschaft, daĂ
jedes Element w â W auf genau eine Weise
w = u1 + Âˇ Âˇ Âˇ + uk ,

u i â Ui ,

âi,

dargestellt werden kann. Dann heiĂt W die direkte Summe11 der Ui , und man schreibt
W = U1 â Âˇ Âˇ Âˇ â Uk =

2.3

k
M

Uj .

j=1

VektorraĚume mit Skalarprodukt

Literatur: Greiner: Quantenmechanik. EinfuĚhrung. Kapitel XVII: Das formale Schema der Quantenmechanik.
In den bisherigen Untersuchungen haben wir uns allgemeine VektorraĚume angeschaut und dabei lediglich
ihre beiden Operationen + : V Ă V â V und Âˇ : K Ă V â V verwendet. Jetzt wollen wir VektorraĚume
studieren, die zusaĚtzlich noch uĚber ein Skalarprodukt verfuĚgen.

2.3.1

Skalarprodukte, Normen, Orthogonalsysteme

Definition 2.22. Sei V ein Vektorraum uĚber K, wobei K = R oder K = C. Eine Abbildung hÂˇ, Âˇi : V ĂV â
K heiĂt Skalarprodukt, wenn die folgenden Eigenschaften gelten fuĚr alle u, v, w â V und alle Îą â K:
â˘ hu, vi = hv, ui (hermitesch12 ),
â˘ hu, v + wi = hu, vi + hu, wi,
â˘ hÎąu, vi = Îą hu, vi,
â˘ hu, ui âĽ 0, und hu, ui = 0 genau dann wenn u = 0.
FuĚr K = R erhalten wir genau die Eigenschaften aus Satz 1.27.
Aus der ersten Bedingung folgt hu, ui â R fuĚr jedes u â U , auch wenn K = C sein sollte. Deshalb ist die
vierte Bedingung sinnvoll.
Man beachte, daĂ ein Skalar im zweiten Faktor nur in konjugierter Form herausgezogen werden kann:
hu, Îąvi = Îą hu, vi .

Standardbeispiele sind fuĚr die FaĚlle von V = Rn bzw. V = Cn
hx, yi =

n
X
j=1

Îžj Îˇj bzw. hx, yi =

n
X

Îžj Îˇj ,

j=1

es gibt aber noch viele weitere VektorraĚume und zugehoĚrige Skalarprodukte.
Frage: Welchen Nutzen bringt uns der Konjugationsstrich uĚber dem zweiten Faktor Îˇj ?
11 direct

sum
Hermite, 1822â1901

12 Charles

58

KAPITEL 2. VEKTORRAĚUME

Definition 2.23. Ein Vektorraum uĚber R bzw. C mit Skalarprodukt heiĂt euklidischer13 Vektorraum14
bzw. unitaĚrer Vektorraum15 .
Weil hu, ui reell und nichtnegativ ist, koĚnnen wir die Wurzel ziehen:
Definition 2.24. Sei V ein Vektorraum mit Skalarprodukt hÂˇ, Âˇi. Die Abbildung
| Âˇ | : V â R,
| Âˇ | : u 7â |u| :=

p
hu, ui

heiĂt Norm von u. Manchmal schreibt man auch kuk oder kukV anstelle von |u|.
Eine weitere Beziehung zwischen Norm und Skalarprodukt wird gegeben durch die Ungleichung von
CauchyâSchwarz:
Satz 2.25. Sei V ein unitaĚrer oder euklidischer Vektorraum. Dann gilt fuĚr alle u, v â V die Ungleichung
| hu, vi | â¤ |u| Âˇ |v|.
Der Beweis verlaĚuft im Prinzip genauso wie im ersten Kapitel, lediglich im Falle eines unitaĚren Vektorraumes
sind einige kleinere AĚnderungen erforderlich.
FuĚr euklidische und unitaĚre VektorraĚume gilt die Parallelogrammgleichung:
|u + v|2 + |u â v|2 = 2(|u|2 + |v|2 ).
Satz 2.26. Die Normfunktion hat folgende Eigenschaften, fuĚr alle u, v â V und alle Îą â K:
1. Es ist |u| âĽ 0, und |u| = 0 genau dann wenn u = 0.
2. Es ist |Îąu| = |Îą| Âˇ |u|.
3. Die Dreiecksungleichung16 gilt: |u + v| â¤ |u| + |v|.
Beweis.

1. Folgt aus der vierten Eigenschaft des Skalarprodukts.
p
p
p
p
2. Folgt aus |Îąu| = hÎąu, Îąui = Îą hu, Îąui = ÎąÎą hu, ui = |Îą|2 hu, ui = |Îą| Âˇ |u|.

3. Ergibt sich aus der Ungleichung von CauchyâSchwarz auf demselben Wege wie im ersten Kapitel.

Definition 2.27. Sei V ein reeller oder komplexer Vektorraum (nicht notwendig euklidisch oder unitaĚr),
und sei | Âˇ | : V â R eine Abbildung, die den Bedingungen aus Satz 2.26 genuĚgt. Dann heiĂt | Âˇ | eine Norm
auf V , und das Paar (V, | Âˇ |) heiĂt normierter Raum17 .
Beispiele fuĚr Normen auf dem Rn sind
|x| :=

n
X
j=1

|Îžj |,

|x| := max |Îžj |,
j=1,...,n

wenn x = (Îž1 , . . . , Îžn ) â Rn .

Frage: Wie sehen die Einheitskreiseâ zu diesen Normen aus ?
â
Definition 2.28. Sei V ein unitaĚrer Raum.
1. Zwei Vektoren u, v â V heiĂen orthogonal zueinander bzw. aufeinander senkrecht18 , u âĽ v, wenn
hu, vi = 0.
13 Euklid

von Alexandria, 3. Jh. v. Chr.
(vector) space
15 unitary space
16 triangle inequality
17 normed space
18 perpendicular to each other
14 euclidean

59

2.3. VEKTORRAĚUME MIT SKALARPRODUKT

2. Eine Familie (uj : j â J) (mit endlich oder unendlich vielen Elementen) heiĂt Orthonormalsystem
(ONS), wenn
hui , uj i = Î´ij =

(

1
0

: i = j,
:i=
6 j.

3. Eine Basis, die gleichzeitig ein ONS ist, heiĂt Orthonormalbasis (ONB).
4. Sei U â V ein Unterraum. Dann heiĂt
U âĽ := {v â V : hv, ui = 0 âu â U }
orthogonales Komplement von U .
5. FuĚr zwei UntervektorraĚume U1 und U2 von V bedeutet die Schreibweise U1 âĽ U2 , daĂ jeder Vektor
aus U1 auf jedem Vektor aus U2 senkrecht steht.
Wegen U âŠ U âĽ = {0} haben wir in den meisten FaĚllen V = U â U âĽ . Ein Beweis dazu steht unten, falls der
Unterraum U endlichdimensional sein sollte. Damit erklaĚrt sich dann auch die Bezeichnung Komplement,
denn dies bedeutet hier ErgaĚnzungsstuĚckâ 19.
â
Der groĂe Vorteil von Orthonormalbasen ist, daĂ sie die Rechnungen vereinfachen (im Vergleich zu einer
herkoĚmmlichen Basis):
Satz 2.29. Sei V ein euklidischer oder unitaĚrer Raum, und sei U = span(u1 , . . . , un ), wobei die uj eine
ONB bilden. Dann hat jeder Vektor u â U die Darstellung
u=

n
X
j=1

hu, uj i uj ,

und fuĚr die Norm haben wir die Formel
2

|u| =

n
X
j=1

| hu, uj i |2 .

Pn
Beweis. Wegen u â span(u1 , . . . , un ) haben wir eine Darstellung u = j=1 Îąj uj mit eindeutig bestimmten
Îąj â K. Wenn wir das Skalarprodukt von u mit einem Basisvektor bilden, erhalten wir wegen der LinearitaĚt
des Skalarprodukts und der OrthonormalitaĚt
hu, ui i =

*

n
X

Îąj uj , ui

j=1

+

=

n
X
j=1

Îąj huj , ui i =

n
X

Îąj Î´ji = Îąi .

j=1

Das beweist die erste Formel. Analog erhalten wir
+
* n
n
n X
n
n
n X
n
X
X
X
X
X
2
Îąj Îąk huj , uk i =
Îąk uk =
Îąj uj ,
|u| = hu, ui =
|Îąj |2 .
Îąj Îąk Î´jk =
j=1

k=1

j=1 k=1

j=1 k=1

j=1

Die gemischten Produkte fallen raus wegen der OrthogonalitaĚt.
Frage: Im R1756 sei ein Untervektorraum U gegeben. Welche Dimension hat U âĽ ?
Frage: Im R1517 sind zwei Vektoren u1 und u2 gegeben, und ein dritter Vektor ist gesucht, der senkrecht
auf u1 und auf u2 stehen soll. Kann man diesen dritten Vektor mittels Kreuzprodukt ermitteln ?
Der naĚchste Satz sagt uns, wie wir von einer gewoĚhnlichen Basis uĚbergehen koĚnnen zu einer Orthonormalbasis:
19 aus

dem Lateinischen: complementum= ErgaĚnzung, VervollstaĚndigung

60

KAPITEL 2. VEKTORRAĚUME

Satz 2.30 (Orthonormalisierungsverfahren von GramâSchmidt20 ). Sei V ein euklidischer oder
unitaĚrer Raum, und sei (v1 , v2 , . . . ) eine endliche oder unendliche linear unabhaĚngige Familie. Wir definieren dann rekursiv
1
w1 ,
|w1 |
n
X
hvn+1 , uj i uj ,
:= vn+1 â

w1 := v1 ,
wn+1

u1 :=

un+1 :=

j=1

1
wn+1 ,
|wn+1 |

ân âĽ 1.

Dann gilt:
1. die Vektoren wn+1 sind nie ~0, also sind die un+1 definiert,
2. die Familie (u1 , u2 , . . . ) ist linear unabhaĚngig und ein Orthonormalsystem,
3. span(v1 , . . . , vn ) = span(u1 , . . . , un ) fuĚr jedes n.
Beweis. UĚbungsaufgabe.
Frage: Warum ist kein wn+1 = 0 ?
Frage: Warum ist hun+1 , uk i = 0 fuĚr 1 â¤ k â¤ n ?

2.3.2

Approximationsprobleme

Literatur: Greiner: Klassische Elektrodynamik. Kapitel I.3.1: Entwicklung beliebiger Funktionen in
vollstaĚndige Funktionssysteme
Nun wollen wir Lote faĚllen auf einen Unterraum. Genauer gesagt, geht es um folgendes Approximationsproblem:
Sei V ein euklidischer oder unitaĚrer Raum, und sei U ein Unterraum von V . Gegeben sei ein v â V . Gesucht
ist dasjenige u â U , das von v den geringsten Abstand hat. Wenn es ein solches u â U gibt, dann heiĂt u
Projektion von v auf U oder auch Proximum.
Wenn U unendlichdimensional sein sollte (was haĚufiger vorkommt als man glaubt), dann ist es keineswegs
selbstverstaĚndlich, daĂ so ein u uĚberhaupt existiert, es gibt sogar Gegenbeispiele. Deshalb beschraĚnken wir
uns auf den Fall eines endlichdimensionalen U .
Satz 2.31. Sei V ein euklidischer oder unitaĚrer Raum, und sei U = span(u1 , . . . , un ) ein Unterraum von
V , mit Orthonormalbasis (u1 , . . . , un ). Sei v â V ein beliebiger Punkt auĂerhalb von U .
1. Dann existiert genau ein u â U , sodaĂ v â u senkrecht auf U steht.
Pn
2. Dieses u wird gegeben durch u = j=1 hv, uj i uj .
3. FuĚr jedes andere uâ˛ â U gilt |v â uâ˛ | > |v â u|.

4. FuĚr den Projektionsfehler |v â u| gilt |v â u|2 = |v|2 â |u|2 .
Pn
Beweis.
1. FuĚr u machen wir den Ansatz u = j=1 Îąj uj . Der Vektor v â u steht senkrecht auf U genau
dann, wenn er senkrecht auf jedem Basisvektor uj steht. Nun ist21
* n
+
X
!
0 = hv â u, uj i = hv, uj i â
Îąk uk , uj = hv, uj i â Îąj ,
k=1

also gibt es genau ein solches u â U .
2. Ist soeben bewiesen worden.
20

JĂ¸rgan Pedersen Gram (1850â1916) und Erhard Schmidt (1876â1959)
Ausrufezeichen uĚber dem = soll bedeuten, daĂ wir uns wuĚnschen, daĂ die genannte Gleichung gelten moĚge.

21 Das

2.3. VEKTORRAĚUME MIT SKALARPRODUKT

61

3. Wenn uâ˛ â U , dann ist wegen u â U auch u â uâ˛ â U . Nun steht v â u senkrecht auf U , also ist
insbesondere hv â u, u â uâ˛ i = 0. Wegen uâ˛ 6= u und des Satzes von Pythagoras haben wir dann
|v â uâ˛ |2 = |(v â u) + (u â uâ˛ )|2 = |v â u|2 + |u â uâ˛ |2 > |v â u|2 .
4. Wir koĚnnen schreiben v = (v â u) + u. Weil v â u und u senkrecht aufeinander stehen (wegen 1.), gilt
aufgrund des Satzes von Pythagoras
|v|2 = |v â u|2 + |u|2 ,
woraus die Behauptung sofort folgt.

Jetzt haben wir alle Werkzeuge beisammen, um den Beweis von V = U â U âĽ nachzuholen:
Satz 2.32. Sei V ein unitaĚrer oder euklidischer Raum, und sei U ein endlichdimensionaler Unterraum
von V . Dann ist V = U â U âĽ .
Beweis. Sei v â V beliebig. Dann gibt es wegen Satz 2.31 genau ein u â U mit v â u âĽ U . Wir koĚnnen
also jeden Vektor v â V in zwei Summanden v â u â U âĽ und u â U zerlegen, und es gibt nur diese eine
Zerlegung. Also ist V = U â U âĽ .
Beispiel 2.33. Sei V = C([â1, 1] â R) der Raum der auf dem Intervall [â1, 1] definierten stetigen und
reellwertigen Funktionen, ausgestattet mit dem Skalarprodukt
Z 1
hf, gi =
f (x)g(x)dx.
â1

qR
1
Dann ist |f | =
|f (x)|2 dx. Wir wollen eine cosâFunktion durch eine quadratische Parabel annaĚhern.
â1
Sei also U = span(1, x2 ) und v = v(x) = cos( Ď2 x). Gesucht ist ein u â U mit |v â u| â min.
LoĚsung: Wir ermitteln mithilfe des GramâSchmidtâVerfahrens eine Orthonormalbasis (u1 , u2 ) fuĚr U :
v2 = v2 (x) = x2 ,
1
1
u1 = u1 (x) = qR
Âˇ1= â ,
1
2
12 dx
â1


1
1
1
w2 = w2 (x) = x2 â x2 , â
Âˇ â = x2 â ,
3
2
2
s
r
2
Z 1
1
8
x2 â
dx =
|w2 | =
,
3
45
â1
r 

1
45
2
x â
.
u2 = u2 (x) =
8
3
v1 = v1 (x) = 1,

Dann ermittelt sich das Proximum u = u(x) wegen Satz 2.31, Teil 2, nach der Formel
u = u(x) = hv, u1 i u1 (x) + hv, u2 i u2 (x),
und fuĚr die Norm haben wir (wegen Satz 2.29) die einfache Darstellung
p
|u| = | hv, u1 i |2 + | hv, u2 i |2 .
Die Koordinaten von u in Bezug auf die Basis (u1 , u2 ) sind dann
â
Z 1
Ď  1
2 2
x Âˇ â dx =
hv, u1 i =
cos
2
Ď
2
â1
und
hv, u2 i =

r

45
8

Z

1

â1

cos


Ď  
1
dx.
x Âˇ x2 â
2
3

62

KAPITEL 2. VEKTORRAĚUME

Im Bronstein finden wir die Formel

 2
Z
2
x
2x
â 3 sin(Îąx) + const.,
x2 cos(Îąx)dx = 2 cos(Îąx) +
Îą
Îą
Îą
mit deren Hilfe wir ausrechnen koĚnnen, daĂ
r  3  
â

Ď 2
10
45 2
hv, u2 i = â
â2 .
+
Ď
2 Ď
2
Damit sind die Koordinaten von u bezuĚglich der Orthonormalbasis (u1 , u2 ) von U bestimmt, und das
gesuchte quadratische Polynom ist gefunden.
Zur Berechnung des Projektionsfehlers |v â u| benutzen wir die Formel |v â u|2 = |v|2 â |u|2 , siehe auch
Satz 2.31, Teil 4. Wir haben
|v|2 =

Z

1

â1



cos

 Ď 2
x
dx.
2

Aus dem Bronstein entnehmen wir die Formel
Z
1
x
sin(2Îąx) + const.,
(cos(Îąx))2 dx = +
2 4Îą
die uns auf |v|2 = 1 fuĚhrt. SchlieĂlich ist nach einiger Rechnerei
|u|2 = | hv, u1 i |2 + | hv, u2 i |2 = 0.810569468 + 0.18883446738 = 0.9994039,
woraus |v âu|2 = 0.000596 und |v âu| = 0.0244 folgen. Wir haben also einen relativen Approximationsfehler
|vâu|
|v| von weniger als 2.5%.
Pn
Bemerkung 2.34. Der Projektion v 7â u gemaĚĂ u = j=1 hv, uj i uj werden wir im 2. Semester bei der
Fourierreihendarstellung von periodischen Funktionen wieder begegnen.

2.4

Ausblick: VektorraĚume in der Physik 1

Wir bringen einige weitere Beispiele.
RaĚume fuĚr Ortsvariablen: das ist praktisch immer der R3 , die Elemente davon beziehen sich auf Orte (oder sind Verbindungsvektoren zweier Orte). Die MaĂeinheit ist zu interpretieren als Meterâ.
â
(Das GegenstuĚck, die Wellenzahlvektoren, behandeln wir absichtlich hier nicht, sondern erst im
uĚbernaĚchsten Kapitel.) Dieser Raum hat die Dimension 3.
RaĚume fuĚr Vektorfelder: an jeden Punkt x des R3 heften wir einen Vektor ~u(x) an, den wir als FluĂvektor interpretieren. Man denke z.B. an einen stroĚmendes Fluid oder an das elektrische Feld (was
im Prinzip dasselbe ist). Die UĚberlagerung verschiedener StroĚmungen ergibt eine Addition in einem
Vektorraum. Wenn wir annehmen, daĂ fuĚr |x| â â die StroĚmung abklingt, dann ist es sinnvoll zu
verlangen, daĂ
Z
|~u(x)|2 dx < â,
xâR3

wobei |~u(x)|2 = (u1 (x))2 + (u2 (x))2 + (u3 (x))2 . Integrale dieses Typs koĚnnen haĚufig als Energie interpretiert werden, womit sie fuĚr die Physik automatisch interessant sind. Den Vektorraum aller solcher
Vektorfelder schreiben wir als L2 (R3 â R3 ), er hat die Dimension unendlich und ein Skalarprodukt
gemaĚĂ
Z
u1 (x)v1 (x) + u2 (x)v2 (x) + u3 (x)v3 (x)dx.
(2.3)
h~u, ~v iL2 :=
xâR3

2.5. AUSBLICK: DIE HELMHOLTZâPROJEKTION

63

VektorraĚume in der Quantenmechanik: wir betrachten ein einzelnes spinloses Teilchen in einem Potential. Der Zustand dieses Teilchens wird quantenmechanisch beschrieben durch eine Wellenfunktion
Ď = Ď(x), die vom Ortsraum R3 nach C abbildet. Wir interpretieren das Betragsquadrat |Ď(x)|2 als
Wahrscheinlichkeitsdichte, das Teilchen an Position x anzutreffen. Sinnvollerweise ist
Z
|Ď(x)|2 dx < â.
xâR3

Alle solchen Wellenfunktionen bilden einen Vektorraum, naĚmlich den L2 (R3 â C). Dieser ist unendlichdimensional, und er hat folgendes Skalarprodukt:
Z
Ď(x)Ď(x)dx.
hĎ, ĎiL2 :=
(2.4)
xâR3

FuĚr M Teilchen jeweils aus dem R3 liegt die Ortsvariable x im R3M . Bei realistischen Werten von
M ist dies meist keine handhabbare Situation mehr, aber mittels Gruppentheorie (Vertauschungen
der Teilchen untereinander sind Elemente einer Permutationsgruppe !) kann man den Aufwand zur
Beschreibung des Systems um einiges reduzieren.
Literatur: Greiner und MuĚller: Quantenmechanik. Symmetrien. Kapitel IX: Darstellungen der Permutationsgruppe und YoungâTableaux.

2.5

Ausblick: die HelmholtzâProjektion

Skalare Felder sind Funktionen Ď von R3 nach R.
Vektorfelder sind Funktionen ~u von R3 nach R3 .
Heute wird der R3 jedesmal ausgestattet mit einem kartesischen Koordinatensystem (keine Polarkoordinaten oder aĚhnliches).
Der Gradient eines skalaren Feldes ist


âĎ âĎ âĎ
.
,
,
âĎ(x) =
âx1 âx2 âx3
Die Divergenz eines Vektorfeldes ~u ist
div ~u(x) =

âu1 âu2
âu3
+
+
.
âx1
âx2
âx3

Ein beruĚhmter Satz von Helmholtz 22 besagt: jedes Vektorfeld ~u â L2 (R3 â R3 ) kann auf eindeutige Weise
zerlegt werden als ~u = ~v + w,
~ wobei ~v , w
~ â L2 (R3 â R3 ) mit
div ~v (x) = 0,

âx â R3 ,

w(x)
~
= âĎ(x),

âx â R3 ,

mit einem geeigneten Skalarfeld Ď. Man sagt auch: ~v ist divergenzfrei bzw. quellenfrei, w
~ ist ein Gradientenfeld bzw. hat ein Potential. Wir haben also die HelmholtzâZerlegung
L2 (R3 â R3 ) = L2div (R3 â R3 ) â L2pot (R3 â R3 ).
Es kommt noch schoĚner: beide Summanden auf der rechten Seite bilden nicht nur eine direkte Summe,
sondern sie stehen auch senkrecht aufeinander im Sinne des Skalarprodukts (2.3), wie wir im 2. Semester
erkennen werden.
Es ist L2div (R3 â R3 ) ein (abgeschlossener) Untervektorraum des Vektorraums L2 (R3 â R3 ). Wenn wir
von einem ~u â L2 (R3 â R3 ) das Lot faĚllen auf L2div (R3 â R3 ), dann loĚsen wir genau eine Approximationsaufgabe im Sinne von Abschnitt 2.3.2. Diese Abbildung heiĂt HelmholtzâProjektion.
22 Hermann von Helmholtz (1821â1894), Mediziner und Physiker, Professor fuĚr Physiologie und Pathologie in KoĚnigsberg,
spaĚter Professor fuĚr Anatomie und Physiologie in Bonn. Formulierte den Energieerhaltungssatz, erfand den Augenspiegel, maĂ
die Fortpflanzungsgeschwindigkeit von Nervenerregungen, lieferte wichtige BeitraĚge zur Hydrodynamik und Elektrodynamik,
begruĚndete die wissenschaftliche Meteorologie.

64

KAPITEL 2. VEKTORRAĚUME

~ B
~ gehoĚren
Anwendungen gibt es dafuĚr in der Elektrodynamik (denn elektrische und magnetische Felder E,
2
3
3
gerade zu L (R â R )) und in der FestkoĚrperphysik. Man stelle sich einen FestkoĚrper vor, der elastische
Schwingungen ausfuĚhrt. Diese koĚnnen longitudinal oder transversal verlaufen, und die dazugehoĚrigen Verschiebungsvektorfelder sind gerade Potentialfelder bzw. divergenzfreie Felder. Die HelmholtzâProjektion
erlaubt es, diese beiden Schwingungstypen getrennt zu betrachten, und man erhaĚlt z.B., daĂ longitudinale
bzw. transversale Wellen unterschiedlich schnell laufen koĚnnen.

2.6

SchluĚsselbegriffe

â˘ Definition eines Vektorraumes,
â˘ FunktionenraĚume als VektorraĚume,
â˘ lineare UnabhaĚngigkeit, Basis, Dimension,
â˘ direkte Summe von VektorraĚumen, Dimensionsformel,
â˘ Definition von Skalarprodukt (im euklidischen Fall und im unitaĚren Fall) und Norm,
â˘ Verfahren von GramâSchmidt,
â˘ LoĚsungsverfahren zu Approximationsproblemen â auch im abstrakten Fall.

Kapitel 3

Matrizen
3.1

Operationen mit Matrizen

Wie immer, sei K auch jetzt ein KoĚrper, zum Beispiel K = R oder K = C. Unter K n , K m usw. verstehen
wir den Vektorraum der Spaltenvektoren mit n bzw. m EintraĚgen. In diesem Kapitel soll es um Matrizen
gehen, also Zahlenschemata der Form
ďŁś
ďŁŤ
a11
a12
...
a1m
ďŁŹ a21
a22
...
a2m ďŁˇ
ďŁˇ
ďŁŹ
aij â K.
A=ďŁŹ .
.. ďŁˇ ,
.
..
ďŁ­ ..
. ďŁ¸
an1
an2
...
anm

Die Menge aller solchen Matrizen bezeichnen wir mit K nĂm . FuĚr m = 1 erhalten wir die Spaltenvektoren

Definition 3.1. FuĚr Matrizen A, B â K nĂm und Zahlen Îą â K definieren wir eine Addition zweier
Matrizen sowie eine Multiplikation einer Zahl mit einer Matrix gemaĚĂ
(A + B)ij := aij + bij ,
(ÎąA)ij := Îąaij ,

1 â¤ i â¤ n,

1 â¤ i â¤ n,

1 â¤ j â¤ m,

1 â¤ j â¤ m.

Die Menge der n Ă mâMatrizen bildet mit diesen beiden Operationen einen Vektorraum uĚber K.
Frage: Was ist das Nullelement dieses Vektorraumes ? Welche Dimension hat er ?
Genauso wie im ersten Kapitel definieren wir eine Multiplikation von Matrizen und Vektoren:
Definition 3.2. FuĚr eine Matrix A â K nĂm und einen Vektor x = (Îž1 , . . . , Îžm )â¤ â K m definieren wir das
Produkt Ax â K n durch
ďŁŤ
ďŁśďŁŤ ďŁś
ďŁŤ Pm
ďŁś
a11
...
a1m
Îž1
j=1 a1j Îžj
ďŁŹ
ďŁˇ
.. ďŁˇ ďŁŹ .. ďŁˇ := ďŁŹ
..
Ax = ďŁ­ ...
ďŁ­
ďŁ¸.
. ďŁ¸ďŁ­ . ďŁ¸
Pm .
an1
...
anm
Îžm
a
Îž
j=1 nj j

Der Vektor x muĂ genau so viele Komponenten haben wie A Spalten hat. Das Ergebnis Ax hat so viele
Komponenten wie A Zeilen hat.
Satz 3.3. Sei A â K nĂm . Dann erzeugt A eine Abbildung fA ,
fA : K m â K n ,
fA : x â
7 fA (x) := Ax.
Diese Abbildung ist linear (bzw. ein Homomorphismus), das heiĂt:
fA (x + y) = fA (x) + fA (y),
fA (Îąx) = ÎąfA (x),

Îą â K,

x, y â K m ,

x â K m.
65

66

KAPITEL 3. MATRIZEN

Beweis. UĚbungsaufgabe.
SpaĚter werden wir sehen, daĂ jede lineare Abbildung von K m auf K n in Form einer Matrixmultiplikation
dargestellt werden kann.
Wir stellen die Matrix als nebeneinandergestellte Spaltenvektoren dar:
ďŁś
ďŁŤ
A = ďŁ­ a1

a2

ÂˇÂˇÂˇ

am ďŁ¸ .

Dann sieht man schnell, daĂ aj = Aej , das heiĂt,
die Spalten der Abbildungsmatrix sind die Koordinaten der Bilder der Einheitsvektoren.
Pm
Daraus ergibt sich fuĚr einen Vektor x = (Îž1 , . . . , Îžm )â¤ = j=1 Îžj ej aufgrund der LinearitaĚt:
ďŁŤ
ďŁś
m
m
m
X
X
X
fA (x) = Ax = A ďŁ­
Îžj ej ďŁ¸ =
Îžj Aej =
Îžj aj .
j=1

j=1

j=1

Das heiĂt, Ax ist nichts anderes als eine Linearkombination der Spalten von A. Die Komponenten von x
sind gerade die Koeffizienten dieser Linearkombination.
Wir verallgemeinern die MatrixâVektorâMultiplikation zu einer MatrixâMatrixâMultiplikation:
Definition 3.4. Sei A â K nĂm und B â K mĂl . Dann definieren wir das Produkt AB â K nĂl durch
(AB)ij :=

m
X

aik bkj ,

k=1

1 â¤ i â¤ n,

1 â¤ j â¤ l.

Man beachte, daĂ das Produkt nur dann definiert ist, wenn B soviele Zeilen hat wie A Spalten hat.
Wenn B nur eine Spalte hat (l = 1), dann erhalten wir die zuvor schon definierte MatrixâVektorâ
Multiplikation.
Durch Hinschauen erkennen wir:
die kâte Spalte von AB ist gleich A Âˇ (kâte Spalte von B)
Diese Multiplikation unterscheidet sich in mindestens zwei Punkten von der herkoĚmmlichen Multiplikation
reeller oder komplexer Zahlen: sie ist im Allgemeinen nicht kommutativ (also AB 6= BA); und Nullteiler
sind moĚglich. Das heiĂt, ein Produkt kann gleich der Nullmatrix sein, obwohl keiner seiner Faktoren gleich
der Nullmatrix ist. Ein Beispiel dafuĚr ist


 

1
â1
1
2
0
0
=
,
â1
1
1
2
0
0


 

1
2
1
â1
â1
1
=
.
1
2
â1
1
â1
1
Satz 3.5. Seien A â K nĂm , B â K mĂl , und seien fA : K m â K n sowie fB : K l â K m die dazugehoĚrigen
Abbildungen. Sei weiterhin fAB : K l â K n die der Produktmatrix AB â K nĂl zugeordnete Abbildung.
Dann ist
fAB = fA âŚ fB ,
das heiĂt fAB ist die NacheinanderausfuĚhrung der Abbildungen fB und fA .
Pl
Beweis. Es seien b1 , . . . , bl die Spalten von B; und sei x = (Îž1 , . . . , Îžl )â¤ = j=1 Îžj ej ein beliebiger Vektor.
Dann ist (wenn wir an die beiden obigen Merkregeln erinnern)
!
!
l
l
X
X
(fA âŚ fB )(x) = fA (fB (x)) = fA (Bx) = fA
Îžk bk = A
Îžk bk
k=1

=

l
X

k=1

Îžk Abk =

l
X

Îžk (AB)ek = (AB)

k=1

Das ist genau die gewuĚnschte Aussage.

k=1

l
X

k=1

Îžk ek = (AB)x = fAB (x).

67

3.2. GLEICHUNGSSYSTEME
Damit koĚnnen wir jetzt zeigen, daĂ die Matrixmultiplikation assoziativ ist:
Satz 3.6. Seien A â K nĂm , B â K mĂl und C â K lĂr . Dann gilt (AB)C = A(BC).
Beweis. Wir beachten die Regel (P Q)ij =
fuĚr die linke Seite gerade


(AB)C



ij

=

l
X

(AB)ik ckj =

k=1

P

l
m
X
X

k=1

pik qkj fuĚr kompatible Matrizen P und Q. Dann ergibt sich

k

ais bsk

s=1

!

ckj =

l X
m
X

ais bsk ckj ,

k=1 s=1

und die rechte Seite ist gleich


m
m

X
X
A(BC)
=
aiu (BC)uj =
aiu
ij

u=1

u=1

l
X

buv cvj

v=1

!

=

m X
l
X

aiu buv cvj ,

u=1 v=1

und wenn man hier u zu einem s umtauft und v zu einem k, dann erhaĚlt man genau den Ausdruck der
anderen Seite. Die Summationszeichen duĚrfen getauscht werden wegen der KommutativitaĚt der Addition
in K.
Definition 3.7. Sei A â K nĂm eine Matrix mit EintraĚgen aij . Dann definieren wir eine transponierte
Matrix1 Aâ¤ â K mĂn und eine adjungierte Matrix2 Aâ â K mĂn durch
(Aâ¤ )ij = aji ,

(Aâ )ij = aji .

Wenn K = R, dann sind die Matrizen Aâ¤ und Aâ identisch.
Satz 3.8. Wenn die Formate der Matrizen A und B zueinander passen, dann ist
(AB)â¤ = B â¤ Âˇ Aâ¤ ,

(AB)â = B â Âˇ Aâ .

Die Standardskalarprodukte koĚnnen geschrieben werden als
hx, yiR = xâ¤ y,

hx, yiC = xâ y = y â x.

Beweis. UĚbungsaufgabe.

3.2

Gleichungssysteme

Wir versuchen jetzt, die MatrixâMultiplikation umzukehren, also eine Divisionâ zu finden. Konkret geht
â
es um Folgendes:
Problem: Gegeben sind zwei Matrizen A â K nĂm , B â K nĂl . Gesucht ist eine Matrix X â K mĂl mit
AX = B.
Es stellen sich dabei fast von selbst einige Fragen:
â˘ Gibt es uĚberhaupt LoĚsungen X ?
â˘ Falls die LoĚsbarkeit von A und B abhaĚngen sollte: welche Bedingungen muĚssen A und B erfuĚllen,
damit es LoĚsungen X gibt ?
â˘ Wieviele LoĚsungen X gibt es ?
â˘ Wie findet man diese LoĚsungen ?
Der Fall l = 1 ist noch am einfachsten: dann ist B ein Spaltenvektor B = b = (Î˛1 , . . . , Î˛n )â¤ , und X ist ein
Spaltenvektor X = x = (Îž1 , . . . , Îžm )â¤ . Wir erhalten in diesem Fall ein lineares Gleichungssystem
a11 Îž1 + Âˇ Âˇ Âˇ + a1m Îžm = Î˛1 ,
..
.
an1 Îž1 + Âˇ Âˇ Âˇ + anm Îžm = Î˛n .
Dieses System wird noch etwas einfacher im Fall n = m:
1 transposed
2 adjoint

matrix
matrix

68

KAPITEL 3. MATRIZEN

Definition 3.9. Eine Matrix A â K nĂn heiĂt invertierbar3, wenn es eine Matrix B â K nĂn gibt mit
BA = In ,
wobei In die n Ă nâEinheitsmatrix bezeichnet: Man schreibt B = Aâ1 , und nennt Aâ1 die zu A inverse4
Matrix. Wenn A invertierbar bzw. nicht invertierbar ist, dann heiĂt A auch regulaĚr5 bzw. singulaĚr6 .
Wenn A invertierbar ist, dann wird eine LoĚsung x des Gleichungssystems Ax = b gegeben durch x = Aâ1 b.
Frage: Kann es sein, daĂ das Gleichungssystem Ax = b bei invertierbarem A mehrere LoĚsungen x hat ?
BegruĚndung ?
Satz 3.10. Es seien A, B â K nĂn . Dann gilt:
1. Wenn A invertierbar ist, so gibt es genau eine inverse Matrix.
2. Wenn A invertierbar ist und AB = I, so ist B = Aâ1 .
3. Wenn A invertierbar ist, so ist auch Aâ1 invertierbar, und (Aâ1 )â1 = A.
4. Wenn A invertierbar ist, so ist auch Aâ¤ invertierbar, und (Aâ¤ )â1 = (Aâ1 )â¤ .
5. Wenn A und B beide invertierbar sind, so ist auch AB invertierbar, und es ist (AB)â1 = B â1 Aâ1 .
Beweis.

1. Folgt direkt aus Satz 1.44.

2. Folgt aus Satz 1.44.
3. Folgt aus Satz 1.44.
4. Wir haben AAâ1 = I. Wenn wir darauf Satz 3.8 anwenden, haben wir (Aâ1 )â¤ Aâ¤ = I â¤ = I, also ist
(Aâ1 )â¤ die Inverse zu Aâ¤ .
5. Folgt direkt aus Lemma 1.48.

Gleichungssysteme der Form AX = B behandeln wir, indem wir versuchen, die Matrix A auf eine einfachere
Form zu bringen. Dabei benutzen wir folgenden Satz.
Satz 3.11. Seien A â K nĂm , B â K nĂl , X â K mĂl und C â K nĂn , wobei C regulaĚr ist. Dann ist X eine
LoĚsung von AX = B genau dann, wenn X eine LoĚsung von CAX = CB ist.
Beweis. Sei AX = B. Dann ist auch CAX = CB.
Sei andererseits CAX = CB. Weil C invertierbar ist, existiert C â1 , und es ist C â1 CAX = C â1 CB, also
AX = B.
Wir wollen also eine Matrix C finden, sodaĂ CA einfacher zu behandeln ist als A. Wir waĚhlen C als Produkt
von sogenannten Eliminationsmatrizen und Permutationsmatrizen, und die angestrebte Form von CA ist
die sogenannte GauĂâJordanâForm bzw. Zeilenstufenform.
Ein Beispiel
ist
ďŁŤ
0
ďŁŹ0
ďŁŹ
ďŁŹ0
ďŁŹ
ďŁŹ0
ďŁŹ
ďŁŹ0
ďŁŹ
ďŁ­0
0
3 invertible
4 inverse
5 regular
6 singular

einer Matrix in Zeilenstufenform, mit beliebigen EintraĚgen an dem mit Ă markierten Stellen,
1
0
0
0
0
0
0

Ă
0
0
0
0
0
0

Ă
1
0
0
0
0
0

Ă
Ă
0
0
0
0
0

Ă
Ă
0
0
0
0
0

Ă
Ă
1
0
0
0
0

Ă
Ă
Ă
1
0
0
0

ďŁś
Ă
ĂďŁˇ
ďŁˇ
ĂďŁˇ
ďŁˇ
ĂďŁˇ
ďŁˇ.
1ďŁˇ
ďŁˇ
0ďŁ¸
0

69

3.2. GLEICHUNGSSYSTEME
Es gibt zwei Sorten von Eliminationsmatrizen: allgemeine und spezielle.
Eine allgemeine Eliminationsmatrix hat die Form
ďŁŤ
1
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
L=ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁ­

..

ďŁś

Îť1j
..
.

.
1

Îťjâ1,j
Îťjj
Îťj+1,j
..
.

ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ,
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁ¸
1

1
..

.

Îťnj

Îťij â K,

Îťjj 6= 0.

Die LeerraĚume sind Nullen.
Eine spezielle Eliminationsmatrix hat eine der folgenden Formen:
ďŁŤ
1
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
L=ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁ­

ďŁŤ
1
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
L=ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁŹ
ďŁ­

..

ďŁś

.
1

Îťij
..

.
1
..

.

ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ,
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁ¸

1

..

ďŁś

.
1
Îťjj
1
..

.

ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ,
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁ¸
1

Îťjj 6= 0.

Frage: Wohin werden von den obigen drei Matrizen L die Einheitsbasisvektoren e~1 , . . . , e~n abgebildet ?
Frage: Warum sind die speziellen Eliminationsmatrizen regulaĚr ? Wie sehen ihre Inversen aus ?
Sei L eine spezielle Eliminationsmatrix des ersten Typs, und A eine beliebige Matrix mit soviel Zeilen, wie
L Spalten hat. Dann ergibt sich das Produkt LA nach folgender Regel:

kâte Zeile von LA =

(

kâte Zeile von A
iâte Zeile von A + Îťij Âˇ (jâte Zeile von A)

: k 6= i,
: k = i.

Frage: Warum soll bei den Eliminationmatrizen des zweiten Typs der Eintrag Îťjj nicht Null sein ?
Frage: Wie sieht das Produkt LA aus, wenn L eine Eliminationsmatrix des zweiten Typs ist ?
Es seien nun zwei oder mehrere spezielle Eliminationsmatrizen gegeben, die ihre ÎťâEintraĚge in derselben
Spalte haben. Wenn wir diese Matrizen multiplizieren, dann erhalten wir eine allgemeine Eliminationsmatrix.
Und weil die speziellen Eliminationsmatrizen regulaĚr sind, sind also auch die allgemeinen Eliminationsmatrizen regulaĚr.
Wir werden solche Matrizen benutzen, um im Produkt CA viele Nullen zu erzeugen. Dazu sind lediglich
die Îťij passend zu waĚhlen. Diesen ProzeĂ bezeichnen wir auch als AusraĚumenâ.
â

70

KAPITEL 3. MATRIZEN

Permutationsmatrizen P = Pij haben die
ďŁŤ
..
.
ďŁŹ 1
ďŁŹ
..
..
ďŁŹ
.
.
ďŁŹ
ďŁŹ
..
ďŁŹ
.
1
ďŁŹ
ďŁŹÂˇ Âˇ Âˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
0
ďŁŹ
ďŁŹ
..
ďŁŹ
.
ďŁŹ
ďŁŹ
..
Pij = ďŁŹ
.
ďŁŹ
ďŁŹ
..
ďŁŹ
ďŁŹ
.
ďŁŹ
ďŁŹÂˇ Âˇ Âˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
1
ďŁŹ
..
ďŁŹ
ďŁŹ
.
ďŁŹ
..
ďŁŹ
ďŁŹ
.
ďŁ­
..
.

Form

ÂˇÂˇÂˇ

ÂˇÂˇÂˇ

ÂˇÂˇÂˇ

1
..
ÂˇÂˇÂˇ

.

ÂˇÂˇÂˇ

1
ÂˇÂˇÂˇ

..
.
..
.
..
.
1
..
.
..
.
..
.
0
..
.
..
.
..
.

ďŁś

ÂˇÂˇÂˇ

ÂˇÂˇÂˇ

ÂˇÂˇÂˇ

ÂˇÂˇÂˇ

1
..

.

ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
Âˇ Âˇ ÂˇďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
Âˇ Âˇ ÂˇďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁ¸
1

Diese Matrix sieht fast wie eine Einheitsmatrix aus, lediglich die Zeilen i und j sind vertauscht worden.
Man uĚberzeugt sich schnell, daĂ:
Die Multiplikation mit einer Permutationsmatrix Pij von links vertauscht die Zeilen i und j.
Die Multiplikation mit einer Permutationsmatrix Pij von rechts vertauscht die Spalten i und j.
Dann gilt also P P = I, also ist P invertierbar.
Das Rechenverfahren zur Behandlung von AX = B verlaĚuft wie folgt:
Man schreibt zunaĚchst die Matrizen A und B nebeneinander, getrennt durch einen senkrechten Strich. Das
ist moĚglich, weil A und B gleichviel Zeilen haben. Unter einer Langzeile verstehen wir eine Zeile, die A und
B umfaĂt.
Folgende Zeilenoperationen sind zulaĚssig:
â˘ Vertauschen zweier Langzeilen,
â˘ Multiplikation einer Langzeile mit einer Zahl 6= 0,
â˘ Addition des Vielfachen einer Langzeile zu einer anderen Langzeile.
Diese drei Operationen werden mithilfe von Permutationsmatrizen und Eliminationsmatrizen mathematisch
beschrieben.
Man wendet diese Operationen solange an, bis man die Matrix links vom Strich auf Zeilenstufenform
gebracht hat.
Sehr anschaulich formuliert, liegt die Zeilenstufenform vor, wenn folgendes gilt: In der Matrix gibt es eine
Treppe von links oben in Richtung nach rechts unten mit folgenden Eigenschaften:
â˘ Jede Stufe ist genau eine Zeile hoch.
â˘ Jede Stufe ist eine oder mehr als eine Spalte breit.
â˘ Im Stufenknick steht eine 1.
â˘ Unterhalb der Stufen und links von den Stufen stehen nur Nullen.
â˘ Oberhalb der Stufen oder rechts von den Stufen steht irgendetwas.
Beispiele dafuĚr sind die Einheitsmatrix oder die Nullmatrix. Ein weiteres Beispiel ist
ďŁŤ
ďŁś
0
1
Ă
Ă
Ă
Ă
Ă
Ă
Ă
ďŁŹ0
0
0
1
Ă
Ă
Ă
Ă
ĂďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ0
0
0
0
0
0
1
Ă
ĂďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ0
0
0
0
0
0
0
1
ĂďŁˇ
ďŁŹ
ďŁˇ.
ďŁŹ0
ďŁˇ
0
0
0
0
0
0
0
1
ďŁŹ
ďŁˇ
ďŁ­0
0
0
0
0
0
0
0
0ďŁ¸
0
0
0
0
0
0
0
0
0

3.2. GLEICHUNGSSYSTEME

71

Hierbei stehen die Ă fuĚr beliebige EintraĚge.

Die Anzahl der Einsen in den Stufenknicken heiĂt auch Rang der Matrix.
Exakt formuliert:
Definition 3.12. Eine Matrix D â K nĂm ist in Zeilenstufenform, wenn es ein 0 â¤ r â¤ n und Zahlen
1 â¤ j1 < j2 < Âˇ Âˇ Âˇ < jr â¤ m gibt mit folgenden Eigenschaften:
â˘ diji = 1 fuĚr 1 â¤ i â¤ r,
â˘ dil = 0 fuĚr 1 â¤ l < ji ,
â˘ dil â K beliebig fuĚr ji < l â¤ m,
â˘ dkl = 0 fuĚr r < k â¤ n und 1 â¤ l â¤ m.
Im oben gezeichneten Beispiel ist r = 5 und (j1 , j2 , . . . , j5 ) = (2, 4, 7, 8, 9).
Die zweiten bis vierten Bedingungen der Definition bedeuten:
â˘ links von den Einsen, aber in derselben Zeile, stehen Nullen,
â˘ rechts von den Einsen, aber in derselben Zeile, stehen Ă,
â˘ ab der Zeile r + 1 stehen nur noch Nullen.
Das GauĂâJordan7âVerfahren verlaĚuft wie folgt:
1. Man setze einen SpaltenzaĚhler s und einen ZeilenzaĚhler z jeweils auf 1.
2. Man suche in der Spalte s unterhalb des zâten Eintrags von oben (einschlieĂlich) nach einem Matrixeintrag 6= 0.
3. (a) Wenn es einen solchen nicht gibt, dann erhoĚhe man den SpaltenzaĚhler s um 1 und gehe zu
Schritt 2.
(b) Wenn es einen Matrixeintrag 6= 0 gibt8 , dann tausche man die dazugehoĚrige Langzeile mit der
zâten Langzeile. Jetzt ist der Eintrag azs 6= 0.
4. Man dividiere die zâte Langzeile durch azs . Jetzt ist der Eintrag azs = 1.
5. Man addiere geeignete Vielfache der zâten Langzeile zu den tieferen Langzeilen, um die EintraĚge
unterhalb von azs zu annullieren ( AusraĚumenâ). SchraĚg links unterhalb von azs und links von azs
â
passiert nichts, weil dort sowieso nur Nullen stehen.
6. Wenn unterhalb von azs die Nullen erzeugt worden sind, erhoĚhe man z und s jeweils um 1.
7. (a) Wenn diese Indizes z und s den Bereich der Matrix A verlassen, ist das Verfahren beendet.
(b) Ansonsten gehe man zu Schritt 2.
Auf diesem Wege kommt man immer zu einer GauĂâJordanâForm. Man kann das Verfahren aber variieren.
Zum Beispiel kann man den Schritt 4 auch spaĚter ausfuĚhren. Man kann auch oberhalb der azs ausraĚumen.
Als Beispiel wollen wir
ďŁŤ
3
1
3
A = ďŁ­1
1
1

bestimmen,
ďŁŤ
3
ďŁ­1
1

die Inverse von
ďŁś
1
1ďŁ¸
3

also das System AX = I3 loĚsen. Wir starten also mit
ďŁś
1
1
1
0
0
3
1
0
1
0ďŁ¸ .
1
3
0
0
1

7 Wilhelm Jordan (1842â1899), nicht zu verwechseln mit Marie Ennemond Camille Jordan (1838â1922), franzoĚsischer
Mathematiker, dem SchoĚpfer der Jordanschen Normalform. Und die JordanâAlgebren der Physik stammen von Pascual
Jordan (1902â1980).
8 Man nennt ihn auch Pivotelement.

72

KAPITEL 3. MATRIZEN

Wir sehen, daĂ wir unterhalb der 3 links oben ausraĚumen koĚnnen. Weil wir Bruchrechnung vermeiden
wollen aus Bequemlichkeit, multiplizieren wir stattdessen die 2. und 3. Langzeile mit passenden Faktoren:
ďŁś
ďŁś
ďŁŤ
ďŁŤ
1
0
0
1
0
0
3
1
1
3
1
1
ďŁ­â3
â9
â3
0
â3
0 ďŁ¸ â ďŁ­0
â8
â2
1
â3
0 ďŁ¸.
0
0
â3
1
0
â3
â3
â3
â9
0
â2
â8
Ausgehend von der â8 an der Position (2,2) raĚumen wir nach oben und unten aus:
ďŁś
ďŁŤ
ďŁŤ
8
0
0
24
0
6
24
8
8
ďŁ­0
â8
â2
1
â3
0ďŁ¸âďŁ­0
â8
â2
0
8
32
â4
0
12
0
0
30

Und jetzt starten wir von der 30 an der Position
ďŁŤ
â120
0
â30
â45
ďŁ­ 0
15
â120
â30
0
0
30
â3
ďŁŤ
â120
0
0
â120
0
âďŁ­ 0
0
0
30
AnschlieĂend
ďŁŤ
1
ďŁ­0
0

9
1
â3

â3
â3
â3

(3,3) und raĚumen nach oben aus:
ďŁś
15
0
â45
0ďŁ¸
â3
12
ďŁś
â48
12
12
12
â48
12ďŁ¸ .
â3
â3
12

ďŁś
0
0ďŁ¸
12

kuĚrzen wir die Langzeilen durch â120, â120 und 30:
ďŁś
48
12
12
â 120
â 120
0
0
120
48
12 ďŁ¸
12
1
0
â 120
â 120
120
3
12
3
0
1
â 30
â 30
30
ďŁś
ďŁŤ
2
1
1
â
â 10
1
0
0
5
10
1 ďŁ¸
2
1
1
0
â 10
.
= ďŁ­0
â 10
5
1
2
1
0
0
1
â 10
â 10
5

Und rechts vom Strich steht die Inverse von A. Man rechnet leicht nach, daĂ tatsaĚchlich AX = I3 gilt.
Frage: Warum ist das so ?
Falls man stattdessen das System
ďŁŤ ďŁś
0
Ax = ďŁ­7ďŁ¸
0

haĚtte loĚsen wollen, dann haĚtte man mit dem Schema
ďŁś
ďŁŤ
0
3
1
1
ďŁ­1
7ďŁ¸
3
1
1
1
3
0
gestartet, und mit
ďŁŤ
1
0
ďŁ­0
1
0
0

genau denselben Langzeilenumformungen wie oben waĚre man auf das Schema
ďŁś
7
â 10
0
14 ďŁ¸
0
5
7
â 10
1

7 â¤
7 14
, 5 , â 10
) abliest.
gekommen, von dem man die LoĚsung x = (â 10

3.3

SchluĚsselbegriffe

â˘ Matrizen als zentrales Beispiel linearer Abbildungen,
â˘ Beziehung zwischen Matrixprodukt einerseits und NacheinanderausfuĚhrung der zugeordneten linearen
Abbildungen andererseits,
â˘ GauĂâJordanâVerfahren.

Kapitel 4

Homomorphismen
Wir setzen jetzt unsere Betrachtungen zur Linearen Algebra fort, und wir werden einigen Gedanken aus
dem EinfuĚhrungskapitel erneut begegnen, diesmal allerdings auf einer abstrakteren Ebene (didaktisches
Spiralprinzip). Es ist also empfehlenswert, gelegentlich nochmal zuruĚckzublaĚttern und den fruĚheren Stoff
mit neuem Blick zu betrachten.

4.1

Allgemeine Eigenschaften

Jede Matrix A â K nĂm begruĚndet eine Abbildung fA : K m â K n , gemaĚĂ der Vorschrift fA (x) := Ax.
Diese Abbildung genuĚgt den Regeln
âx, y â K m ,

fA (x + y) = fA (x) + fA (y),

âÎť â K,

fA (Îťx) = ÎťfA (x),

âx â K m .

Wir benutzen dabei aber gar nicht, daĂ der Ausgangsvektorraum der K m ist. Weil diese beiden Eigenschaften so wichtig sind, haben Abbildungen dieses Typs einen eigenen Namen bekommen:
Definition 4.1. Seien U und V VektorraĚume uĚber K, und sei f : U â V eine Abbildung mit den Eigenschaften
f (x + y) = f (x) + f (y),
f (Îťx) = Îťf (x),

âx, y â U,

âÎť â K,

(4.1)

âx â U.

(4.2)
1

Dann sagen wir, daĂ f eine lineare Abbildung bzw. ein Homomorphismus ist. Die Menge aller Homomorphismen von U nach V bezeichnen wir mit Hom(U â V ) bzw. L(U â V ). Wenn U = V , dann reden wir
auch von Endomorphismen2 und schreiben L(U ) anstatt von L(U â U ). Falls V = K ist, bezeichnet man
die Homomorphismen auch als lineare Funktionale3 .
Anstelle von Homomorphismen ist auch die Bezeichnung lineare Operatoren4 gebraĚuchlich. In der Literatur
sind die Schreibweisen L(U, V ) oder L(U ; V ) weit verbreitet anstelle unserer Notation L(U â V ), die
hoffentlich besser erklaĚrt, wie hier eigentlich abgebildet wird.
Die beiden zentralen Eigenschaften (4.1) und (4.2) einer linearen Abbildung druĚcken wir wieder als kommutative Diagramme aus:
f

(x, y) âââââ (f (x), f (y))
ďŁŚ
ďŁŚ
ďŁŚ
ďŁŚ
+y(in U)
+y(in V )
x + y âââââ
f

f (x) + f (y)
= f (x + y)

f

x âââââ
ďŁŚ
ďŁŚ(in U)
ÎťÂˇy
Îťx âââââ
f

Einige Eigenschaften ergeben sich direkt aus der Definition:
1 homomorphism
2 endomorphism
3 linear
4 linear

functionals
operators

73

f (x)
ďŁŚ
ďŁŚ
ÎťÂˇy(in

V)

Îťf (x)
= f (Îťx)

74

KAPITEL 4. HOMOMORPHISMEN

Satz 4.2. Sei f â L(U â V ). Dann gilt
â˘ f (0) = 0,
Pn
Pn
â˘ f ( j=1 Îťj xj ) = j=1 Îťj f (xj ).

Beweis. Sollten Sie selber koĚnnen.

Beispiel 4.3.
1. Sei A â K nĂm , dann definieren wir eine Abbildung f : K m â K n gemaĚĂ f (x) = Ax;
und es ist dann f â Hom(K m â K n ).
2. Sei U = C 1 ([0, 1] â R) der Raum der auf [0, 1] einmal stetig differenzierbaren Funktionen, und
d
entsprechend V = C([0, 1] â R). Dann ist die Abbildung dx
: U â V eine lineare Abbildung.
3. Sei U = C 2 (R â R), V = C(R â R) und L â L(U â V ) der zur Schwingungsdifferentialgleichung
d
d2
2
gehoĚrige Differentialoperator L = dt
2 + k dt + Ď , wobei k und Ď gewisse positive Konstanten sind.
Die aĚquivalente Sprechweise in der Physik lautet: fuĚr die Schwingungsdifferentialgleichung gilt das
â
Superpositionsprinzipâ.
4. AĚhnliches gilt fuĚr die Partiellen Differentialoperatoren âł und â.
1
5. Sei U = C(R â R) und
R x V = C (R â R). Dann ist der Integrationsoperator, der jedem f â U eine
Stammfunktion x 7â 0 f (t)dt zuordnet, eine lineare Abbildung.

6. Sei U = C(R â R), V = R, und Î´ : U â V die Abbildung f 7â f (0), die jeder Funktion ihren Wert
im Nullpunkt zuordnet. Diese sogenannte Diracsche5 DeltaâDistribution ist ein lineares Funktional,
das z.B. fuĚr die Beschreibung von Punktmassen verwendet wird.
7. Sei U = R3 der Raum der Ortsvariablen und V = R. Lineare Abbildungen k : U â V werden in der
Physik haĚufig als Wellenzahlvektoren bezeichnet.
Im Folgenden werden wir allgemeine Homomorphismen als Abbildungen zwischen zwei VektorraĚumen U
und V studieren.
Satz 4.4. Es seien U und V zwei VektorraĚume. Sei (b1 , . . . , bm ) eine Basis fuĚr U , und sei (v1 , . . . , vm )
eine beliebige Familie von Vektoren in V .
Dann gibt es genau einen Homomorphismus f â L(U â V ) mit f (bj ) = vj fuĚr jedes j. Dieser Homomorphismus wird gegeben durch die Formel
f (Îą1 b1 + Âˇ Âˇ Âˇ + Îąm bm ) = Îą1 v1 + Âˇ Âˇ Âˇ + Îąm vm .
Beweis. Weil (b1 , . . . , bm ) eine Basis ist, gibt es zu jedem u â U genau einen Satz von Koeffizienten
(Îą1 , . . . , Îąm ) mit u = Îą1 b1 + Âˇ Âˇ Âˇ + Îąm bm . Die obige Formel beschreibt einen Homomorphismus (Beweis
selbst vervollstaĚndigen !) und es ist f (bj ) = vj , wie verlangt.
Es bleibt noch zu zeigen, daĂ dieses f der einzige Homomorphismus mit f (bj ) = vj ist. Sei nun g ein
weiterer Homomorphismus mit g(bj ) = vj fuĚr jedes j. Dann ist wegen Satz 4.2
g(u) = g(Îą1 b1 + Âˇ Âˇ Âˇ + Îąm bm ) = Îą1 g(b1 ) + Âˇ Âˇ Âˇ + Îąm g(bm ) = Îą1 v1 + Âˇ Âˇ Âˇ + Îąm vm = f (u),
also ist g = f .
Damit koĚnnen wir jetzt eine offengebliebene Frage des vorigen Kapitels beantworten. Damals wurde behauptet, daĂ jede lineare Abbildung f : K m â K n in Form einer Matrix dargestellt werden kann.

Gegeben sei eine lineare Abbildung f â L(K m â K n ).

Gesucht ist eine Matrix A â K nĂm mit Ax = f (x) fuĚr jedes x â K m .

Um eine solche Matrix zu finden, waĚhlen wir eine Basis in K m , zum Beispiel die Standardbasis (e1 , . . . , em ).
Seien vj die Bilder der Einheitsvektoren, vj = f (ej ), j = 1, . . . , m. Wenn wir diese Spaltenvektoren nebeneinanderstellen, erhalten wir eine Matrix A. Diese ist genau die gesuchte Matrix, denn gemaĚĂ der Regel
die Spalten sind die Koordinaten der Bilder der Basisvektoren
5 Paul

Adrien Maurice Dirac (1902â1984), Nobelpreis 1933 zusammen mit Erwin SchroĚdinger

4.1. ALLGEMEINE EIGENSCHAFTEN

75

ist Aej = vj , also f (ej ) = vj . Und der vorige Satz sagt uns dann, daĂ die Gleichung f (u) = Au nicht nur
fuĚr u â {e1 , . . . , em } gilt, sondern fuĚr jeden Vektor u â U .

Frage: Was ist der Unterschied zwischen den folgenden beiden Formulierungen:
f ist eine lineare Abbildungen vom R7 in den R7â
â
f ist eine lineare Abbildungen vom R7 auf den R7â ?
â
Definition 4.5. Seien A und B Mengen, und f : A â B eine Abbildung.
â˘ f heiĂt injektiv6 , wenn aus f (x) = f (xâ˛ ) folgt: x = xâ˛ .

â˘ f heiĂt surjektiv7 , wenn es zu jedem y â B mindestens ein x â A mit y = f (x) gibt.
â˘ f heiĂt bijektiv8 , wenn f injektiv und surjektiv ist.
â˘ Die identische Abbildung idA : A â A wird definiert durch idA (x) := x fuĚr jedes x â A.
Wir betonen, daĂ A und B einfach nur Mengen sind; sie brauchen keine VektorraĚume zu sein.
Satz 4.6. Identische Abbildungen sind bijektiv.
Beweis. ist recht einfach und soll deshalb eine UĚbungsaufgabe sein.
Satz 4.7. Seien A, B, C Mengen, f eine Abbildung von A in B, g eine Abbildung von B in C. Dann gilt:
1. Wenn g âŚ f surjektiv ist, dann auch g.
2. Wenn g âŚ f injektiv ist, dann auch f .
Beweis. Wir benutzen die Beweismethode der Kontraposition: 9
Zu Teil 1: Wenn g die Menge C nicht ausschoĚpft, dann kann g âŚ f die Menge C erst recht nicht ausschoĚpfen.

Zu Teil 2: Wenn f nicht injektiv ist, dann bildet f zwei verschiedene Elemente von A auf dasselbe Element
von B ab. Dann kann auch g âŚ f nicht injektiv sein.
Satz 4.8. Seien A, B Mengen, und sei f : A â B eine Abbildung. Dann gilt:
f ist genau dann bijektiv, wenn es eine Abbildung g : B â A gibt mit
f âŚ g = idB ,

g âŚ f = idA .

Beweis. =ââ:
â
Sei f bijektiv. Dann ist f surjektiv, also gibt es fuĚr jedes b â B mindestens ein a â A mit f (a) = b. Und
weil f auch injektiv ist, ist dieses Element a eindeutig. Die gesuchte Abbildung g ist gerade diejenige, die
b auf a sendet.
â=â:
â
Wegen Satz 4.6 ist idB bijektiv, also ist auch f âŚ g = idB bijektiv, also ist f âŚ g surjektiv, und wegen Satz 4.7
ist dann auch f surjektiv.
Wegen Satz 4.6 ist idA bijektiv, also ist auch g âŚ f = idA bijektiv, also ist g âŚ f injektiv, und wegen Satz 4.7
ist dann auch f injektiv.
Satz 4.9. Seien U , V , W VektorraĚume uĚber einem KoĚrper K, und sei f â L(U â V ), g â L(V â W ).
1. Die identische Abbildung idU : u 7â u ist ein bijektiver Homomorphismus idU â L(U â U ).
2. Durch g âŚ f : u 7â g(f (u)) wird ein Homomorphismus g âŚ f â L(U â W ) definiert.
6 injective
7 surjective
8 bijective
9 Das bedeutet: Seien A und B Aussagen, und wir wollen beweisen: Wenn A, dann Bâ. AĚquivalent dazu ist die Aussage
â
Wenn B falsch ist, dann ist A auch falschâ. Diese zweite Variante ist manchmal einfacher zu beweisen.
â

76

KAPITEL 4. HOMOMORPHISMEN

Beweis. Das Ergebnis 1. ist ziemlich leicht und deshalb dem Leser uĚberlassen.
Zum Beweis von 2. benutzen wir (4.1) und (4.2) fuĚr f und g:
(g âŚ f )(x + y) = g(f (x + y)) = g(f (x) + f (y)) = g(f (x)) + g(f (y)) = (g âŚ f )(x) + (g âŚ f )(y).
Analog
(g âŚ f )(Îťx) = g(f (Îťx)) = g(Îťf (x)) = Îťg(f (x)) = Îť(g âŚ f )(x).
Definition 4.10. Bijektive Homomorphismen zwischen VektorraĚumen heiĂen Isomorphismen10 .
Wenn es zwischen zwei VektorraĚumen U und V einen Isomorphismus gibt, dann nennt man beide RaĚume
zueinander isomorph11 .
Im ersten Kapitel hatten wir bereits zwei zueinander isomorphe VektorraĚume behandelt: naĚmlich einen
Vektorraum der Verschiebungspfeile in der Ebene (nennen wir ihn U ), und den Raum V = R2 , der aus
Zahlenpaaren besteht. Der Zusammenhang zwischen ~u â U und einem dazugehoĚrigen Zahlenpaar wird
durch eine Basis (~b1 , ~b2 ) fuĚr U vermittelt. Wenn eine solche Basis gewaĚhlt ist, koĚnnen wir jedes ~u zerlegen

im Sinne von ~u = Îž1~b1 + Îž2~b2 . Die Abbildung ~u 7â ÎžÎž12 ist eine Abbildung von U nach R2 , und diese
Abbildung ist linear und bijektiv, also ein Isomorphismus. Wir haĚtten auch eine andere Basis (~bâ˛1 , ~bâ˛2 ) anstatt
(~b1 , ~b2 ) waĚhlen koĚnnen, und das haĚtte uns einen anderen Isomorphismus geliefert. Es gibt also unendlich
viele Isomorphismen zwischen U und R2 . In der Schule ist vermutlich der Eindruck entstanden, daĂ U als
Vektorraum der Verschiebungspfeile in der Ebene und der R2 schon irgendwie derselbe Vektorraumâ sind.
â
Dieser Eindruck ist in dem Sinne richtig, daĂ beide VektorraĚume isomorph zueinander sind, und isomorph
bedeutet eben gleichgestaltig.
Satz 4.11. Seien A, B â K nĂn Matrizen, und fA , fB â L(K n ) die zugehoĚrigen linearen Abbildungen.
Dann ist die Komposition fA âŚ fB : K n â K n gegeben durch (fA âŚ fB )(x) = ABx.

Zueinander inverse Isomorphismen fA und fB werden durch zueinander inverse Matrizen A und B beschrieben.
Beweis. UĚbungsaufgabe. Man nutze Satz 3.5.

4.2

Geometrische Aspekte

Wir brauchen noch einige weitere Begriffe.
Definition 4.12. Sei f â Hom(U â V ). Dann heiĂt
â˘ ker f := {u â U : f (u) = 0} der Nullraum bzw. Kern12 von f ,
â˘ img f := {f (u) â V : u â U } der Bildraum bzw. das Bild13 von f .
Analog definieren wir fuĚr eine Matrix A â K nĂm
â˘ ker A := {x â K m : Ax = 0},
â˘ img A := {Ax â K n : x â K m }.
Satz 4.13. Wenn f â Hom(U â V ), dann ist ker f ein Unterraum von U und img f Unterraum von V .
Beweis. Es ist f (0) = 0, also ist 0 â ker f und 0 â img f . Wenn nun u1 , u2 â ker f sind, dann ist
f (u1 + u2 ) = f (u1 ) + f (u2 ) = 0, also auch u1 + u2 â ker f . Analog beweist man Îąu â ker f , falls Îą â K
und u â ker f . GemaĚĂ Satz 2.6 ist dann ker f ein Unterraum von U .
Seien nun v1 , v2 â img f . Dann gibt es u1 , u2 â U mit f (u1 ) = v1 , f (u2 ) = v2 . Dann gilt f (u1 + u2 ) =
f (u1 ) + f (u2 ) = v1 + v2 , also ist v1 + v2 â img f . Analog zeigt man Îąv â img f , wenn v â img f und Îą â K.
Nun verwendet man nochmal den Satz 2.6, und der Beweis ist fertig.
10 isomorphism
11 isomorph
12 kernel
13 image

to each other

4.2. GEOMETRISCHE ASPEKTE

77

Definition 4.14. Die Dimension des Bildes von f heiĂt Rang14 von f :
rang f := dim img f.
Der Rang einer Matrix A wird definiert durch
rang A := dim img A.
Die Dimension des Kernes von f heiĂt Defekt15 von f ,
def f := dim ker f.
Satz 4.15. Sei f â Hom(U â V ) und sei (u1 , . . . , um ) eine Familie in U . Dann haben wir die Aussagen:
1. f injektiv ââ ker f = {0},
2. f surjektiv ââ img f = V ,
3. (f (u1 ), . . . , f (um )) ist unabhaĚngig in V =â (u1 , . . . , um ) sind unabhaĚngig in U ,
4. (u1 , . . . , um ) sind unabhaĚngig in U und f ist injektiv =â (f (u1 ), . . . , f (um )) sind in V unabhaĚngig,
5. (u1 , . . . , um ) erzeugen U =â (f (u1 ), . . . , f (um )) erzeugen img f ,
6. (u1 , . . . , um ) erzeugen U und f ist surjektiv =â (f (u1 ), . . . , f (um )) erzeugen V .
Beweis.
1. Wenn f injektiv ist, dann gibt es nur ein u â U mit f (u) = 0, naĚmlich u = 0. Also ist
ker f = {0}.

Sei nun andererseits ker f = {0}. Wenn f (u) = f (uâ˛ ) ist, dann ist f (u â uâ˛ ) = 0, also u â uâ˛ â ker f =
{0}, also u = uâ˛ .

2. So ist SurjektivitaĚt definiert.
Pm
Pm
3. Sei j=1 Îąj uj = 0. Zu zeigen waĚre Îąj = 0 fuĚr jedes j. Wir haben 0 = f (0) = f ( j=1 Îąj uj ) =
Pm
j=1 Îąj f (uj ). Nun sind aber die f (uj ) linear unabhaĚngig, also sind alle Îąj = 0.
Pm
Pm
4. Sei j=1 Îąj f (uj ) = 0, zu zeigen waĚre Îąj = 0 fuĚr jedes j. Nun ist wegen der LinearitaĚt f ( j=1 Îąj uj ) =
Pm
0. Weil f injektiv ist, haben wir mit 1. ker f = {0}, also muĂ
j=1 Îąj uj = 0 sein. Da nun die
(u1 , . . . , um ) linear unabhaĚngig sind, ist Îąj = 0 fuĚr jedes j.
5. Sei v â img f . Dann gibt es ein u âP
U mit f (u) = v. Weil die uj den Originalraum
U erzeugen, gibt
P
P
m
es Konstanten Îą1 , . . . , Îąm mit u = j=1 Îąj uj . Dann ist v = f (u) = f ( j Îąj uj ) = j Îąj f (uj ). Also
ist v als Linearkombination der (f (u1 ), . . . , f (um )) dargestellt.
6. Folgt direkt aus 2. und 5.

Ein Teilergebnis dieses Satzes ist so wichtig, daĂ wir es nochmal hinschreiben sollten:
Folgerung 4.16. Sei f â Hom(U â V ) ein injektiver Homomorphismus. Dann gilt
(u1 , . . . , um ) linear unabhaĚngig in U ââ (f (u1 ), . . . , f (um )) linear unabhaĚngig in V.
Die InjektivitaĚt von f ist hierbei die entscheidende Voraussetzung !
Als weitere fast triviale Folgerung ergibt sich:
Satz 4.17.

1. Die Spalten einer Matrix sind ein Erzeugendensystem fuĚr das Bild.

2. Der Rang einer Matrix ist die Maximalzahl linear unabhaĚngiger Spalten dieser Matrix.
Rang und Defekt einer Abbildung sind nicht unabhaĚngig voneinander:
14 rank
15 defect

78

KAPITEL 4. HOMOMORPHISMEN

Satz 4.18 (Dimensionsformel). Sei f â Hom(U â V ). Dann ist
def f + rang f = dim U.
FuĚr Matrizen A â K nĂm bedeutet das
def A + rang A = m.
Beweis. Sei (u1 , . . . , us ) eine Basis fuĚr ker f . Nach dem BasisergaĚnzungssatz koĚnnen wir diese Basis
von ker f zu einer Basis von ganz U ergaĚnzen: (u1 , . . . , us , us+1 , . . . , um ). Dann erzeugen die Vektoren
(f (u1 ), . . . , f (um )) den Raum img f , wegen Satz 4.15, Teil 5. Weil f (u1 ) = Âˇ Âˇ Âˇ = f (us ) = 0, kann man diese
Vektoren im Erzeugendensystem weglassen, und kommt zu einem Erzeugendensystem (f (us+1 ), . . . , f (um ))
fuĚr img f .
Als naĚchstes wollen wir zeigen, daĂ diese Vektoren (f (us+1 ), . . . , f (um )) linear unabhaĚngig sind.
P
Pm
Pm
Sei nun also m
j=s+1 Îąj f (uj ) = 0. Dann ist f ( j=s+1 Îąj uj ) = 0, also ist
j=s+1 Îąj uj â ker f . Aber ker f
hatte gerade die Basis (u1 , . . . , us ), und die Vektoren (u1 , . . . , us , us+1 , . . . , um ) sind linear unabhaĚngig. Das
kann nur sein, wenn alle Îąj = 0 sind, fuĚr j = s + 1, . . . , m.
Damit ist (f (us+1 ), . . . , f (um )) eine Basis fuĚr img f .
Die Dimensionsformel ergibt sich nun durch AbzaĚhlen.

Abbildung 4.1: Pseudo-schematische Veranschaulichung der Dimensionsformel, mit dim U = m = 7 und
dim ker f = s = 3
Damit koĚnnen wir nun die Abbildungseigenschaften der einer invertierbaren Matrix zugehoĚrigen Abbildung
beschreiben:
Satz 4.19. Seien U, V VektorraĚume uĚber K, und sei dim U = dim V . Sei f â Hom(U â V ). Dann sind
die folgenden Aussagen aĚquivalent:
1. f ist Isomorphismus,
2. f ist injektiv,
3. f ist surjektiv,
4. f bildet eine Basis (u1 , . . . , um ) von U auf eine Basis (f (u1 ), . . . , f (um )) von V ab.

4.2. GEOMETRISCHE ASPEKTE

79

Beweis. 2 ââ 3:

Wir haben dim ker f + dim img f = dim U = dim V . Wenn f injektiv ist, dann ist ker f = {0}, also
dim ker f = 0, also dim img f = dim V , also img f = V , also ist f surjektiv. Wenn f surjektiv ist, dann ist
img f = V , also dim img f = dim V , also muĂ dim ker f = 0 sein, also ist ker f = {0}, also ist f injektiv.
1 ââ 2, 3:

Das ist genau Definition 4.10.
2, 3 =â 4:
Es ist (u1 , . . . , um ) unabhaĚngig in U , und f ist injektiv: also ist wegen Satz 4.15, Teil 4 auch
(f (u1 ), . . . , f (um )) eine linear unabhaĚngige Familie in V .
Weiterhin erzeugt (u1 , . . . , um ) den Raum U , und f ist surjektiv: also erzeugt wegen Satz 4.15, Teil 6 auch
(f (u1 ), . . . , f (um )) den Raum V .
Also ist (f (u1 ), . . . , f (um )) eine Basis von V .
4 =â 3:
Es liegen f (u1 ), . . . , f (um ) in img f , aber diese m Vektoren sind eine linear unabhaĚngige Familie, denn
sie bilden eine Basis von V . Also ist dim img f âĽ m. Wegen dim V = m folgt dann img f = V , also ist f
surjektiv.
Wir uĚbertragen die Ergebnisse dieses Satzes auf Matrizen:
Satz 4.20. Sei A â K nĂn und f â Hom(K n â K n ) die von A erzeugte lineare Abbildung. Dann sind die
folgenden Aussagen aĚquivalent:
1. f ist Isomorphismus,
2. A ist invertierbar,
3. rang A = n,
4. def A = 0,
5. Die Spalten von A erzeugen den K n ,
6. Die Spalten von A sind linear unabhaĚngig,
7. Die Spalten von A sind eine Basis des K n .
Beweis. 1 ââ 2 :

Siehe Satz 4.11.
1 ââ 3 ââ 4 :

Die Aussage rang A = n ist gleichbedeutend mit img A = K n , also mit der SurjektivitaĚt von f , gemaĚĂ
Satz 4.15. Und def A = 0 bedeutet ker A = {0}, also die InjektivitaĚt von f , ebenfalls wegen Satz 4.15.
Die AĚquivalenz von 1, 3 und 4 folgt nun aus Satz 4.19.
1 ââ 7 :

Die Spalten sind gerade die Bilder (f (e1 ), . . . , f (em )) der Standardbasisvektoren (e1 , . . . , em ). Die Behauptung folgt aus Satz 4.19, von Aussage 1 zu Aussage 4.
5 =â 7 :
Nach Satz 2.14 enthaĚlt das System der n Spaltenvektoren eine Basis des K n . Wegen dim K n = n muĂ jede
Basis aber aus genau n Vektoren bestehen, also ist das System der n Spaltenvektoren schon eine Basis.
7 =â 5 und 7 =â 6 :
So sind Basen definiert !
6 =â 7 :
Nach dem BasisergaĚnzungssatz kann man das linear unabhaĚngige System der n Spaltenvektoren zu einer
Basis des K n ergaĚnzen. Diese hat aber genau n Vektoren, also muĚssen die n Spaltenvektoren von A schon
eine Basis sein. (Oder man nimmt die zweite Behauptung des Austauschsatzes von Steinitz.)

80

KAPITEL 4. HOMOMORPHISMEN

Folgerung 4.21. Sei A â K nĂm und C â K nĂn , wobei C invertierbar sei.
Dann haben A und CA denselben Rang.

Beweis. Das folgt aus Folgerung 4.16: wenn die Matrix A einige linear unabhaĚngige Spalten enthaĚlt, dann
sind die entsprechenden Spalten von CA auch linear unabhaĚngig, und umgekehrt. Die Maximalzahl linear
unabhaĚngiger Spalten ist aber gerade gleich dem Rang.
Satz 4.22. Sei A â K nĂm , und sei AĚ eine GauĂâJordanâTransformierte von A. Das heiĂt, AĚ ergibt
sich aus A durch Zeilenumformungen und hat Zeilenstufenform.
Dann haben A und AĚ denselben Rang, und A ist invertierbar genau dann, wenn n = m und AĚ die Gestalt
einer oberen Dreiecksmatrix hat.
Beweis. Wir erhalten AĚ als Produkt LA, wobei L eine regulaĚre Matrix ist, die wir als Produkt von Eliminationsmatrizen und Permutationsmatrizen schreiben koĚnnen. Dann haben AĚ und A denselben Rang.
Wir wissen: wenn AĚ eine obere Dreiecksmatrix ist, dann koĚnnen wir das GauĂâJordanâVerfahren weiterfuĚhren, indem wir noch nach oben ausraĚumen, solange bis eine Matrix AĚË = In entsteht, also LA = In .
Dann ist A invertierbar.
Andererseits, wenn nun AĚ auf Zeilenstufenform gebracht wurde, aber keine Dreiecksgestalt hat, dann ist
die unterste Zeile von AĚ eine Nullzeile. Die Spalten von AĚ spannen dann nicht den K n auf, also ist AĚ nicht
invertierbar. Dann ist rang AĚ < n, also auch rang A < n, also ist A nicht invertierbar.
12 Fragen: Sei U = R7 und V = R8 . Wieviele injektive (surjektive) (bijektive) (identische) Abbildungen
gibt es von U nach V ? Von V nach U ? Von V nach V ? (Alle Abbildungen seien linear.)

4.3

Lineare Gleichungen

Es geht um Gleichungen f (u) = v, wobei f â Hom(U â V ) eine lineare Abbildung zwischen zwei VektorraĚumen U und V ist. Dieses f und ein Vektor v â V sind gegeben, und u â U ist gesucht. Folgende
Fragen ergeben sich naheliegenderweise:
â˘ Gibt es uĚberhaupt LoĚsungen ?
â˘ Wenn ja, wieviele sind es ?
â˘ Manchmal gibt es fuĚr gewisse v â V eine LoĚsung u, fuĚr andere v â V aber keine LoĚsung u. Welche
Bedingungen muĂ v erfuĚllen, damit es (mindestens) eine LoĚsung gibt ?
â˘ Wie findet man die LoĚsungen ?
Satz 4.23. Seien U und V VektorraĚume uĚber K, f â Hom(U â V ) und v â V gegeben.
â˘ Wenn u1 , u2 â U LoĚsungen von f (u) = v sind, dann ist u1 â u2 eine LoĚsung von f (u) = 0.
â˘ Wenn u1 eine LoĚsung von f (u) = v und u2 eine LoĚsung von f (u) = 0 sind, dann ist u1 + u2 eine
LoĚsung von f (u) = v.
Beweis. Das sollten Sie selber koĚnnen.
Wir erhalten die folgende Merkregel:
Die allgemeine LoĚsung des inhomogenen Problems
=
eine spezielle LoĚsung des inhomogenen Problems
+
die allgemeine LoĚsung des homogenen Problems.

4.4. BASISTRANSFORMATIONEN

81

Die allgemeine LoĚsung des homogenen Problems wird gerade beschrieben durch den Kern von f . Demnach
besitzt die Gleichung f (u) = v also LoĚsungen in einer linearen Mannigfaltigkeit mit der Dimension def f =
dim U â rang f ; aber nur, wenn sie uĚberhaupt eine LoĚsung besitzt.
Satz 4.24. Seien U und V VektorraĚume uĚber K, und zwar endlichdimensional. Sei f â Hom(U â V ).
Dann gilt:
1. Die Gleichung f (u) = v hat fuĚr jedes v â V mindestens eine LoĚsung u â U ââ rang f = dim V .
2. Die Gleichung f (u) = v hat fuĚr jedes v â V maximal eine LoĚsung u â U ââ rang f = dim U .
3. Die Gleichung f (u) = v hat fuĚr jedes v â V genau eine LoĚsung u â U ââ rang f = dim U = dim V .
Beweis.

1. Trivial wegen img f = V .

2. Wenn es maximal eine LoĚsung gibt, dann ist ker f = {0}, also 0 = def f = dim U â rang f und
umgekehrt.
3. Folgt sofort aus 1. und 2.

Wenn wir diesen Satz auf den speziellen Fall von Matrizen beziehen, erhalten wir:
Satz 4.25. Sei A â K nĂm . Dann gilt fuĚr Gleichungssysteme Ax = y mit gegebenem y â K n und gesuchtem
x â K m:
1. Das Gleichungssystem Ax = y ist loĚsbar ââ y â img A ââ rang A = rang(A|y), wobei (A|y)
die Matrix bezeichnet, die durch Nebeneinanderstellen von A und y entsteht. Es gibt dann def A =
m â rang A linear unabhaĚngige LoĚsungen fuĚr das homogene Problem Ax = 0.
2. Ax = y hat fuĚr jedes y â K n mindestens eine LoĚsung x â K m ââ rang A = n ââ die Spalten von
A erzeugen den K n .
3. Ax = y hat fuĚr jedes y â K n maximal eine LoĚsung x â K m ââ rang A = m ââ die Spalten von A
sind linear unabhaĚngig.
4. Ax = y hat fuĚr jedes y â K n genau eine LoĚsung x â K m ââ rang A = m = n ââ die Spalten von
A sind eine Basis des K n .
Beweis.

1. Hinschauen. Die Spalten von A spannen img A auf.

2. Siehe 4.24, Teil 1.
3. Siehe 4.24, Teil 2. Der Rang von A ist die Anzahl der linear unabhaĚngigen Spalten von A, aber A hat
genau m Spalten.
4. Folgt aus 2. und 3.

4.4

Basistransformationen

Problem: Gegeben sei U = K m mit 2 Basen: (b1 , . . . , bm ) und (bâ˛1 , . . . , bâ˛m ). Wir stellen uns B als alte
Basis vor, und B â˛ als neue Basis. FuĚr einen Vektor u â U haben wir dann zwei Darstellungen:
u=

m
X

Îžj bj =

m
X

Îžjâ˛ bâ˛j ,

j=1

j=1

mit Koeffizienten Îžj , Îžjâ˛ â K. Wir fassen diese Koeffizienten zu Spaltenvektoren zusammen:
ďŁŤ ďŁś
ďŁŤ â˛ďŁś
Îž1
Îž1
ďŁŹ .. ďŁˇ
ďŁŹ .. ďŁˇ
â˛
x=ďŁ­ . ďŁ¸ ,
x =ďŁ­ . ďŁ¸ .
Îžm

B

â˛
Îžm

Bâ˛

82

KAPITEL 4. HOMOMORPHISMEN

Wie kann man die neue Koordinatenspalte xâ˛ aus der alten Koordinatenspalte x bestimmen ?
LoĚsung: Weil die Basisvektoren aus dem K m stammen, kann man sie als Spaltenvektoren interpretieren.
Wenn man diese Spaltenvektoren nebeneinanderstellt, erhaĚlt man quadratische Matrizen B und B â˛ . Diese
sind regulaĚr, weil die Spaltenvektoren linear unabhaĚngig sind. Dann laĚĂt sich die Darstellungsformel fuĚr u
gerade schreiben als
u = Bx = B â˛ xâ˛ ,
woraus wir sofort
xâ˛ = (B â˛ )â1 Bx
erhalten. Wir ziehen daraus eine Merkregel (und stellen uns dabei vor, daĂ x bezuĚglich der Standardbasis
(e1 , e2 , . . . , em ) dargestellt wird, also B = Im die Einheitsmatrix ist):
Im K m kann man die Multiplikation einer Zahlenspalte x mit einer invertierbaren Matrix ansehen als
Umrechnung der Koordinaten des von x bezuĚglich der Standardbasis dargestellten Vektors
auf eine neue Basis.
Einerseits: die alte Basis war jetzt (e1 , . . . , em ), und der jâte neue Basisvektor lautet bâ˛j = B â˛ ej .
Andererseits: die alten Koordinaten (bezuĚglich (e1 , . . . , em )) eines Punktes hatten wir x genannt, und dann
lauten die neuen Koordinaten desselben Punktes jetzt xâ˛ = (B â˛ )â1 x.
Wir bekommen also zwei verschiedene Umrechnungsregeln fuĚr die Basisvektoren und fuĚr die Koordinaten.
Problem: Seien U = K m , V = K n und f â Hom(U â V ). Bezogen auf die Standardbasen (e1 , . . . , em )
und (e1 , . . . , en ) von U bzw. V wird die lineare Abbildung f durch eine Matrix A â K nĂm dargestellt:
f (u) = Au, wenn u als Spaltenvektor interpretiert wird.
V
V
U
Wir geben uns noch jeweils eine weitere Basis von U bzw. V vor: (bU
1 , . . . , bm ) fuĚr U und (b1 , . . . , bn )
fuĚr V . Wenn wir diese Spaltenvektoren in der angegebenen Reihenfolge nebeneinanderstellen, erhalten wir
regulaĚre Matrizen BU â K mĂm und BV â K nĂn . Sei nun v = f (u), also v = Au. Die Vektoren u und v
koĚnnen wir auch in den neuen Basen darstellen:

u=

m
X

ÎžjU bU
j ,

v=

n
X

ÎžjV bVj ,

j=1

j=1

wobei ÎžjU , ÎžjV â K. Diese Koeffizienten koĚnnen wir wieder in Spaltenform anordnen:
ďŁŤ UďŁś
ďŁŤ VďŁś
Îž1
Îž1
ďŁŹ .. ďŁˇ
ďŁŹ .. ďŁˇ
U
V
x =ďŁ­ . ďŁ¸ ,
x =ďŁ­ . ďŁ¸ .
U
Îžm

ÎžnV

BU

BV

Wie sieht nun die Abbildungsmatrix bezuĚglich der neuen Basen aus ? Gesucht ist also eine Matrix AĚ â
K nĂm mit
xV = AĚxU .
Und diese Beziehung soll natuĚrlich fuĚr jeden Koeffizientenvektor xU gelten, wenn xV den Koeffizientenvektor
des zugehoĚrigen Bildvektors v darstellt.
V
LoĚsung: Die Darstellung der Vektoren u und v als Linearkombinationen der Basisvektoren bU
j und bj kann
man auch schreiben als

u = BU xU ,

v = BV xV .

Zusammen mit v = Au erhalten wir damit
xV = BVâ1 v = BVâ1 Au = (BVâ1 ABU )xU = AĚxU .
Im Hinblick auf die obige Merkregel ist dieses Dreierprodukt von Matrizen nicht uĚberraschend: die beiden
aĚuĂeren Matrizen dienen dem Umrechnen zwischen verschiedenen Basen, und A erzeugt die Abbildung.
Wir fassen die Ergebnisse zusammen:

4.4. BASISTRANSFORMATIONEN

83

Satz 4.26. Mit den obigen Bezeichnungen ist AĚ = BVâ1 ABU . Die Bilder der neuen Basisvektoren bU
j
werden gegeben durch
U
f (bU
j ) = Abj =

n
X

aĚkj bVk ,

k=1

wobei aĚkj gerade die EintraĚge der Matrix AĚ sind.
Beweis. Es ist lediglich der zweite Teil noch offen. Nun ist aber
U
f (bU
j ) = Abj = A Âˇ (jâte Spalte von BU ) = jâte Spalte von(ABU ) = (ABU )ej

= BV BVâ1 ABU ej = BV AĚej = BV Âˇ (jâte Spalte von AĚ)
n
X
=
aĚkj bVk .
k=1

Im wichtigen Sonderfall U = V = K n haben wir die Freiheit, BU und BV unabhaĚngig voneinander zu
waĚhlen, normalerweise nicht.
Definition 4.27. Zwei Matrizen A, AĚ â K nĂn heiĂen aĚhnlich16 , wenn es eine invertierbare Matrix B â
K nĂn gibt mit AĚ = B â1 AB.
Es stellt sich die Frage, wie man die Matrix B waĚhlen kann/soll, damit AĚ moĚglichst einfach oder moĚglichst
schoĚn wird. Die Antwort darauf ist mit unseren jetzigen Kenntnissen noch nicht moĚglich und muĂ auf ein
spaĚteres Kapitel vertagt werden.
Satz 4.28. Sei A â K nĂm , BU â K mĂm , BV â K nĂn , wobei BU und BV invertierbar seien. Dann haben
A und AĚ = BVâ1 ABU denselben Rang.
Beweis. Wir definieren eine lineare Abbildung f â Hom(K m â K n ) durch f (u) := Au. Dann ist rang A =
rang f = dim img f . Nun beschreiben A und AĚ die gleiche Abbildung, lediglich in verschiedenen Basen. Da
der Rang einer Matrix sich interpretieren laĚĂt als Dimension des Bildraumes, muĚssen die RaĚnge von A und
AĚ gleich sein.
Der Rang einer Matrix ist gleich der Anzahl der linear unabhaĚngigen Spalten, man redet auch vom Spaltenrang.
Definition 4.29. Der Zeilenrang einer Matrix ist gleich der Anzahl der linear unabhaĚngigen Zeilen.
Satz 4.30. FuĚr jede Matrix stimmen Zeilenrang und Spaltenrang uĚberein.
Beweis. FuĚr den Zeilenrang bzw. Spaltenrang einer Matrix A schreiben wir Zeilenrang(A) bzw.
Spaltenrang(A). Sei A die fragliche Matrix mit Rang (Spaltenrang) r. Wir waĚhlen geeignete Matrizen
BU und BV (zusammengebaut aus Eliminationsmatrizen und Permutationsmatrizen) so, daĂ


I
0
BVâ1 ABU = AĚ = r
â K nĂm .
0
0
Nun gilt fuĚr die beiden Rangtypen, durch zweimaliges Anwenden von Satz 4.28:
r = Spaltenrang(A) = Spaltenrang(AĚ) = Zeilenrang(AĚ) = Spaltenrang((AĚ)â¤ )
= Spaltenrang(BUâ¤ Aâ¤ (BVâ1 )â¤ ) = Spaltenrang(BUâ¤ Aâ¤ (BVâ¤ )â1 )
= Spaltenrang(Aâ¤ )
= Zeilenrang(A).

Bemerkung 4.31. Es empfiehlt sich (Spiralprinzip der Didaktik), den Abschnitt 1.5.3 erneut zu lesen,
und dabei insbesondere das Dreierprodukt R = AR0 Aâ¤ (in der Notation von Satz 1.67) mit dem jetzigen
Dreierprodukt AĚ = B â1 AB wie in Definition 4.27 zu vergleichen. Welche Gemeinsamkeiten finden Sie ?
16 similar

84

KAPITEL 4. HOMOMORPHISMEN

4.5

Differentialgleichungen

Ziel dieses einfuĚhrenden Abschnittes soll es sein, die LoĚsungsmenge von Differentialgleichungen mithilfe der
Homomorphismentheorie zu beschreiben. Dabei orientieren wir uns an 2 typischen Beispielen:
Beispiel 4.32. Eine Zerfalls/Wachstumsgleichung mit Zufuhr und Anfangsbedingung wird gegeben durch
uâ˛ (t) + Îąu(t) = f (t),

u(0) = u0 ,

Îą â R.

Eine Schwingungsgleichung mit DaĚmpfung und Anregung und 2 Anfangsbedingungen ist zum Beispiel
uâ˛â˛ (t) + kuâ˛ (t) + Ď 2 u(t) = f (t),

u(0) = u0 ,

uâ˛ (0) = u1 ,

k, Ď > 0.

Definition 4.33. Sei U = C n (R â R) und V = C(R â R). Hierbei ist u â U genau dann, wenn u nâmal
differenzierbar ist, und die nâte Ableitung ist noch eine stetige Funktion. Seien a0 , a1 , . . . , anâ1 stetige
Funktionen von R nach R.
Eine Abbildung
L : U â V,
L : u 7â Lu = Lu(t) :=

dn
dnâ1
d
u(t)
+
a
(t)
u(t) + Âˇ Âˇ Âˇ + a1 (t) u(t) + a0 (t)u(t)
nâ1
n
nâ1
dt
dt
dt

heiĂt Differentialoperator der Ordnung n.
Wenn f â V gegeben ist, dann heiĂt
Lu = f
lineare Differentialgleichung der Ordnung n, und typischerweise ist die Funktion u gesucht. Falls f âĄ 0
(also f (t) = 0 fuĚr jedes t â R), dann reden wir von einer homogenen Gleichung, ansonsten von einer
inhomogenen Gleichung.
Frage: Was sind dim U und dim V ?
Im allgemeinen Fall haben wir es mit einem Anfangswertproblem
Lu = f,

(u(0), uâ˛ (0), . . . , u(nâ1) (0)) = (u0 , u1 , . . . , unâ1 )

(4.3)

zu tun, wobei f und die uj gegeben sind, und das u ist gesucht.
Satz 4.34. Wenn uâ eine spezielle LoĚsung des inhomogenen Problems Lu = f ist, dann liegen alle LoĚsungen
zu Lu = f in der Menge
uâ + ker L := {u â U : u â uâ â ker L}.
Beweis. Siehe Abschnitt 4.3.
Satz 4.35. Es ist dim ker L = n. Die Anfangswertabbildung, die eine Funktion u auf ihre verallgemeinerten
Anfangswerte zur Zeit 0 abbildet,
AWA : ker L â Rn ,

AWA : u 7â (u(0), uâ˛ (0), uâ˛â˛ (0), . . . , u(nâ1) (0)),
ist bijektiv.
Der Beweis dazu verteilt sich auf mehrere Schritte. Leider koĚnnen wir ihn mit unseren Kenntnissen nicht in
der hier behaupteten Allgemeinheit fuĚhren, sodaĂ wir uns stattdessen beim Beweis der SurjektivitaĚt auf die
Schwingungsgleichung beschraĚnken und den Beweis des allgemeinen Falls auf das dritte Semester vertagen.
Auf jeden Fall erhalten wir aus diesem Satz ein allgemeines Verfahren zur Bestimmung der LoĚsung des
Anfangswertproblems (4.3):
1. Finde eine spezielle LoĚsung uâ = uâ (t) zu Lu = f , zum Beispiel durch geschicktes Raten. Die
Anfangswerte fuĚr t = 0 sind im Moment egal.

4.5. DIFFERENTIALGLEICHUNGEN

85

2. Bestimme ker L, also alle Funktionen u â U mit Lu = 0.
3. WaĚhle dann aus ker L ein Element uh = uh (t) mit besonderen Anfangswerten, naĚmlich mit
(nâ1)
(nâ1)
(uâ (0), uâ˛â (0), . . . , uâ
(0)) + (uh (0), uâ˛h (0), . . . , uh
(0)) = (u0 , u1 , . . . , unâ1 ).
4. Setze u(t) = uâ (t) + uh (t).
Aus Satz 4.35 ergibt sich, daĂ es immer eine LoĚsung u gibt, und daĂ die so konstruierte LoĚsung tatsaĚchlich
die einzige LoĚsung ist.

Abbildung 4.2: Die drei VektorraĚume U = C n (R â R), V = C(R â R) und Rn , sowie die beiden linearen
Abbildungen L und AWA.
FuĚr den Beweis der InjektivitaĚt der Anfangswertabbildung brauchen wir ein Hilfsresultat:
Lemma 4.36. Sei u â C n (R â R) LoĚsung zu Lu = 0 mit den verallgemeinerten Anfangsbedingungen
(u(0), uâ˛ (0), . . . , u(nâ1) (0)) = (0, 0, . . . , 0). Dann ist u(t) = 0 fuĚr jedes t â R.
Beweis. Wir definieren uns eine (mathematische) Energie

1
E(t) :=
(u(t))2 + (uâ˛ (t))2 + . . . + (u(nâ1) (t))2
2

und untersuchen deren ZeitâAbleitung:

E â˛ = uuâ˛ + uâ˛ uâ˛â˛ + . . . + u(nâ1) u(n)
â¤ |u| Âˇ |uâ˛ | + |uâ˛ | Âˇ |uâ˛â˛ | + . . . + |u(nâ1) | Âˇ |u(n) |.

(4.4)

FuĚr saĚmtliche reellen Zahlen p und q gilt die Youngsche Ungleichung |p| Âˇ |q| â¤ 12 (p2 + q 2 ), was wir bald
sehr oft gebrauchen koĚnnen. Wir duĚrfen auch noch annehmen, daĂ die Koeffizientenfunktionen a0 , a1 , . . . ,
anâ1 im Betrag durch eine (notfalls groĂe) Zahl A beschraĚnkt sind:
|aj (t)| â¤ A

âj,

ât.

86

KAPITEL 4. HOMOMORPHISMEN

Pnâ1
Pnâ1
Dann ist (wegen Lu = 0) auch |u(n) (t)| = | j=0 aj (t)u(j) (t)| â¤ A j=0 |u(j) (t)|. Wir koĚnnen hier (Spiralprinzip) die Ungleichung von CauchyâSchwarz im Rn verwenden:
ďŁŤ
ďŁś ďŁŤ ďŁś
*
|u(t)|
1 +
nâ1
p
X
p
p
â
ďŁŹ
ďŁˇ ďŁŹ .. ďŁˇ
..
(j)
|u (t)| = ďŁ­
ďŁ¸, ďŁ­ . ďŁ¸ â¤ 2E(t) Âˇ 12 + Âˇ Âˇ Âˇ + 12 = 2E(t) Âˇ n,
.
j=0
1
|u(nâ1) (t)|
p
â p
also |u(n) (t)| â¤ nA 2E(t). Weiterhin ist |u(nâ1) (t)| â¤ 2E(t). Somit erhalten wir aus (4.4) und der
Ungleichung von Young

 â
 1

â â
1  (nâ2) 2
1 2
(u
) + (u(nâ1) )2 + 2E Âˇ nA 2E
u + (uâ˛ )2 +
(uâ˛ )2 + (uâ˛â˛ )2 + . . . +
2
2
2
â
â¤ 2E + 2 nAE.
â
Zur einfacheren Schreibweise setzen wir Îą := 2 + 2 nA und bekommen also E â˛ (t) â¤ ÎąE(t) fuĚr alle t â R.
Eâ˛ â¤

Ein freundlicher Geist erscheint uns im Traum und gibt uns den Hinweis, die Ableitung der Funktion
t 7â E(t)eâÎąt zu untersuchen:
E(t)eâÎąt

â˛

= E â˛ (t)eâÎąt + E(t) Âˇ (âÎą)eâÎąt â¤ ÎąE(t)eâÎąt â ÎąE(t)eâÎąt = 0 â¤ 0.

Also ist diese Funktion schwach monoton fallend, und somit ist, fuĚr t âĽ 0,
E(t)eâÎąt â¤ E(0)eâÎąÂˇ0 .
Nun ist allerdings E(0) = 0, also auch E(t) â¤ 0 fuĚr t âĽ 0. Weiterhin zeigt uns die Definition der Funktion
E, daĂ E gar nicht negativ werden kann. Damit ist also E(t) = 0 fuĚr jedes t âĽ 0.

Das erzwingt dann aber u(t) = 0 fuĚr alle t âĽ 0, nach Definition von E. Analog argumentiert man fuĚr
negative t.

Damit ist die InjektivitaĚt der verallgemeinerten Anfangswertabbildung fast schon gezeigt:
Satz 4.37. Die Abbildung
AWA : ker L â Rn ,

AWA : u 7â (u(0), uâ˛ (0), . . . , u(nâ1) (0))
ist injektiv und es ist dim ker L â¤ n.
Beweis. Es seien u und uĚ aus ker L mit AWA u = AWA uĚ. Sei z = u â uĚ. Dann ist z â ker L und
(z(0), z â˛ (0), . . . , z (nâ1) (0)) = (0, 0, . . . , 0). Nach Lemma 4.36 haben wir z(t) = 0 fuĚr jedes t â R, also
u = uĚ. Also ist die Anfangswertabbildung injektiv.
Die Dimensionsaussage folgt aus Satz 4.15, Teil 4, denn dim Rn = n.
Unsere jetzigen Kenntnisse reichen nicht aus, um die SurjektivitaĚt der Anfangswertabbildung im allgemeinen
Fall zu zeigen. Wir beschraĚnken uns stattdessen auf den Spezialfall der Schwingungsgleichung.
Satz 4.38. Sei U = C 2 (R â R), V = C(R â R) und Ď, k â R. Dann ist die Abbildung
 2

d
d
2
AWA : ker
+ k + Ď â R2 ,
dt2
dt
AWA : u 7â (u(0), uâ˛ (0))

surjektiv.
Beweis. Wir zeigen dim ker L = 2. Dann ist die Anfangswertabbildung eine injektive Abbildung von einem
zweidimensionalen Raum in einen zweidimensionalen Raum, nach Satz 4.19 also auch surjektiv.
Weil u â ker L, erfuĚllt u die Differentialgleichung
uâ˛â˛ (t) + kuâ˛ (t) + Ď 2 u(t) = 0.

4.6. AUSBLICK: LINEARE ABBILDUNGEN IN DER PHYSIK

87

Im Rest des Beweises konstruieren wir zwei linear unabhaĚngige LoĚsungen u+ , uâ â C 2 (R â R), also zwei
linear unabhaĚngige Elemente von ker L. Dann ist die Dimension von ker L also mindestens gleich 2, nach
Satz 4.37 also genau gleich 2.
Wir machen fuĚr die Funktion u den Ansatz
u(t) = eÎťt ,

t â R,

mit waĚhlbarem Parameter Îť. Wenn wir diesen Ansatz in die homogene Gleichung einsetzen, bekommen wir

eÎťt Îť2 + kÎť + Ď 2 = 0.

Weil die Exponentialfunktion nie Null wird, muĚĂte Îť2 + kÎť + Ď 2 = 0 sein, also
r
k
k2
Îť = Îť1,2 = â Âą
â Ď2.
2
4
Fall 1:

k2
4

â Ď2 > 0

Dann sind Îť1 , Îť2 â R und verschieden, also sind die Funktionen u+ (t) := eÎť1 t und uâ (t) := eÎť2 t linear
unabhaĚngig. Wir haben also zwei linear unabhaĚngige LoĚsungen u der homogenen Schwingungsgleichung
gefunden.
Fall 2:

k2
4

â Ď2 = 0

Dann ist Îť1 = Îť2 und die beiden Funktionen t 7â eÎť1 t und t 7â eÎť2 t sind linear abhaĚngig.

Man rechnet aber schnell nach, daĂ eine weitere LoĚsung gegeben wird durch t 7â teÎť1 t . Damit sind erneut
zwei linear unabhaĚngige LoĚsungen der Schwingungsgleichung gefunden.
Fall 3:

k2
4

â Ď2 < 0

In diesem Fall sind Îť1 und Îť2 nicht reell. Wir haben stattdessen
s
k
k2
Îť1,2 = Âľ Âą iÎ˝, Âľ = â , Î˝ =
â Ď2 ,
2
4
und dann gilt
eÎť1 t = eÂľt (cos(Î˝t) + i sin(Î˝t)),

eÎť2 t = eÂľt (cos(Î˝t) â i sin(Î˝t)).

Man rechnet nach, daĂ zwei linear unabhaĚngige LoĚsungen der Schwingungsgleichung gegeben werden durch
u+ (t) := eÂľt cos(Î˝t),

uâ (t) := eÂľt sin(Î˝t).

In allen drei FaĚllen kamen wir auf dim ker L âĽ 2, und der Beweis ist damit beendet.

4.6

Ausblick: Lineare Abbildungen in der Physik

Die uĚblichen Differentialoperatoren sind typische Beispiele fuĚr lineare Abbildungen zwischen FunktionenvektorraĚumen:
Gradient: fuĚr ein Skalarfeld Ď : R3 â R ist der Gradient definiert als


âĎ âĎ âĎ
âĎ =
.
,
,
âx1 âx2 âx3
Die Abbildung Ď 7â âĎ ist linear. Wir haben z.B.

?
â â Hom L2 (R3 â R) â L2 (R3 â R3 ) ,

dabei sollten wir aber aufpassen wegen des Definitionsbereiches von â: dieser Operator kann nur auf
solche Funktionen Ď aus dem L2 (R3 â R) angewandt werden, fuĚr die tatsaĚchlich âĎ im Bildraum
L2 (R3 â R3 ) liegt. (Welche Funktionen Ď das sind, wollen wir hier nicht weiter eroĚrtern. Wir deuten
lediglich diese Baustelle (bzw. dieses Minenfeld) durch ein Fragezeichen an.) Mit den Bezeichnungen
der HelmholtzâZerlegung haben wir dann
img â = L2pot (R3 â R3 ).

88

KAPITEL 4. HOMOMORPHISMEN

Divergenz: fuĚr ein Vektorfeld ~u : R3 â R3 ist die Divergenz definiert als
div ~u =

âu2 âu3
âu1
+
+
.
âx1
âx2
âx3

Die Abbildung ~u 7â div ~u ist linear. Wir haben z.B.

?
div â Hom L2 (R3 â R3 ) â L2 (R3 â R) ,

und auch hier sollten wir aufpassen wegen des Definitionsbereiches von div. Mit den Bezeichnungen
der HelmholtzâZerlegung haben wir dann
ker div = L2div (R3 â R3 ).
LaplaceâOperator: Dieser ist definiert als âł = div âŚâ. Aufgrund von Satz 4.9, Teil 2, ist er dann
automatisch linear.
Lineare Differentialoperatoren haben die wunderschoĚne physikalische Eigenschaft, daĂ fuĚr ihre LoĚsungen das Superpositionsprinzip gilt.
P und Q: Wir betrachten ein einzelnes spinloses Teilchen. Sein quantenmechanischer Zustand wird beschrieben durch eine Wellenfunktion Ď = Ď(t, x) : Rt Ă R3x â C. Diese liegt (fuĚr jede Zeit t) im
Vektorraum L2 (R3 â C), der ausgestattet ist mit dem Skalarprodukt hÂˇ, ÂˇiL2 wie in (2.4).
Wenn ein Teilchen einen Spin hat (mit Wert + 21 oder â 21 ), dann brauchen wir zwei Wellenfunktionen zur Beschreibung dieses Teilchens, also haben wir Ď = (Ď+ , Ďâ ) â (L2 (R3 â C))2 , und das
Skalarprodukt ist dann hĎ, Ďi := hĎ+ , Ď+ iL2 + hĎâ , Ďâ iL2 .

Bei M Teilchen (ohne Spin) nehmen wir als Vektorraum den L2 (R3M â C) mit naheliegendem
Skalarprodukt, und wenn diese Teilchen miteinander wechselwirken oder Spins haben koĚnnen, dann
wird es richtig kompliziert.
Alle diese VektorraĚume sind dem jeweiligen physikalischen System zugeordnet und heiĂen SystemhilbertraĚume. Ihre Dimension ist praktisch immer gleich unendlich.
Nun schauen wir uns MeĂgroĚĂen an, wie z.B. Ort, Impuls, Energie. Jeder solchen MeĂgroĚĂe (Observable) entspricht eine lineare Abbildung, man sagt auch linearer Operator. Diese Operatoren bilden
den Systemhilbertraum H (bzw. einen Untervektorraum davon) in H ab (bzw. in den H Ă H Ă H,
denn wir leben ja in einer dreidimensionalen Welt).
Ortsoperator Q: Es ist Q : Ď 7â QĎ, wobei (QĎ)(x) = xĎ(x). Wegen x = (x1 , x2 , x3 )â¤ â R3
zerlegen wir Q = (Q1 , Q2 , Q3 ) mit (Qj Ď)(x) = xj Ď(x).
Impulsoperator P : Es ist P : Ď 7â P Ď, wobei
(P Ď)(x) =

~
âĎ(x),
i

h
ist das PlanckâWirkungsquantum, mit h = 6.6260755 Âˇ 10â34Js. Wir zerlegen analog
und ~ = 2Ď
zu Q auch P als P = (P1 , P2 , P3 ).

Operator zur potentiellen Energie: Sei V = V (x) ein Potential (fuĚr das Wasserstoffatom z.B.
das CoulombâPotential), dann gehoĚrt dazu ein Multiplikationsoperator MV : Ď 7â MV Ď, wobei
(MV Ď)(x) = V (x)Ď(x).
â
Operator zur Gesamtenergie: Wir ersetzen E durch i~ ât
.

Die Energiegleichung E = Ekin + Epot =
i~

p2
2m

+ V wird dann zu

~2
â
Ď(t, x) = â
âł Ď(t, x) + V (x)Ď(t, x),
ât
2m

was die beruĚhmte SchroĚdingergleichung ist.
Die Wellenfunktion Ď ist als solche meĂtechnisch nicht auffindbar. Messen kann man nur die uĚblichen
GroĚĂen fuĚr ein Teilchen: Aufenthaltsort, Impuls, Energie, Drehimpuls usw., und Quantenmechanisch
sind diese MeĂwerte dann Eigenwerte der entsprechenden Operatoren. FuĚr den Ortsoperator Q1
waĚre ein solcher Eigenwert also eine Zahl xĚ1 â R mit Q1 Ď = xĚ1 Ď. Diese Eigenwerte zu Operatoren

4.6. AUSBLICK: LINEARE ABBILDUNGEN IN DER PHYSIK

89

entsprechen genau den Eigenwerten zu selbstadjungierten Matrizen, wie wir sie im zweiten Semester
behandeln werden.
Zum AbschluĂ erinnern wir daran, daĂ die Matrizenmultiplikation nicht kommutativ ist. Etwas aĚhnliches beobachten wir auch hier: die Operatoren P und Q vertauschen nicht miteinander, denn es ist
Pj âŚ Qj 6= Qj âŚ Pj , fuĚr j = 1, 2, 3. Die physikalische Konsequenz davon ist die UnschaĚrfenrelation
von Heisenberg, die besagt, daĂ man fuĚr ein Teilchen seinen Aufenthaltsort und seinen Impuls nicht
gleichzeitig beliebig genau messen kann.
Wir verlassen den Zoo der Differentialoperatoren und schauen uns einige physikalische GroĚĂen an:
WaĚrmeleitungstensor: Wir haben ein unbewegliches Medium, in dem WaĚrme stroĚmen kann, z.B. einen
FestkoĚrper (aber kein Gas oder Fluid). Die Temperatur am Ort x nennen wir T (x), was ein skalares
Feld ergibt. Der Gradient davon, also âT (x), ist ein Vektor, der am Punkt x angeheftet ist und in
Richtung des steilsten Temperaturanstiegs zeigt. Dieser Temperaturunterschied fuĚhrt normalerweise
zu einem WaĚrmefluĂ, der durch ein Vektorfeld beschrieben wird, das wir j nennen. Im Allgemeinen
gilt dann
j = âÎťâT,
wobei Îť meist eine Zahl ist, die die WaĚrmeleitfaĚhigkeit des Stoffes beschreibt. Es ist aber auch moĚglich,
daĂ der Stoff in unterschiedlichen Richtungen die WaĚrme unterschiedlich gut leitet. Zum Beispiel
ist es denkbar, daĂ Holz die WaĚrme besser parallel zur Faser leitet als quer dazu. Oder vielleicht
liegt ein Kristall vor, bei dem das Verhalten entlang der verschiedenen Achsen unterschiedlich ist.
Dann brauchen j und âT nicht mehr parallel zueinander sein, und Îť ist eine Matrix, die auch
WaĚrmeleitfaĚhigkeitstensor genannt wird. Wir stellen uns vor, daĂ dieser Tensor die Ursacheâ âT
â
auf die Wirkungâ j abbildet. Der isotrope Fall (daĂ alle Richtungen gleichberechtigt sind) ist mit
â
enthalten, wenn wir fuĚr Îť ein Vielfaches der Einheitsmatrix erlauben.
Verzerrungstensor: Ein FestkoĚrper wird irgendwie belastet und verformt sich ein wenig. Das kann man
sich so vorstellen, daĂ das Teilchen, was im Ruhezustand an der Position x war, durch die Verformung
gewandert ist an die Stelle x + u(x). Der Vektor u(x) heiĂt Verschiebungsvektor ; wir stellen ihn uns
als kurz vor. Bei einer Verschiebung des gesamten KoĚrpers ist u konstant, was uns nicht interessiert.
Bei einer Drehung des gesamten KoĚrpers ist (nach einer geeigneten Wahl des Koordinatensystems)
u(x) = Ax, wobei A eine Drehmatrix aus der Gruppe SO(3) ist. Das interessiert uns auch nicht, denn
wir wollen nur die Verzerrungen beschreiben, weshalb wir voraussetzen, daĂ unser KoĚrper solche
Bewegungen nicht ausfuĚhrt. (Das typische Beispiel ist ein Stab, an dessen beiden Enden gezogen
wird, sodaĂ er laĚnger wird, gleichzeitig aber duĚnner.) Unter dieser Voraussetzung bekommen wir,
daĂ ein kurzer Verbindungsvektor ~z zweier Punkte abgebildet wird auf einen Vektor Îľ~z, wobei Îľ der
sogenannte Verzerrungstensor ist, der den alten Verbindungsvektor vor der Deformation zum neuen
Verbindungsvektor nach der Deformation sendet.
Spannungstensor: Wir haben einen verformten FestkoĚrper und waĚhlen in seinem Innern ein kleines
FlaĚchenstuĚck, und auf einer Seite davon denken wir uns das Material gedanklich markiert. Dieses
gedanklich markierte Material kann auf das uĚbrige Material eine Kraft ausuĚben, die uĚber das kleine
FlaĚchenstuĚck vermittelt wird. Diese Kraft kann senkrecht zur FlaĚche druĚcken (Druckspannung), oder
senkrecht zur FlaĚche ziehen (Zugspannung) oder tangential zur FlaĚche schieben (Schubspannung),
oder alles vermischt bewirken. Der Spannungstensor Ď ist derjenige Tensor (also diejenige Matrix),
der den AuĂennormalenvektor des kleinen FlaĚchenstuĚcks auf den Kraftvektor abbildet.
ElastizitaĚtstensor: Wir stellen uns vor, daĂ zwischen Verzerrung und Spannung ein linearer Zusammenhang besteht, der dann wie folgt aussieht:
Ď = CÎľ.
Hierbei sind Ď und Îľ die obigen Tensoren zweiter Stufe mit jeweils 3 Ă 3 = 9 EintraĚgen, also ist C ein
Tensor vierter Stufe mit 9 Ă 9 = 81 EintraĚgen, der ElastizitaĚtstensor genannt wird. Aus SymmetriegruĚnden sind viele von diesen EintraĚgen gleich, sodaĂ nur 21 Materialkonstanten uĚbrigbleiben. Und
fuĚr einen isotropen FestkoĚrper sind es nur zwei Parameter, und es entsteht:


2
Cijkl = KÎ´ij Î´kl + G Î´ik Î´jl + Î´il Î´jk â Î´ij Î´kl .
3

90

KAPITEL 4. HOMOMORPHISMEN
Wir nennen K den Kompressionsmodul und G den Schermodul.
Wenn dieser FestkoĚrper jetzt beginnt zu schwingen, dann bekommen wir die Differentialgleichung fuĚr
die Verschiebungsvektorfunktion ~u = ~u(t, x):


G
â 2 ~u
â div ~u,
Ěş 2 = G âł ~u + K +
ât
3
wobei Ěş die Materialdichte bezeichnet.

Knobelaufgabe: Man zerlege mittels HelmholtzâProjektion das Verschiebungsvektorfeld ~u in einen longitudinalen und einen transversalen Anteil. Man beobachte, wie diese Zerlegung die obige komplizierte
Differentialgleichung erheblich vereinfacht. Man errate die Ausbreitungsgeschwindigkeiten der beiden Wellentypen.

4.7

Ausblick: VektorraĚume in der Physik 2

Sei U ein Vektorraum uĚber dem KoĚrper K. Die Menge aller linearen Abbildungen von U in den Vektorraum
K 1 heiĂt Dualraum von U . Dieser Dualraum ist seinerseits wiederum ein Vektorraum uĚber dem KoĚrper K.
Man stelle sich vor, daĂ ein Element des Dualraumes nichts anderes tut, als auf Vektoren aus U zu warten,
sie aufzufressen und Zahlen daraus zu machen.
Als Beispiel fuĚr U betrachten wir den R3 , dann ist K = R. Die Elemente von U nennen wir x = (x1 , x2 , x3 )â¤
als Spaltenvektoren. Wir stellen uns diese Vektoren als Ortsvektoren vor, mit MaĂeinheit Meter. Der Dual1
raum dazu besteht aus Zeilenvektoren k = (k1 , k2 , k3 ) mit MaĂeinheit Meter
. Jedes solche k â R3k erzeugt
eine Abbildung von R3x in den einheitenlosen R gemaĚĂ kx = k1 x1 + k2 x2 + k3 x3 . Diese Vektoren k nennt
man gelegentlich Wellenzahlvektoren.
Als typisches Beispiel denken wir an einen triklinen Kristall. Dieser hat 3 Achsen, deren Winkel zueinander
keine rechten Winkel sind, und die charakteristischen KantenlaĚngen sind unterschiedlich lang. Auf diesem
Wege bekommen wir eine Gitterstruktur, und an jedem Gitterknotenpunkt sitzt eine Elementarzelle. Wir
fuĚhren ein Koordinatensystem ein, wobei die Basisvektoren entlang der Gitterlinien zeigen. Die BasisvektorenlaĚnge ist genau gleich dem Abstand zweier benachbarter Gitterpunkte entlang einer Gitterlinie.
Wir bekommen auf diesem Wege drei Basisvektoren b1 , b2 , b3 , die keine ONB des R3 bilden. Jeder Vektor x
kann dann geschrieben werden als x = Îž 1 b1 + Îž 2 b2 + Îž 3 b3 (hierbei hat es sich eingebuĚrgert, die Indizes an die
Koordinaten oben zu schreiben, nicht unten). Wenn wir noch einen weiteren Vektor y = Îˇ 1 b1 + Îˇ 2 b2 + Îˇ 3 b3
haben und uns fuĚr das Skalarprodukt xÂˇy interessieren, dann ist dieses leider nicht gleich Îž 1 Îˇ 1 + Îž 2 Îˇ 2 + Îž 3 Îˇ 3 .
Stattdessen entstehen weitere AĚrgersummanden aus gemischten Produkten, eben weil keine ONB vorliegt.
Als Ausweg betrachten wir eine weitere Basis (b1 , b2 , b3 ), diesmal fuĚr den Dualraum, mit Indizes oben statt
unten. Wir verlangen, daĂ fuĚr die Skalarprodukte mit den Elementen der alten Basis gilt:
bj Âˇ bk = Î´kj ,

j, k = 1, 2, 3,

wobei rechts das Kroneckersymbol steht. Wenn wir jetzt y in der dualen Basis (man nennt sie auch
reziproke Basis) entwickeln, y = Îˇ1 b1 + Îˇ2 b2 + Îˇ2 b2 , dann ist tatsaĚchlich x Âˇ y = Îž 1 Îˇ1 + Îž 2 Îˇ2 + Îž 3 Îˇ3 .

Die reziproke Basis erzeugt in natuĚrlicher Weise ein neues Gitter, das in Bezug auf das Kristallgitter das
reziproke Gitter genannt wird. FuĚr die Untersuchung von z.B. Gitterschwingungen spielt dieses reziproke
Gitter eine ganz wichtige Rolle.
Wir rechnen damit noch ein biĂchen. Als Hauptregeln halten wir dabei fest:
â˘ Punkte Âˇ bezeichnen das Skalarprodukt von Vektoren,
â˘ GroĚĂen in lateinischen Buchstaben sind Vektoren (abgesehen von Indizes),
â˘ GroĚĂen in griechischen Buchstaben sind reelle Zahlen,
â˘ alle Summationen laufen von 1 bis 3 (die Summenkonvention von Einstein wird hier nicht praktiziert,
da im ersten Semester vielleicht zu verwirrend),
â˘ GroĚĂen mit unteren Indizes heiĂen kovariant, mit oberen Indizes heiĂen sie kontravariant (das ist
im allgemeinen Fall nicht ganz korrekt, in der hier vorliegenden Situation aber richtig. In Wirklichkeit beziehen sich die Begriffe kovariantâ und kontravariantâ darauf, nach welcher Formel sich die
â
â
betreffenden GroĚĂen transformieren bei einem Basiswechsel).

91

4.7. AUSBLICK: VEKTORRAĚUME IN DER PHYSIK 2

Gegeben sei jetzt eine kovariante Basis (b1 , b2 , b3 ) fuĚr den R3 , und wir suchen die dazugehoĚrige kontravariante Basis (b1 , b2 , b3 ). Das definierende Gleichungssystem ist eindeutig loĚsbar, sodaĂ wir schon wissen, daĂ
es diese kontravariante Basis tatsaĚchlich gibt, es fehlt bloĂ noch der Rechenweg. Wir setzen
Î˛ jk := bj Âˇ bk ,

Î˛jk := bj Âˇ bk ,

j, k = 1, 2, 3.

Die kovarianten Metrik-Koeffizienten Î˛jk sind bekannt, die kontravarianten Metrik-Koeffizienten Î˛ jk sind
es noch nicht.
P
Wir basteln uns einige Vektoren als Spielzeug, naĚmlich cj := l Î˛ jl bl . Dann ist
X
X
cj Âˇ b k =
Î˛ jl bl Âˇ bk =
Î˛ jl Î´lk = Î˛ jk = bj Âˇ bk .
l

l

Wir haben also (cj â bj ) âĽ bk fuĚr jedes k, demnach muĂ cj â bj = 0 â R3 sein, also cj = bj . Das bedeutet
X
bj =
Î˛ jl bl ,
l

und jetzt brauchen wir die cj nicht mehr. Wenn es uns gelingt, die Î˛ jl auszurechnen, dann haben wir die
gesuchte kontravariante Basis (b1 , b2 , b3 ). Dazu beobachten wir, daĂ
X
X
Î˛ jl Î˛lk =
Î˛ jl bl Âˇ bk = bj Âˇ bk = Î´kj ,
l

l

was nichts anderes bedeutet, als daĂ die Matrix der Î˛ jl und die Matrix der Î˛jl invers zueinander sind. Und
Matrizen invertieren koĚnnen wir ja.
Als naĚchstes Projekt bestimmen wir die Kooordinaten eines
(die beiden Basen
P
P Vektors. Sei x gegeben
natuĚrlich auch), und die Koordinaten Îž j und Îžj gemaĚĂ x = j Îž j bj sowie x = j Îžj bj seien gesucht. Man
verifiziert schnell, daĂ die LoĚsung gegeben wird durch folgende Formeln:
Îž j = x Âˇ bj ,

Îžj = x Âˇ bj ,

j = 1, 2, 3.

Und als SchluĂprojekt betrachten wir Basistransformationen: sei mit (b1 , b2 , b3 ) eine weitere kovariante
Basis gegeben:
X
bj :=
Îąkj bk ,
(4.5)
k

und wir wollen wissen, wie die Transformationsformeln fuĚr die anderen GroĚĂen sind. Die Unter- und Oberstriche bedeuten keinerlei Konjugationen oder aĚhnliches, sondern sollen lediglich das EinfuĚhren weiterer
Buchstaben vermeiden.
Die umgekehrte Transformation ist dann
X
Îąlk bl
bk =
l

mit noch unbekannten Îąlk , die sich aber bestimmen lassen durch Einsetzen und Koeffizientenvergleich:
X
Îąlk Îąm
bk =
l bm ,
l,m

P
m
also muĂ l Îąlk Îąm
sein, und demnach sind die Matrizen der Îąlk und Îąlk invers zueinander. Dann muĂ
l = Î´k P
m
â1
= I folgt Aâ1 A = I.
aber auch die Gleichung l Îąlk Îąm
l = Î´k gelten, denn aus AA
1

2

3

Nun suchen wir Formeln fuĚr die reziproke Basis (b , b , b ). Mit Phantasie, gestuĚtzt durch Probieren anhand
P
l
echter Zahlen (oder durch einen Ansatz), kommt man zur Vermutung, daĂ b = k Îąlk bk sein koĚnnte. Wir
testen dies anhand folgender Rechnung:
!
!
X
X
X
X
X
X
l
k
l k
l
l
Îąm
bj Âˇ
Îąlk bk =
Îąlk bk =
Âˇ
Îąm
Îąm
Îąm
j bm
j Îą k bm Âˇ b =
j Îąk Î´m =
j Îąm = Î´j ,
k

m

k

m,k

m,k

m

92

KAPITEL 4. HOMOMORPHISMEN

also gilt tatsaĚchlich
X
l
b =
Îąlk bk .

(4.6)

k

FuĚr die Transformation von Koordinaten eines Vektors x haben wir x =
l

l

Îž =xÂˇb =xÂˇ

X

Îąlk bk ,

P

l

l

Îž bl mit

k

woraus wir
X
l
Îž =
Îąlk Îž k

(4.7)

k

bekommen. Und fuĚr die kovarianten Koordinaten von x haben wir x =
Îž j = x Âˇ bj = x Âˇ

X

Îąkj bk

P

j

j

Îž j b mit

k

mit der Konsequenz
X
Îžj =
Îąkj Îžk .

(4.8)

k

Wir erkennen, daĂ (4.5) und (4.8) sehr aĚhnlich aussehen: die Transformation erfolgt mittels der Îą. SaĚmtliche
GroĚĂen, die sich so transformieren, bezeichnet man als kovariante Tensoren.
Und wir erkennen weiterhin, daĂ auch (4.6) und (4.7) einander sehr aĚhneln: die Transformation erfolgt
mittels der Îą. SaĚmtliche GroĚĂen, die sich nach diesem Schema transformieren bei Basiswechsel, nennt man
kontravariante Tensoren.
Das waren die AnfangsgruĚnde der Tensorrechnung, mehr Informationen findet man z.B. in Klingbeil,
Tensorrechnung fuĚr Ingenieure.

4.8

SchluĚsselbegriffe

â˘ Definition linearer Abbildungen, Differentialoperatoren als lineare Abbildungen,
â˘ Begriffe injektiv, surjektiv, bijektiv, Isomorphismus,
â˘ Kern, Bild, Rang,
â˘ Dimensionsformel und Folgerungen daraus,
â˘ Struktur der LoĚsungsmenge linearer Gleichungssysteme,
â˘ aĚhnliche Matrizen und deren Beziehung zu Basistransformationen,
â˘ LoĚsungsalgorithmus fuĚr lineare gewoĚhnliche Differentialgleichungen mit konstanten Koeffizienten, anhand der Schwingungsdifferentialgleichung.

Kapitel 5

Normierte RaĚume, Reelle Zahlen,
Folgen, Reihen
Die bisherigen Kapitel 2,3,4 behandelten die Lineare Algebra, deren Leitbegriffe Vektorraum und lineare
Abbildung lauten. In diesem und dem naĚchsten Kapitel soll es nun um die Analysis gehen, insbesondere wollen wir uns dann mit dem Differenzieren und Integrieren vertraut machen. Diese Techniken beruhen auf dem
Begriff des Grenzwert s, der also die Leitidee der Analysis ist. Wir entfernen uns aber nicht uĚbermaĚĂig weit
von der linearen Algebra, denn auch in VektorraĚumen kann man Folgen und deren Grenzwerte anschauen.

5.1

Folgen im Rd und Cd

Aus der Schule ist (so ungefaĚhr) bekannt, was darunter zu verstehen ist, wenn man sagt, daĂ eine Folge
(a1 , a2 , . . . ) reeller Zahlen gegen einen Grenzwert aâ konvergiert. Wir wollen den Konvergenzbegriff auf die
RaĚume Rd und Cd uĚbertragen; und deshalb fuĚhren wir in diesen RaĚumen eine Norm ein, um AbstaĚnde
messen zu koĚnnen.
Definition 5.1. Im Rd und im Cd definieren wir die pythagoraĚischen Normen kÂˇkRd und kÂˇkCd mittels
q
p
kzkCd := |z1 |2 + . . . + |zd |2 .
kxkRd := x21 + . . . + x2d ,
Im Folgenden schreiben wir immer K d mit K = R oder K = C.

Diese Normen besitzen die gleichen Eigenschaften wie in Satz 2.26 beschrieben. Unter dem Abstand zweier
Punkte verstehen wir die Norm von deren Differenzvektor.
Unter einer Folge im Raum K d verstehen wir einen Ausdruck der Form (a1 , a2 , a3 , . . . ) =: (an )nâN mit
an â K d . Mathematisch praĚzise werden wir den Begriff dann in Definition 5.16 festlegen.
Definition 5.2. Eine Folge (an )nâN â K d heiĂt konvergent mit Grenzwert aâ 1 , wenn
lim kan â aâ kK d = 0.

nââ

Das ist mathematisch definiert als:
âÎľ > 0 âN0 (Îľ) : ân âĽ N0 (Îľ) : kan â aâ kK d < Îľ.
Wir lesen diese Formelzeile folgendermaĂen: FuĚr jedes2 positive Îľ existiert ein N0 (das von Îľ abhaĚngen
â
darf ), sodaĂ fuĚr jedes n âĽ N0 gilt, daĂ kan â aâ kK d kleiner als Îľ istâ.
Zu verstehen ist diese Formelzeile wie folgt:
Wir koĚnnen den Abstand von an und aâ beliebig klein bekommen (kan â aâ kK d < Îľ)
â und zwar beliebig klein (âÎľ > 0) â
wenn wir nur vorher sicherstellen, daĂ n genuĚgend groĂ (âN0 (Îľ) : ân âĽ N0 (Îľ)) ist.
Dabei sind alle an mit n âĽ N0 (Îľ) hoĚchstens Îľ von aâ entfernt, nicht bloĂ einige an (ân âĽ N0 (Îľ)).
1 convergent
2

with limit aâ
die gelegentliche Lesung fuĚr alle anstelle fuĚr jedes ist grammatisch und inhaltlich falsch.

93

94

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Eine Merkregel fuĚr den Konvergenzbegriff ist:
Ein Element aâ ist Grenzwert einer Folge (an )nâN ,
wenn es zu jeder Umgebung von aâ ein N0 gibt,
sodaĂ das FolgenendstuĚck ab N0 in dieser Umgebung liegt.
Eine Umgebung von aââ ist ein Intervall mit Mittelpunkt aâ . Ein FolgenendstuĚck ab N0â sind alle
â
â
Folgenglieder ab aN0 .
Der Zweck des folgenden Satzes besteht auch darin, die Argumentationstechnik der sogenannten Epsiâ
lontikâ einzuuĚben. Wir erkennen, daĂ man durch konsequentes Anwenden der ÎľâN0 âSprache tatsaĚchlich
nuĚtzliche Aussagen zeigen kann. Wenn man sich an diese Ausdrucksweise erstmal gewoĚhnt hat, dann ist es
gar nicht mehr so schwierig:
Satz 5.3. Konvergente Folgen haben genau einen Grenzwert.
Beweis. Wir nehmen das Gegenteil an und machen uns auf die Suche nach einem Widerspruch: Sei (an )nâN
eine konvergente Folge mit 2 verschiedenen Grenzwerten aâ und aââ . Dann ist laut Definition
âÎľ > 0 âN0,â (Îľ) : ân âĽ N0,â (Îľ) : kan â aâ kK d < Îľ,

âÎľ > 0 âN0,ââ (Îľ) : ân âĽ N0,ââ (Îľ) : kan â aââ kK d < Îľ.
Nun waĚhlen wir Îľ :=

1
17

kaâ â aââ kK d > 0. Sei nun n âĽ N0,â (Îľ) und n âĽ N0,ââ (Îľ). Dann haben wir

kaâ â aââ kK d = kaâ â an + an â aââ kK d â¤ kaâ â an kK d + kan â aââ kK d < Îľ + Îľ =
und jetzt duĚrfen wir durch die positive Zahl kaâ â aââ kK d kuĚrzen: also 1 <

2
17 ,

2
kaâ â aââ kK d ,
17

was absurd ist.

Definition 5.4. Sei (an )nâN eine Folge von Elementen einer Menge M , und sei (k(n))nâN eine streng
monoton wachsende Folge natuĚrlicher Zahlen. Dann heiĂt die Folge
(ak(n) )nâN
Teilfolge3 der Folge (an )nâN .
Satz 5.5. Wenn eine Folge gegen einen Grenzwert konvergiert, dann konvergiert auch jede ihrer Teilfolgen
gegen denselben Grenzwert.
Beweis. UĚbungsaufgabe.
Satz 5.6. Seien (an )nâN und (bn )nâN zwei konvergente Folgen in K d mit Grenzwerten aâ und bâ , und
seien Îą, Î˛ â K. Dann konvergiert auch die Folge (Îąan + Î˛bn )nâN , und zwar gegen Îąaâ + Î˛bâ .
Beweis. ZunaĚchst haben wir fuĚr saĚmtliche n âĽ 1, daĂ
k(Îąan + Î˛bn ) â (Îąaâ + Î˛bâ )kK d â¤ kÎą(an â aâ )kK d +kÎ˛(bn â bâ )kK d = |Îą| kan â aâ kK d +|Î˛| kbn â bâ kK d .
Laut Voraussetzung und Definition ist
âÎľ > 0 âN0,a (Îľ) : ân âĽ N0,a (Îľ) : kan â aâ kK d < Îľ,
âÎľ > 0 âN0,b (Îľ) : ân âĽ N0,b (Îľ) : kbn â bâ kK d < Îľ.
Sei nun ein positives Îľ gegeben, und wir suchen ein N0,+ (Îľ), sodaĂ fuĚr jedes n âĽ N0,+ (Îľ) gilt, daĂ
!

k(Îąan + Î˛bn ) â (Îąaâ + Î˛bâ )kK d < Îľ.
Wenn wir dieses N0,+ (Îľ) erfolgreich gebaut haben, dann ist der Beweis komplett. Mit der obigen Zerlegung
erkennen wir, daĂ folgendes N0,+ (Îľ) unsere WuĚnsche erfuĚllt:





Îľ
Îľ
, N0,b
,
N0,+ (Îľ) := max N0,a
2|Îą|
2|Î˛|
zumindest in dem Fall, daĂ weder Îą noch Î˛ gleich Null ist. Dieser Sonderfall Îą Âˇ Î˛ = 0 sei den Studierenden
als UĚbungsaufgabe ans Herz gelegt.
3 sub-sequence

5.1. FOLGEN IM RD UND CD

95

Warnung 5.7. Die Regel
lim (Îąan + Î˛bn ) = Îą lim an + Î˛ lim bn

nââ

nââ

nââ

gilt nur dann, wenn beide Grenzwerte auf der rechten Seite existieren ! Als Beispiel betrachte man Îą = Î˛ = 1
sowie (a1 , a2 , a3 , . . . ) = (1, 2, 1, 2, 1, 2, 1, 2, . . . ) und (b1 , b2 , b3 , . . . ) = (6, 5, 6, 5, 6, 5, 6, 5, . . . ).
Definition 5.8. Sei M â K d . Ein Punkt aâ â K d heiĂt HaĚufungspunkt von M 4 , wenn es eine Folge
(an )nâN â M \ {aâ }

gibt mit aâ = limnââ an .

Die folgende Vereinigungsmenge heiĂt AbschluĂ5 von M :
M = M âŞ {alle HaĚufungspunkte von M }.
Beispiele:
â˘ K d = R und M = (0, 1],

â˘ K d = R2 und M = {x : kxkK d < R}.

Definition 5.9. Eine Menge M ist abgeschlossen6 , wenn jeder HaĚufungspunkt von M in M liegt.
Beispiele: FuĚr K d = R betrachte man M = [0, 1] bzw. M = [0, 1) bzw. M = [0, â).
Definition 5.10. Sei x0 â K d und Îľ â R+ . Die Menge
BÎľ (x0 ) = {x â K d : kx â x0 kK d < Îľ}

heiĂt ÎľâUmgebung7 von x0 â K d oder auch ÎľâBall um x0 .

Definition 5.11. Sei M â K d eine Menge. Ein Punkt x0 â K d heiĂt innerer Punkt8 von M , wenn
es eine Umgebung BÎľ (x0 ) â M gibt. Ein Punkt x0 â K d heiĂt Randpunkt9 von M , wenn in jeder Îľâ
Umgebung BÎľ (x0 ) ein Punkt in M und ein Punkt in K d \ M liegen. Die Menge aller Randpunkte wird mit
âM bezeichnet.
Definition 5.12. Eine Menge M â K d ist offen10 , wenn jeder Punkt von M ein innerer Punkt von M
ist.
Satz 5.13. Eine Menge M ist abgeschlossen genau dann, wenn âM â M .

Eine Menge M â K d ist offen genau dann, wenn K d \ M abgeschlossen ist.
Beweis. Das schaffen Sie selbst.
Man sagt auch umgangssprachlich:
Bei einer abgeschlossenen Menge gehoĚrt der Rand dazu.
Bei einer offenen Menge gehoĚrt der Rand nicht dazu.
Als SonderfaĚlle haben wir M = â und M = K d . Diese Mengen sind gleichzeitig offen und abgeschlossen.
Es gibt auch Mengen in R, die weder offen noch abgeschlossen sind, z.B. M = (0, 1].
Definition 5.14. Eine Menge M â K d heiĂt beschraĚnkt11 , wenn es eine Konstante C â R gibt, sodaĂ
kxkK d â¤ C fuĚr jedes x â M gilt. Also
âC â R : âx â M : kxkK d â¤ C.
Das bedeutet anschaulich, daĂ jede beschraĚnkte Menge in einer passend groĂ gewaĚhlten Kugel untergebracht
werden kann. Der Kugelradius C darf von der betreffenden Menge M abhaĚngen.
Frage: Sei M = Q â R1 die Menge der rationalen Zahlen im Vektorraum R1 . Was sind die Randpunkte
von M ? die HaĚufungspunkte ? die inneren Punkte ?
4 cluster

point of M

5 closure
6 closed
7

Îľâneighbourhood
point
9 boundary point
10 open
11 bounded
8 inner

96

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

5.2

Folgen und Reihen in normierten RaĚumen

Das charakteristische Merkmal der VektorraĚume Rd und Cd ist, daĂ sie endlichdimensional sind. In den
Betrachtungen des vorigen Abschnitts haben wir aber diese EndlichdimensionalitaĚt nirgendwo benutzt.
Deshalb werden wir jetzt allgemeinere VektorraĚume verwenden, die auch unendlichdimensional sein koĚnnen.
Dabei lassen wir uns von den Anwendungen in der Physik leiten, denn der fuĚr die Quantenmechanik hoĚchst
wichtige Vektorraum L2 (R3 â C) ist ja unendlichdimensional.
Um AbstaĚnde in VektorraĚumen messen zu koĚnnen, ist es hilfreich, eine Norm zu haben, was uns dann zum
Konzept der normierten RaĚume fuĚhren wird. Insbesondere betrachten wir folgende normierten RaĚume:
Rd , Cd , K pĂq , C k (R â Rd ), . . . .
Da ab jetzt U ein beliebiger Vektorraum sein wird, uĚber den nichts bekannt ist (abgesehen von den Eigenschaften, die in der Definition von abstrakten VektorraĚumen gefordert werden), koĚnnen wir auch den
Begriff einer Norm nur abstrakt definieren:
Definition 5.15. Sei U ein Vektorraum uĚber einem KoĚrper K, K = R oder K = C. Eine Abbildung
kÂˇk : U â R,
kÂˇk : u 7â kuk
heiĂt Norm von U , wenn gilt:
â˘ kuk âĽ 0 fuĚr jedes u â U ; und kuk = 0 genau dann, wenn u = 0,
â˘ kÎťuk = |Îť| Âˇ kuk, fuĚr jedes u â U und jedes Îť â K,
â˘ ku + vk â¤ kuk + kvk fuĚr jegliche u, v â U .
Das Paar (U, kÂˇk) heiĂt normierter Raum12 .
Jeder normierte Raum ist ein Vektorraum. Die Umkehrung gilt nicht. Anschaulich bedeutet kuk die LaĚngeâ
â
des Vektors u.
Beispiele: FuĚr U = Rd sind folgende Normen gebraĚuchlich:
q
kxk2 := x21 + Âˇ Âˇ Âˇ + x2d ,
kxk1 := |x1 | + Âˇ Âˇ Âˇ + |xd |,
kxkâ := max |xj |.
j=1,...,d

Analog fuĚr U = Cd . FuĚr Matrizen A â K pĂq benutzt man unter anderem
sX
|aij |2 ,
oder auch kAk := max |aij |.
kAk :=
i,j

i,j

FuĚr den Funktionenraum U = C([a, b] â R) erscheinen folgende Normen plausibel:
kf kâ := max |f (x)|,
xâ[a,b]
s
Z b
kf k2 :=
f (x)2 dx,
a

kf k1 :=

Z

a

b

|f (x)|dx.

In Wirklichkeit ist die kÂˇkâ âNorm etwas anders definiert; die obige Formel ist aber nicht falsch, wenn die
Funktion f auf [a, b] stetig ist (wie wir es ja vorausgesetzt hatten).
Und fuĚr den Raum U = C 1 ([a, b] â R) verwendet man im Allgemeinen kf kC 1 := kf kâ + kf â˛ kâ .
Definition 5.16. Sei M eine Menge. Eine Abbildung
N â M,

n 7â an

heiĂt Folge13 von Elementen aus M .
12 normed

space

13 sequence

5.2. FOLGEN UND REIHEN IN NORMIERTEN RAĚUMEN

97

Mit Hilfe der jetzt bereitgestellten Begriffe Normâ und Folgeâ definieren wir anschlieĂend die Begriffe
â
â
Konvergenzâ, Teilfolge,â HaĚufungspunktâ, AbschluĂâ, abgeschlossene Mengeâ, ÎľâUmgebungâ, inâ
â
â
â
â
â
â
nerer Punktâ, Randpunktâ, offene Mengeâ, beschraĚnkte Mengeâ genauso wie im vorigen Abschnitt (wir
â
â
â
ersetzen einfach uĚberall K dâ durch Uâ). Die SaĚtze 5.3, 5.5, 5.6 und 5.13 gelten auch jetzt.
â
â
Beispiel 5.17. Sei U der Vektorraum der auf [0, 1] vernuĚnftig integrierbarenâ Funktionen (diese ungenaue
â
Formulierung ist unvermeidlich), sei fn = fn (x) := xn , und sei fâ (x) := 0 fuĚr 0 â¤ x < 1 sowie fâ (1) := 1.
â˘ fuĚr jedes feste x â [0, 1] ist limnââ fn (x) = fâ (x), mit Konvergenz im normierten Raum R,
â˘ wir haben auch limnââ fn = fâ mit Konvergenz in der kÂˇk1 âNorm fuĚr U , denn kfn â fâ k1 =

1
n+1 ,

â˘ sei g = g(x) := 0 die Nullfunktion auf [0, 1], also g(x) = 0 insbesondere auch fuĚr x = 1. Dann
1
. (Nun ist aber
ist limnââ fn = g mit Konvergenz in der kÂˇk1 âNorm fuĚr U , denn kfn â gk1 = n+1
g 6= fâ . Wie versoĚhnen Sie das mit dem zweiten â˘ und mit Satz 5.3 ?)
â˘ gemessen in der kÂˇkâ âNorm konvergiert die Folge der fn nirgendwohin, auch nicht nach fâ oder g,
denn kfn â fâ kâ = kfn â gkâ = 1 fuĚr alle n.

5.2.1

VollstaĚndigkeit

Aus Beispiel 5.17 lernen wir, daĂ wir im Falle eines unendlichdimensionalen Vektorraumes U bei der Wahl
der Norm von U Vorsicht walten lassen sollten. Ansonsten koĚnnte es passieren, daĂ der entstehende normierte Raum nicht vollstaĚndig ist, was einige unangenehme Konsequenzen mit sich braĚchte.
Definition 5.18. Eine Folge (an )nâN â U heiĂt CauchyâFolge14 genau dann, wenn:
âÎľ > 0 âN0 (Îľ) : ân, m âĽ N0 (Îľ) : kan â am kU < Îľ.
Definition 5.19. Ein normierter Raum (U, kÂˇkU ) heiĂt vollstaĚndig15 , wenn jede CauchyâFolge (an )nâN â
U einen Grenzwert aâ â U hat. Ein vollstaĚndiger normierter Raum heiĂt auch Banachraum16 . Ein
vollstaĚndiger euklidischer bzw. unitaĚrer Raum heiĂt Hilbertraum17 .
Jeder Hilbertraum ist ein Banachraum. Jeder Banachraum ist ein normierter Raum. Die Umkehrungen
gelten beide nicht.
Beispiel: Der Raum (Q, | Âˇ |) ist ein normierter Vektorraum uĚber dem KoĚrper Q, aber nicht vollstaĚndig.
Als Gegenbeispiel betrachten wir
(a1 , a2 , . . . ) = (3, 3.1, 3.14, 3.141, 3.1415, 3.14159, . . .),
definiert als Folge der abgehackten Dezimalbruchentwicklung von Ď. Der Grenzwertâ dieser Folge ist Ď 6â Q.
â
Diese Cauchyfolge hat also keinen Grenzwert in Q ! Wir stellen uns dies so vor, daĂ genau an der Stelle,
wo der Grenzwert dieser Folge sein muĚĂte (wenn es ihn gaĚbe), der Vektorraum Q leider ein Loch hat.
Satz 5.20. Der Raum (R, | Âˇ |) als normierter Raum uĚber dem KoĚrper R ist vollstaĚndig. Er ist sogar der
kleinste vollstaĚndige Raum, der als Erweiterung von Q gewonnen werden kann.
Beweis. Wir koĚnnen ihn hier nicht fuĚhren, da wir die reellen Zahlen nirgends definiert hatten. Stattdessen
begnuĚgen wir uns mit der Bemerkung, daĂ R genau als VervollstaĚndigung von Q definiert wurde.
Die linearen RaĚume Rd , Cd sind mit jeder der Normen kÂˇk1 , kÂˇk2 und kÂˇkâ vollstaĚndig, genauso wie der
Raum der Matrizen mit jeder der obigen Normen. Dies folgt alles aus der VollstaĚndigkeit von R.
Satz 5.21. Der Raum der auf dem Intervall [a, b] stetigen Funktionen, C([a, b] â R), versehen mit der
kÂˇkâ âNorm, ist vollstaĚndig.
Beweis. Kommt spaĚter.
14 Cauchyâsequence
15 complete
16
17

Stefan Banach, 1892â1945, polnischer Mathematiker
David Hilbert, 1862â1943, deutscher Mathematiker

98

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Frage: Man zeige durch ein Beispiel, daĂ der Raum C([a, b] â R), versehen mit der kÂˇk1 âNorm, nicht
vollstaĚndig ist. Analog fuĚr die kÂˇk2 âNorm.

Die UnvollstaĚndigkeit des Raumes der stetigen Funktionen unter der kÂˇk1 â bzw. kÂˇk2 âNorm ist unerwuĚnscht.
Andererseits sind diese Normen so wichtig, daĂ man nicht auf sie verzichten kann. Eine solche Funktion
koĚnnte z.B. ein Geschwindigkeitsfeld sein, und eine physikalisch naheliegende Norm waĚre dann die kinetische
Energie (oder vielmehr die Wurzel daraus), die diesem Geschwindigkeitsfeld innewohnt. Das ist aber exakt
eine L2 âNorm. Der Ausweg ist, den Raum der stetigen Funktionen solange zu ergaĚnzen, bis ein vollstaĚndiger
Raum entsteht. Diese RaĚume bezeichnet man dann als L1 ([a, b] â R) bzw. L2 ([a, b] â R); und sie bestehen
aus all jenen Funktionen, fuĚr die die Integrale (im Lebesgue18 âSinne)
Z

b

x=a

|f (x)|dx

bzw.

Z

b

x=a

|f (x)|2 dx

endlich sind. FuĚr die Theorie des LebesgueâIntegrales ist bei den Mathematikstudierenden die gesamte
MaĂtheorieâVorlesung des dritten Semesters reserviert, und wir verzichten hier auf saĚmtliche Einzelheiten.
Satz 5.22. Konvergente Folgen sind CauchyâFolgen.
Beweis. Sei (an )nâN eine konvergente Folge mit Grenzwert aâ . Wir haben:
âÎľ > 0 âN0 (Îľ) : ân âĽ N0 (Îľ) : kan â aâ k < Îľ.
Wir wollen zeigen, daĂ:
âÎľ > 0 âN1 (Îľ) : ân, m âĽ N1 (Îľ) : kan â am k < Îľ.
Gesucht ist ein N1 (Îľ), sodaĂ die Aussage in der vorigen Zeile gilt. Wir waĚhlen N1 (Îľ) := N0 (Îľ/2). Dieses
N1 hat die gewuĚnschten Eigenschaften, denn: Seien nun n, m âĽ N1 (Îľ). Dann haben wir
kan â am k = kan â aâ + aâ â am k â¤ kan â aâ k + kaâ â am k <

Îľ Îľ
+ = Îľ.
2 2

Also ist die Folge (an )nâN tatsaĚchlich eine CauchyâFolge.
Die Umkehrung dieses Satzes gilt bekanntlich nicht.
Satz 5.23. CauchyâFolgen sind beschraĚnkt.
Beweis. Man waĚhle in der Definition der CauchyâFolgen Îľ := 1, m := N0 (1), schreibe sich die Definition
des CauchyâFolgenâBegriffs genau hin, und schon steht es da.
Satz 5.24. Sei (an )nâN eine konvergente Folge in einem normierten Raum U . Dann gilt
lim an

nââ

U

= lim kan kU .
nââ

Beweisskizze. In jedem normierten Raum U gilt die Ungleichung
kxkU â kykU â¤ kx â ykU .
Wir setzen x := limnââ an und y := an , und jetzt ist es nicht mehr schwierig.
Frage: Warum gilt (5.1) ?
18 Henri

LeĚon Lebesgue, 1875â1941

(5.1)

99

5.2. FOLGEN UND REIHEN IN NORMIERTEN RAĚUMEN

5.2.2

Reihen in normierten RaĚumen

Definition 5.25. Sei (U, kÂˇk) ein normierter Raum und (an )nâN eine Folge in U . Unter einer Reihe19
â
X

an

n=0

verstehen wir zwei Dinge (gleichzeitig):
â˘ einerseits die Folge der Partialsummen (SN )N âN , wobei SN =

PN

n=0

an ,

â˘ andererseits den Grenzwert S = limN ââ SN , aber nur, falls dieser existiert.
Pâ
Wenn die Folge der Partialsummen (SN )N âN gegen S konvergiert, dann
sagt man, daĂ die Reihe n=0 an
Pâ
konvergiert und den Wert S hat. Ansonsten sagt man, daĂ die Reihe n=0 an divergiert.
Pâ
Definition 5.26. Sei n=0 an eine Reihe von Elementen aus einem normierten Raum U . Wenn die Reihe
von reellen Zahlen
â
X
kan kU
n=0

konvergiert, dann nennt man die Reihe
Beispiel 5.27. Sei U = R.

Pâ

n=0

an absolut konvergent20.

P
k
1. Die geometrische Reihe â
k=0 q konvergiert genau dann, wenn |q| < 1. Dann ist ihr Grenzwert gleich
1âqN +1
1
1âq , denn fuĚr die Partialsumme SN gilt die Formel SN =
1âq , und somit ist limN ââ SN = 1,
aber nur wenn |q| < 1.
Pâ
2. Die harmonische Reihe k=1 k1 divergiert, denn wenn N = 2k ist mit k â N+ , dann ist SN > k2 , und
somit ist limN ââ SN = +â.
Pâ 1
1 2
3. Die Reihe
k=1 k2 konvergiert (warum ?), und zwar gegen 6 Ď (Beweis im 2. Semester mittels
Fourierreihen).
4. Die Reihe 1 â 12 + 31 â 14 + 51 â . . . konvergiert (und zwar gegen ln 2, vgl. Satz 6.72), aber nicht absolut
(warum ?).

Frage: Vollenden Sie die Argumentation dieses Beispiels in den Unterpunkten 1., 2. und 3. Die Ungleichung
1
1
k2 < k(kâ1) koĚnnte nuĚtzlich sein.
Alle wichtigen Funktionen (Winkelfunktionen, Arcusfunktionen, Exponentialfunktionen, Logarithmusfunktionen) lassen sich mit Potenzreihen definieren (wie man das genau macht, schauen wir uns spaĚter an):
Beispiel 5.28 (Potenzreihe). Sei U = C([â1, 1] â R) oder U = C(B1 (0) â C), versehen mit der
kÂˇkâ âNorm. Die Reihe
â
X
1 k
x ,
k!

k=0

|x| â¤ 1,

konvergiert in U , und zwar gegen die Exponentialfunktion (Beweis folgt). Konvergenz gilt auch in der kÂˇk1 â
bzw. kÂˇk2 âNorm.
In der Signaltechnik sind Reihen von Winkelfunktionen wichtig:
Beispiel 5.29 (Fourierreihe). Sei U = L2 ([âĎ, Ď] â R). Im 2. Semester werden wir erkennen: Die Reihe


4
sin(3x) sin(5x) sin(7x)
sin(x) +
+
+
+ ...
Ď
3
5
7
konvergiert in der kÂˇk2 âNorm gegen die 2Ďâperiodisch fortgesetzte Funktion ( Rechtecksignalâ)
â
(
1
: 0 < x < Ď,
x 7â S(x) :=
â1 : â Ď < x < 0.
19 series
20 absolutely

convergent

100

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Frage: Warum konvergiert diese Reihe nicht in der kÂˇkâ âNorm ?
Pâ
Satz 5.30. Sei n=0 an eine Reihe, die im normierten Raum U konvergiert. Dann ist limnââ an = ~0U .

Beweis. Die Folge (SN )N âN der Partialsummen konvergiert, ist also eine CauchyâFolge, wegen Satz 5.22.
Die Definition des Begriffes CauchyâFolgeâ sagt uns jetzt:
â
âÎľ > 0 âN0 (Îľ) : âN, M âĽ N0 (Îľ) : kSN â SM kU < Îľ.
Wir waĚhlen nun M := N + 1 und erhalten
âÎľ > 0 âN0 (Îľ) : âN âĽ N0 (Îľ) : kaN +1 kU < Îľ.
Also streben die aN gegen ~0U , denn kaN +1 kU = aN +1 â ~0U

U

.

Warnung 5.31. Die Umkehrung dieses Satzes ist falsch (und ein beliebter Fehler).
Absolute Konvergenz ist besser als Konvergenz â das ist die naĚchste Aussage. Siehe auch Abschnitt 5.3.3.
Pâ
Satz 5.32. Wenn die Reihe n=0 an in einem BanachâRaum U absolut konvergiert, dann konvergiert sie
dort auch (im Sinne von Definition 5.25).
Beweis. Die Folge (ka1 k , ka1 k + ka2 k , ka1 k + ka2 k + ka3 k , . . . ) konvergiert, ist also eine CauchyâFolge.
Das bedeutet:
âÎľ > 0 âN0 (Îľ) : âm, n âĽ N0 (Îľ) (m âĽ n) : kan k + kan+1 k + Âˇ Âˇ Âˇ + kam k < Îľ.
Sei nun (SN )N âN die Partialsummenfolge der Reihe

Pâ

n=0

an . Dann ist fuĚr m > n

kSm â Sn k = kan+1 + an+2 + Âˇ Âˇ Âˇ + am k â¤ kan+1 k + kan+2 k + Âˇ Âˇ Âˇ + kam k .
Also ist (SN )N âN eine CauchyâFolge und folglich konvergent, denn U ist ein Banachraum.
Eine Reihe in einem Banachraum konvergiert genau dann,
wenn beliebig lange ReihenendteilstuĚcke beliebig klein werden.
Pâ
PM
Ein ReihenendstuĚck waĚre n=N an , und ein ReihenendteilstuĚck ist n=N an = SM â SN â1 . Hierbei darf
die Anzahl der Summanden (also M â N + 1) beliebig groĂ sein. Die Konvergenz der Reihe ist ja gerade
gleichbedeutend damit, daĂ die Partialsummenfolge eine CauchyâFolge ist.
P
Pâ
Satz 5.33. Seien â
Raum mit Reihenn=0 an und
n=0 bn zwei konvergente Reihen in einem normierten
Pâ
werten sa und sb . Seien weiterhin Îą, Î˛ â K. Dann konvergiert auch die Reihe n=0 (Îąan + Î˛bn ), und zwar
gegen den Wert Îąsa + Î˛sb .
Beweis. Folgt sofort aus dem entsprechenden Satz fuĚr Folgen, also Satz 5.6.

5.2.3

Konvergenzkriterien

Um zu entscheiden, ob Reihen konvergieren, benutzt man uĚblicherweise Vergleichskriterien:
Pâ
Satz 5.34 (Allgemeines Majorantenkriterium). Sei
n=0 an eine Reihe in einem Banachraum U
(konvergent oder divergent). Wenn es eine Folge (Îłn )nâN reeller Zahlen gibt mit den zwei Eigenschaften:
â˘ es existiert ein N , sodaĂ kan kU â¤ Îłn fuĚr jedes n âĽ N ,
Pâ
â˘ die Reihe n=0 Îłn konvergiert,

dann konvergiert auch die Reihe

Pâ

n=0

an , sogar absolut.

5.2. FOLGEN UND REIHEN IN NORMIERTEN RAĚUMEN

101

Beweis. Das Konvergenzverhalten einer Reihe (divergent/konvergent/absolut konvergent) aĚndert sich nicht,
wenn man am Anfang einige an weglaĚĂt. P
Wir duĚrfen also annehmen, daĂ die Reihe erst mit aN beginnt.
Pâ Sei
â
nun (Sn )nâĽN die Partialsummenfolge zu n=N kan kU , und (Ďn )nâĽN die Partialsummenfolge zu n=N Îłn .
Weil die Reihe uĚber die Îłn konvergiert, wissen wir, daĂ die Ďn eine CauchyâFolge bilden:
âÎľ > 0 âN0 (Îľ) : ân, m âĽ N0 (Îľ) : |Ďn â Ďm | < Îľ.
Nun ist (fuĚr n âĽ m)
|Sn â Sm | = kam+1 kU + Âˇ Âˇ Âˇ + kan kU â¤ Îłm+1 + Âˇ Âˇ Âˇ + Îłn = |Ďn â Ďm |.
Also ist auch (Sn )nâĽN eine CauchyâFolge in R.
Beispiel 5.35. Wir betrachten im Raum U = C((ââ, â) â R) die FourierâReihe
sin(x) +

sin(2x) sin(3x) sin(4x)
+
+
+ ...
4
9
16

und fragen nach ihrer Konvergenz in der kÂˇkâ âNorm. Wir haben
an =

sin(nx)
n2

und kan kU = n12 . Wir koĚnnen jetzt Îłn := n12 waĚhlen, was eine konvergente Reihe ergibt. Also konvergiert
obige Fourierreihe.
Pâ 1
konvergiert wegen n! = 1 Âˇ 2 Âˇ 3 Âˇ Âˇ Âˇ Âˇ n âĽ 2nâ1 .
Beispiel 5.36. Die Reihe n=1 n!

Folgerung 5.37. Wir vergleichen eine beliebige Reihe mit der geometrischen Reihe:

Pâ
â˘ Wenn es C > 0, N â N und p < 1 gibt mit kan kU â¤ Cpn fuĚr alle n âĽ N , dann konvergiert n=0 an
absolut in U ,
Pâ
â˘ Wenn es C > 0, N â N und p > 1 gibt mit kan kU âĽ Cpn fuĚr alle n âĽ N , dann divergiert n=0 an .

Der erste Teil folgt aus Satz 5.34, der zweite folgt aus Satz 5.30.
Pâ
Satz 5.38 (Wurzelkriterium). Sei n=0 an eine Reihe in einem Banachraum U . Sei q â R definiert
durch
q
q = lim n kan kU
nââ

wobei wir voraussetzen, daĂ dieser Limes existiert. Dann gilt:
q < 1: die Reihe

q > 1: die Reihe

Pâ

n=0

Pâ

n=0

an konvergiert absolut,
an divergiert,

q = 1: das Konvergenzverhalten ist unbekannt.
p
Beweis. Wir setzen p := 21 (1 + q). FuĚr q < 1 haben wir ab einem N0 die Ungleichung n kan kU â¤ p < 1,
p
und fuĚr q > 1 haben wir ab einem N0 die Ungleichung n kan kU âĽ p > 1. Nun liften wir das in die n-te
Potenz und verwenden Folgerung 5.37.
Das folgende Quotientenkriterium ist gelegentlich einfacher anzuwenden. Andererseits gibt es Beispiele, in
denen das Quotientenkriterium versagt, aber das Wurzelkriterium noch eine Aussage liefert.
P
Satz 5.39 (Quotientenkriterium). Sei â
n=0 an eine Reihe in einem Banachraum U , und keiner der
~
Summanden sei 0. Sei q â R definiert durch
q = lim

nââ

kan+1 kU
,
kan kU

wobei wir voraussetzen, daĂ dieser Limes existiert. Dann gilt:

102

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

q < 1: die Reihe
q > 1: die Reihe

Pâ

n=0

Pâ

n=0

an konvergiert absolut,
an divergiert,

q = 1: das Konvergenzverhalten ist unbekannt.
Beweis. Sei q < 1. Wir setzen p := 12 (1 + q) < 1, also q < p < 1. Dann gibt es ein N0 , sodaĂ fuĚr m âĽ N0 die
Quotienten kam+1 kU / kam kU unterhalb von p liegen. Multiplikation passend vieler Ungleichungen diesen
Typs liefert
kaN0 +n kU â¤ kaN0 kU pn ,

n âĽ 0.

Wir benutzen nun Folgerung 5.37 mit C = kaN0 kU , und die absolute Konvergenz der Reihe
sofort.
FuĚr den Fall q > 1 verwende man den zweiten Teil von Folgerung 5.37.

Pâ

n=0

an folgt

Beispiel 5.40. Sei U = R. FuĚr jedes x â (0, â) betrachten wir die Reihe reeller Zahlen
â
X
1
(â2x)k
k!
k=0

und fragen, fuĚr welche x Konvergenz vorliegt. Wir haben (fuĚr fixiertes x)
an =

1
(â2x)n ,
n!

also |an | =

1
(2x)n ,
n!

und bekommen demnach
|an+1 |
= lim
nââ
nââ |an |

q = lim

1
n+1
(n+1)! (2x)
1
n
n! (2x)

= lim

nââ

2x
= 0,
n+1

also Konvergenz. Damit konvergiert diese Potenzreihe fuĚr jedes x â (0, â) absolut.

5.3

Folgen und Reihen reeller Zahlen

Die Menge der reellen Zahlen bildet zusammen mit der uĚblichen Betragsfunktion einen vollstaĚndigen normierten Raum. Das bedeutet, das alle Ergebnisse des vorigen Abschnitts auch auf Folgen und Reihen von
reellen Zahlen angewandt werden koĚnnen.
Andererseits haben die reellen Zahlen auch Eigenschaften, die man in einem beliebigen normierten Raum
meist nicht zur VerfuĚgung hat. Insbesondere:
â˘ reelle Zahlen kann man ordnen,
â˘ reelle Zahlen kann man multiplizieren.
Aus diesem Grunde ist die Theorie der Folgen und Reihen reeller Zahlen etwas reichhaltiger.
Wir erinnern an ein einfaches Ergebnis, das zum Beispiel mittels vollstaĚndiger Induktion gezeigt werden
kann (und uĚbrigens auch in C gilt):
Satz 5.41 (Binomische Formel). FuĚr reelle Zahlen a, b und n â N0 gilt
(a + b)n =

n  
X
n k nâk
a b
.
k

k=0

Hierbei steht

n
k



fuĚr den Binomialkoeffizienten,

 
n
n!
n Âˇ (n â 1) Âˇ . . . Âˇ (n â k + 1)
=
=
,
k
k!(n â k)!
1 Âˇ 2 Âˇ... Âˇk

0! = 1.

5.3. FOLGEN UND REIHEN REELLER ZAHLEN

5.3.1

103

Schranken und Grenzen

In allgemeinen normierten RaĚumen hatten wir beschraĚnkte Mengen definiert. Im R haben wir zusaĚtzlich
noch einseitig beschraĚnkte Mengen:
Definition 5.42. Sei M â R eine Menge reeller Zahlen. Wenn es eine Zahl S â R gibt mit der Eigenschaft,
daĂ x â¤ S fuĚr jedes x â M , dann heiĂt S obere Schranke21 von M . In diesem Falle sagt man, daĂ die
Menge M nach oben beschraĚnkt22 ist. Entsprechend definiert man untere Schranken23 und den Begriff
nach unten beschraĚnkt24 .
Jede Zahl, die groĚĂer ist als eine obere Schranke einer Menge, ist ebenfalls eine obere Schranke dieser
Menge.
Definition 5.43. Sei M eine Menge reeller Zahlen. Wenn es eine Zahl S â R gibt, die gleichzeitig obere
Schranke von M und Element von M ist, dann heiĂt S Maximum25 von M . Analog definiert man das
Minimum26 .
Eine Menge kann zwar unendlich viele verschiedene obere oder untere Schranken haben, aber hoĚchstens ein
Maximum bzw. Minimum.
Beispiel 5.44. Die Menge M = {x â R : 0 < x < 1} =: (0, 1) hat weder Minimum noch Maximum. Sie ist
nach oben beschraĚnkt durch S = 1 und nach unten beschraĚnkt durch S = 0.
Definition 5.45. Sei M â R eine nach oben beschraĚnkte Menge. Eine Zahl S â R heiĂt Supremum bzw.
kleinste obere Schranke bzw. obere Grenze27 wenn folgendes gilt:
1. S ist obere Schranke von M ,
2. jede kleinere Zahl als S ist keine obere Schranke von M .
Entsprechend definiert man die Begriffe Infimum bzw. groĚĂte untere Schranke bzw. untere Grenze28 .
Beispiel 5.46. Die Menge M = (0, 1) hat Infimum 0 und Supremum 1.
Die Menge M = {x â Q : x2 < 2} hat in den rationalen Zahlen weder Supremum noch Infimum, obwohl sie
nach oben und unten beschraĚnkt ist.
Satz 5.47. Jede nichtleere nach oben beschraĚnkte Menge reeller Zahlen hat genau ein Supremum in R.
Jede nichtleere nach unten beschraĚnkte Menge reeller Zahlen hat genau ein Infimum in R.
Bevor wir zum Beweis dieses zentralen Satzes kommen, schieben wir ein nuĚtzliches Ergebnis ein:
Satz 5.48. Sei (an )nâN eine monoton wachsende Folge reeller Zahlen, die nach oben beschraĚnkt ist. Dann
hat diese Folge einen Grenzwert, und dieser Grenzwert ist gleich dem Supremum der Menge {an : n â N}.
Hierbei verstehen wir unter monoton wachsend29 , daĂ an â¤ an+1 fuĚr jedes n gilt. Ein entsprechender Satz
gilt fuĚr monoton fallende Folgen, die nach unten beschraĚnkt sind.
Beweis. Sei an â¤ an+1 und an â¤ S fuĚr jedes n â N. Wir wollen zeigen, daĂ die Folge (an )nâN eine
CauchyâFolge ist, also
âÎľ > 0 âN0 (Îľ) : ân, m âĽ N0 (Îľ) : |an â am | < Îľ.
Wir nehmen das Gegenteil an und suchen einen Widerspruch. Das Gegenteil ist
âÎľ0 > 0 : âN ân = n(N ), m = m(N ) âĽ N : |an â am | âĽ Îľ0 .
21 upper

bound
from above
23 lower bounds
24 bounded from below
25 maximum
26 minimum
27 supremum, least upper bound
28 infimum, greatest lower bound
29 monotonically increasing
22 bounded

104

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Wir kommen auf diesem Wege zu einer steigenden Folge von Indizes
k1 < k2 < k3 < k4 < k5 < k6 . . .
mit limjââ kj = â, sodaĂ
|ak2 â ak1 | âĽ Îľ0 ,

|ak4 â ak3 | âĽ Îľ0 ,

|ak6 â ak5 | âĽ Îľ0 , . . . .

Andererseits ist ak1 â¤ ak2 â¤ ak3 â¤ ak4 â¤ . . . . Dann muĂ zwangslaĚufig die Folge der an die Schranke
S irgendwann uĚberschreiten. Das ist ein Widerspruch. Also ist die Folge (an )nâN eine CauchyâFolge und
somit konvergent. Die Aussage uĚber das Supremum sieht man schnell.
Beweis des Satzes 5.47. Wir zeigen nur die Aussage uĚber das Supremum.
Man uĚberlegt sich schnell, daĂ eine Menge keine zwei verschiedenen Suprema haben kann.
Die Menge hat mindestens ein Element und mindestens eine obere Schranke; diese taufen wir x1 und S1 .
Es gilt x1 â¤ S1 . Wenn x1 = S1 sein sollte, dann ist dies gerade das Supremum, und wir waĚren fertig. Sei
also jetzt x1 < S1 . Wir taufen das arithmetische Mittel dieser beiden Zahlen Z.
Es gibt 2 FaĚlle:
Z ist obere Schranke von M : dann setzen wir x2 := x1 und S2 := Z.
Z ist keine obere Schranke von M : dann gibt es ein Element von M , das zwischen Z und S1 liegt.
Wir nennen dieses Element x2 , und setzen S2 := S1 .
Auf jeden Fall haben wir jetzt 2 Zahlen x1 , x2 , die Element von M sind, und zwei obere Schranken S1 und
S1 . Es gilt
x1 â¤ x2 â¤ S2 â¤ S1 ,

|x2 â S2 | â¤

1
|x1 â S1 |.
2

Mit dem Intervall [x2 , S2 ] koĚnnen wir verfahren wie eben. Wir finden ein Element x3 â M und eine obere
Schranke S3 , sodaĂ gilt
x1 â¤ x2 â¤ x3 â¤ S3 â¤ S2 â¤ S1 ,

|x3 â S3 | â¤

1
|x1 â S1 |.
4

Induktiv setzen wir dieses Verfahren fort und erhalten eine monoton wachsende Folge (xn )nâN und eine
monoton fallende Folge (Sn )nâN . Da die wachsende Folge (xn )nâN nach oben beschraĚnkt ist durch S1 , hat
diese Folge einen Grenzwert xâ . Analog hat die monoton fallende Folge (Sn )nâN einen Grenzwert S â .
Da aber zusaĚtzlich die Folge der Differenzen (xn â Sn )nâN nach 0 strebt, muĂ xâ = S â sein.
FuĚr jedes Îľ > 0 gilt dann:

â˘ im Intervall (xâ â Îľ, xâ ) ist mindestens ein Element von M ,
â˘ im Intervall (xâ , xâ + Îľ) ist mindestens eine obere Schranke von M .
Die erste Aussage bedeutet, daĂ keine Zahl unterhalb von xâ jemals obere Schranke von M sein kann. Die
zweite Aussage bedeutet, daĂ oberhalb von xâ kein Element von M sein kann. Insbesondere ist dann xâ
eine obere Schranke von M ; und nach der ersten Aussage ist dies die kleinste obere Schranke von M .
Das folgende Ergebnis ist fuĚr spaĚtere Zwecke gedacht:
Satz 5.49 (Bolzano30 âWeierstraĂ31 ). Sei M â R ein Intervall mit folgenden beiden Eigenschaften:
â˘ M ist beschraĚnkt,
â˘ M ist abgeschlossen.
30
31

Bernard Placidus Johann Nepomuk Bolzano, 1781â1848
Karl Theodor Wilhelm WeierstraĂ, 1815â1897

5.3. FOLGEN UND REIHEN REELLER ZAHLEN

105

Sei (an )nâN eine Folge, die in M liegt; also an â M fuĚr jedes n â N. Dann besitzt diese Folge eine Teilfolge,
die gegen einen Grenzwert in M strebt.
Beweis. Wir teilen das Intervall M =: M1 in zwei gleichgroĂe Teilintervalle. In einem davon liegen unendlich
viele Glieder der Folge (an )nâN . Dieses Teilintervall nennen wir M2 . Nun teilen wir M2 in zwei gleichgroĂe
Teilintervalle. In einem davon liegen unendlich viele Folgenglieder, dieses nennen wir M3 . In diesem Stile
fahren wir fort und erhalten eine Folge von Intervallen
M1 â M2 â M3 â Âˇ Âˇ Âˇ ,
von denen jedes halb so lang ist wie das vorhergehende. In jedem dieser Intervalle liegen unendlich viele
Elemente der Folge.
Aus M1 waĚhlen wir ein Element ak1 der Folge. Aus M2 waĚhlen wir ein weiteres Element ak2 der Folge,
das von ak1 verschieden sein soll. Aus M3 waĚhlen wir ein Folgenglied ak3 , das von den beiden vorigen
verschieden sein soll. Wir bekommen auf diesem Wege eine Teilfolge (akj )jâN . Weil alle Glieder dieser
Teilfolge ab j = N0 im Teilintervall MN0 enthalten sind, ist diese Folge offensichtlich eine CauchyâFolge.
Diese hat einen reellen Grenzwert, der nach Konstruktion ein HaĚufungspunkt des Intervalles ist. Weil das
Intervall abgeschlossen ist, liegt der Grenzwert im Intervall.
Bemerkung 5.50. Dieser Satz kann verallgemeinert werden. Es reicht, daĂ M â Rd eine beschraĚnkte und
abgeschlossene Menge ist, damit jede Folge in M eine in M konvergente Teilfolge enthaĚlt.
Definition 5.51. Sei U ein Banachraum. Eine Menge M â U heiĂt kompakt32 , wenn jede Folge in M
eine Teilfolge enthaĚlt, die einen Grenzwert in M hat.
Wir fassen zusammen:
â˘ in jedem Banachraum gilt: wenn eine Teilmenge kompakt ist, dann ist sie auch beschraĚnkt und
abgeschlossen (das uĚberlegt man sich schnell).
â˘ im Rd gilt: wenn eine Teilmenge beschraĚnkt und abgeschlossen ist, dann ist sie auch kompakt (das
haben wir in Bemerkung 5.50 schon erwaĚhnt).
â˘ in jedem unendlichdimensionalen Banachraum gilt: es gibt Teilmengen, die beschraĚnkt und abgeschlossen sind, aber leider nicht kompakt.
Relevant fuĚr die Physik ist diese Betrachtung aus folgendem Grund: in gewisser Hinsicht ist die Quantenmechanik eine Form von linearer Algebra in unendlichdimensionalen RaĚumen. Jetzt beobachten wir aber,
daĂ in solchen RaĚumen die Begriffe kompakt sowie beschraĚnkt & abgeschlossen logisch nicht aĚquivalent
sind. Das ist unangenehm, aber unvermeidlich. FuĚr Mathematikstudierende gibt es im vierten Semester
eine Funktionalanalysisvorlesung, in der (mit hohem Aufwand !) solche Fragen eroĚrtert und beantwortet
werden. Auf die Einzelheiten muĚssen wir aus ZeitgruĚnden verzichten.

5.3.2

Beispiele fuĚr konvergente Folgen

Bei Folgen und Reihen gibt es prinzipiell (mindestens) zwei Fragen zu beantworten:
Konvergiert die Folge/Reihe uĚberhaupt ? Antworten dazu werden gegeben durch Majorantenkriterien oder durch Satz 5.48.
Wohin konvergiert sie ? Oder: wie lautet der Grenzwert genau ? Ein Werkzeug fuĚr die Antwort
zu dieser Frage kommt jetzt.
Satz 5.52 (Sandwichprinzip). Es seien (an )nâN , (bn )nâN , (cn )nâN Folgen reeller Zahlen mit den Eigenschaften, daĂ
â˘ an â¤ bn â¤ cn fuĚr jedes n â N,
â˘ die Limites limnââ an und limnââ cn existieren und sind beide gleich g â â R.
32 compact

106

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Dann existiert auch der Grenzwert limnââ bn ; und er ist ebenfalls gleich g â .
Beweis. FuĚr jedes Îľ > 0 suchen wir ein N0 (Îľ), sodaĂ |bn â g â | < Îľ gilt fuĚr jedes n âĽ N0 (Îľ).

Wir wissen aus der Voraussetzung, daĂ

âÎľ > 0 âN1 (Îľ) : ân âĽ N1 (Îľ) : |an â g â | < Îľ,
âÎľ > 0 âN2 (Îľ) : ân âĽ N2 (Îľ) : |cn â g â | < Îľ.
Wir waĚhlen N0 (Îľ) := max(N1 (Îľ), N2 (Îľ)). Dann haben wir fuĚr n âĽ N0 (Îľ):
g â â Îľ < an â¤ bn â¤ cn < g â + Îľ,
was den Beweis vollendet.
Als Anwendung des Sandwichprinzips haben wir folgendes Ergebnis:
â
Lemma 5.53. Es ist limnââ n n = 1.
â
Beweis. Offensichtlich ist n n âĽ 1 fuĚr n âĽ 17. Also koĚnnen wir fuĚr jedes solche n schreiben:
â
n
n = 1 + xn , wobei xn âĽ 0.
Wir wenden die binomische Formel an:
 
 
n  
X
n k
n(n â 1) 2
n 0
n 2
n = (1 + xn )n =
xn âĽ
xn +
x =1+
xn .
k
0
2 n
2
k=0

x2n nach xn umstellen, kommen wir auf
Wenn wir diese Ungleichung n âĽ 1 + n(nâ1)
2
r
2
0 â¤ xn â¤
, fuĚr jegliches n âĽ 17.
n
Damit sind die xn eingequetscht
zwischen zwei Folgen, die beide nach Null streben. Also ist limnââ xn = 0
â
und somit limnââ n n = 1.
Satz 5.54. Seien (an )nâN und (bn )nâN zwei konvergente Folgen reeller Zahlen mit Grenzwerten aâ und bâ .
Dann konvergiert auch die Folge (an bn )nâN , und ihr Grenzwert ist aâ bâ .
Wenn zusaĚtzlich noch bn 6= 0 fuĚr jedes n â N und bâ 6= 0, dann konvergiert auch die Folge (an /bn )nâN , und
ihr Grenzwert ist aâ /bâ .
Beweis. Wir beweisen nur die Aussage uĚber das Produkt, die zweite Aussage laĚĂt sich aĚhnlich zeigen. Die
entscheidende Idee ist das EinfuĚgen einer fruchtbaren Null :
|an bn â aâ bâ | = |an bn â an bâ + an bâ â aâ bâ | â¤ |(an â aâ )bâ | + |an (bn â bâ )|.
Nun sind die Folgen (an )nâN und (bn )nâN beschraĚnkt (wegen Satz 5.22 und Satz 5.23), also
|an | â¤ A,

|bn | â¤ B,

n â N.

Dieselbe BeschraĚnkung gilt dann auch fuĚr die Grenzwerte. Wir haben somit
|an bn â aâ bâ | â¤ B|an â aâ | + A|bn â bâ |,
woraus sich die Behauptung unmittelbar ergibt.
Bemerkung 5.55. Mit einem aĚhnlichen Beweis kann man zeigen: die Produktfolge einer Nullfolge und
einer beschraĚnkten Folge ist wieder eine Nullfolge. Hierbei verstehen wir unter einer Nullfolge eine solche
Folge, die nach Null strebt.
Beispiel 5.56. Seien k, x â R. Die Folge (nk xn )nâN konvergiert gegen 0 fuĚr |x| < 1 und divergiert fuĚr
|x| > 1. Wir fassen dies zusammen in folgender Merkregel (wobei wir sprachlich etwas unscharf sind):
Im Konfliktfall ist ein Exponentialterm staĚrker als ein Potenzterm.

5.3. FOLGEN UND REIHEN REELLER ZAHLEN

5.3.3

107

Nichtabsolute Konvergenz und Umordnungen

Wir kennen aus dem Majorantenkriterium bereits einen Mechanismus, der die Konvergenz einer Reihe
bewirkt: die Folge der Summanden klingt schnell genug ab.
Es gibt noch einen weiteren Mechanismus, der die Konvergenz einer Reihe nach sich zieht: die AusloĚschung.
P
Satz 5.57 (Leibniz33 âKriterium). Sei â
n=0 an eine Reihe reeller Zahlen. Wenn die an die folgenden
Bedingungen erfuĚllen:
â˘ die Folge der |an | strebt streng monoton nach Null,
â˘ an an+1 < 0 fuĚr jedes n â N,
dann konvergiert die Reihe

Pâ

n=0

an .

Man beachte, daĂ die Folge der Summanden an auch extrem langsam nach Null streben darf, ohne die
Reihenkonvergenz in Gefahr zu bringen.
Beweis. Die zweite Bedingung besagt, daĂ die Summanden an alternierende Vorzeichen haben. Zum Beweis
betrachten wir die Partialsummen sn . Seien die a2n positiv, und die a2n+1 negativ. Dann haben wir
s2n+2 â s2n = a2n+2 + a2n+1 < 0,
also ist die Folge (s0 , s2 , s4 , . . . ) streng monoton fallend. Analog zeigt man, daĂ die Folge (s1 , s3 , s5 , . . . )
streng monoton steigend ist. Weiterhin ist sn+2 â sn+1 > 0, sodaĂ die fallende Folge (s0 , s2 , . . . ) nach
unten beschraĚnkt ist durch s1 . Also konvergiert sie. Analog ist die wachsende Folge (s1 , s3 , . . . ) nach oben
beschraĚnkt durch s0 , also konvergent. Beide Grenzwerte muĚssen uĚbereinstimmen wegen an â 0.
Wir bleiben noch etwas bei solchen Reihen mit Summanden wechselnden Vorzeichens. Es gilt (wie wir
spaĚter sehen werden)
ln 2 = 1 â

1
1 1 1 1 1 1 1 1
+ â + â + â + â
â ... .
2 3 4 5 6 7 8 9 10

Durch Verdoppelung ist dann
2 2 2 2 2 2 2 2
2
+ â + â + â + â
â ...
2 3 4 5 6 7 8 9 10
2 1 2 1 2 1 2 1
= 2 â 1 + â + â + â + â â ...
3 2 5 3 7 4 9 5

2 ln 2 = 2 â

Wenn man die BruĚche mit gleichem Nenner zusammenfaĂt, folgt
2 ln 2 = 1 â

1
1 1 1 1 1 1 1 1
+ â + â + â + â
â Âˇ Âˇ Âˇ = ln 2.
2 3 4 5 6 7 8 9 10

Wo steckt der Fehler ?
Pâ
Satz 5.58. Sei S eine beliebige
Zahl. Dann gibt es eine Reihe n=0 an , die sich durch Umsortieren
Pâ reelle nâ1
1
der Summanden der Reihe n=1 (â1)
n ergibt und den Reihenwert S hat.

Die Umsortierung haĚngt natuĚrlich von S ab. Der Satz besagt also, daĂ zumindest bei dieser Reihe das
Kommutativgesetz und das Assoziativgesetz leider nicht gelten.
Beweisskizze. Die Teilreihen 1 + 13 + 51 + . . . und â 12 â 14 â 16 â . . . der positiven und negativen Summanden
divergieren beide.
Dieses PhaĚnomen kann nicht auftreten, wenn die Ausgangsreihe absolut konvergiert.
Pâ
Satz 5.59. Sei n=0 an eine absolut konvergierende Reihe. Dann konvergiert auch jede Umordnung dieser
Reihe absolut und hat denselben Reihenwert. Es gilt auch die Umkehrung: wenn jede Umordnung der Reihe
denselben Reihenwert hat, dann ist die Ausgangsreihe absolut konvergent.
33

Gottfried Wilhelm von Leibniz, 1646â1716

108

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Beweis. Lassen wir weg.
Die Frage derPKonvergenz von
Pâumgeordneten Reihen ist keineswegs akademisch. Wenn man zum Beispiel
â
zwei Reihen
a
und
n=0 n
k=0 bk multiplizieren will, dann moĚchte man die einzelnen Produkte an bk
bilden, in eine geeignete Reihenfolge bringen, und in dieser Reihenfolge dann aufsummieren. Nun gibt es
aber keine Reihenfolge dieser Teilprodukte, die richtigerâ waĚre als eine andere Reihenfolge. Falls bei einer
â
anderen Anordnung der Teilprodukte sich ein anderer Reihenwert ergaĚbe, waĚre dies natuĚrlich schlecht.
Pâ
Pâ
Satz 5.60.
k=0 bk zwei absolut konvergente Reihen. Dann konvergiert die
n=0 an und
PâEs seien
Reiheâ
a
b
bei
beliebiger
Anordnung
der Summanden an bk gegen denselben Wert, naĚmlich
n,k=0 n k
P
âPâ
â
( n=0 an )( k=0 bk ).
Beweis. Lassen wir weg.

Es hat sich eine spezielle Anordnung eingebuĚrgert, die auf die sogenannte CauchyâProduktreihe fuĚhrt. Das
Konzept der CauchyâProduktreihe wird sich als ganz natuĚrlich herausstellen, wenn wir uns im naĚchsten
Abschnitt Potenzreihen anschauen.
Pâ
Pâ
Definition 5.61. Es seien n=0 an und k=0 bk zwei absolut konvergente Reihen. Wir definieren eine
Folge (cm )mâN0 gemaĚĂ
X

cm :=

k+n=m

Dann heiĂt

5.4

an bk = a0 bm + a1 bmâ1 + Âˇ Âˇ Âˇ + amâ1 b1 + am b0 ,

Pâ

m=0 cm

die CauchyâProduktreihe der beiden Reihen

m â N0 .
Pâ

n=0

an und

Pâ

k=0 bk .

Potenzreihen

Definition 5.62. Unter einer Potenzreihe34 verstehen wir eine Reihe der Form
â
X

n=0

an (z â z0 )n ,

an , z, z0 â C.

Hierbei denken wir uns z0 als fixiert und z als variabel. FuĚr den Summanden mit n = 0 vereinbaren wir,
daĂ 00 := 1.
Ein Beispiel ist die geometrische Reihe
â
X

z n,

n=0

z â C.

Diese konvergiert genau fuĚr |z| < 1, wie man zum Beispiel mithilfe des Wurzelkriteriums sieht. Das Konvergenzgebiet ist in diesem Falle also eine Kreisscheibe mit Radius 1 um z0 = 0. GemaĚĂ Definition 5.10
verwenden wir fuĚr diese Kreisscheibe die Schreibweise B1 (0).
Diese Potenzreihe kann man von 2 Standpunkten aus betrachten.
â˘ Einerseits kann man jeden Summanden z n als stetige Funktion ansehen, also zum Beispiel als Element
eines Vektorraumes U = C(BR (0) â C) (wobei R eine passend gewaĚhlte positive Zahl sei), diesen
Raum mit der kÂˇkâ âNorm ausstatten (wodurch er zu einem Banachraum wird), und dann nach der
Konvergenz der Reihe in diesem Banachraum fragen.
â˘ Andererseits koĚnnte man z â C festhalten. Dann erhaĚlt man eine Reihe von komplexen Zahlen, die
konvergieren kann oder auch nicht.
Wir vermerken nur kurz, daĂ die geometrische Reihe genau dann im Banachraum U = C(BR (0) â C)
konvergiert, wenn R < 1, und werden uns ab jetzt auf den zweiten Aspekt konzentrieren.
34 power

series

109

5.4. POTENZREIHEN

Pâ
Definition 5.63. Sei n=0 an (z â z0 )n eine Potenzreihe, und sei M â C eine beliebige Menge. Wir sagen,
daĂ die Potenzreihe auf der Menge M gleichmaĚĂig gegen P (z) konvergiert35, wenn gilt:
âÎľ > 0 âN0 (Îľ) : ân âĽ N0 (Îľ), âz â M :

n
X

k=0

ak (z â z0 )k â P (z) < Îľ.

Entscheidend ist dabei, daĂ die Schranke N0 (Îľ) nicht von z â M abhaĚngt, sondern fuĚr alle solchen z
dasselbe N0 (Îľ) verwendet werden kann. Dann kann man die letzte Formelzeile umschreiben zu
âÎľ > 0 âN0 (Îľ) : ân âĽ N0 (Îľ) : sup

zâM

n
X

k=0

ak (z â z0 )k â P (z) < Îľ,

und das wiederum kann logisch aĚquivalent umformuliert werden zu
âÎľ > 0 âN0 (Îľ) : ân âĽ N0 (Îľ) :

n
X

k=0

ak (Âˇ â z0 )k â P (Âˇ)

< Îľ,
â

denn genauso ist die kÂˇkâ âNorm in Bezug auf die Menge M definiert. Weil wir es jetzt mit Funktionen zu
tun haben, ist das z in der Notation verschwunden und ein JokerâZeichen Âˇ an seine Stelle getreten.
Die gleichmaĚĂige Konvergenz ist gleichbedeutend mit der Konvergenz in der kÂˇkâ âNorm.
P
n
Beispiel 5.64. Wir betrachten nochmal die geometrische Reihe â
n=0 z . Diese konvergiert auf der Menge
M1 := {z â C : |z| < 1}, aber dort nicht gleichmaĚĂig. Die Konvergenz wird beliebig langsam, wenn z
nach 1 strebt. Wir haben aber gleichmaĚĂige Konvergenz, wenn wir die Kreisscheibe M1 durch eine kleinere
Kreisscheibe Mr := {z â C : |z| < r} mit r < 1 ersetzen (warum ?).
P
n
Satz 5.65. Sei â
n=0 an (z â z0 ) eine Potenzreihe. Sei eine reelle Zahl t definiert durch
p
t := lim n |an |,
nââ

wobei wir voraussetzen, daĂ dieser Limes existiert oder daĂ die Folge der
gilt:
1. die Potenzreihe konvergiert fuĚr jedes z â C mit |z âz0 | <

1
t

p
n
|an | gegen +â divergiert. Dann

absolut (aber nicht unbedingt gleichmaĚĂig),

2. die Potenzreihe konvergiert in jeder kleineren Kreisscheibe Mr := {z â C : |z â z0 | < r} (r <
gleichmaĚĂig,

1
t)

3. die Potenzreihe divergiert fuĚr jedes z â C mit |z â z0 | > 1t ,
4. wenn t = 0 ist, dann konvergiert die Potenzreihe absolut in ganz C, und auf jeder kompakten Teilmenge
von C konvergiert sie gleichmaĚĂig,
5. wenn t = â, dann konvergiert die Potenzmenge nur fuĚr z = z0 ,
6. im Konvergenzgebiet stellt die Potenzreihe eine stetige Funktion dar.
p
Man braucht nicht voraussetzen,
daĂ die Folge n |an | einen Grenzwert hat. Es reicht, t als groĚĂten HaĚufungsp
punkt der Menge { n |an | : n â N} zu definieren, und die restlichen Aussagen des Satzes gelten nach wie
vor.
Wenn jeder Koeffizient an 6= 0 ist, dann kann man die Zahl t auch uĚber die (gelegentlich einfacher handhabbare) Formel
t := lim

nââ

|an+1 |
|an |

p
bestimmen. Falls dieser Limes existieren sollte, dann existiert auch der Grenzwert limnââ n |an | und beide
p
|
nicht existiert, aber limnââ n |an | gibt es doch.
sind gleich. Es kann uĚbrigens passieren, daĂ limnââ |a|an+1
n|
Ein Beispiel ist (a0 , a1 , a2 , a3 , . . . ) = (1, 2, 1, 2, 1, 2, . . . ).
Die Zahl

1
t

heiĂt auch Konvergenzradius.

110

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Abbildung 5.1: Zum Konvergenzverhalten von Potenzreihen. Der gepunktete Kreis umschlieĂt die offene
Kreisscheibe B1/t (z0 ), der durchgezogene Kreis umschlieĂt die abgeschlossene Kreisscheibe Br (z0 ), mit
0 < r < 1/t.
Beweis. FuĚr die Punkte 1 bis 5 benutzt man das Wurzelkriterium bzw. Quotientenkriterium. Punkt 6 wird
spaĚter bewiesen, siehe auch Satz 5.21.
Insgesamt erhalten wir folgendes Verhalten (vgl. Abbildung 5.1). Sei hierbei eine positive Zahl r mit r < 1/t
fest gewaĚhlt.
fuĚr |z â z0 | > 1/t: Divergenz der Potenzreihe,
fuĚr |z â z0 | = 1/t: Konvergenzverhalten unbekannt,
fuĚr |z â z0 | < 1/t: Konvergenz ist absolut, jedoch nicht gleichmaĚĂig,
fuĚr |z â z0 | < r: Konvergenz ist absolut und gleichmaĚĂig.
P
n
Satz 5.66. Sei â
n=0 an (z â z0 ) eine
Pâ Potenzreihe mit Konvergenzradius R. Dann hat die durch termweise
Differentiation entstehende Reihe n=0 nan (z â z0 )nâ1 denselben Konvergenzradius.
Beweis. UĚbungsaufgabe.

BemerkungP5.67. Im zweiten Semester werden wir sehen: sei P = P (z) eine Funktion, die durch die
n
Potenzreihe â
n=0 an (z â z0 ) dargestellt wird, fuĚr z â B1/t (z0 ). Dann ist P dort sogar differenzierbar, und
wir haben die Formel
!â˛
â
â
â
X
X
X
â
â˛
â˛
n
P (z) =
an (z â z0 )
=
(an (z â z0 )n ) =
nan (z â z0 )nâ1 , falls z â B1/t (z0 ).
n=0

n=0

n=0

Die durch â markierte Umformung versteht sich nicht von selbst, sondern wird noch zu beweisen sein.
Die gleichmaĚĂige Konvergenz von Potenzreihen ist hierbei ein wichtiges Hilfsmittel (aber noch nicht ganz
ausreichend).
35 converges

uniformly to P (z)

5.5. BEISPIEL: DIE EXPONENTIALFUNKTION

111

Wie multipliziert man zwei Potenzreihen ? Der folgende Satz sagt uns, daĂ die Multiplikation in naheliegender Weise (Zusammenfassen aller gleichen Potenzen) tatsaĚchlich das richtige Ergebnis liefert:
Pâ
Pâ
Satz 5.68. Seien n=0 an (z â z0 )n und k=0 bk (z â z0 )k zwei Potenzreihen mit Konvergenzradien ra und
rb . Wir definieren eine Folge (cm )mâN0 von Koeffizienten durch
cm :=

X

an b k

n+k=m

Pâ
und setzen rc := min(ra , rb ). Dann konvergiert die Potenzreihe m=0 cm (z â z0 )m fuĚr |z â z0 | < rc , und
dort gilt
!
! â
â
â
X
X
X
m
k
n
cm (z â z0 ) =
bm (z â z0 ) .
an (z â z0 )
m=0

n=0

k=0

Beweis. Die links stehende Reihe ist gerade die CauchyâProduktreihe der beiden rechts stehenden Reihen,
aufgrund der speziellen Definition der cm .

5.5

Beispiel: Die Exponentialfunktion

Die wichtigste Funktion der Physikerinnen und Physiker hat folgende zentrale Eigenschaften:
â
X
1 k
exp(z) =
z ,
k!
k=0

z â C,

exp(u + v) = exp(u) exp(v),
u, v â C,

z n
,
z â C,
exp(z) = lim 1 +
nââ
n
â
p
wenn x = â Q+ , p, q â N+ , dann exp(x) = q ep ,
wobei e = 2.718281828459 . . .,
q
wenn z = x + iy â C, x, y â R,
dann exp(z) = ex (cos y + i sin y),

(5.2)
(5.3)
(5.4)
(5.5)
(5.6)

die wir jetzt zeigen werden. Gleichung (5.2) gilt, weil genau so die Exponentialfunktion definiert wird. Die
(absolute und auf Kompakta gleichmaĚĂige) Konvergenz der Reihe auf der rechten Seite ergibt sich schnell
aus dem Quotientenkriterium, analog zu Beispiel 5.40.
Gleichung (5.4) stellt die Beziehung her zur stetigen Verzinsung eines Kapitals (das ist der historisch traditionsreichste Zugang zur Exponentialfunktion), und der Beweis davon wird die Hauptarbeit in diesem
Abschnitt sein. Das Additionstheorem (5.3) zeigt man entweder durch banales Rechnen (Definition (5.2) einsetzen, maximal moĚgliches Ausmultiplizieren aller Klammern, gleiche Terme wegstreichen bis zur Gleichung
0 = 0, Nachweis der AĚquivalenz der Umformungen), was aber eine lange Rechnung werden kann, weshalb
wir diesen Weg hier nicht verfolgen. Oder man beweist anstatt (5.4) gleich eine etwas allgemeinere Aussage
(naĚmlich Lemma 5.69), und dann folgt (5.3) innerhalb weniger Zeilen. Aus dem Additionstheorem (5.3)
folgt dann (5.5) sofort. Und (5.6) zeigenâ wir spaĚter dadurch, daĂ wir die Funktionen sin und cos genauso
â
wie in (5.6) definieren, und hinterher beweisen, daĂ die so definierten analytischen Winkelfunktionen die
schulbekannten geometrischen Eigenschaften besitzen.
Wie angekuĚndigt, beginnen wir mit einer BonustrackâVersion von (5.4). Der Beweis der Originalgleichung (5.4) haĚtte uĚbrigens im Prinzip denselben Arbeitsaufwand erfordert.
Lemma 5.69. Sei (zn )nâN eine Folge komplexer Zahlen mit Grenzwert zâ â C. Dann gilt
â

zn n X 1 k
1+
=
z .
nââ
n
k! â

lim

k=0

Beweis. Zur Vereinfachung der Schreibweisen setzen wir

zn n
,
an := 1 +
n

aâ :=

â
X
1 k
z .
k! â

k=0

112

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

Diese Reihe fuĚr aâ konvergiert (BegruĚndung analog zu Beispiel 5.40). FuĚr an haben wir aus der binomischen
Formel die Darstellung
n  
X
n
zn k
.
an =
n
k
k=0

Hier wuĚrden wir jetzt gern in jedem Summanden separat zur Grenze n â â uĚbergehen wollen. Wir halten
also ein k fest und untersuchen die Folge der kâten Summanden auf ihr Verhalten fuĚr n â â:
  
n
zn k
n(n â 1) Âˇ . . . Âˇ (n â k + 1) znk
lim
= lim
Âˇ k
nââ k
nââ
n
k!
n
n(n â 1) Âˇ . . . Âˇ (n â k + 1) znk
Âˇ
= lim
nââ
nk
k!


 
n(n â 1) Âˇ . . . Âˇ (n â k + 1)
znk
= lim
Âˇ lim
nââ
nââ k!
nk

zâk
,
k!
aber jetzt haben wir noch die Schwierigkeit zu bewaĚltigen, daĂ die Summanden in der Summe an =
P
n
k=0 . . . immer zahlreicher werden fuĚr n â â. Diesen Beweisversuch brechen wir ab.
=1Âˇ

FuĚr einen zweiten Beweisversuch gehen wir auf die Definition der Konvergenz zuruĚck: fuĚr jedes Îľ > 0 suchen
wir ein N0 (Îľ) mit
|aâ â an | < Îľ

ân âĽ N0 (Îľ).

Sei N â N groĂ, und sei n > N . Dann haben wir die Zerlegung
N
â
N  
X
X
1 k X n  zn k
1 k
|aâ â an | â¤
+
zâ â
z +
k!
n
k! â
k
k=0

k=N +1

k=0

Pâ

  
n
X
n
zn k
.
n
k

k=N +1

1 k
â konvergent
k=0 k! z
P
â

(mit Reihensumme aâ ), wie schon eingangs geschrieben. Also muĂ das
Nun ist
ReihenendstuĚck | k=N +1 . . . | (also der mittlere Summand auf der rechten Seite) kleiner sein als 31 Îľ, wenn
wir N groĂ genug waĚhlen.
Weiterhin koĚnnen wir annehmen, daĂ |zn | â¤ |zâ | + 1 fuĚr alle n > N , denn die Folge der zn konvergiert nach
zâ , siehe auch Satz 5.23. Also ist
  
n
n
n
X
X
X
nk (|zâ | + 1)k
n(n â 1) Âˇ . . . Âˇ (n â k + 1) |zn |k
n
zn k
Âˇ
Âˇ
â¤
â¤
n
nk
k!
nk
k!
k
k=N +1

<

k=N +1
â
X

k=N +1

k=N +1

k

Îľ
(|zâ | + 1)
â¤ ,
k!
3

wenn N groĂ ist. Nun halten wir Îľ und N fest und kuĚmmern uns um den ersten Summanden:
  
N
N
N  
X
X
1 k
1 k X n  zn k
n
zn k
â¤
zâ â
zâ â
k!
n
k!
n
k
k
k=0

k=0

k=0

(wegen der Dreiecksungleichung), und solche Differenzen hatten wir schon bei unserem ersten Beweisversuch
erforscht. Wir finden also ein N0 (Îľ) âŤ N , sodaĂ fuĚr n âĽ N0 (Îľ) gilt:
  
N
X
1 k
Îľ
n
zn k
â¤ .
zâ â
k!
n
3
k
k=0

Damit ist der Beweis komplett.
Definition 5.70 (EndguĚltige Definition der Exponentialfunktion). Die Euler36 sche Zahl e =
2.718281828459045 . . . wird definiert durch
n X

â
1
1
=
.
e := lim 1 +
nââ
n
k!
k=0

36 Leonhard

Euler, 1707â1783

113

5.5. BEISPIEL: DIE EXPONENTIALFUNKTION
Die Exponentialfunktion z 7â exp(z) definieren wir (anders als im ersten Kapitel) durch
exp(z) :=

â
X
1 k
z ,
k!

z â C.

k=0

Nun beweisen wir (5.3).
Satz 5.71 (Additionstheorem). Die Exponentialfunktion erfuĚllt
u, v â C.

exp(u + v) = exp(u) exp(v),
Beweis. Es ist



v n 
u n  
Âˇ lim 1 +
lim 1 +
nââ
nââ
n
n

u n 
v n 
= lim
1+
1+
nââ
n
n

u
v
uv n
= lim 1 + + + 2
nââ
n n n
n

u + v + uv
n
.
= lim 1 +
nââ
n

exp(u) Âˇ exp(v) =



uv
n

Wir setzen zn := u + v +

u+v+
1+
nââ
n
lim

uv
n

wegen Lemma 5.69
wegen Satz 5.54

â C. Dann ist limnââ zn = zâ := u + v, und nach Lemma 5.69 haben wir

n

=

â
X
1
(u + v)k = exp(u + v),
k!

k=0

was zu beweisen war.
Wir kommen jetzt zu (5.5).
Aus dem Additionstheorem haben wir zum Beispiel exp(2z) = exp(z) exp(z) = (exp(z))2 , und nach dem
Prinzip der vollstaĚndigen Induktion koĚnnen wir zeigen, daĂ
exp(mz) = (exp(z))m ,
Wenn wir hierin z :=
exp(1/q) =

1
q

z â C,

m â N+ .

(5.7)

und m := q setzen, bekommen wir e = exp(1) = (exp(1/q))q , also

â
q
e = e1/q ,

q â N+ .

Wenn wir (5.7) nocheinmal verwenden, mit m = p und z = 1/q, dann folgt
p

exp(p/q) = (exp(1/q))p = e q ,

p, q â N+ .

Wir haben also die Gleichung exp(x) = ex gezeigt fuĚr alle positiven rationalen Zahlen. Analog zeigt man
diese Gleichung fuĚr negative rationale Zahlen. Nun ist z 7â exp(z) eine stetige Funktion (wie wir im Satz 5.65
vorhin noch nicht bewiesen haben), und man definiert x 7â ex fuĚr irrationale x als Grenzwert der Folge
xn 7â exn , wobei die xn eine Folge rationaler Zahlen sind, die gegen x streben.
Damit haben wir gezeigt:

Satz 5.72. FuĚr x â R gilt:
ex =

â

X
1 n
x n
.
x = lim 1 +
nââ
n!
n
n=0

Wenn wir in Zukunft ez schreiben mit z â C, dann meinen wir damit stets exp(z). SpaĚter werden wir dafuĚr
sorgen, daĂ
exp(iĎ) = cos(Ď) + i sin(Ď),

Ď â R,

gilt. Diese Formel hatten wir bisher einige Male benutzt, aber noch nicht bewiesen.

114

5.6

KAPITEL 5. NORMIERTE RAĚUME, REELLE ZAHLEN, FOLGEN, REIHEN

SchluĚsselbegriffe

â˘ Definition und Beispiele fuĚr Normen,
â˘ Begriffe Konvergenz, HaĚufungspunkt, offene und abgeschlossene Mengen,
â˘ Definition CauchyâFolge,
â˘ Definition Reihe, Summenformel fuĚr die geometrische Reihe,
â˘ Quotientenkriterium, Wurzelkriterium, Leibnizkriterium,
â˘ Supremum/Infimum im Gegensatz zu Maximum/Minimum,
â˘ Definition absolute und gleichmaĚĂige Konvergenz,
â˘ Konvergenzradius von Potenzreihen,
â˘ Definition der Exponentialfunktion.

Kapitel 6

Funktionen
Definition 6.1. Eine eindeutige Abbildung f von einer Menge Df in eine Menge V heiĂt Funktion1 . Die
Menge Df heiĂt Definitionsbereich2 von f . Wenn f ein x â Df auf y â V abbildet, dann heiĂt y der Wert3
der Funktion f an der Stelle x. Der Wert y heiĂt auch Bild von x.
Die Menge Wf â V saĚmtlicher tatsaĚchlich angenommener Funktionswerte heiĂt Wertebereich4 von f .

Sei W â V eine beliebige Menge. Die Menge aller x â Df mit f (x) â W heiĂt Urbild5 von W .

Hierbei verstehen wir unter
â wird. Der
â eindeutig, daĂ jedem x â Df genau ein Wert y = f (x) zugeordnet
populaĚren Schreibweise 9 = Âą3 wollen wir uns hier nicht anschlieĂen. In diesem Skript ist 9 = 3.
Die Mengen Df und V koĚnnen ganz beliebige Mengen sein. In den meisten FaĚllen wird es sich in dieser
Vorlesung um Teilmengen von R oder C handeln, aber man kann auch beliebige BanachraĚume betrachten.
Beispiel 6.2. Eine eindeutige Abbildung von N nach R ist eine Funktion. Diese Abbildung koĚnnte man
schreiben als n â
7 an . Man sagt dazu auch Folge reeller Zahlenâ. Das ist uns eine eigene Merkregel wert:
â
Jede Folge kann als eine Funktion angesehen werden.

Ab jetzt bezeichnen wir mit z und w stets komplexe Zahlen und mit x, y, u, v stets reelle Zahlen.

6.1

Grenzwerte von Funktionen

Wie in Beispiel 6.2 schon dargelegt, koĚnnen wir mit ein wenig Phantasie Folgen als Funktionen interpretieren. Und dann liegt es nahe, die Definition des Grenzwertes einer Folge (Definition 5.2) als Anleitung zu
benutzen, um eine Definition des Grenzwertes einer Funktion zu gewinnen. Wir gehen erst sprachlich vor,
und dann wird die formelhafte Definition uns fast entgegenfallen.
Zuerst erinnern wir an die Bedeutung der Schreibweise limnââ an = aâ :
Ein Element aâ ist Grenzwert einer Folge (an )nâN ,
wenn es zu jeder Umgebung von aâ ein N0 gibt,
sodaĂ das FolgenendstuĚck ab N0 in dieser Umgebung liegt.
Wir werden kreativ und vereinbaren die Sprechweise, daĂ die Menge {N0 , N0 + 1, . . . } von natuĚrlichen
Zahlen eine Umgebung von ââ ist. (SelbstverstaĚndlich ist â 6â N.) Weiterhin erinnern wir daran, daĂ die
â
Folge beschrieben wird durch die Vorschrift n 7â an . Damit bekommen wir die Neuformulierung:
1 function
2 domain
3 value
4 range
5 pre-image

115

116

KAPITEL 6. FUNKTIONEN
Ein Element aâ ist Grenzwert einer Folge (an )nâN ,
wenn es zu jeder Umgebung von aâ eine Umgebung von â gibt,
sodaĂ die Folgenvorschrift n 7â an diese Umgebung von â in jene Umgebung von aâ abbildet.

Und nun ersetzen wir sprachlich:
f â â aâ ,

Funktion â Folge,

z â n,

f â a,

z â â â,

und erhalten die Bedeutung der Schreibweise limzâzâ f (z) = f â :
Ein Element f â ist Grenzwert einer Funktion f im Punkt z â ,
wenn es zu jeder Umgebung von f â eine Umgebung von z â gibt,
sodaĂ die Funktionsvorschrift z 7â f (z) diese Umgebung von z â in jene Umgebung von f â abbildet.
Um nun zur Formeldefinition zu kommen, denken wir nur noch daran, daĂ die genannte Umgebung von
f â ein ÎľâBall um f â â C sein kann, und die genannte Umgebung von z â wird ein Î´0 âBall um z â â C
sein. Und um es ganz exakt zu machen, denken wir noch uĚber die Definitionsbereiche nach: offensichtlich
muĂ z â Df sein, ansonsten gaĚbe es f (z) gar nicht. Andererseits braucht z â nicht in Df liegen; aber
ein HaĚufungspunkt von Df muĂ z â schon sein, ansonsten koĚnnen die z nicht gegen z â konvergieren und
gleichzeitig im Definitionsbereich von f verbleiben.
Definition 6.3. Sei Df â C eine Menge, und f : Df â C eine Funktion. Sei z â â C ein HaĚufungspunkt
von Df . Wir sagen, daĂ die Funktion f im Punkt z â den Grenzwert6 f â â C hat, wenn gilt:
âÎľ > 0 âÎ´0 (Îľ) > 0 : âz â Df mit z 6= z â und |z â z â | < Î´0 (Îľ) : |f (z) â f â | < Îľ.
Wir vergleichen diese Definition mit der des Grenzwerts einer Folge:
lim an = aâ ââ âÎľ > 0 âN0 (Îľ) > 0 : ân

nââ

lim f (z) = f â ââ âÎľ > 0 âÎ´0 (Îľ) > 0

zâz â

: âz â Df \ {z â }

mit n âĽ N0 (Îľ)

: |an â aâ | < Îľ,

mit |z â z â | < Î´0 (Îľ) : |f (z) â f â | < Îľ.

Mit den oben angefuĚhrten Ersetzungen gehen diese beiden Definitionen ineinander uĚber. Die Menge {z â
Df : z 6= z â , |z â z â | < Î´0 (Îľ)} beschreibt eine (punktierte) Umgebung von z â , waĚhrend die Menge {n : n âĽ
N0 (Îľ)} eine Umgebung von â beschreibt.
Bemerkung 6.4. Weil isolierte Punkte (auch Einsiedlerpunkte genannt) keine HaĚufungspunkte des Definitionsbereiches sind, ist ein Grenzwert dort nicht definiert (und auch nicht sinnvoll definierbar).
In Definition 6.3 tauchte in der Formelzeile die EinschraĚnkung z 6= z â auf, die vermutlich uĚberraschend ist.
Wir betrachten zu ihrer ErklaĚrung die Funktion f : R â C, die durch
(
1
: z 6= 13,
f (z) :=
37 : z = 13
gegeben ist. Sei z â := 13. Der Graph von f legt den Wunsch nahe, limzâzâ f (z) = 1 zu haben, und dies
wird durch die EinschraĚnkung z 6= z â uĚberhaupt erst moĚglich. Wenn man auf die Passage mit z =
6 z ââ in
â
Definition 6.3 verzichtete, dann wuĚrde limzâzâ f (z) gar nicht existieren.
Bemerkung 6.5. Die Definition eines Grenzwertes einer Funktion laĚĂt sich direkt auf Abbildungen zwischen zwei BanachraĚumen U und V uĚbertragen. Man muĂ nur die Betragsstriche | Âˇ | durch Normstriche
kÂˇkU und kÂˇkV ersetzen.
Den folgenden Satz beweist man genauso wie den entsprechenden Satz fuĚr Folgen (Satz 5.3).
Satz 6.6. Wenn eine Funktion in einem Punkt einen Grenzwert hat, dann ist dieser eindeutig.
Das folgende Ergebnis ist nuĚtzlich, wenn man durch Funktionen dividieren will:
Lemma 6.7. Es sei limzâzâ f (z) = f â , wobei f â 6= 0 ist. Dann finden wir eine gelochte Umgebung
{z : z â Df , 0 < |z â z â | < Î´}, sodaĂ in dieser Umgebung f (z) 6= 0 gilt.
6 limit

6.1. GRENZWERTE VON FUNKTIONEN

117

Beweis. Man waĚhle Îľ = 12 |f â | und beachte die Dreiecksungleichung nach untenâ in (5.1).
â
Es gibt noch eine zweite Definition des Grenzwertes; aber wir werden hier nicht beweisen, daĂ beide Definitionen aĚquivalent sind:
Satz 6.8. Die Funktion f hat in einem HaĚufungspunkt z â des Definitionsgebietes den Grenzwert f â genau
dann, wenn gilt:
FuĚr jede Folge (zn )nâN mit zn â Df \ {z â } (fuĚr jedes n) und limnââ zn = z â ist limnââ f (zn ) = f â .
Die folgenden Definitionen sogenannter uneigentlicher Grenzwerte und einseitiger Grenzwerte beziehen sich
nur auf reelle Funktionen.
Definition 6.9. Sei f : R â R eine Funktion und f â â R. Wir sagen, daĂ f gegen f â strebt fuĚr x â +â,
limxâ+â f (x) = f â , wenn:
âÎľ > 0 âR0 (Îľ) > 0 : âx âĽ R0 (Îľ) : |f (x) â f â | < Îľ.
Analog schreiben wir limxâââ f (x) = f â , wenn
âÎľ > 0 âR0 (Îľ) > 0 : âx â¤ âR0 (Îľ) : |f (x) â f â | < Îľ.
Unter rechtsseitigen bzw. linksseitigen Grenzwerten limxâxâ + f (x) bzw. limxâxâ â f (x) verstehen wir die
uĚblichen Grenzwerte einer Funktion an einer Stelle xâ , wobei die Laufvariable sich nur von rechts bzw. nur
von links an xâ annaĚhern darf.
SchlieĂlich definieren wir noch die bestimmte Divergenz einer Funktion gegen +â oder ââ:

Definition 6.10. Sei f : Df â R eine Funktion und xâ ein HaĚufungspunkt von Df . Wir sagen, daĂ f
bestimmt gegen +â divergiert fuĚr x â xâ , wenn gilt:
âR > 0 âÎ´0 (R) > 0 : âx â Df \ {xâ }, |x â xâ | < Î´0 (R) : f (x) > R.
Analog definieren wir bestimmte Divergenz gegen ââ.

SinngemaĚĂ definiert man einseitige bestimmte Divergenz.
Warnung 6.11. Die populaĚre Redeweise 1/x2 strebt/konvergiert nach +â fuĚr x â 0â ist unsinnig.
â
PraĚziser ist: 1/x2 divergiert (bestimmt) nach +â fuĚr x â 0â.
â
FuĚr das Rechnen mit Grenzwerten haben wir das folgende Ergebnis.
Satz 6.12. In jeder der folgenden Formeln steht lim uĚberall fuĚr eines der Symbole limxâxâ , limxâxâ â ,
limxâxâ + , limxâââ , limxâ+â im Sinne der obigen Definitionen, jedoch nicht Definition 6.10.
Wenn lim f (x) und lim g(x) existieren, dann existieren auch
1. lim(Îąf (x) + Î˛g(x)) = Îą lim f (x) + Î˛ lim g(x),
2. lim(f (x)g(x)) = (lim f (x))(lim g(x)),
(x)
=
3. lim fg(x)

lim f (x)
lim g(x) ,

falls g(x) 6= 0.

Beweis. Wegen Satz 6.8 duĚrfen wir die entsprechenden SaĚtze fuĚr Zahlenfolgen verwenden, also Satz 5.6 und
Satz 5.54.
FuĚr Limites im Sinne von Definition 6.10 nutzen wir spaĚter die Regel von BernoulliâlâHospital.
FuĚr komplexe Funktionen koĚnnen einseitige Grenzwerte nicht definiert werden, weil Begriffe wie links von
â
z ââ keinen Sinn haben, denn die komplexen Zahlen kann man nicht ordnen. Weiterhin hat es keinen Sinn,
+â und ââ zu unterscheiden (denn sie sind gleich).
Stattdessen definiert man die bestimmte Divergenz allgemein:

Definition 6.13. Sei f : Df â C eine Funktion und z â â C ein HaĚufungspunkt von Df . Wir sagen, daĂ
f bestimmt gegen â divergiert fuĚr z â z â , wenn gilt:
âR > 0 âÎ´0 (R) > 0 : âz â Df \ {z â }, |z â z â | < Î´0 (R) : |f (z)| > R.

118

KAPITEL 6. FUNKTIONEN

6.2

Stetigkeit

Ab jetzt betrachten wir bis auf weiteres komplexe Funktionen.
Definition 6.14. Sei f : Df â C eine Funktion und z â â C nicht nur ein HaĚufungspunkt des Definitionsbereiches Df , sondern zusaĚtzlich noch ein Element von Df . Wir sagen, daĂ die Funktion f an der Stelle
z â stetig7 ist, wenn folgendes gilt:
â˘ limzâzâ f (z) existiert,
â˘ und limzâzâ f (z) = f (z â ).
Wenn eine dieser beiden Bedingungen verletzt ist, heiĂt die Funktion f unstetig an der Stelle z â8 .
Bemerkung 6.15. Die Forderung, daĂ z â ein Element von Df und ein HaĚufungspunkt von Df sein soll,
klingt zunaĚchst doppelt gemoppelt; sie ist es aber nicht: der Punkt z â koĚnnte ja ein Einsiedlerpunkt von Df
sein, und dann wird der Stetigkeitsbegriff sinnlos.
Wir koĚnnen die Stetigkeit von f an der Stelle z â auch als kommutatives Diagramm ausdruĚcken, im Sinne
von limn f (zn ) = f (limn zn ):
ďŁź
f
ďŁ´
(z1 , z2 , . . . ) âââââ (f (z1 ), f (z2 ), . . . )
ďŁ´
ďŁ´
ďŁ´
ďŁŚ
ďŁŚ
ďŁ´
ďŁ´
ďŁŚ
ďŁŚ
ďŁ˝
limn y
ylimn
(6.1)
ďŁ´
ďŁ´
ďŁ´
f (limn zn )
ďŁ´
ďŁ´
zâ
âââââ
ďŁ´
ďŁž
= limn f (zn )
f

Beispiel 6.16. Die Funktion f = f (z) = 1/z, Df = C \ {0} ist im Punkt z â = 0 undefiniert und unstetig.
Die Funktion g = g(x) = ln x mit Dg = R+ ist fuĚr negative Zahlen undefiniert, und es hat auch nicht viel
Sinn, dort nach Stetigkeit/Unstetigkeit zu fragen.
Definition 6.17. Wenn f : Df â C in jedem Punkt z â â Df stetig ist, dann heiĂt f stetig in Df 9 . Den
Vektorraum aller in D stetigen Funktionen bezeichnen wir mit C(D), C(D â C) oder C(D â R).
Satz 6.18. Sei f : Df â C, und sei z â â Df ein HaĚufungspunkt von Df . Dann sind aĚquivalent:
1. f ist stetig in z â ,
2. limzâzâ f (z) = f (z â ),
3. limnââ f (zn ) = f (z â ) fuĚr jede Folge (zn )nâN â Df \ {z â }, die gegen z â konvergiert,
4. jedes Îľ > 0 hat ein Î´0 (Îľ) > 0, sodaĂ fuĚr jedes z â Df mit |z â z â | < Î´0 (Îľ) gilt: |f (z) â f (z â )| < Îľ.
Beweis. Ergibt sich aus Satz 6.8 und Definition 6.14.
Im weiteren Verlauf des Unterkapitels 6.2 wollen wir auch folgende Frage studieren:
Sei A â Df eine Menge. Welche Eigenschaften von A vererben sich auf f (A), wenn f stetig ist ?
Wir wissen bereits: wenn A eine in Df konvergente Folge ist, dann ist (bei entsprechender Numerierung
der Elemente) auch die Bildmenge f (A) eine in Wf konvergente Folge.
Es gibt Eigenschaften, die sich nicht vererben, naĚmlich Offenheit und Abgeschlossenheit:
Frage: Man gebe ein Beispiel einer stetigen Funktion von R nach R an, bei der das Bild einer offenen
Menge nicht unbedingt wieder eine offene Menge ist.
Man gebe ein weiteres Beispiel einer stetigen Funktion von R nach R an, bei der das Bild einer abgeschlossenen Menge nicht unbedingt wieder eine abgeschlossene Menge ist.
Andererseits gilt fuĚr Offenheit und Abgeschlossenheit die Vererblichkeit ruĚckwaĚrtsâ:
â
7

continuous at z â
at z â
9 continuous in D
f

8 discontinuous

119

6.2. STETIGKEIT
Satz 6.19. Sei f : C â C eine Funktion. Dann sind aĚquivalent:
1. f ist stetig auf C,
2. das Urbild jeder offenen Menge ist offen,
3. das Urbild jeder abgeschlossenen Menge ist abgeschlossen.

Beweis. Lassen wir weg. Die AĚquivalenz von 2. und 3. folgt uĚbrigens sofort aus dem zweiten Teil von
Satz 5.13.
Bemerkung 6.20. Analog wie bei Bemerkung 6.5 halten wir auch hier fest, daĂ man die punktweise
Stetigkeit im Sinne von Definition 6.14 auch fuĚr Abbildungen f : U â V zwischen BanachraĚumen definieren
kann. Man muĂ nur uĚberall die Betragsstriche | Âˇ | passend durch Normstriche kÂˇkU und kÂˇkV ersetzen, vor
allem in Satz 6.18, Teil 4. Und die Stetigkeit in einem Gebiet im Sinne von Definition 6.17 laĚĂt sich auch
fuĚr solche allgemeineren Abbildungen definieren. Satz 6.19 gilt dann ebenfalls.
Satz 6.21. Die folgenden Aussagen gelten sowohl fuĚr Stetigkeit in einem Punktâ als auch fuĚr Stetigkeit
â
â
in einem Gebietâ.
â˘ Wenn f und g stetig sind, dann auch Îąf + Î˛g fuĚr jegliche Îą, Î˛ â C.
â˘ Wenn f und g stetig sind, dann auch f Âˇ g.
â˘ Wenn f und g stetig sind und zusaĚtzlich noch g(x) 6= 0 gilt, dann ist auch

f
g

stetig.

Beweis. Folgt beinahe sofort aus Satz 6.12.
Satz 6.22. Seien f : Df â Wf und g : Dg â Wg stetige Funktionen mit Wg â Df . Dann ist die zusammengesetzte Funktion (Komposition10 ) f âŚ g : Dg â Wf definiert und wieder stetig.
Beweis. Sei xâ â Dg und y â = g(xâ ) â Df . Die Stetigkeiten von g in xâ und von f in y â bedeuten: wenn
(xn )nâN und (yn )nâN Folgen sind mit Grenzwert xâ und y â , dann ist
lim g(xn ) = g( lim xn ) = g(xâ ),

nââ

nââ

lim f (yn ) = f ( lim yn ) = f (y â ).

nââ

nââ

Nun sei (xn )nâN gewaĚhlt, und sei yn := g(xn ). Dann hat diese Folge den Grenzwert y â , und es folgt




lim (f âŚ g)(xn ) = lim (f (g(xn )) = lim f (yn ) = f lim yn = f lim g(xn )
nââ
nââ
nââ
nââ
nââ
 



= f g lim xn
= (f âŚ g) lim xn .
nââ

nââ

Also kommutiert f âŚ g mit lim, und das wollten wir haben.
Beispiel 6.23.

â˘ Konstante Funktionen sind trivialerweise stetig.

â˘ Lineare Funktionen der Form x 7â x sind stetig.
â˘ Polynome sind stetig (folgt aus den ersten beiden â˘).
â˘ Gebrochen rationale Funktionen sind dort stetig, wo der Nenner nicht Null wird.
Satz 6.24. Sei f eine stetige Funktion mit Definitionsbereich Df in einem Banachraum U und Wertebereich Wf in einem Banachraum V . Dann ist das Bild einer jeden kompakten Menge wieder kompakt.
Beweis. Sei M â Df eine in U kompakte Menge. Wir wollen zeigen, daĂ die Bildmenge f (M ) in V kompakt
ist.
Sei also (vn )nâN eine Folge in f (M ). Gesucht ist eine Teilfolge davon, die einen Grenzwert in f (M ) hat. Zu
jedem vn gibt es ein un â M mit f (un ) = vn . Die Folge (un )nâN ist eine Folge in M , und M ist kompakt.
Also gibt es eine Teilfolge (unk )kâN , die gegen einen Grenzwert uâ â M konvergiert. Weil f in uâ stetig ist,
muĂ limkââ f (unk ) = f (uâ ) sein. Nun ist aber f (unk ) = vnk , und diese vnk bilden eine Teilfolge der Folge
(vn )nâN . Wegen uâ â M ist f (uâ ) â f (M ). Also strebt die Teilfolge (vnk )kâN gegen ein Element aus f (M ).
Folglich ist f (M ) kompakt.
10 composed

function

120

KAPITEL 6. FUNKTIONEN

Die Kompaktheit einer Menge in Df vererbt sich also auf die entsprechende Bildmenge. Dies gilt auch fuĚr
stetige Funktionen zwischen unendlichdimensionalen BanachraĚumen.
Folgerung 6.25. Jede auf einer kompakten Menge stetige Funktion ist dort beschraĚnkt.
Das Bild einer kompakten Menge unter einer stetigen Funktion ist abgeschlossen.
Beweis. Hinter Definition 5.51 hatten wir vermerkt: Kompakte Mengen sind beschraĚnkt. Kompakte Mengen
sind abgeschlossen. Fertig.
Jetzt untersuchen wir fuĚr reelle Funktionen f : Df â Wf â R, welche Bedingungen der Definitionsbereich
Df erfuĚllen sollte, damit die Menge Wf der tatsaĚchlich angenommenen Werte ein Maximum oder Minimum
enthaĚlt. Wir beginnen mit zwei Beispielen.
Beispiel: Sei f = f (x) = exp(x) mit Definitionsbereich Df = R. Es ist Df abgeschlossen, aber nicht
beschraĚnkt. Und die Menge der tatsaĚchlich angenommenen Werte ist Wf = (0, â), die kein kleinstes
Element enthaĚlt. Es existiert also min Wf nicht.
Beispiel: Sei f = f (x) = tan(x) mit Definitionsbereich Df = (âĎ/2, Ď/2). Der Definitionsbereich Df ist
beschraĚnkt, aber nicht abgeschlossen. Und tatsaĚchlich ist Wf = (ââ, â) = R, was ebenfalls kein kleinstes
Element besitzt.
Jeweils fuĚr sich alleine reichen die BeschraĚnktheit und die Abgeschlossenheit des Definitionsbereiches also
nicht aus, um die Existenz von min Wf (oder analog max Wf ) zu sichern. Aber beide zusammen bringen
uns zum gewuĚnschten Ziel, wie der folgende Satz vom Maximum zeigt (einen analogen Satz vom Minimum
kann jeder selbst formulieren):
Satz 6.26 (Satz vom Maximum). Sei f : Df â R eine stetige Funktion, und sei Df 6= â kompakt. Dann
nimmt f auf Df sein Supremum und sein Infimum an. Das heiĂt, es gibt xâ , xâ â Df sodaĂ
f (xâ ) â¤ f (x) â¤ f (xâ )

âx â Df .

Beweis. Der Wertebereich Wf ist kompakt (wegen Satz 6.24) und nichtleer (wegen Df 6= â). Er ist also
nach oben beschraĚnkt, hat wegen Satz 5.47 demnach ein Supremum. Weil Wf abgeschlossen ist (wegen
Folgerung 6.25), ist dieses Supremum von Wf sogar ein Maximum, also besitzt Wf ein groĚĂtes Element
f (xâ ). Analog argumentiert man fuĚr die Aussage uĚber das Minimum.
Bemerkung 6.27. Sei U = C([a, b] â R) der Raum der auf [a, b] stetigen und reellwertigen Funktionen.
Wir hatten die kÂˇkâ âNorm bisher als
kf kâ := max |f (x)|
xâ[a,b]

definiert. Dabei hatten wir den (denkbaren) Fall ignoriert, daĂ es dieses Maximum gar nicht gaĚbe: sei es,
weil |f | auf [a, b] unbeschraĚnkt waĚre, sei es, weil |f | zwar beschraĚnkt waĚre und ein Supremum haĚtte, aber
kein Maximum. Der Satz vom Maximum sagt uns jetzt, daĂ das Maximum von |f | immer existiert. Die
eigentlich korrekte Definition der Norm ist
kf kâ := sup |f (x)|.
xâ[a,b]

Reellwertige stetige Funktionen springen nicht â das ist das Ergebnis des folgenden Satzes.
Satz 6.28 (Zwischenwertsatz11 ). Sei f : Df â R stetig auf dem kompakten Intervall [a, b]. Dann nimmt
f auf [a, b] jeden Wert zwischen f (a) und f (b) an. Wenn insbesondere f (a) und f (b) verschiedenes Vorzeichen haben, dann gibt es eine Nullstelle zwischen a und b.
Beweis. Es reicht, die Aussage uĚber die Nullstelle zu beweisen (Warum ?)
Wir setzen a0 = a und b0 = b. OBdA sei f (a0 ) < 0 und f (b0 ) > 0. Nimm Z := 12 (a0 + b0 ).
Fall 1: angenommen, daĂ f (Z) = 0: Dann sind wir fertig.
Fall 2: angenommen, daĂ f (Z) < 0: Dann setzen wir a1 = Z und b1 = b0 .
11 intermediate

value theorem

121

6.2. STETIGKEIT
Fall 3: angenommen, daĂ f (Z) > 0: Dann setzen wir a1 = a und b1 = Z.
Das Ergebnis ist in Fall 2 und Fall 3 jeweils f (a1 ) < 0 < f (b1 ) und |a1 â b1 | = 21 |a0 â b0 |.

In diesem Stil verfahren wir induktiv weiter und finden zwei Folgen (an )nâN , (bn )nâN so daĂ:
a0 â¤ a1 â¤ a2 â¤ a3 â¤ . . . ,

b0 âĽ b1 âĽ b2 âĽ b3 âĽ . . . ,
an < bn ân,
1
|bn+1 â an+1 | = |bn â an |
2
f (an ) < 0 < f (bn ) ân.

ân,

Die Folgen (an )nâN und (bn )nâN streben gegen einen gemeinsamen Grenzwert xâ , siehe Satz 5.48. Wegen
f (an ) < 0 und der Stetigkeit (in der Interpretation als kommutatives Diagramm gemaĚĂ (6.1)) ist
f (xâ ) = f ( lim an ) = lim f (an ) â¤ 0,
nââ

nââ

analog zeigt man f (xâ ) âĽ 0. Also ist f (xâ ) = 0.
Frage: Kennen Sie ein Beispiel fuĚr eine stetige Funktion f : C â C, fuĚr die dieser Satz falsch ist ?

Wir uĚbertragen die Begriffe injektivâ und monotonâ auf Funktionen:
â
â
Definition 6.29. Eine Funktion f : Df â Wf heiĂt injektiv12 , wenn aus y1 = f (x1 ), y2 = f (x2 ) â Wf
und y1 = y2 immer x1 = x2 folgt.
Sei I â R ein Intervall. Eine Funktion f : I â R heiĂt monoton wachsend13 , wenn aus x1 â¤ x2 stets
f (x1 ) â¤ f (x2 ) folgt.

Eine Funktion f : I â R heiĂt streng monoton wachsend14 , wenn aus x1 < x2 stets f (x1 ) < f (x2 ) folgt.

Analog werden (streng) monoton fallende Funktionen definiert.

Satz 6.30. Sei f : Df â Wf eine Funktion, wobei Df , Wf Intervalle aus R seien.
1. Die Funktion f ist surjektiv auf Wf . Wenn f injektiv ist, dann existiert eine Umkehrfunktion g : Wf â
Df mit g âŚ f = idDf und f âŚ g = idWf .
2. Wenn f streng monoton ist, dann ist f injektiv. Die Umkehrfunktion ist wieder streng monoton.
3. Wenn f streng monoton und stetig ist, dann ist auch g stetig.
Beweis. 1. und 2. sind sehr leicht, siehe auch Satz 4.8.
Bei 3. nehmen wir oBdA an, daĂ f waĚchst. Sei y0 = f (x0 ) â Wf . Zu zeigen waĚre, daĂ die Umkehrfunktion
g in y0 stetig ist. Angenommen, y0 sei kein Randpunkt von Wf . Dann ist auch x0 kein Randpunkt von Df ,
und fuĚr kleines Îľ > 0 ist [x0 â Îľ, x0 + Îľ] â Df . Wir suchen zu diesem Îľ ein Î´0 (Îľ), sodaĂ aus |y â y0 | < Î´0 (Îľ)
die Ungleichung |g(y) â g(y0 )| < Îľ folgt.

Nun ist das Bild des Intervalles [x0 â Îľ, x0 + Îľ] unter der Abbildung f aber gerade gleich dem Intervall
[f (x0 â Îľ), f (x0 + Îľ)]. Wir waĚhlen Î´0 (Îľ) = min(f (x0 ) â f (x0 â Îľ), f (x0 + Îľ) â f (x0 )) und sind fertig.

Ein anderer Beweis zu 3. beruht darauf zu begruĚnden, daĂ man bei dem kommutativen Diagramm fuĚr die
Definition der Stetigkeit von f die f âPfeile umdrehen darf (es ist nicht so trivial wie es aussieht).
Definition 6.31. Die Umkehrfunktion15 einer injektiven Funktion f bezeichnet man mit f â1 .

Diese Bezeichnung ist miĂverstaĚndlich (denn f â1 koĚnnte ja auch die Division f1 ausdruĚcken wollen), aber
trotzdem allgemein gebraĚuchlich. Wenn y im Wertebereich von f liegt, dann wird die Urbildmenge von y
oft als f â1 (y) geschrieben, und zwar auch fuĚr nicht-injektive f . Das ist dann eine dritte Bedeutung von
f â1 .
12 injective
13 monotonically

increasing
strictly monotonically increasing
15 inverse function
14

122

KAPITEL 6. FUNKTIONEN

Beispiel 6.32. Wir setzen Df = {x â R : x âĽ 0} = RâĽ0 und f = f (x) = xn mit n â N. Dann ist
Wf = RâĽ0 . Die Funktion x 7â x ist offensichtlich stetig, nichtnegativ-wertig und streng monoton wachsend,
also ist auch die Funktion f stetig und streng monoton wachsend, denn sie ist das mehrfache Produkt der
Funktion x 7â x mit sich selbst. Nach Satz 6.30 hat dann f eine Umkehrfunktion g, die wir Wurzelfunktion
nennen und es ist
g : RâĽ0 â RâĽ0 ,
â
g : x 7â n x.
Man beachte, daĂ die Wurzelfunktion lediglich fuĚr nichtnegative reelle Zahlen definiert ist. Die manchmal
anzutreffenden Aussagen der Form
â
3
â8 = â2
sind verwegener Unsinn. Man meditiere zum Beispiel uĚber der Zeile
â2 =

â
â
1
1
2
1
6
3
â8 = (â8) 3 = (â8) 6 = (â8)2 6 = 64 6 = 64 = +2.

Satz 6.33. Die Funktion z 7â exp(z) ist stetig auf C.

Beweis. Wir koĚnnten auf das entsprechende Ergebnis aus Satz 5.65 verweisen, das allerdings immer noch
seines Beweises harrt.
Oder wir rechnen es schnell aus, wobei wir mit der Stetigkeit in z0 = 0 anfangen. Wir wollen zeigen, daĂ
lim | exp(z) â exp(0)| = 0

zâ0

ist, weshalb wir |z| â¤ 1 voraussetzen koĚnnen. Nun ist
0 â¤ | exp(z) â exp(0)| =

â
X
zk
k=0

k!

â1 â¤

â¤ |z| exp(1);

â
X
|z|k
k=1

k!

= |z|

â
X

k=0

â

X |z|k
|z|k
â¤ |z|
= |z| exp(|z|)
(k + 1)!
k!
k=0

und wenn wir jetzt das Sandwichprinzip anwenden, ist die Stetigkeit im Nullpunkt gezeigt.
Sei nun z0 â C beliebig, und wir wollen beweisen, daĂ
lim | exp(z) â exp(z0 )| = 0.

zâz0

Nun ist aber
lim | exp(z) â exp(z0 )| = lim | exp(z0 )| Âˇ | exp(z â z0 ) â exp(0)| = | exp(z0 )| Âˇ lim | exp(z â z0 ) â 1| = 0.

zâz0

zâz0

zâz0

Also ist die Exponentialfunktion auch in z0 â C stetig.
Weil die Exponentialfunktion x 7â ex fuĚr reelle x noch dazu streng monoton wachsend ist, koĚnnen wir eine
Umkehrfunktion dazu betrachten:
Definition 6.34. Die Umkehrfunktion der Funktion
exp : R â R+ ,
exp : x â
7 exp(x) = ex
heiĂt Logarithmus16 (zur Basis e) und wird geschrieben als
ln : R+ â R,
ln : x â
7 ln(x).
Aus den Eigenschaften der Exponentialfunktion erhalten wir schnell die folgenden Eigenschaften der Logarithmusfunktion:
16 logarithm

123

6.3. DIFFERENZIERBARKEIT

Lemma 6.35. Die Logarithmusfunktion ist stetig und streng monoton auf R+ . FuĚr x, y â R+ gilt dann
ln(x Âˇ y) = ln(x) + ln(y),

ln(x/y) = ln(x) â ln(y),
ln(1) = 0.

Wir wollen als naĚchstes beliebige Potenzen ax definieren, wobei a â R+ und x â R ist.
Definition 6.36 (Allgemeine Potenz). Sei a â R+ und x â R. Dann definieren wir
ax := exp(x ln a).
Diese Funktion ist stetig, denn sie ist eine Komposition von stetigen Funktionen.
Wenn jetzt x = n â N, dann hat die Schreibweise ax zwei Definitionen: einerseits an = aÂˇ. . .Âˇa (n Faktoren),
andererseits an = exp(n ln a). Diese widersprechen einander nicht, denn es ist
exp(n ln a) = exp(ln a + Âˇ Âˇ Âˇ + ln a) = exp(ln a) Âˇ . . . Âˇ exp(ln a) = a Âˇ . . . Âˇ a.
Satz 6.37. Seien a, b â R+ und x, y â R. Dann gilt
ln(ax ) = x ln a,
(a Âˇ b)x = ax Âˇ bx ,

ax+y = ax Âˇ ay ,
1
aâx = x ,
a
(ax )y = a(xÂˇy) .

Beweis. Die erste Eigenschaft folgt sofort aus der Definition von ax , und die weiteren ergeben sich dann
unmittelbar.
FuĚr a1/n erhalten wir gerade die Wurzelfunktion

â
n
a, denn (a1/n )n = a((1/n)Âˇn) = a.

Warnung 6.38. Die obigen Regeln fuĚr Exponentialfunktionen der Form x 7â ax gelten nur fuĚr reelle x
und positive a. Analog gelten die Regeln fuĚr die Logarithmusfunktion nur fuĚr positive Argumente von ln.
Gedankenlose Ausdehnung auf negative oder gar komplexe a fuĚhrt in den meisten FaĚllen zu schweren Fehlern. Auf dieselben Probleme wird man stoĂen, wenn man die Logarithmusfunktion fuĚr negative oder komplexe Zahlen definieren will.

6.3

Differenzierbarkeit

Definition 6.39. Sei f : Df â Wf eine Funktion, wobei Df , Wf â R oder Df , Wf â C.

Sei z0 â Df ein Punkt im Inneren von Df , d.h. nicht auf dem Rand âDf . Wir sagen, daĂ f im Punkte z0
differenzierbar17 ist, wenn der Grenzwert
lim

zâz0

f (z) â f (z0 )
z â z0

(im Sinne von Definition 6.3)

existiert. In diesem Falle nennen wir den Grenzwert Ableitung von f im Punkt z0 18 und schreiben dafuĚr
f â˛ (z0 ) =

df
d
d
(z0 ) =
f (z0 ) =
f (z)
dz
dz
dz

z=z0

.

Sei G â Df eine Menge. Wenn f in jedem (inneren) Punkt von G differenzierbar ist, dann sagen wir, daĂ
f differenzierbar in G ist. In diesem Falle ist die Ableitung f â˛ eine Funktion, die in G definiert ist. Wenn
diese Ableitung zusaĚtzlich stetig sein sollte, dann nennt man f stetig in G differenzierbar19 .
17
18
19

differentiable at z0
derivative of f at z0
continuously differentiable in G

124

KAPITEL 6. FUNKTIONEN

Wenn f â˛ wieder differenzierbar sein sollte, dann heiĂt f zweimal differenzierbar20 . Analog definiert man
hoĚhere Ableitungen.
FuĚr die Menge der auf G kâmal stetig differenzierbaren Funktionen mit reellen Werten schreibt man
C k (G â R).
Es gibt zwei verschiedene Theorien differenzierbarer Funktionen. Einerseits diejenige, wo Df und Wf Teilmengen von R sind, andererseits diejenige mit Df , Wf â C.

Diese zwei Theorien sind total verschieden voneinander. So ist zum Beispiel eine komplex differenzierbare
Funktion automatisch unendlich oft differenzierbar. Man redet in diesem Zusammenhang auch von holomorphen oder analytischen21 Funktionen. Weiterhin gilt zum Beispiel: wenn eine komplex differenzierbare
Funktion in einer kleinen Kreisscheibe uĚberall den Wert 0 hat, dann hat sie uĚberall in C den Wert 0. Dieses
Verhalten findet man bei reell differenzierbaren Funktionen bekanntlich nicht.
AuĂerdem ist zum Beispiel die Funktion z 7â |z|4 lediglich im Nullpunkt komplex differenzierbar und sonst
nirgendwo.
Holomorphe Funktionen werden im 3. Semester eingehend studiert werden; ein groĂer Teil der jetzigen
Untersuchungen beschraĚnkt sich deshalb auf reell differenzierbare Funktionen.
Definition 6.40. Sei f : Df â Wf differenzierbar in Df und x0 â Df . Sei âx = dx â R (bzw. C) eine
beliebige Zahl. Wir setzen voraus, daĂ der Ball B|âx| (x0 ) in Df enthalten ist. Dann definieren wir:
âf = f (x0 + âx) â f (x0 ),
df = f â˛ (x0 ) Âˇ âx = f â˛ (x0 ) Âˇ dx.
Der Ausdruck df heiĂt Differential von f 22 .
Die Differenz âf â df beschreibt den Fehler, den man begeht, wenn man f in der NaĚhe des Punktes x0
durch eine lineare Funktion annaĚhert. Die Differenzierbarkeit von f in x0 bedeutet gerade, daĂ dieser Fehler
von hoĚherer als erster Ordnung nach 0 konvergiert, wenn âx nach 0 strebt.
Definition 6.41. Seien u und v zwei Funktionen, definiert nahe x = x0 . Wir sagen, daĂ
u(x) = O(v(x)),

x â x0 ,

wenn es eine Konstante C > 0 gibt, sodaĂ in einer Umgebung von x = x0 gilt:
|u(x)| â¤ C|v(x)|.
Wir sagen, daĂ
u(x) = o(v(x)),

x â x0

wenn u(x) = O(v(x)) fuĚr x â x0 und wenn zusaĚtzlich u in x0 eine staĚrkere Nullstelle hat als v, also
lim

xâx0

u(x)
= 0.
v(x)

Diese AusdruĚcke O und o heiĂen Landau23 âSymbole. SinngemaĚĂ genauso definiert man Landau-Symbole
fuĚr den Fall x â â.
Wenn wir an einer Stelle O(v(x)) schreiben, und an einer anderen Stelle des Textes nochmal O(v(x))
schreiben, dann soll das nicht bedeuten, daĂ beide Terme identisch sind, sondern nur, daĂ sie sich gleich
verhalten. Das bedeutet, daĂ der erste Term abgeschaĚtzt werden kann als â¤ C1 |v(x)|, und daĂ der zweite
Term abgeschaĚtzt werden kann als â¤ C2 |v(x)|. Hierbei sind C1 und C2 positive Konstanten, deren genauer
Wert oft nicht uĚbermaĚĂig interessant ist.
20 twice

differentiable
holomorph, analytic
22 Es gibt noch eine andere Bedeutung des Begriffes Differential. Gelegentlich wird diejenige lineare Abbildung, die âx auf
f â˛ (x0 )Âˇâx abbildet, als Differential bezeichnet. Der obige Differentialbegriff versteht unter df also den Ausdruck f â˛ (x0 )Âˇâx, der
andere Differentialbegriff versteht unter df aber den Term f â˛ (x0 ). An dieser Stelle ist es vielleicht interessant zu vermerken, daĂ
mit der umgangssprachlichen Bezeichnung Bronsteinâ gelegentlich zwei verschiedene Werke gemeint sind: eine Ausgabe des
â
B.G.TeubnerâVerlags, und eine Ausgabe des Verlags Harri Deutsch. Beide haben ihre Wurzeln in der sowjetischen Erstauflage
von 1937, aber aus juristischen GruĚnden darf nur die letztere Bronsteinâ genannt werden, und beide vertreten unterschiedliche
â
Auffassungen, was ein Differential denn nun eigentlich ist.
23 Edmund Georg Hermann Landau, 1877â1938, nicht zu verwechseln mit Lev Davidovich Landau, 1908â1968, bekannt
fuĚr den gemeinsam mit Evgenni Mikhailovich Lifshitz geschriebenen Kurs zur Theoretischen Physik
21

125

6.3. DIFFERENZIERBARKEIT
Beispiel 6.42. Es gilt
sin(x) = O(x),

x â 0,

3

sin(x) = x + O(x ),
2

x â 0,

cos(x) = 1 + O(x ) = 1 + o(x),
2x + 5
= O(1), x â +â.
x+1

x â 0,

Lemma 6.43. FuĚr x â x0 haben wir die folgenden Rechenregeln:
â˘ wenn u in einer Umgebung von x0 beschraĚnkt ist, dann ist u(x) = O(1),
â˘ O(v(x)) + O(v(x)) = O(v(x)),
â˘ O(v1 (x)) Âˇ O(v2 (x)) = O(v1 (x) Âˇ v2 (x)),
â˘ O((x â x0 )2 ) = o(x â x0 ).
Hierbei sind die letzten drei â˘ von links nach rechts zu lesen, sonst begibt man sich evtl. in die Schwierigkeiten, die in Warnung 5.7 beschrieben wurden. Die Beweise dieser Regeln folgen schnell aus der Definition
der LandauâSymbole.
Satz 6.44 (WeierstraĂâZerlegung). Sei f eine Funktion, definiert in einer Umgebung von x0 .
1. Sei f differenzierbar in x0 . Dann ist fuĚr x in der NaĚhe von x0
f (x) = f (x0 ) + f â˛ (x0 ) Âˇ (x â x0 ) + R(x; x0 )
mit R(x; x0 ) = o(|x â x0 |) fuĚr x â x0 .
2. Wenn es eine Zahl A â R gibt (die nicht von x abhaĚngt) mit
f (x) = f (x0 ) + A Âˇ (x â x0 ) + o(|x â x0 |),

x â x0 ,

dann ist f in x0 differenzierbar, und es ist f â˛ (x0 ) = A.
Beweis.

1. Wir haben
R(x; x0 )
f (x) â f (x0 )
=
â f â˛ (x0 ),
x â x0
x â x0

und die rechte Seite strebt nach 0 falls x â x0 .
2. Es ist
f (x) â f (x0 )
o(|x â x0 |)
=A+
,
x â x0
x â x0
und die rechte Seite hat einen Limes fuĚr x â x0 , naĚmlich A. Also ist f differenzierbar in x0 .
SpaĚter werden wir sehen, daĂ sogar R(x; x0 ) = O(|x â x0 |2 ) gilt, wenn f in einer Umgebung von x0 zweimal
stetig differenzierbar ist.
Als Merkregel halten wir fest:
Wenn f (x) = f (x0 ) + A(x â x0 ) + o(x â x0 ) fuĚr x â x0 ,
dann muĂ f in x0 differenzierbar sein,
und f â˛ (x0 ) muĂ gleich A sein.
Damit werden wir demnaĚchst die Ableitungsformeln fuĚr Summen, Produkte, Verkettungen und Umkehrfunktionen zeigen, wobei wir ausgiebigen Gebrauch von den Rechenregeln fuĚr die LandauâSymbole machen
werden.

126

KAPITEL 6. FUNKTIONEN

Satz 6.45. Differenzierbare Funktionen sind stetig.
Beweis. Sieht man sofort aus Satz 6.44.
Beispiel 6.46. Die Ableitungen von f = f (x) = 1 und g = g(x) = x lauten f â˛ = f â˛ (x) = 0 und
g â˛ = g â˛ (x) = 1.
Satz 6.47. Die Exponentialfunktion z0 7â exp(z0 ) ist fuĚr komplexe z0 differenzierbar, und es ist expâ˛ (z0 ) =
exp(z0 ).
Beweis. Wir zeigen zunaĚchst, daĂ expâ˛ (0) = 1. Dazu muĚssen wir nur beweisen, daĂ
exp(z) â exp(0) â 1 Âˇ (z â 0) = o(|z â 0|),

z â 0.

Nun ist aber
exp(z) â 1 â z
=
zâ0

zk
k=2 k!

Pâ

z

â¤

â
X

k=1

â

â

k=0

k=0

X |z|k
X |z|k
|z|k
= |z|
â¤ |z|
= |z| exp(|z|)
(k + 1)!
(k + 2)!
k!

und dies strebt nach 0 fuĚr z â 0, denn die Exponentialfunktion ist stetig, siehe Satz 6.33.

Sei nun z0 â C beliebig. Wir wollen zeigen, daĂ

exp(z) â exp(z0 ) â exp(z0 ) Âˇ (z â z0 ) = o(|z â z0 |),

z â z0 .

Offensichtlich ist nun wegen des Additionstheorems der Exponentialfunktion
exp(z) â exp(z0 ) â exp(z0 ) Âˇ (z â z0 )
exp(z â z0 ) â 1 â 1 Âˇ (z â z0 )
= | exp(z0 )| Âˇ
,
z â z0
(z â z0 ) â 0
und die rechte Seite geht nach 0 fuĚr (z â z0 ) â 0, denn die Exponentialfunktion ist im Ursprung differenzierbar.
Satz 6.48. Seien f und g in z0 differenzierbar, und Îą, Î˛ â C. Dann ist auch Îąf + Î˛g in z0 differenzierbar,
und es ist
(Îąf + Î˛g)â˛ (z0 ) = Îąf â˛ (z0 ) + Î˛g â˛ (z0 ).
Beweis. Wir haben fuĚr z â z0 die Entwicklungen
f (z) = f (z0 ) + f â˛ (z0 ) Âˇ (z â z0 ) + Rf (z; z0 ),

g(z) = g(z0 ) + g â˛ (z0 ) Âˇ (z â z0 ) + Rg (z; z0 ).
Dann folgt sofort

(Îąf + Î˛g)(z) = (Îąf + Î˛g)(z0 ) + (Îąf â˛ (z0 ) + Î˛g â˛ (z0 )) Âˇ (z â z0 ) + (ÎąRf (z; z0 ) + Î˛Rg (z; z0 )).

Also ist die Menge der auf einem Gebiet differenzierbaren Funktionen ein Vektorraum.
Satz 6.49 (Produktregel). Seien f und g in z0 differenzierbar. Dann ist auch f Âˇ g in z0 differenzierbar,
und es ist
(f Âˇ g)â˛ (z0 ) = f â˛ (z0 )g(z0 ) + f (z0 )g â˛ (z0 ).
Beweis. Wir haben
f (z) = f (z0 ) + f â˛ (z0 ) Âˇ (z â z0 ) + Rf (z; z0 ),

g(z) = g(z0 ) + g â˛ (z0 ) Âˇ (z â z0 ) + Rg (z; z0 ).
Dann liefert eine simple Multiplikation

(f Âˇ g)(z) = (f (z0 ) + f â˛ (z0 ) Âˇ (z â z0 ) + Rf (z; z0 )) Âˇ (g(z0 ) + g â˛ (z0 ) Âˇ (z â z0 ) + Rg (z; z0 ))

= (f Âˇ g)(z0 ) + (f â˛ (z0 )g(z0 ) + f (z0 )g â˛ (z0 )) Âˇ (z â z0 ) + Rf (z; z0 ) Âˇ O(1) + O(1) Âˇ Rg (z; z0 ),

und der Beweis ist komplett.

127

6.3. DIFFERENZIERBARKEIT
Bevor wir jetzt zur Regel fuĚr die Division kommen, betrachten wir zuerst die Funktion w 7â
Kettenregel.
Lemma 6.50. Die Funktion w0 7â


1
w0

â˛

=â

1
w0

1
w

und die

ist differenzierbar fuĚr w0 6= 0, und die Ableitung ist

1
.
w02

Beweis. Sei w nahe w0 . Dann koĚnnen wir deshalb w 6= 0 annehmen. Nun ist




1
w0 â w
w0 â w
1
1
1
1
w0 â w w0 â w
1
=
+
+
+
+
=
â
=
â
w
w0
w w0
w0
w0 w
w0
w02
w0 w
w02


1
1
w0 â w 1
1
=
â 2 Âˇ (w â w0 ) +
â
w0
w0
w0
w w0
1
1
w0 â w w0 â w
=
â 2 Âˇ (w â w0 ) +
Âˇ
w0
w0
w0
ww0
1
1
â 2 Âˇ (w â w0 ) + O(|w â w0 |2 ).
=
w0
w0
Also muĂ der Wert der Ableitung an der Stelle w0 gerade â w12 sein.
0

Satz 6.51 (Kettenregel). Seien f und g zwei Funktionen mit Wg â Df . Sei g differenzierbar in z0 und
f differenzierbar in w0 = g(z0 ). Dann ist die Komposition f âŚ g differenzierbar in z0 , und es ist
(f âŚ g)â˛ (z0 ) = f â˛ (w0 ) Âˇ g â˛ (z0 ).
Beweis. Wir haben
g(z) = g(z0 ) + g â˛ (z0 ) Âˇ (z â z0 ) + Rg (z; z0 ),
f (w) = f (w0 ) + f â˛ (w0 ) Âˇ (w â w0 ) + Rf (w; w0 ).
Wir setzen die erste Gleichung in die zweite ein, und waĚhlen dabei w = g(z):
(f âŚ g)(z) = (f âŚ g)(z0 ) + f â˛ (w0 ) Âˇ (g â˛ (z0 ) Âˇ (z â z0 ) + Rg (z; z0 )) + Rf (w, w0 )

= (f âŚ g)(z0 ) + f â˛ (w0 ) Âˇ g â˛ (z0 ) Âˇ (z â z0 ) + O(1)Rg (z; z0 ) + Rf (w; w0 ).

Es bleibt noch zu zeigen, daĂ Rf (w; w0 ) = o(|z â z0 |). Wenn w = w0 sein sollte, dann ist Rf (w; w0 ) = 0
und wir waĚren in diesem Falle fertig. Ansonsten duĚrfen wir durch w â w0 dividieren und haben
Rf (w; w0 )
Rf (w; w0 ) w â w0
=
Âˇ
z â z0
w â w0
z â z0
FuĚr z â z0 strebt w nach w0 . Dann geht der erste Faktor nach 0 und der zweite strebt zu g â˛ (z0 ), also
lim

zâz0 ,
w(z)6=w(z0 )

Rf (w; w0 )
= 0,
z â z0

was den Beweis der Kettenregel vollendet.
Satz 6.52 (Divisionsregel). Seien die Funktionen f und g in z0 differenzierbar, und sei g 6= 0 in einer
Umgebung von z0 . Dann ist auch die Funktion f /g in z0 differenzierbar, und es gilt
 â˛
1
1
f
(z0 ) = f â˛ (z0 ) Âˇ
â f (z0 ) Âˇ
Âˇ g â˛ (z0 ).
g
g(z0 )
g(z0 )2
Beweis. Folgt sofort aus der Produktregel, Lemma 6.50 und der Kettenregel.

128

KAPITEL 6. FUNKTIONEN

Man kann sich die Divisionsregel auch folgendermaĂen plausibel machen: seien f und g differenzierbare
Funktionen, wobei g 6= 0 ist in einer Umgebung von z0 . Angenommen, eine gute Fee erzaĚhlt uns, daĂ auch
die Funktion w := fg differenzierbar waĚre. Wir koĚnnen ja immer schreiben f = w Âˇ g, und die Auskunft der
Fee gestattet es uns, hier die Produktregel anzuwenden: f â˛ = wâ˛ Âˇ g + w Âˇ g â˛ . Das braucht man dann nur
noch nach wâ˛ umzustellen.
Jetzt wollen wir Umkehrfunktionen ableiten. Sei eine differenzierbare Funktion f gegeben. Angenommen,
eine gute Fee sagt uns, daĂ f eine Umkehrfunktion g hat, und daĂ g sogar differenzierbar ist. Dann haben
wir fuĚr alle z in der NaĚhe von z0 die IdentitaĚt
g(f (z)) = z,
und wenn wir das gemaĚĂ Kettenregel ableiten auf beiden Seiten, bekommen wir an der Stelle z0 dann
g â˛ (f (z0 )) Âˇ f â˛ (z0 ) = 1.
Mit der Notation w0 := f (z0 ) schreibt sich das dann als g â˛ (w0 ) =
daĂ das Dividieren erlaubt waĚre.

1
f â˛ (z0 ) ,

unter der stillen Voraussetzung,

Nun stellen wir uns auf den Standpunkt, daĂ uns heute noch keine gute Fee besucht hat:
Satz 6.53 (Existenz der Umkehrfunktion). Sei f : Df â Wf eine stetige Funktion, z0 â Df und
w0 = f (z0 ). Wenn f in einer Umgebung von z0 stetig differenzierbar ist und f â˛ (z0 ) 6= 0 ist, dann existiert
in der NaĚhe von w0 eine Umkehrfunktion g = g(w) zur Funktion f .
Achtung: uĚber die Differenzierbarkeit von g wird jetzt noch nichts behauptet (erst im Satz 6.54) !
Beweisskizze. Wir schauen uns zuerst die reelle Situation an, also x und x0 anstatt z und z0 . Nach Voraussetzung existiert f â˛ nahe x0 und ist stetig. Wegen dieser Stetigkeit, f â˛ (x0 ) 6= 0 und Lemma 6.7 hat dann
die Ableitung f â˛ in einer kompletten Umgebung von x0 immer dasselbe Vorzeichen. Sei nun xĚ ein Punkt
in dieser Umgebung von x0 , sodaĂ f â˛ (xĚ) dasselbe Vorzeichen hat wie f â˛ (x0 ). FuĚr x nahe xĚ haben wir aus
der WeierstraĂâZerlegung, daĂ
Rf (x; xĚ)
f (x) â f (xĚ)
= f â˛ (xĚ) +
.
x â xĚ
x â xĚ
Hier ist der erste Summand auf der rechten Seite nicht Null, und der Betrag des zweiten ist kleiner als
|f â˛ (xĚ)|, wenn x nahe genug an xĚ ist. Also hat die rechte Seite das gleiche Vorzeichen wie f â˛ (xĚ), also wie
f â˛ (x0 ). Das heiĂt: wenn f â˛ (x0 ) > 0 ist, dann gilt fuĚr x > xĚ, daĂ f (x) > f (xĚ) ist; und fuĚr x < xĚ gilt
entsprechend f (x) < f (xĚ). Also ist dann f in dieser kompletten Umgebung streng monoton. Dann koĚnnen
wir Satz 6.30 anwenden und erhalten, daĂ es eine stetige und streng monotone Umkehrfunktion g gibt.
In der komplexen Situation, also z, z0 â C, koĚnnen wir mit Monotonie nicht argumentieren, weil man in C
keine Ordnung findet. Ein Beweis kommt dann im zweiten Semester (Satz uĚber implizite Funktionen).
Wir vermerken noch, daĂ man auf die geforderte Stetigkeit von f â˛ nicht verzichten kann, wie das Beispiel
(
2 + x + x2 sin(xâ2 ) : x 6= 0,
y = f (x) =
2
:x=0
zeigt. Die Ableitung f â˛ existiert auf ganz R; und es ist f â˛ (0) = 1. Trotzdem gibt es keine Umkehrfunktion in
der NaĚhe von y = 2, denn die Funktion f = f (x) ist abwechselnd monoton wachsend und monoton fallend,
wenn x nach 0 strebt. Nahe x = 0 wechselt das Monotonieverhalten von f unendlich oft.
Satz 6.54 (Ableitung der Umkehrfunktion). Sei f : Df â Wf stetig, z0 â Df , w0 = f (z0 ). Sei
weiterhin f in z0 differenzierbar, und sei f â˛ (z0 ) 6= 0. Wir setzen auĂerdem voraus, daĂ die Funktion f in
einer Umgebung von z0 eine Umkehrfunktion g hat. Dann ist g in w0 differenzierbar, und es ist
g â˛ (w0 ) =

1
.
f â˛ (z0 )

Beweis. Wir wissen, daĂ
f (z) = f (z0 ) + f â˛ (z0 ) Âˇ (z â z0 ) + Rf (z; z0 )

(6.2)

6.4. MITTELWERTSATZ UND TAYLORSCHER SATZ

129

R (z;z )

gilt, falls |z â z0 | klein genug ist. Hierbei ist limzâz0 fzâz00 = 0. Das bedeutet: wenn z nahe genug an z0
ist, dann ist der letzte Summand in (6.2) 1000 mal kleiner als der mittlere (es sei denn, der mittlere waĚre
identisch gleich Null; aber das ist unmoĚglich wegen f â˛ (z0 ) 6= 0). Dann haben wir fuĚr solche z
0.999|f â˛(z0 )| â¤

|f (z) â f (z0 )|
â¤ 1.001|f â˛(z0 )|.
|z â z0 |

(6.3)

Wir koĚnnen also |f (z) â f (z0 )| und |z â z0 | gegeneinander vergleichen. Nun setzen wir w = f (z) und
w0 = f (z0 ), also z = g(w) und z0 = g(w0 ). Wenn w 6= w0 , dann ist nach dieser Formel also auch z 6= z0 .
Wir entwickeln g = g(w) in eine Reihe:
g(w) = z
= z0 + (z â z0 ) = g(w0 ) + (z â z0 )
= g(w0 ) +

f (z) â f (z0 ) â Rf (z; z0 )
w â w0
Rf (z; z0 )
= g(w0 ) + â˛
â
.
f â˛ (z0 )
f (z0 )
f â˛ (z0 )

Es bleibt zu zeigen, daĂ der Restterm schneller als von erster Ordnung (in |w â w0 |) nach 0 strebt, wenn
w â w0 . Das ergibt sich aus
Rf (z;z0 )
f â˛ (z0 )

w â w0

=

1
1
Rf (z(w); z0 ) z(w) â z0
z â z0
Rf (z(w); z0 )
Âˇ
= â˛
.
Âˇ
Âˇ
Âˇ
f â˛ (z0 )
z(w) â z0
w â w0
f (z0 ) f (z) â f (z0 )
z(w) â z0

Der erste Faktor ist beschraĚnkt, der zweite wegen (6.3) auch, und der dritte strebt nach 0 fuĚr w â w0 .
Mit diesem Satz koĚnnen wir die Ableitungen von Logarithmus und allgemeinen Potenzen bestimmen:
Satz 6.55. FuĚr x > 0 ist
(ln x)â˛ = x1 .
FuĚr a â R+ und x â R ist
d x
a = (ln a)ax ,
dx
d x
a = xaxâ1 .
da
Beweis. Sei y = ln x, dann ist x = exp(y) und schlieĂlich
(ln x)â˛ =

1
1
1
=
= .
(exp(y))â˛
exp(y)
x

Der zweite Teil ergibt sich aus ax = exp(x ln a), der Kettenregel und den Regeln fuĚr die Ableitung der
Exponentialfunktion und Logarithmusfunktion.

6.4

Mittelwertsatz und Taylorscher Satz

Die Betrachtungen dieses Abschnitts setzen reelle Funktionen voraus, und die meisten Ergebnisse werden
falsch, wenn man stattdessen komplexe Funktionen betrachtet.
Definition 6.56. Sei f : I â R eine Funktion, definiert auf einem Intervall I â R. Sei x0 â I ein innerer
Punkt oder ein Randpunkt. Wir sagen, daĂ f im Punkt x0 ein lokales Maximum24 hat, wenn in einer
Umgebung von x0 die Ungleichung f (x) â¤ f (x0 ) gilt.

Analog definiert man ein lokales Minimum25 .

Wenn f in einem inneren Punkt x0 â I ein lokales Maximum oder Minimum hat, dann redet man auch
von einem lokalen Extremum26 .
24 local

maximum
minimum
26 local extremum
25 local

130

KAPITEL 6. FUNKTIONEN

Satz 6.57. Sei f : I â R eine Funktion mit einem lokalen Extremum in einem inneren Punkt x0 â I.
Wenn f in x0 differenzierbar ist, dann ist f â˛ (x0 ) = 0.
Warnung 6.58. Die Aussage des Satzes wird falsch, wenn x0 ein Randpunkt des Intervalles sein darf !
Beweis. Sei das Extremum ein Maximum (der Fall des Minimums geht aĚhnlich). In der NaĚhe von x0 gilt
dann f (x) â¤ f (x0 ), und somit gilt
(
â¤ 0 : x âĽ x0 ,
f (x) â f (x0 )
x â x0
âĽ 0 : x â¤ x0 .
Wenn x â x0 , dann muĂ der Grenzwert des Differenzenquotienten existieren. Der linksseitige Grenzwert
ist âĽ 0, der rechtsseitige ist â¤ 0. Also ist f â˛ (x0 ) = 0.
Satz 6.59 (Satz von Rolle27). Die Funktion f : I â R sei auf dem abgeschlossenen Intervall I = [a, b]
stetig und auf dem offenen Intervall (a, b) differenzierbar. Sei weiterhin f (a) = f (b) = 0. Dann existiert
ein Îž â (a, b) mit f â˛ (Îž) = 0.
Beweis. Wir unterscheiden drei FaĚlle.
f hat in (a, b) wenigstens einen positiven Wert: Nach dem Satz vom Maximum nimmt dann f irgendwo im Intervall (a, b) ein positives Maximum f (Îž) an. Nach dem vorigen Satz ist dann f â˛ (Îž) = 0.
f hat in (a, b) keinen positiven, jedoch mindestens einen negativen Wert: Dann kann man aĚhnlich wie im ersten Fall argumentieren.
f ist identisch 0: Dann ist f â˛ (Îž) = 0 fuĚr jedes Îž â (a, b).
In jedem der drei FaĚlle haben wir ein Îž im Intervallinneren mit der Eigenschaft f â˛ (Îž) = 0 gefunden.
Satz 6.60 (Verallgemeinerter Mittelwertsatz). Seien die Funktionen f, g : I â R auf dem abgeschlossenen Intervall I = [a, b] stetig und auf dem offenen Intervall (a, b) differenzierbar. Weiterhin sei g â˛ (x) 6= 0
auf (a, b) und g(a) 6= g(b). Dann gibt es ein Îž â (a, b) mit
f (b) â f (a)
f â˛ (Îž)
= â˛ .
g(b) â g(a)
g (Îž)
Beweis. Man setze
h(x) = f (x) â f (a) â

f (b) â f (a)
(g(x) â g(a))
g(b) â g(a)

und wende den Satz von Rolle auf die Funktion h an.
Satz 6.61 (Mittelwertsatz28 ). Sei die Funktion f : I â R auf dem abgeschlossenen Intervall I = [a, b]
stetig und auf dem offenen Intervall (a, b) differenzierbar. Dann gibt es ein Îž â (a, b) mit
f (b) â f (a)
= f â˛ (Îž).
bâa
Beweis. Man benutze den verallgemeinerten Mittelwertsatz mit g(x) = x.
Diese beiden unscheinbaren MittelwertsaĚtzchen haben viele wichtige und tiefgreifende Anwendungen. Die
ersten sind zusammengefaĂt im folgenden Satz:
Satz 6.62. Sei f : I â R stetig auf I = [a, b] und differenzierbar auf (a, b).
27

Michel Rolle, 1652â1719
value theorem

28 mean

6.4. MITTELWERTSATZ UND TAYLORSCHER SATZ

131

1. Wenn |f â˛ (x)| â¤ K fuĚr jedes x â (a, b), dann ist |f (x1 ) â f (x2 )| â¤ K|x1 â x2 | fuĚr beliebige x1 , x2 â
[a, b].29
2. Wenn f â˛ (x) = 0 fuĚr jedes x â (a, b), dann ist f auf [a, b] konstant.
3. Wenn limxâbâ f â˛ (x) existiert, dann ist f in x = b linksseitig differenzierbar und f â˛ ist in b linksseitig
stetig. Analog fuĚr a anstelle von b.
4. Die Funktion f ist monoton wachsend auf [a, b] genau dann, wenn f â˛ (x) âĽ 0 fuĚr jedes x â (a, b).
5. Wenn f â˛ (x) > 0 fuĚr jedes x â (a, b), dann ist f streng monoton wachsend auf [a, b].
Die Umkehrung der letzten Aussage ist falsch.
Beweis.
1. Wir haben f (x1 ) â f (x2 ) = f â˛ (Îž) Âˇ (x1 â x2 ) fuĚr ein gewisses Îž zwischen x1 und x2 .
2. Ergibt sich sofort aus 1. mit K = 0.
3. FuĚr x < b haben wir wegen des Mittelwertsatzes mit einem unbekannten Îž â (x, b)
f (x) â f (b)
= f â˛ (Îž).
xâb
Wenn x von links nach b strebt, dann strebt auch Îž = Îž(x) nach b (Sandwichprinzip). Also existiert
der Grenzwert der rechten Seite (fuĚr x â bâ), und somit existiert auch der Grenzwert der linken
Seite.
4. Wenn f monoton wachsend ist, dann hat die Differenz f (x1 ) â f (x2 ) dasselbe Vorzeichen wie x1 â x2 .
Also ist der Quotient dieser beiden Differenzen âĽ 0. UĚbergang zum Grenzwert liefert f â˛ (x) âĽ 0.
Wenn nun f â˛ (x) âĽ 0 ist fuĚr jedes x â (a, b) und x1 > x2 , so ist auch f (x1 ) âĽ f (x2 ) wegen des
Mittelwertsatzes.

5. Der Beweis verlaĚuft analog zum Beweis von 4. Ein Gegenbeispiel fuĚr die Umkehrung ist z.B. [a, b] =
[â1, 1] und f = f (x) = x7 .

Eine weitere wichtige Anwendung des verallgemeinerten Mittelwertsatzes ist die Regel von Bernoulli30 â
de lâHospital31 :
Satz 6.63. Seien f, g : [a, b] â R stetige Funktionen mit folgenden Eigenschaften:
1. limxâbâ f (x) = limxâbâ g(x) = 0,
2. g â˛ (x) 6= 0 fuĚr x â (a, b),
3. f und g sind differenzierbar in (a, b),
4. der Grenzwert
f â˛ (x)
xâbâ g â˛ (x)
lim

existiert und hat den Wert S.
29 Solche

Funktionen heiĂen dehnungsbeschraĚnkt oder Lipschitzstetig, nach Rudolf Otto Sigismund Lipschitz, 1832â

1903
30 Johann I Bernoulli, 1667â1748, nicht zu verwechseln mit seinem Sohn Johann II Bernoulli, 1710â1790, oder seinem
Enkel Johann III Bernoulli, 1744â1807, oder 5 weiteren beruĚhmten Bernoullis
31 Guillaume Francois Antoine Marquis de LâHospital, 1661â1704

132

KAPITEL 6. FUNKTIONEN

Dann existiert auch der Grenzwert
lim

xâbâ

f (x)
g(x)

und hat ebenfalls den Wert S. Dies gilt auch fuĚr bestimmte Divergenz gegen S = â.
Eine entsprechende Aussage gilt natuĚrlich auch fuĚr rechtsseitige Grenzwerte oder beidseitige Grenzwerte.
Warnung 6.64. Die Voraussetzung 4. ist entscheidend. Es gibt Beispiele, in denen limxâbâ
aber

â˛

(x)
limxâbâ fgâ˛ (x)

f (x)
g(x)

existiert,

gibt es nicht.

Beweis von Satz 6.63. Sei x < b. Dann gibt es ein Îž â (x, b) mit
f (x)
f (x) â f (b)
f â˛ (Îž)
=
= â˛ .
g(x)
g(x) â g(b)
g (Îž)
Wenn x nach b strebt, dann strebt auch Îž = Îž(x) nach b. Die rechte Seite konvergiert nach Voraussetzung 4.,
also auch die linke Seite.
Frage: Welchen Wert hat
ex â 1
?
xâ0 ln(1 + x)
lim

Es gibt wichtige Variationen der Regel von Bernoulliâde lâHospital:
Satz 6.65. Seien f, g : (a, b) â R stetige Funktionen mit folgenden Eigenschaften:
1. limxâbâ f (x) = limxâbâ g(x) = â,
2. g â˛ (x) 6= 0 fuĚr x â (a, b),
3. f und g sind differenzierbar in (a, b),
4. der Grenzwert
f â˛ (x)
xâbâ g â˛ (x)
lim

existiert und hat den Wert S.
Dann existiert auch der Grenzwert
lim

xâbâ

f (x)
g(x)

und hat ebenfalls den Wert S. Dies gilt auch fuĚr bestimmte Divergenz gegen S = â.
Beweis. Lassen wir weg.
Die beiden letzten SaĚtze gelten sinngemaĚĂ auch fuĚr b = +â. Zum Beweis betrachtet man dann
f (tâ1 )
f (x)
= lim
xâ+â g(x)
tâ0+ g(tâ1 )
lim

und wendet die herkoĚmmliche Regel von Bernoulliâde lâHospital an.
Und wir haben noch eine weitere Folgerung aus dem Verallgemeinerten Mittelwertsatz:

6.4. MITTELWERTSATZ UND TAYLORSCHER SATZ

133

Satz 6.66 (Taylor32 scher Satz). Die Funktion f : I â R sei auf dem abgeschlossenen Intervall I = [a, b]
stetig und auf dem offenen Intervall (a, b) n + 1âmal differenzierbar. Seien x, x0 â (a, b). Dann gibt es ein
Îž zwischen x und x0 sodaĂ
f (x) = f (x0 ) + f â˛ (x0 ) Âˇ (x â x0 ) +

1
1 â˛â˛
f (x0 ) Âˇ (x â x0 )2 + Âˇ Âˇ Âˇ + f (n) (x0 ) Âˇ (x â x0 )n + Rn (x; x0 )
2!
n!

mit einem Restterm der Form
Rn (x; x0 ) =

1
f (n+1) (Îž) Âˇ (x â x0 )n+1 .
(n + 1)!

Der Anteil f (x0 ) + Âˇ Âˇ Âˇ +

1 (n)
(x0 )(x
n! f

(6.4)

â x0 )n heiĂt auch nâtes TaylorâPolynom von f an der Stelle x0 33 .

Beweis. Seien x und x0 fixiert, und t â (a, b) variabel. Wir basteln uns mit Phantasie zwei Funktionen:
F (t) = f (x) â f (t) â f â˛ (t)(x â t) â
G(t) =

1 â˛â˛
1
f (t)(x â t)2 â Âˇ Âˇ Âˇ â f (n) (t)(x â t)n ,
2!
n!

(x â t)n+1
.
(n + 1)!

Die Funktion F wurde genau so gewaĚhlt, daĂ F (x0 ) = Rn (x; x0 ) ist. Die Funktion G ist monoton im
Intervall zwischen x und x0 , also duĚrfen wir den verallgemeinerten Mittelwertsatz anwenden:
F (x) â F (x0 )
F â˛ (Îž)
= â˛ ,
G(x) â G(x0 )
G (Îž)
fuĚr ein gewisses unbekanntes Îž zwischen x und x0 .
Wir rechnen im Folgenden die einzelnen Terme aus. ZunaĚchst ist F (x) = 0 und G(x) = 0. Weiterhin ist


d 1 (k)
1
1
k
= f (k+1) (t)(x â t)k â
f (t)(x â t)
f (k) (t)(x â t)kâ1 ,
dt k!
k!
(k â 1)!
1
F â˛ (t) = â f (n+1) (t)(x â t)n ,
n!
(x â t)n
.
Gâ˛ (t) = â
n!
Insgesamt ist dann
F (x0 ) =

F â˛ (Îž)
(x â x0 )n+1
(n+1)
G(x
)
=
f
(Îž)
,
0
Gâ˛ (Îž)
(n + 1)!

und genau das war unser Ziel.
Beispiel 6.67. Sei f = f (x) = (1 + x)Îą mit x â (â1, 1) = I und Îą â R. Mit x0 = 0 haben wir
f (x0 ) = 1,

f â˛ (x0 ) = Îą,

f â˛â˛ (x0 ) = Îą(Îą â 1),

und somit ergibt sich die nuĚtzliche NaĚherung
(1 + x)Îą = 1 + Îą Âˇ x +

Îą(Îą â 1) 2
x + O(|x|3 ),
2

x â 0.

An dieser Stelle koĚnnen wir eine in der Physik uĚbliche Sprechweise betrachten: fuĚr eine genuĚgend oft
differenzierbare Funktion f und x nahe genug bei x0 wollen wir f (x) annaĚhern. Dabei stellen wir uns vor,
daĂ f eine hochkomplizierte Funktion ist, insbesondere viel komplizierter als Polynome. Wir wollen f durch
das Taylorpolynom mit Entwicklungspunkt x0 annaĚhern, und bei der Wahl von x0 lassen wir uns von zwei
Gesichtspunkten leiten:
â˘ es sollten x und x0 nahe beieinander sein, damit das Restglied Rn klein ist,
32
33

Brook Taylor, 1685â1731
nth order Taylor polynomial of f at x0

134

KAPITEL 6. FUNKTIONEN

â˘ es sollte x0 so gewaĚhlt sein, daĂ wir tatsaĚchlich die Ableitungen f (j) (x0 ) mit geringem Arbeitsaufwand
bestimmen koĚnnen.
FuĚr die gesuchten NaĚherungen gibt es dann mehrere Varianten:
Nullte NaĚherung: f (x) â f (x0 )
Erste NaĚherung: f (x) â f (x0 ) + f â˛ (x0 )(x â x0 )
Zweite NaĚherung: f (x) â f (x0 ) + f â˛ (x0 )(x â x0 ) + 12 f â˛â˛ (x0 )(x â x0 )2 ,
und so weiter. Der Unterschied zwischen linker und rechter Seite ist jeweils gleich R0 (x; x0 ), R1 (x; x0 ) bzw.
R2 (x; x0 ). Sinnvoll sind diese NaĚherungen, wenn dieser Fehler Rn deutlich kleiner ist als der letzte noch
mitgenommene Summand; und wuĚnschenswerterweise sollte die (k + 1)âte NaĚherung besser sein als die kâte
NaĚherung. Manchmal tut uns aber die Natur diesen Gefallen nicht, z.B. dann nicht, wenn der Graph von
f heftige AusschlaĚgeâ aufweist, aber trotzdem noch oft genug differenzierbar ist (dann kann f (n+1) (Îž) fuĚr
â
groĂe n riesig werden).
Wir tragen einige Formeln zusammen. Sei dazu f â C([a, b] â R) und x, x0 â (a, b). Dann ist
f (x) = f (x0 ) + f â˛ (x0 ) Âˇ (x â x0 ) + o(|x â x0 |),
â˛

f (x) = f (x0 ) + f (Îž) Âˇ (x â x0 ),
f (x) = f (x0 ) + f â˛ (Îž) Âˇ (x â x0 ),

x â x0 ,

1
f (x) = f (x0 ) + f â˛ (x0 ) Âˇ (x â x0 ) + f â˛â˛ (Îž) Âˇ (x â x0 )2
2

(WeierstraĂâZerlegung)
(Mittelwertsatz)
(Taylorscher Satz mit n = 0)
(Taylorscher Satz mit n = 1).

FuĚr die ersten drei Formeln benoĚtigen wir noch f â C 1 ([a, b] â R) als Voraussetzung, und fuĚr die vierte
Formel sogar f â C 2 ([a, b] â R). Die beiden Zwischenstellen Îž in den beiden Taylorschen SaĚtzen sind im
Allgemeinen nicht gleich. Wir beobachten, daĂ der Taylorsche Satz mit n = 0 nichts anderes ist als der
Mittelwertsatz, und der Taylorsche Satz mit n = 1 erlaubt eine praĚzisere Beschreibung des Restterms in
der WeierstraĂâZerlegung, falls f zweimal stetig differenzierbar ist.
Als eine weitere Anwendung des Taylorschen Satzes haben wir zum Beispiel das folgende Kriterium fuĚr
das Vorliegen eines Extremums:
Satz 6.68. Sei die Funktion f auf dem Intervall I n + 1âmal stetig differenzierbar. In einem inneren
Punkte x0 â I sei
f â˛ (x0 ) = f â˛â˛ (x0 ) = Âˇ Âˇ Âˇ = f (n) (x0 ),

f (n+1) (x0 ) 6= 0.

Dann gilt:
n ungerade und f (n+1) (x0 ) > 0: f hat in x0 ein Minimum,
n ungerade und f (n+1) (x0 ) < 0: f hat in x0 ein Maximum,
n gerade: f hat in x0 kein Extremum, sondern die Funktion x 7â f (x) â f (x0 ) wechselt im Punkt x0 das
Vorzeichen.
Beweis. Wir haben
f (x) = f (x0 ) +

1
f (n+1) (Îž)(x â x0 )n+1 .
(n + 1)!

Weil f (n+1) stetig ist, haben f (n+1) (Îž) und f (n+1) (x0 ) das gleiche Vorzeichen, falls Îž nahe genug bei x0 ist,
siehe Lemma 6.7.
Wenn die Funktion f unendlich oft differenzierbar ist, dann koĚnnen wir einen Versuch wagen, in der
Taylorschen Formel den GrenzuĚbergang n â â zu vollziehen. Das klappt aber nur, wenn das Restglied
Rn nach 0 strebt fuĚr n â â.

6.4. MITTELWERTSATZ UND TAYLORSCHER SATZ

135

Satz 6.69. Die Funktion f sei auf dem Intervall I unendlich oft differenzierbar. Wir setzen voraus, daĂ
es einen inneren Punkt x0 â I und ein kleines positives Î´ gibt, sodaĂ


1 (n)
f (Îž) Î´ n = 0.
lim
max
nââ |Îžâx0 |â¤Î´ n!
Dann gilt: fuĚr jedes x â I mit |x â x0 | â¤ Î´ konvergiert die Restgliedfolge der Rn (x; x0 ) fuĚr n â â nach 0,
und es ist
f (x) =

â
X
1 (k)
f (x0 )(x â x0 )k ,
k!
k=0

wobei diese Reihe fuĚr |x â x0 | â¤ Î´ absolut konvergiert.
Diese Reihe heiĂt auch Taylorreihe34 .
Beweis. Wir haben fuĚr jedes n â N die Zerlegung
f (x) = pn (x; x0 ) + Rn (x; x0 )
n
X
1
1 (k)
f (x0 )(x â x0 )k +
f (n+1) (Îž)(x â x0 )n+1 ,
=
k!
(n + 1)!
k=0

wobei das unbekannte Îž von x und n abhaĚngt. Falls |x â x0 | â¤ Î´, dann ist
lim Rn (x; x0 ) = 0,

nââ

also muĂ der Grenzwert limnââ pn (x; x0 ) auch existieren und den Wert f (x) annehmen.
Wir kehren noch einmal zuruĚck zur Funktion f = f (x) = (1 + x)Îą mit Îą â R und â1 < x < 1. Wir setzen
x0 = 0 und haben
f (n) (x) = Îą(Îą â 1) Âˇ . . . Âˇ (Îą â n + 1)(1 + x)Îąân ,

â1 < x < 1.

Definition 6.70. FuĚr Îą â R und n â N definieren wir den Binomialkoeffizienten
 
Îą(Îą â 1) Âˇ . . . Âˇ (Îą â n + 1)
Îą
:=
.
n
n!

Îą
n



als

Dann koĚnnen wir die Funktion f (x) = (1 + x)Îą schreiben als
(1 + x)Îą = pn (x; 0) + Rn (x; 0)


n  
X
Îą k
Îą
=
x +
(1 + Îž)Îąâ(n+1) xn+1 ,
k
n+1
k=0

â1 < x < 1,

wobei das unbekannte Îž zwischen 0 und x liegt und von x und n abhaĚngt.
Wir haĚtten gerne, daĂ
â  
X
Îą n
?
x ,
(1 + x)Îą =
n
n=0

â1 < x < 1.

Um dies zu beweisen, reicht es aber nicht aus zu zeigen, daĂ der Grenzwert limnââ pn (x; 0) existiert (das
tut er fuĚr â1 < x < 1, wie man mit dem Quotientenkriterium schnell sieht).

Wir brauchen mehr. NaĚmlich, daĂ dieser Grenzwert nicht bloĂ schnoĚde existiert, sondern auch noch den
Wert f (x) = (1 + x)Îą hat. Dies ist aber gleichbedeutend mit
?

lim Rn (x; 0) = 0.

nââ
34 Taylor

series

136

KAPITEL 6. FUNKTIONEN

Sei x â (â1, 1) fixiert und n so groĂ, daĂ Îą â n < 0. Dann ist
|Îą Âˇ (Îą â 1) Âˇ . . . Âˇ (Îą â n)|
(1 â |x|)Îąâ(n+1) |x|n+1
(n + 1)!

n+1
|Îą Âˇ (Îą â 1) Âˇ . . . Âˇ (Îą â n)|
|x|
= (1 â |x|)Îą
(n + 1)!
1 â |x|

|Rn (x; 0)| â¤

=: Sn (x),

wobei Sn fuĚr Schrankeâ steht. Der Quotient Sn+1 /Sn strebt nach
â
Damit koĚnnen wir zeigen 35 , daĂ
lim Rn (x; 0) = 0

nââ

fuĚr |x| <

|x|
1â|x| ,

und dies ist < 1 fuĚr |x| <

1
2.

1
.
2

Auf diesem Wege haben wir bewiesen, daĂ
(1 + x)Îą =

â  
X
Îą

n

n=0

xn ,

Îą â R,

â

1
1
<x< .
2
2

(6.5)

Dies ist die beruĚhmte binomische Reihe. Im dritten Semester werden wir staĚrkere Werkzeuge kennenlernen,
die uns garantieren, daĂ die Formel (6.5) auch fuĚr â1 < x < 1 gilt.
Warnung 6.71. Die Funktion
(
2
eâ1/x
: x 6= 0,
f (x) =
0
:x=0
ist auf ganz R definiert, dort stetig und uĚberall unendlich oft differenzierbar. SaĚmtliche Ableitungen f (n) (0)
sind gleich 0. Die Folge der Werte der Taylorpolynome pn (x; 0) konvergiert fuĚr n â â, naĚmlich nach 0.
Aber offensichtlich ist
0 = lim pn (x; 0) 6= f (x)
nââ

falls x 6= 0, denn fuĚr solche x ist f (x) 6= 0.
Frage: Wie sieht der Graph dieser Funktion f aus ?
Satz 6.72. FuĚr â1 < x â¤ 1 gilt
ln(1 + x) =

â
X

(â1)nâ1

n=1

xn
.
n

Drei Beweise fuĚr Neugierige.
1
Ein erster Beweis verlaĚuft so: wir besorgen uns die Potenzreihe fuĚr die Funktion x 7â 1+x
, wobei die Summenformel der geometrischen Reihe nuĚtzlich ist. Dann integrieren wir diese Potenzreihe summandenweise
(und im zweiten Semester zeigen wir dann, daĂ diese summandenweise Integration tatsaĚchlich zulaĚssig ist).

Ein zweiter Beweis funktioniert sehr aĚhnlich zum Beweis von (6.5), aber er hat den Nachteil, daĂ wir die
Konvergenz der Restgliedfolge nach Null nur fuĚr â 21 < x < 1 hinbekommen, da die Restgliedformel (6.4)
(auch bekannt als die LagrangeâForm des Restglieds) hier nicht kraĚftig genug ist.
Als Alternative verschaffen wir uns im dritten Beweis eine andere Restglieddarstellung, die zwar etwas
haĚĂlicher ist, aber zumindest bei der Logarithmusfunktion leistungsfaĚhiger. DafuĚr schauen wir uns den
Beweis von Satz 6.66 nochmal an, jetzt aber waĚhlen wir G = G(t) = x â t. Dann lassen wir den dort
vorgefuĚhrten Beweis nochmal durchlaufen, und als Ergebnis ergibt sich dann
n

xâÎž
1 (n+1)
(Îž) Âˇ
Âˇ (x â x0 )n+1 ,
Rn (x; x0 ) = F (x0 ) = f
n!
x â x0
35 Wenn wir etwas sorgfaĚltiger arbeiten und uns uĚberlegen, wo Îž liegen kann, dann bekommen wir lim
nââ Rn (x; 0) = 0
sogar fuĚr â 21 < x < 1.

137

6.5. ELEMENTARE FUNKTIONEN

mit einem Îž zwischen x und x0 (mehr ist von Îž nicht bekannt). Dies ist die CauchyâForm des Restglieds,
und sicherlich hat Îž jetzt nicht mehr denselben Wert wie in (6.4). FuĚr unsere Logarithmusfunktion ist
f (x) = ln(1 + x) sowie
f (n) (x) = (â1)nâ1 (n â 1)!

1
,
(1 + x)n

und daraus ergibt sich dann (mit x0 = 0)
|Rn (x; 0)| â¤

xâÎž
1
1
Âˇ
Âˇ n! Âˇ
n!
|1 + Îž|n+1 x â 0

n

Âˇ |x â 0|n+1 =

xâÎž
|x|
Âˇ
|1 + Îž| 1 + Îž

n

.

Wegen â1 < x < 1 ist auf jeden Fall 1 + Îž > 0.

Sei nun 0 < x < 1. Dann haben wir 0 < Îž < x < 1, also folgt |x â Îž| < |x| und 1 + Îž > 1; somit ist dann
|Rn (x; 0)| < |x|n+1 .

Sei nun â1 < x < 0. Dann haben wir â1 < x < Îž < 0, also folgt |x â Îž| = |x| â |Îž| und 1 + Îž = 1 â |Îž|;
somit ist dann
n

|x|
|x| â |Îž|
|Rn (x; 0)| =
Âˇ
1 â |Îž|
1 â |Îž|
n

|x|
|x| Âˇ (1 â |Îž|) + |x| Âˇ |Îž| â |Îž|
=
Âˇ
1 â |Îž|
1 â |Îž|
n

1 â |x|
|x|
1 â |x| < 1 â |Îž|
Âˇ |x| â |Îž| Âˇ
=
1 â |Îž|
1 â |Îž|
|x|
â¤
Âˇ |x|n
1 â |Îž|
|x|n+1
,
â¤
1 â |x|
also gilt wegen |x| < 1 tatsaĚchlich limnââ Rn (x; 0) = 0.

Offen bleibt (in allen drei Beweisen) der Fall x = 1, den wir nicht weiter verfolgen wollen.
Insbesondere erhalten wir die fruĚher schon benutzte IdentitaĚt
ln 2 = 1 â

1 1 1 1
+ â + Âą ....
2 3 4 5

Wir fassen die beiden Stolperfallen bei der Konvergenz von Taylorreihen zusammen:
â˘ Es kann geschehen, daĂ bei unpassender Wahl von x die Taylorreihe divergiert. Zum Beispiel hat die
1
, entwickelt am Punkt x0 = 0, die Taylorreihe
Funktion f = f (x) = 1âx
f (x) =

â
X

xn ,

n=0

die nun leider fuĚr x = 13 divergiert. Die Taylorreihe konvergiert nur fuĚr â1 < x < 1.
â˘ Es kann geschehen, daĂ die Taylorreihe zwar konvergiert, aber nicht zur Funktion f . Siehe Warnung 6.71 fuĚr ein Beispiel dafuĚr. Dieses zweite Problem tritt in der Praxis allerdings seltener auf.

6.5

Elementare Funktionen

6.5.1

Der geometrische Zugang zu den Winkelfunktionen

Die Winkelfunktion sin, cos und tan 36 fuĚr Winkel zwischen 0 und
Dreieck definiert:
sin =
36 sine,

Gegenkathete
,
Hypotenuse

cosine, tangent

cos =

Ankathete
,
Hypotenuse

tan =

Ď
2

werden wie uĚblich am rechtwinkligen

sin
Gegenkathete
=
.
cos
Ankathete

138

KAPITEL 6. FUNKTIONEN

Wenn nun Ď â R beliebig ist, dann definiert man sin Ď und cos Ď wie folgt:

In einem kartesischen Koordinatensystem startet man im Punkt (1, 0) und laĚuft auf dem Einheitskreis im
Gegenuhrzeigersinn den Winkel Ď ab. Die kartesischen Koordinaten desjenigen Punktes, an dem man dann
ankommt, definieren die Zahlen cos Ď (als xâKoordinate) und sin Ď (als yâKoordinate). Falls cos Ď 6= 0,
sin Ď
dann definiert man tan Ď := cos
Ď . Die LaĚnge des auf dem Einheitskreis zuruĚckgelegten Weges ist gerade
gleich Ď.
Man sieht dann durch Hinschauen:
Satz 6.73. Die geometrisch definierten Winkelfunktionen sin und cos sind stetig und haben folgende Eigenschaften fuĚr jedes Ď â R:
sin(Ď + 2Ď) = sin Ď,

cos(Ď + 2Ď) = cos Ď,

sin(Ď + Ď) = â sin Ď,
Ď

sin
â Ď = cos Ď,
2
sin(Ď â Ď) = sin Ď,

cos(Ď + Ď) = â cos Ď,
Ď

cos
â Ď = sin Ď,
2
cos(Ď â Ď) = â cos Ď,

sin(âĎ) = â sin Ď,

cos(âĎ) = cos Ď,

sowie

Ď
sin Ď +
= cos Ď,
2
2
2
sin Ď + cos Ď = 1.
Die Funktionen sin und cos haben die PeriodenlaĚnge 2Ď, waĚhrend tan die halbe PeriodenlaĚnge Ď hat.

Abbildung 6.1: Zum Additionstheorem des Sinus

139

6.5. ELEMENTARE FUNKTIONEN
Satz 6.74 (Additionstheoreme). FuĚr Îą, Î˛ â R gilt
sin(Îą + Î˛) = sin Îą cos Î˛ + cos Îą sin Î˛,
cos(Îą + Î˛) = cos Îą cos Î˛ â sin Îą sin Î˛,




ÎąâÎ˛
Îą+Î˛
sin
,
sin Îą â sin Î˛ = 2 cos
2
2




ÎąâÎ˛
Îą+Î˛
sin
.
cos Îą â cos Î˛ = â2 sin
2
2
Beweis. Die letzten beiden Formeln folgen aus den ersten beiden (wie ?)

Wir beginnen mit dem Beweis des Additionstheorems fuĚr den Sinus im Falle von 0 < Îą < Ď/2 und
0 < Î˛ < Ď/2, vgl. Abbildung 6.1. Wir zeichnen ein Dreieck P RS mit Innenwinkel Îą bei P und Innenwinkel
Î˛ bei R. Wir nennen die StreckenlaĚngen u := RS und v := SP . Dann drehen wir das Dreieck P RS
um den Mittelpunkt der Strecke P R um Ď, wobei S auf einen Punkt Q abgebildet wird. Es entsteht ein
Parallelogramm P QRS mit Innenwinkel Îą + Î˛ bei P . Der FlaĚcheninhalt dieses Parallelogramms ist
A = uv Âˇ sin(Îą + Î˛).

(6.6)

Der LotfuĂpunkt von S auf P R sei getauft auf den Namen Z. FuĚr den FlaĚcheninhalt des rechtwinkligen
Dreiecks P ZS haben wir dann
AP ZS =

1
uv
1
P Z Âˇ ZS = Âˇ (v cos Îą) Âˇ (u sin Î˛) =
cos Îą sin Î˛.
2
2
2

FuĚr den FlaĚcheninhalt des rechtwinkligen Dreiecks ZRS folgt analog
AZRS =

1
1
uv
ZR Âˇ ZS = Âˇ (u cos Î˛) Âˇ (v sin Îą) =
sin Îą cos Î˛.
2
2
2

Der GesamtflaĚcheninhalt des Parallelogramms ist dann
A = 2(AP ZS + AZRS ) = uv Âˇ (cos Îą sin Î˛ + sin Îą cos Î˛).
Wir vergleichen dies mit (6.6), und das gewuĚnschte Additionstheorem fuĚr den Sinus steht da, zumindest
fuĚr den Fall 0 < Îą, Î˛ < Ď/2. Weil Sinus/Cosinus ungerade/gerade Funktionen sind, bekommen wir das
Additionstheorem fuĚr den Sinus auch im Fall âĎ/2 < Îą, Î˛ < 0.
Als naĚchstes wollen wir das Additionstheorem des Sinus zeigen im Falle, daĂ einer der beiden Winkel im
Intervall (0, Ď/2) liegt, und der andere im Intervall (âĎ/2, 0). Mit ein wenig UĚberlegung erkennt man, daĂ
es genuĚgt zu zeigen:
sin(Î˛ â Îą) = sin Î˛ cos Îą â cos Î˛ sin Îą,

0 < Îą < Î˛ < Ď/2.

Dazu zeichnen wir ein rechtwinkliges Dreieck P QR mit rechtem Winkel bei Q und Winkel Î˛ bei P , und
darin enthalten ein weiteres rechtwinkliges Dreieck P QT mit rechtem Winkel bei Q und Winkel Îą bei P .
Wir taufen StreckenlaĚngen: w := P R, v := P T , u := P Q. Siehe Abbildung 6.2. Dann haben wir
u = w cos Î˛,

u = v cos Îą

=â

v=w

cos Î˛
.
cos Îą

Der FlaĚcheninhalt des Dreiecks P T R ist einerseits
AP T R =

w2 cos Î˛
1
wv sin(Î˛ â Îą) =
sin(Î˛ â Îą).
2
2 cos Îą

Andererseits haben wir P T R als Differenzdreieckâ zweier rechtwinkliger Dreiecke, also
â

cos Î˛
1
w2 
1
cos Î˛ sin Î˛ â cos Î˛
sin Îą
AP T R = AP QR â AP QT = uw sin Î˛ â uv sin Îą =
2
2
2
cos Îą

w2 cos Î˛ 
=
cos Îą sin Î˛ â cos Î˛ sin Îą .
2 cos Îą

Das wollten wir zeigen. Also gilt das Additionstheorem des Sinus jetzt sogar im Falle von âĎ/2 â¤ Îą, Î˛ â¤
Ď/2, denn diese Winkelfunktionen sind stetig, also koĚnnen wir zum abgeschlossenen Intervall uĚbergehen.

140

KAPITEL 6. FUNKTIONEN

Abbildung 6.2: Zum Additionstheorem des Sinus
Aufgrund von sin(Îł + Ď) = â sin Îł und cos(Îł + Ď) = â cos Îł haben wir das Additionstheorem dann endlich
fuĚr alle Îą, Î˛ â R.
Und das Additionstheorem fuĚr den Cosinus ergibt sich dann sofort gemaĚĂ
Ď

 Ď


cos(Îą + Î˛) = sin
â (Îą + Î˛) = sin
â Îą + (âÎ˛)
2

2Ď

Ď
â Îą cos(âÎ˛) + cos
â Îą sin(âÎ˛)
= sin
2
2
= cos Îą cos Î˛ â sin Îą sin Î˛,
wobei wir hier Satz 6.73 benutzt haben.
Aus den Additionstheoremen und einfachen geometrischen UĚberlegungen ergeben sich dann die Formeln
fuĚr die Ableitungen:
Satz 6.75. FuĚr alle Ď â R gilt
(sin Ď)â˛ = cos Ď,

(cos Ď)â˛ = â sin Ď.

Beweis. Geometrisch ist klar, daĂ sin 0 = 0 und cos 0 = 1.
Wir zeigen als erstes, daĂ (sinâ˛ )(0) = 1.
Sei 0 < Ď <

Ď
2.

Einfache FlaĚcheninhaltsuĚberlegungen am Kreis zeigen, daĂ

sin Ď < Ď < tan Ď ââ 1 <

Ď
tan Ď
1
<
=
.
sin Ď
sin Ď
cos Ď

Das Sandwichprinzip liefert dann limĎâ0
(sinâ˛ )(0) = 1.

sin Ď
Ď

= 1. Wegen sin 0 = 0 ist das aber gerade die Formel fuĚr

6.5. ELEMENTARE FUNKTIONEN

141

Sei nun Ď0 â R beliebig und Ď 6= Ď0 . Aus Satz 6.74 erhalten wir dann






ĎâĎ0
ĎâĎ0
sin Ď â sin Ď0
Ď + Ď0 sin
Ď + Ď0 sin
2
2
= 2 cos
= cos
.
ĎâĎ0
Ď â Ď0
2
Ď â Ď0
2
2
FuĚr Ď â Ď0 strebt die rechte Seite dann gegen cos Ď0 .

Die Ableitungsformel fuĚr den Cosinus ergibt sich aus der Ableitungsformel fuĚr den Sinus und cos Ď =
sin(Ď/2 â Ď).
Weil (modulo Vorzeichen) eine Winkelfunktion die Ableitung der anderen ist und umgekehrt, sind die
Winkelfunktionen sin und cos unendlich oft differenzierbar. Dann stellt sich naturgemaĚĂ die Frage nach
der Taylorreihe, und ob sie konvergiert, und wenn ja, gegen welchen Grenzwert. Diese Frage werden wir im
naĚchsten Abschnitt beantworten (obwohl wir es auch jetzt schon koĚnnten).
Wir vermerken nur kurz zum SchluĂ, daĂ die Sinusfunktion die (einzige) LoĚsung des Anfangswertproblems
y â˛â˛ + y = 0,

y(0) = 0,

y â˛ (0) = 1

ist; und die Kosinusfunktion ist die (wiederum einzige) LoĚsung des Anfangswertproblems
y â˛â˛ + y = 0,

y(0) = 1,

y â˛ (0) = 0.

Die Einzigkeit der LoĚsungen der genannten Anfangswertproblem war gerade Inhalt von Lemma 4.36.

6.5.2

Der analytische Zugang zu den Winkelfunktionen

Die AusfuĚhrungen des vorigen Abschnitts haben einen kleinen Nachteil: sie sind logisch etwas fragwuĚrdig
angeordnet. Dazu sollten wir kurz uĚber das Konzept der Definition nachdenken: eine Definition bedeutet,
daĂ der betreffende Begriff geborenâ wird. Bekanntlich kann niemand mehrfach geboren werden, und
â
genauso kann auch kein Begriff mehrfach definiert werden. Weiterhin verwendet man beim Definieren neuer
Begriffe andere Begriffe, die schon vorher definiert worden waren, sodaĂ sich im Laufe eines dreisemestrigen
Kurses ein groĂer Stammbaum von Begriffen ergibt.
Im vorigen Abschnitt hatten wir allerdings einige Begriffe benutzt, von denen wir gar nicht wissen, wie wir
sie definieren sollen:
â˘ Winkel,
â˘ LaĚnge eines Kreisbogens,
â˘ FlaĚcheninhalt eines Dreiecks bzw. Kreissektors,
und sicherlich noch einige andere mehr.
Es hat sich deshalb in der Mathematik eingebuĚrgert, die Winkelfunktionen auf dem Wege der Analysis einzufuĚhren und, darauf aufbauend, die geometrischen Begriffe hinterher zu definieren. Zum Beispiel definiert
man FlaĚcheninhalte und KurvenlaĚngen als bestimmte Integrale.
Wir stellen uns jetzt auf den Standpunkt, von einer Zahl namens Ď noch nie etwas gehoĚrt zu haben. Wir
werden ihr erst zum SchluĂ wiederbegegnen. Wir wissen auch gar nicht, was ein Kreis ist.
Wir beginnen mit einem kleinen Satz zur Exponentialfunktion.
Satz 6.76. FuĚr z â C ist exp(z) = exp(z). FuĚr x â R ist | exp(ix)| = 1.
Beweis. Die erste Behauptung erhalten wir aus
exp(z) =

â
â
X
X
zk
(z)k
=
=
= exp(z).
k!
k!
k!

â
X
zk
k=0

k=0

k=0

Nun ist einerseits
exp(ix) exp(âix) = exp(ix â ix) = exp(0) = 1,

142

KAPITEL 6. FUNKTIONEN

andererseits nach der ersten Behauptung
exp(ix) = exp(ix) = exp(âix).
Insgesamt ergibt sich exp(ix)exp(ix) = 1, also | exp(ix)| = 1.
Definition 6.77. FuĚr x â R definieren wir zwei Funktionen c = c(x) und s = s(x) durch
c(x) = â exp(ix),

s(x) = â exp(ix).

Satz 6.78. Diese Funktionen haben die folgenden Eigenschaften fuĚr beliebige x, y â R:
1
(exp(ix) + exp(âix)),
2
c(0) = 1,

1
(exp(ix) â exp(âix)),
2i
s(0) = 0,

c(x) =

s(x) =

s(âx) = âs(x),

c(âx) = c(x),
2

2

c (x) + s (x) = 1,
c(x + y) = c(x)c(y) â s(x)s(y),

s(x + y) = s(x)c(y) + s(y)c(x),

â˛

sâ˛ (x) = c(x).

c (x) = âs(x),

Beweis. Die erste Zeile folgt aus exp(ix) = exp(âix).
Aus exp(0) = 1 folgt c(0) = 1 und s(0) = 0.
Die câFunktion ist gerade, denn wegen des Satzes 6.76 haben wir
c(âx) = â exp(âix) = âexp(ix) = â exp(ix) = c(x),
und analog argumentiert man fuĚr die sâFunktion. Aus Satz 6.76 bekommt man auch c2 (x) + s2 (x) = 1.
Die Additionstheoreme fuĚr c und s folgen aus
c(x + y) + is(x + y) = exp(i(x + y)) = exp(ix) exp(iy)
= (c(x) + is(x)) (c(y) + is(y))
= (c(x)c(y) â s(x)s(y)) + i(c(x)s(y) + s(x)c(y))
und Vergleich von Realteil und ImaginaĚrteil auf beiden Seiten.
Die Beziehung fuĚr die Ableitungen laĚĂt sich aĚhnlich zeigen. Wir starten mit den Formeln
c(x) = â exp(ix) =

1
(exp(ix) + exp(âix)),
2

s(x) = â exp(ix) =

1
(exp(ix) â exp(âix)).
2i

Die rechten Seiten sind differenzierbare Funktionen, also muĚssen auch die linken Seiten differenzierbar sein.
Dann erhalten wir nach kurzer Rechnung câ˛ (x) = âs(x) und sâ˛ (x) = c(x), was den Beweis komplettiert.
Satz 6.79. FuĚr jedes x â R gilt
c(x) =

â
X

(â1)k

k=0

x2k
,
(2k)!

s(x) =

â
X

k=0

(â1)k

x2k+1
.
(2k + 1)!

Diese Reihen konvergieren fuĚr jedes x â R absolut.
Beweis. Wir haben c(x) = 21 (exp(ix) + exp(âix)) und s(x) =
exp(ix) =

â n n
X
i x
,
n!
n=0

Der Rest ist Arithmetik.

exp(âix) =

â
X
(â1)n in xn
.
n!
n=0

1
2i (exp(ix)

â exp(âix)) sowie

143

6.5. ELEMENTARE FUNKTIONEN
Als naĚchstes untersuchen wir die Funktionen c und s auf Monotonie. Wir haben fuĚr c die Potenzreihe
c(x) = 1 â

x4
x6
x8
x2
+
â
+
â ...,
2!
4!
6!
8!

und es gilt die AbschaĚtzung
c(x) â¤ 1 â

x2
x4
+ ,
2!
4!

wenn x2 â¤ 56,

1 6
x +
denn fuĚr solche x ist der Reihenrest â 6!
Summand dieser Leibnizreihe ist negativ.

1 8
8! x â

eine Leibnizreihe, also nicht positiv, denn der erste

FuĚr x = 2 haben wir also c(2) â¤ â 13 . Andererseits ist die câFunktion stetig und c(0) = 1. Wegen des
Zwischenwertsatzes hat die Funktion c also mindestens eine Nullstelle zwischen x = 0 und x = 2. KoĚnnte
vielleicht c dort mehr als eine Nullstelle besitzen ? Dazu beobachten wir, daĂ im Intervall (0, 2] gilt, daĂ






x2
x2
x4
x2
+ (positiv) > x 1 â
,
s(x) = x 1 â
+
â +... = x 1 â
6
120
6
6
und dies ist positiv fuĚr 0 < x â¤ 2. Wegen câ˛ = âs ist dann aber c auf dem Intervall [0, 2] streng monoton
fallend, und demnach besitzt c auf diesem Intervall genau eine Nullstelle. Man kann errechnen, daĂ sie
folgenden Wert hat:
p2 = 1.570796326794897 . . ..

Definition 6.80. Wir definieren
Ď := 2p2 .
Das ist einfach nur eine Schreibweise, aber Assoziationen zu schulischen Bildungserlebnissen sind nicht
unbeabsichtigt.
Wegen c(p2 ) = 0 und c2 (x) + s2 (x) = 1 muĂ |s(p2 )| = 1 sein. Weil aber sâ˛ (x) = c(x) und c > 0 auf dem
Intervall [0, p2 ), gilt demnach s(p2 ) = 1. Mit der neuen Schreibweise Ď2 = p2 erhalten wir so
 Ď
= i.
exp i
2
Potenzieren gibt exp(iĎ) = â1 und exp(2iĎ) = 1, also auch c(Ď) = â1, s(Ď) = 0, c(2Ď) = 1, s(2Ď) = 0.

Die Additionstheoreme fuĚr die Funktionen c und s liefern dann

Ď
c x+
= c(x) Âˇ 0 â s(x) Âˇ 1 = âs(x),
2
c(x + Ď) = c(x) Âˇ (â1) â s(x) Âˇ 0 = âc(x),
c(x + 2Ď) = c(x) Âˇ 1 â s(x) Âˇ 0 = c(x),

Ď
= s(x) Âˇ 0 + c(x) Âˇ 1 = c(x),
s x+
2
s(x + Ď) = s(x) Âˇ (â1) + c(x) Âˇ 0 = âs(x),
s(x + 2Ď) = s(x) Âˇ 1 + c(x) Âˇ 0 = s(x).

Satz 6.81. FuĚr jedes x â R gilt c(x) = cos(x), s(x) = sin(x) sowie
exp(ix) = cos(x) + i sin(x).

(6.7)

Hierbei sind cos und sin geometrisch definiert.
Beweis. Die Funktionen c und cos sind LoĚsung des Anfangswertproblems
y â˛â˛ + y = 0,

y(0) = 1,

y â˛ (0) = 0,

und die Funktionen s und sin sind LoĚsung des Anfangswertproblems
y â˛â˛ + y = 0,

y(0) = 0,

y â˛ (0) = 1.

Da es aber (wegen Lemma 4.36) nur jeweils eine einzige LoĚsung geben kann, muĂ c(x) = cos(x) und
s(x) = sin(x) gelten. Die Gleichung (6.7) folgt dann aus der Definition von c und s.

144

KAPITEL 6. FUNKTIONEN

Definition 6.82 (Komplexe Winkelfunktionen). FuĚr z â C definieren wir die Winkelfunktionen
cos(z) =

1
(exp(iz) + exp(âiz)),
2

sin(z) =

1
(exp(iz) â exp(âiz)).
2i

Offensichtlich sind diese Funktionen in ganz C unendlich oft differenzierbar, und Satz 6.74 sowie Satz 6.79
gelten auch fuĚr komplexe Argumente.
Frage: Man bestimme supzâC | cos(z)| und supzâC | sin(z)|.
Satz 6.83. Die einzigen Nullstellen der komplexen Winkelfunktionen sind zsin,n = nĎ fuĚr den Sinus und
zcos,n = Ď2 + nĎ fuĚr den Cosinus, wobei n â Z.
Wenn exp(iz) = 1, dann ist z = 2nĎ fuĚr ein geeignetes n â Z.

Beweis. Sei z = x + iy mit x, y â R. FuĚr den Cosinus haben wir zum Beispiel

1
1
â(exp(iz) + exp(âiz)) = â eix eây + eâix ey
2
2
 1

1 ây
=
e cos x + ey cos x = cos x eây + ey .
2
2

â(cos z) =

Die Aussage uĚber die Nullstellen der Sinusfunktion bekommt man auf aĚhnlichem Wege.
Wenn nun exp(iz) = 1, dann ist auch exp(âiz) = 1, und somit ist sin(z) = 0, also z = kĎ fuĚr ein k â Z.
Die ungeraden k kommen nicht in Frage (warum ?), also bleiben nur die z = 2nĎ uĚbrig, und fuĚr diese gilt
exp(iz) = 1 tatsaĚchlich.
Definition 6.84. FuĚr z â C mit z 6â
tan z =

Ď
2

+ ĎZ definieren wir die Tangensfunktion als

sin z
.
cos z

Satz 6.85. Die Tangensfunktion besitzt die folgenden Eigenschaften (uĚberall dort, wo sie definiert ist):
tan(z + Ď) = tan z,
tan(âz) = â tan z,
tan z + tan w
,
tan(z + w) =
1 â tan z tan w
1
tanâ˛ (z) = 1 + tan2 z =
.
cos2 z
Beweis. Diese Eigenschaften ergeben sich direkt aus den analogen Eigenschaften von sin und cos.
Durch Division der Potenzreihen von sin und cos bekommt man die folgende Potenzreihe des Tangens im
Ursprung mit Konvergenzradius Ď2 . Weitere Terme der Potenzreihe ergeben sich durch Rechnen.
2
17 7
62 9
1382 11
21844 13
929569 15
1
z +
z +
z +
z +
z
tan z =z + z 3 + z 5 +
3
15
315
2835
155925
6081075
638512875
+ O(z 17 ),
z â 0.
Satz 6.86 (Arcusfunktionen).
1. Die Sinusfunktion bildet das Intervall [â Ď2 , Ď2 ] bijektiv auf [â1, 1] ab. Die Umkehrfunktion
h Ď Ďi
arcsin : [â1, 1] â â ,
2 2
1
.
1ây 2

heiĂt Arcussinus und hat die Ableitung (arcsin y)â˛ = â

2. Die Cosinusfunktion bildet das Intervall [0, Ď] bijektiv auf [â1, 1] ab. Die Umkehrfunktion
arccos: [â1, 1] â [0, Ď]
1
.
1ây 2

heiĂt Arcuscosinus und hat die Ableitung (arccos y)â˛ = â â

145

6.5. ELEMENTARE FUNKTIONEN
3. Die Tangensfunktion bildet das Intervall (â Ď2 , Ď2 ) bijektiv auf R ab. Die Umkehrfunktion
 Ď Ď
arctan: R â â ,
2 2
heiĂt Arcustangens und hat die Ableitung (arctan y)â˛ =

1
1+y 2 .

Beweis. Das schaffen Sie alleine.
Frage: Wie lassen sich die Ableitungsformeln fuĚr die Arcusfunktionen und die Formel fuĚr die Allgemeine
binomische Reihe (siehe (6.5)) so kombinieren, daĂ mit sehr geringem Arbeitsaufwand Potenzreihen fuĚr die
Arcusfunktionen erhalten werden koĚnnen ? Insbesondere sollten Sie folgende Formel gefunden haben:
arctan x =

â
X
(â1)n 2n+1
x
,
2n + 1
n=0

â1 â¤ x â¤ 1.

Wir beschlieĂen den Abschnitt mit einigen kulturellâhistorischen Betrachtungen. Wenn wir in der
ArcustangensâPotenzreihe x = 1 einsetzen, dann bekommen wir die erstaunliche Formel
1 1 1 1
Ď
= 1 â + â + â ...,
4
3 5 7 9
die rein theoretisch verwendet werden koĚnnte, um Ď auszurechnen; aber in Wirklichkeit ist sie weitgehend
unbrauchbar, denn man benoĚtigt mindestens 10.000 Summanden, um Ď auf 4 Stellen nach dem Komma zu
ermitteln. Diese Formel wurde in Kontinaleuropa 1682 gefunden von Leibniz, aber sie war im 14. Jahrhundert schon in Indien bekannt.
Bemerkenswert ist, daĂ man nach einigen Transformationen (elementar, aber nur mit GespuĚr zu finden)
deutlich schneller konvergierende Reihen angeben kann, die tatsaĚchlich fuĚr die Bestimmung von Ď genutzt
wurden. Wir folgen hierbei den Darlegungen von John Machin aus dem Jahr 1706. Und zwar haben wir
tan x + tan y
= tan(x + y),
1 â tan x Âˇ tan y
also auch (wenn wir t = tan x und s = tan y setzen)


t+s
arctan
= x + y = arctan t + arctan s.
1 â ts
Sei a 6â {1, 1/2} eine positive Zahl, und seien t :=

a
aâ1

sowie s :=

â1
2aâ1 .

Dann haben wir37

1
(2a â 1) â (a â 1)
a
1
â
=1+
=1+
a â 1 2a â 1
(a â 1)(2a â 1)
(a â 1)(2a â 1)
a
â1
=1â
Âˇ
= 1 â ts,
a â 1 2a â 1

t+s=1+

also bekommen wir dann




Ď
1
a
â arctan
.
= arctan
4
aâ1
2a â 1
Jetzt waĚhlen wir a clever, naĚmlich a = 120, und es ergibt sich dann
120
1
Ď
= arctan
â arctan
.
4
119
239
Den ersten Arcustangens koĚnnen wir deutlich verschoĚnern. Es ist naĚmlich
5
2 Âˇ 12
120
120
2 Âˇ 5 Âˇ 12
=
=
=
5
119
144 â 25
12 Âˇ 12 â 5 Âˇ 5
Âˇ
1 â 12

5
12

=

5
12

+

5
12

1â

5
12

Âˇ

5
12

,

also entsteht
arctan
37 dies

120
5
5
5
= arctan
+ arctan
= 2 arctan .
119
12
12
12

ist eine sehr schoĚne UĚbung darin, Klammern moĚglichst nicht auszumultiplizieren

146

KAPITEL 6. FUNKTIONEN

Das war so entzuĚckend, daĂ wir diesen Umformungstrick nochmal wiederholen:
2 Âˇ 51
10
2Âˇ1Âˇ5
5
=
=
=
12
24
5Âˇ5â1Âˇ1
1 â 15 Âˇ

1
5

=

1
5

+ 51
,
1 â 15 Âˇ 51

und das liefert uns dann
1
1
1
5
= arctan + arctan = 2 arctan ,
arctan
12
5
5
5
und insgesamt entsteht somit die Formel von John Machin:
Ď
1
1
= 4 arctan â arctan
.
4
5
239
Der groĂe Vorteil dieser Darstellung gegenuĚber der LeibnizâReihe aus dem Jahr 1682 ist jetzt, daĂ die
Argumente der Arcustangensfunktionen deutlich naĚher an der Null sind, sodaĂ die Potenzreihen jetzt
wesentlich schneller konvergieren. Jeder neue Summand ist offenkundig mindestens 25 mal kleiner als sein
VorgaĚnger, und tatsaĚchlich hat Machin im Jahr 1706 mit dieser Formel die Zahl Ď ohne technische Hilfsmittel
auf 100 Dezimalstellen berechnet.

6.5.3

Die Hyperbelfunktionen

Definition 6.87. Die Funktionen
sinh : C â C,
cosh: C â C,


Ďi
tanh : C \
+ ĎiZ â C,
2

1
(exp(z) â exp(âz)),
2
1
cosh : z 7â cosh(z) = (exp(z) + exp(âz)),
2
sinh(z)
exp(z) â exp(âz)
tanh : z 7â tanh(z) =
=
,
cosh(z)
exp(z) + exp(âz)
sinh : z 7â sinh(z) =

heiĂen Sinus hyperbolicus, Cosinus hyperbolicus, Tangens hyperbolicus38 .
Der Name kommt von der Formel cosh2 (x) â sinh2 (x) = 1 fuĚr x â R, die besagt, daĂ die Punkte der Form
(cosh x, sinh x) auf dem Ast einer Hyperbel liegen.
Nach diesen Definitionen untersuchen wir die uĚblichen Eigenschaften
â˘ Additionstheoreme,
â˘ Symmetrieeigenschaften (z.B. Invarianz der Funktionen unter der Spiegelung x 7â âx oder unter
einer Verschiebung x 7â x + p),
â˘ Ableitungen,
â˘ Potenzreihendarstellungen,
â˘ Umkehrfunktionen (aber nur in R, nicht in C !).
Satz 6.88. Im Definitionsbereich dieser Funktionen gelten die folgenden Eigenschaften:
sinh(z + w) = sinh z cosh w + cosh z sinh w,
cosh(z + w) = cosh z cosh w + sinh z sinh w,
tanh z + tanh w
tanh(z + w) =
,
1 + tanh z tanh w
sinh(âz) = â sinh z,

cosh(âz) = cosh z,

sinhâ˛ (z) = cosh z,

coshâ˛ (z) = sinh z,

sinh(z + 2Ďi) = sinh z,
sinh z = âi sin(iz),
â
X
z 2k+1
sinh z =
,
(2k + 1)!

cosh(z + 2Ďi) = cosh z,
cosh z = cos(iz),
â
X
z 2k
cosh z =
.
(2k)!

k=0

38 hyperbolic

sine, hyperbolic cosine, hyperbolic tangent

k=0

tanh(âz) = â tanh z,
1
,
tanhâ˛ (z) =
cosh2 (z)
tanh(z + Ďi) = tanh z,
tanh z = âi tan(iz),

6.5. ELEMENTARE FUNKTIONEN

147

Beweis. Eine wunderbare Gelegenheit, den Umgang mit elementaren Funktionen zu uĚben !
FuĚr den Tangens hyperbolicus bekommen wir auf bekanntem Wege die Reihendarstellung
1
2
17 7
62 9
1382 11
21844 13
929569 15
tanh z =z â z 3 + z 5 â
z +
z â
z +
z â
z
3
15
315
2835
155925
6081075
638512875
+ O(z 17 ),
z â 0.
Der Konvergenzradius ist

Ď
2.

Satz 6.89 (Areafunktionen). Die Funktion y = sinh(x) bildet das Intervall R = (ââ, â) bijektiv auf
sich ab und hat dort die Umkehrfunktion


p
x = Arsinh(y) = ln y + y 2 + 1 .
Die Funktion y = cosh(x) bildet [0, â) bijektiv auf [1, â) ab und hat dort die Umkehrfunktion


p
x = Arcosh(y) = ln y + y 2 â 1 .

Die Funktion y = tanh(x) bildet R = (ââ, â) bijektiv auf (â1, 1) ab und hat dort die Umkehrfunktion
x = Artanh(y) =

1 1+y
ln
.
2 1ây

Diese Funktionen heiĂen Area sinus hyperbolicus, Area cosinus hyperbolicus, Area tangens hyperbolicus39 .
Beweis. UĚbungsaufgabe.
Die Namen cosh und sinh erklaĚren sich wie folgt.
Am Einheitskreis (beschrieben durch die Gleichung x2 + y 2 = 1) betrachten wir den Sektor mit den Ecken
(1, 0) und (x, y). Dann gilt: falls dieser Sektor den FlaĚcheninhalt Ď/2 besitzt, so ist (x, y) = (cos Ď, sin Ď).
Siehe Abbildung 6.3.
An der Einheitshyperbel (beschrieben durch die Gleichung x2 â y 2 = 1) betrachten wir den Sektor mit
den Ecken (1, 0) und (x, y). Dann gilt: falls dieser Sektor den FlaĚcheninhalt Ď/2 besitzt, so ist (x, y) =
(cosh Ď, sinh Ď). Siehe Abbildung 6.4.

6.5.4

Wurzeln aus komplexen Zahlen

In diesem Abschnitt ziehen wir Wurzeln aus komplexen Zahlen. Diese bilden allerdings keine Wurzelfunktion, denn die Bedingung der Eindeutigkeit ist verletzt.
Satz 6.90. Sei n â N.
1. Dann gibt es genau n verschiedene komplexe Zahlen Îś0 , . . . , Îśnâ1 als LoĚsungen der Gleichung
z n = 1.
Dies sind die Zahlen


j
,
Îśj = exp 2Ďi
n

j = 0, 1, . . . , n â 1.

2. Sei w = |w| exp(iĎ) â C. Dann gibt es genau n verschiedene komplexe Zahlen z0 , . . . , znâ1 als
LoĚsungen der Gleichung
z n = w.

39

Diese werden gegeben durch zj = z0 Îśj , j = 0, 1, . . . , n â 1, wobei
 Ď
p
z0 = n |w| exp i
.
n
p
Hierbei bedeutet n |w| die gewoĚhnliche nichtnegative Wurzel aus einer nichtnegativen reellen Zahl.

Nicht Arcus . . .

148

KAPITEL 6. FUNKTIONEN

Abbildung 6.3: Ein Sektor am Einheitskreis

Abbildung 6.4: Ein Sektor an der Einheitshyperbel

6.6. VERFAHREN ZUR NUMERISCHEN LOĚSUNG NICHTLINEARER GLEICHUNGEN
Beweis.

149

1. Sei z n = 1. Wir koĚnnen z schreiben als z = |z| exp(iĎ) mit Ď â R. Dann ist
1 = z n = |z|n exp(inĎ),

also |z| = 1 und somit exp(inĎ) = 1.

Wir wissen aus Satz 6.83: wenn exp(iĎ) = 1, dann ist Ď = 2jĎ fuĚr ein j â Z. Die Relation nĎ = 2jĎ
fuĚhrt gerade auf die Darstellung der Îśj .
2. Man rechnet nach, daĂ diese zj tatsaĚchlich LoĚsungen sind. Sei nun z â eine weitere LoĚsung von z n = w.
â
Dann ist der Quotient zz0 aber eine LoĚsung von z n = 1. Und diese Gleichung hat lediglich die LoĚsungen
Îś0 , . . . , Îśnâ1 . Also kann es auĂer den z0 , . . . , znâ1 keine weiteren LoĚsungen der Gleichung z n = w
geben.

Die Zahlen Îś0 , . . . , Îśnâ1 heiĂen n-te Einheitswurzeln, und die Zahl Eins wird auch gern als Einheit bezeichnet.

6.6

Verfahren zur numerischen LoĚsung nichtlinearer Gleichungen

Lineare Gleichungen und Systeme von linearen Gleichungen koĚnnen mittels Standardmethoden geloĚst werden (und zwar exakt ); und die Theorie der Matrizen stellt viele Hilfsmittel bereit, die LoĚsungen naĚher zu
beschreiben.
DemgegenuĚber ist die Situation bei nichtlinearen Gleichungen viel schwieriger:
â˘ Man kann manchmal nur hoffen, daĂ es uĚberhaupt LoĚsungen gibt.
â˘ Oft weiĂ man nicht, wieviele LoĚsungen es sind.
â˘ Es ist keineswegs einfach, diese LoĚsungen dann auch noch zu finden.
â˘ Und schlieĂlich kann man nur in wenigen FaĚllen die LoĚsungen exakt ermitteln; meist muĂ man sich
mit NaĚherungswerten begnuĚgen.
Beispiel 6.91. Wir versuchen die Gleichungen
y = ey â 2

x = cos(x),
zu loĚsen, wobei x, y > 0. Wir erhalten die Wertetabellen
x
0
Ď/2

y
0
1
2

cos(x)
1
0

ey â 2
â1
0.718
5.39

Das fuĚhrt uns auf die Idee einer Iteration,
x0 := 0,

xn+1 := cos(xn ),

Damit erhalten wir dann
n xn
0 0
1 1
2 0.54
3 0.86
bzw.
4 0.65
5 0.79
6 0.70
7 0.76
8 0.72

bzw.

y0 := 1.5,

n
0
1
2
3
4

yn
1.5
2.48
9.96
21191.5
E

yn+1 := exp(yn ) â 2.

150

KAPITEL 6. FUNKTIONEN

Im ersten Fall strebt die Folge gegen 0.739085133, im zweiten Fall gegen â. Wir unternehmen einen zweiten
Versuch fuĚr den miĂgluĚckten zweiten Fall: y + 2 = ey , also sei jetzt
y0 := 1.5,

yn+1 := ln(yn + 2).

Das fuĚhrt uns auf
n
0
1
2
3
4

yn
1.5
1.25
1.18
1.157
1.149

mit Grenzwert 1.146193. Dieser ist eine LoĚsung zu y = ey â 2.
Warum versagt das eine Verfahren, aber das andere nicht ?

6.6.1

Das Halbierungsverfahren

ZunaĚchst erinnern wir an ein einfaches Verfahren, das in solchen Situationen immer funktioniert. Wir
hatten es schon beim Beweis des Zwischenwertsatzes benutzt.
Wir suchen zum Beispiel die Nullstelle von f = f (y) = ey â 2 â y. Dazu starten wir von zwei Werten y0 und
y1 , fuĚr die f (y0 ) und f (y1 ) verschiedenes Vorzeichen haben. Dazwischen muĂ eine Nullstelle von f liegen,
denn f ist stetig. Als naĚchstes probieren wir y2 := 21 (y0 + y1 ), und so weiter:
n
0
1
2
3
4
5

yn
1
2
1.5
1.25
1.125
1.1875

f (yn )
â0.28
3.39
0.98
0.24
â0.045
...

Die Fehlerschranke fuĚr die Nullstelle verbessert sich mit jedem Schritt um den Faktor
konvergiert langsam, dafuĚr aber auf jeden Fall.

6.6.2

1
2.

Das Verfahren

Funktionaliteration und der Banachsche Fixpunktsatz

Definition 6.92. Sei F eine Abbildung einer Menge M in sich. Wir sagen, daĂ xâ â M ein Fixpunkt40
von F ist, wenn F (xâ ) = xâ .
Satz 6.93 (Banachscher Fixpunktsatz41 ). Sei M â U abgeschlossen, U ein Banachraum, und F : M â
M mit kF (x) â F (y)kU â¤ Îą kx â ykU fuĚr alle x, y â M , wobei Îą < 1.

Dann hat F genau einen Fixpunkt in M , F (xâ ) = xâ , der gefunden werden kann durch den Algorithmus
x0 â M beliebig,
xn+1 := F (xn ),

n â N0 .

Wir haben die FehlerschaĚtzung
kxâ â xn kU â¤

Îąn
kx1 â x0 kU .
1âÎą

Beweis. ZunaĚchst haben wir
kx2 â x1 kU = kF (x1 ) â F (x0 )kU â¤ Îą kx1 â x0 kU ,

kx3 â x2 kU = kF (x2 ) â F (x1 )kU â¤ Îą kx2 â x1 kU â¤ Îą2 kx1 â x0 kU ,
..
.
kxn+1 â xn kU â¤ Îąn kx1 â x0 kU .
40 fixed
41

point
Banach fixed point theorem

(6.8)

6.6. VERFAHREN ZUR NUMERISCHEN LOĚSUNG NICHTLINEARER GLEICHUNGEN

151

Sei nun n < m. Dann ergibt sich
kxm â xn kU = k(xm â xmâ1 ) + (xmâ1 â xmâ2 ) + Âˇ Âˇ Âˇ + (xn+1 â xn )kU
â¤ kxm â xmâ1 kU + kxmâ1 â xmâ2 kU + Âˇ Âˇ Âˇ + kxn+1 â xn kU
â¤ (Îąmâ1 + Îąmâ2 + Âˇ Âˇ Âˇ + Îąn ) kx1 â x0 kU
â
X
â¤
Îąk kx1 â x0 kU
k=n
n

=

Îą
kx1 â x0 kU â 0,
1âÎą

falls n â â. Also ist (xn )nâN eine Cauchyfolge. Weil der Raum U als Banachraum keine LoĚcher hat, besitzt
diese Folge einen Grenzwert xâ , limnââ kxn â xâ kU = 0. Und weil M abgeschlossen ist, liegt xâ in M .

Wenn wir in der eben gezeigten Ungleichung
kxm â xn kU â¤

Îąn
kx1 â x0 kU ,
1âÎą

m > n,

den Index m nach Unendlich schicken und Satz 5.24 clever einsetzen, dann bekommen wir genau die
FehlerschaĚtzung (6.8).
Dieser Grenzwert xâ ist Fixpunkt der Abbildung F , denn
xâ = lim xn = lim xn+1 = lim F (xn ) = F ( lim xn ) = F (xâ ).
nââ

nââ

nââ

nââ

Hierbei haben wir im vorletzten Schritt die Stetigkeit von F (als kommutatives Diagramm) benutzt. Und die
Stetigkeit von F wiederum folgt aus Satz 6.18 (Teil 4) mit Î´0 (Îľ) = Îľ/Îą, in Verbindung mit Bemerkung 6.20.
Es kann keinen zweiten Fixpunkt geben, denn falls xââ = F (xââ ), dann gilt
kxââ â xâ kU = kF (xââ ) â F (xâ )kU â¤ Îą kxââ â xâ kU ,
was nur fuĚr xâ = xââ moĚglich ist.
Damit ist der Banachsche Fixpunktsatz bewiesen.
Beispiel 6.94. Sei F : [a, b] â [a, b], und |F â˛ (x)| â¤ Îą < 1 auf [a, b]. Dann gibt es genau ein x â [a, b] mit
F (x) = x.
Warum ist das so ?
Beispiel 6.95. Bezogen auf das Beispiel der Gleichung x = cos(x) heiĂt das: [a, b] = [0.1, 1], F (x) = cos(x),
also |F â˛ (x)| = | sin(x)|. Auf dem Intervall [0.1, 1] ist die Sinusfunktion echt kleiner als 1, also koĚnnen
wir argumentieren wie eben. (Die Wahl der Intervallgrenzen 0.1 und 1 hat sich dabei nach Probieren als
zweckmaĚĂig herausgestellt.)
Frage: Warum kam es fuĚr die Iteration yn+1 := exp(yn ) â 2 im obigen Beispiel zur Divergenz ?
Ein Beispiel aus der Physik
Wir betrachten ein Elektron in einem Potentialtopf. Die Ortsvariable x ist aus dem R1 , und das Potential
V ist
(
V0 : â a < x < a,
V (x) =
0
: |x| âĽ a,
wobei a > 0 und V0 < 0 gegeben sind. Wenn E der Energiewert des Elektrons ist und m seine Masse, dann
erfuĚllt die Wellenfunktion Ď : R â C des Elektrons im stationaĚren Fall die Differentialgleichung


Z â
~2 d2
â
+
V
(x)
Ď(x)
=
EĎ(x),
x
â
R,
|Ď(x)|2 dx = 1.
2m dx2
x=ââ
Wir betrachten nur gebundene ZustaĚnde, also ist V0 < E < 0. Zuerst skalieren wir die Konstanten weg,
also fuĚhren wir reelle Parameter Îş, Îş0 und K ein mit
E =: â

~2 2
Îş ,
2m

V0 =: â

~2 2
Îş ,
2m 0

E â V0 =:

~2 2
K ,
2m

152

KAPITEL 6. FUNKTIONEN

und es ergeben sich dann die Differentialgleichungen

 2
d
2
ââ < x < âa,
â Îş Ď(x) = 0,
dx2
 2

d
ââ < x < âa,
+ K 2 Ď(x) = 0,
dx2

 2
d
a < x < +â.
â Îş2 Ď(x) = 0,
dx2
Die Parameter erfuĚllen die Nebenbedingung K 2 = Îş20 â Îş2 , und Îş0 ist gegeben, waĚhrend K gesucht ist.
Die LoĚsungen der Differentialgleichungen sind
ďŁą
Îşx
âÎşx
ďŁ´
: â â < x < âa,
ďŁ˛ a1 e + a2 e
Ď(x) = b1 eiKx + b2 eâiKx : â a < x < a,
ďŁ´
ďŁł Îşx
c1 e + c2 eâÎşx
: a < x < â.

Râ
Die Bedingung x=ââ |Ď(x)|2 dx = 1 erzwingt dann c1 = 0 und a2 = 0. Es bleiben also noch vier Parameter
a1 , b1 , b2 und c2 uĚbrig. An den UĚbergangsstellen x = Âąa soll Ď stetig sein und Ďâ˛ ebenfalls. Dies ergibt vier
Gleichungen fuĚr die vier Parameter. Dieses Gleichungssystem ist linear und homogen. Damit wir allerdings
eine physikalisch relevante LoĚsung bekommen, muĂ der Rang der Matrix weniger als vier sein (ansonsten
haĚtten wir nur die LoĚsung a1 = b1 = b2 = c2 = 0, die keinen physikalischen Zustand beschreibt).
Aus physikalischen UĚberlegungen ergibt sich: weil V symmetrisch ist (denn V (âx) = V (x)), ist die Wellenfunktion Ď entweder symmetrisch (also Ď(âx) = Ď(x)) oder antisymmetrisch (also Ď(âx) = âĎ(x)).
Im symmetrischen Fall ist b1 = b2 und a1 = c2 , und wir erhalten (nach einiger Rechnung im Selbststudium)
aus der Rangbedingung an die Matrix die notwendige Bedingung
tan(Ka) =

Îş
.
K

Und im antisymmetrischen Fall ist b1 = âb2 und a1 = âc2 , und die Rangbedingung an die Matrix liefert
uns jetzt
tan(Ka) = â

K
.
Îş

Wir sehen K als gesucht an und werfen jetzt Îş heraus: es ist 0 â¤ Îş â¤ Îş0 und Îş =
folgende hochgradig nichtlineare Gleichungen nach K aufzuloĚsen:
p
Îş20 â K 2
âK
!
!
,
tan(Ka) = p
tan(Ka) =
.
K
Îş20 â K 2

p
Îş20 â K 2 , also sind

Eine Arbeitsstrategie koĚnnte die folgende sein:

â˘ man plotte jeweils die linke Gleichungsseite als Funktion von K, und die rechte Gleichungsseite auch,
â˘ man bestimme geometrisch die Anzahl der Kreuzungspunkte der Graphen (diese Anzahl gibt uns an,
wieviele Energieniveaus in diesem Potentialtopf moĚglich sind),
â˘ die tatsaĚchlichen Werte fuĚr K koĚnnte man mit dem Halbierungsverfahren bestimmen, oder mit einer
behutsam eingesetzten Funktionaliteration. Aus diesen Werten fuĚr K ermitteln wir dann die Energieniveaus, die ein Elektron haben kann. Genau an diesen Energieniveaus waren wir interessiert.

6.6.3

Das Newtonverfahren

Definition 6.96. Wir sagen, daĂ eine Folge (xn )nâN von Elementen eines Banachraums linear gegen einen
Grenzwert xâ konvergiert, wenn
kxn+1 â xâ kU â¤ Îą kxn â xâ kU

6.6. VERFAHREN ZUR NUMERISCHEN LOĚSUNG NICHTLINEARER GLEICHUNGEN

153

gilt fuĚr alle n, mit einem gewissen Îą < 1, das nicht von n abhaĚngt.
Wir sagen, daĂ die Folge (xn )nâN quadratisch konvergiert, wenn
kxn+1 â xâ kU â¤ C kxn â xâ k2U
gilt, falls xn nahe bei xâ ist. Hierbei ist C eine positive Konstante, die nicht von n abhaĚngt, aber auch
groĚĂer als eins sein darf.
Das Newtonverfahren ist ein Beispiel fuĚr ein quadratisch konvergentes Iterationsverfahren.
Noch ein Wort zur Schreibweise: fuĚr Abbildungen mit gesuchtem Fixpunkt xâ schreiben wir GroĂ F , und
fuĚr Funktionen mit gesuchter Nullstelle xâ schreiben wir klein f .
Das Newtonverfahren im R1
Sei f : R â R eine zweimal stetig differenzierbare Funktion mit Nullstelle xâ , und sei f â˛ (xâ ) 6= 0. Dann ist
das Newtonverfahren wie folgt definiert:
x0 nahe genug an xâ ,
xn+1 := xn â

f (xn )
,
f â˛ (xn )

n = 0, 1, . . .

Wenn die Folge der xn konvergieren sollte gegen einen Grenzwert, dann strebt die Folge (xn+1 â xn )nâN
der Differenzen benachbarter Folgenglieder nach 0, also geht die Folge (f (xn ))nâN ebenfalls nach 0.
Satz 6.97. Es gibt ein Î´ > 0, sodaĂ fuĚr das Intervall M = [xâ â Î´, xâ + Î´] gilt:
1. Die Iterationsvorschrift des Newtonverfahrens bildet M in sich ab.
2. Das Newtonverfahren konvergiert quadratisch gegen xâ .
Beweis. ZunaĚchst ist f â˛ (xâ ) 6= 0. Wir waĚhlen nun Î´ so klein, daĂ im Intervall M uĚberall f â˛ (x) 6= 0 ist. Das
geht wegen Lemma 6.7 und der Stetigkeit von f â˛ . Als naĚchstes basteln wir uns eine Hilfsfunktion
F (x) := x â

f (x)
,
f â˛ (x)

x â M.

Eine Division durch Null kann fuĚr x â M nicht eintreten. FuĚr diese Hilfsfunktion rechnet man schnell nach,
daĂ
F (xâ ) = xâ ,

F â˛ (x) =

f (x)f â˛â˛ (x)
,
(f â˛ (x))2

F â˛ (xâ ) = 0.

Wir duĚrfen fordern, daĂ Î´ so klein ist, daĂ im Intervall M gilt: |F â˛ (x)| â¤ 12 , wegen der Stetigkeit von F â˛ .

Das Newtonverfahren wird nun beschrieben durch die Vorschrift xn+1 := F (xn ). Sei xn â M , also |xn âxâ | â¤
Î´. Der Mittelwertsatz liefert dann, daĂ |xn+1 â xâ | â¤ 12 |xn â xâ | â¤ 21 Î´, also xn+1 â M . Damit kann
das Banach-Fixpunkt-Verfahren fuĚr die Funktion F durchgefuĚhrt werden und wir bekommen sofort die
Konvergenz gegen limnââ xn = xâ sowie eine Fehlerschranke:
|xn+1 â xn | â¤

1
2nâ1

Î´.

Diese Schranke koĚnnen wir aber gewaltig verbessern. Dazu gehen wir zuruĚck zur Funktion f , fuĚr die wir
eine Taylorentwicklung im Entwicklungspunkt xn veranstalten:
1
0 = f (xâ ) = f (xn + (xâ â xn )) = f (xn ) + f â˛ (xn )(xâ â xn ) + f â˛â˛ (Îžn )(xâ â xn )2 .
2
Hierbei liegt Îžn zwischen xn und xâ ; mehr ist vom Îžn nicht bekannt. Wir dividieren durch f â˛ (xn ) und
sortieren um:
f (xn )
f â˛â˛ (Îžn ) â
+ xâ â xn + â˛
(x â xn )2 ,
â˛
f (xn )
2f (xn )
f â˛â˛ (Îžn ) â
(x â xn )2 .
xn+1 â xâ =
2f â˛ (xn )
0=

Weil dieser Bruch beschraĚnkt auf M ist, ist dies genau die gewuĚnschte quadratische Konvergenz.

154

KAPITEL 6. FUNKTIONEN

Beispiel 6.98 (Babylonisches Wurzelziehen). Wir wollen die Wurzel aus 2 ziehen. Sei also f (x) =
x2 â 2, und x0 = 1. Dann haben wir die Iterationsvorschrift


x2n â 2
2
1
xn+1 := xn â
xn +
.
=
2xn
2
xn
Diese Iterationsvorschrift war schon den alten Sumerern in Babylon bekannt, vor etwa 4000 Jahren. Eine
entsprechende Formel gibt es auch fuĚr hoĚhere Wurzeln; dann ist f (x) = xn â a. Diese Formel geht auf
Heron 42 zuruĚck. Es ergibt sich folgende Tabelle. Man beobachte die Verdopplung der Anzahl korrekter
Dezimalstellen, worin sich die quadratische Konvergenz widerspiegelt.
n
0
1
2
3
4
5
6

6.7

xn
1
1.5
1.416666666666667
1.41421568627451
1.41421356237469
1.414213562373095
1.414213562373095

Anzahl richtiger Stellen
1
1
3
6
12
âĽ 16
âĽ 16

SchluĚsselbegriffe

â˘ Definition Grenzwert und Stetigkeit einer Funktion,
â˘ Satz vom Maximum, Zwischenwertsatz,
â˘ Definition von Ableitung und LandauâSymbolen,
â˘ Produktregel, Kettenregel, Ableitung der Umkehrfunktion,
â˘ Mittelwertsatz, Regel von Bernoulli und de lâHospital,
â˘ Taylorsatz und seine Beziehung zu Extremwerten,
â˘ komplexe Winkelfunktionen, Arcusfunktionen, Hyperbelfunktionen, Areafunktionen,
â˘ Wurzeln aus komplexen Zahlen im Gegensatz zur Wurzelfunktion,
â˘ Banachscher Fixpunktsatz und Newtonverfahren.

42 Heron

von Alexandria, lebte irgendwann zwischen 150 vor Christus und 250 nach Christus.

Anhang A

Algebraische Strukturen
Name

Operationen

Halbgruppe

âŚ

Gruppe

âŚ
jede Gleichung a âŚ x = b bzw.
y âŚ a = b ist loĚsbar

Ring

+, â und Âˇ

KoĚrper

+, â, Âˇ und /

Vektorraum

Vektor + Vektor = Vektor
Zahl Âˇ Vektor = Vektor

normierter
Raum

wie Vektorraum U ,
aber zusaĚtzlich noch
Norm kÂˇk : U â RâĽ0

euklidischer/
unitaĚrer
Raum
Banachraum/
vollstaĚnd.
normierter
Raum
Hilbertraum

wie Vektorraum U ,
aber zusaĚtzlich noch reelles /
komplexes Skalarprodukt
hÂˇ, Âˇi,
p
und Norm kukU := hu, ui
wie normierter Raum

wie euklidischer bzw.
unitaĚrer Raum

Beispiele und Anmerkungen
Bsp: (N0 , +)
Bsp: Evolutionsoperator eines Diffusionsprozesses
Anm: âŚ braucht nicht kommutativ zu sein
Bsp: (Z, +)
Bsp: Verschiebungsgruppe
Bsp: Gruppe der Drehungen
Bsp: alle invertierbaren Matrizen des RnĂn
Bsp: (Z, +, Âˇ)
Bsp: Restklassenring bei Division durch k â Z6=0
Bsp: Polynomring
Bsp: Matrizenring RnĂn
Anm: + ist kommutativ, aber Âˇ nicht unbedingt
Bsp: Q, R und C
Bsp: Restklassenring bei Division durch Primzahl
Bsp: gebrochen rationale Funktionen
Bsp: Rn als Menge der Verschiebungspfeile
Bsp: Rn als Zahlenspalten
Bsp: Funktionenraum L2 ([a, b] â R)
Bsp: Matrizen RnĂm
Bsp: Menge der linearen Abbildungen Hom(U â V )

Bsp: Rn mit kxk1 = |x1 | + Âˇ Âˇ Âˇ + |xn |
Bsp: Rn mit kxk2 = (x21 + Âˇ Âˇ Âˇ + x2n )1/2
Bsp: Rn mit kxkâ = max{|x1 |, . . . , |xn |}
Anm: Norm entstammt genau dann einem Skalarprodukt, wenn die Parallelogrammgleichung gilt
Pn
Bsp: Rn mit hx, yi = j=1 xj yj
P
Bsp: Cn mit hx, yi = nj=1 xj yj
Rb
Bsp: C([a, b] â R) mit hf, gi = a f (x)g(x)dx
Rb
Bsp: C([a, b] â C) mit hf, gi = a f (x)g(x)dx
Anm: jede Cauchyfolge konvergiert
Bsp: Rn und Cn mit jeder Norm
Bsp: L2 ([a, b] â R)
Bsp: C([a, b] â R) mit kÂˇkâ

Anm: jede Cauchyfolge konvergiert
Anm: per Definition ist jeder Hilbertraum gleichzeitig euklidisch/unitaĚr und Banachraum
Bsp: L2 ([a, b] â R)

155

156

ANHANG A. ALGEBRAISCHE STRUKTUREN

Es gibt noch weitere physikalisch relevante algebraische Strukturen, die im Skript zwar gelegentlich verwendet wurden, ohne aber definiert worden zu sein. Insbesondere sind Algebren interessant. Grob gesprochen,
ist eine (assoziative) Algebra ein Vektorraum, der gleichzeitig ein Ring ist. Das heiĂt, zusaĚtzlich zu den
beiden Vektorraumoperationen Vektor + Vektor = Vektorâ und Zahl Âˇ Vektor = Vektorâ gibt es noch
â
â
eine Ringoperation Vektor mal Vektor = Vektorâ, die in jedem Faktor linear ist und teilweise noch weitere
â
Eigenschaften hat. FuĚr diese weitere Multiplikation verwenden wir das Symbol âŚ.
Name

Operationen

Beispiele und Anmerkungen

Algebra

wie Vektorraum,
und zusaĚtzlich
Vektor âŚ Vektor = Vektor

Anm: âŚ braucht weder kommutativ noch assoziativ
sein, und ein neutrales Element braucht es auch nicht
geben
Bsp: R3 mit Kreuzprodukt

assoziative
Algebra

LieâAlgebra

wie Algebra

wie Algebra,
schreibe [x, y] statt xâŚy

Anm: âŚ ist jetzt assoziativ
Bsp: Matrizen aus RnĂn mit âŚ = Matrizenmultipl.
Bsp: Hom(U â U ) mit âŚ = NacheinanderausfuĚhrung
Bsp: L1 (R â R) mit âŚ = â,
und â ist dasRFaltungsprodukt
â
(f â g)(x) = y=ââ f (x â y)g(y)dy
Anm: per Definition ist
[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0 und [x, x] = 0
Bsp: R3 mit Kreuzprodukt
Bsp: Hom(U â U ) mit [A, B] := A âŚ B â B âŚ A,
hierbei ist âŚ = NacheinanderausfuĚhrung
Bsp: Drehimpulsoperatoren der Quantenmechanik

Ein Buch, das uĚber mehrere hundert Seiten hinweg LieâAlgebren diskutiert, ist:
Literatur: Greiner und MuĚller: Quantenmechanik. Symmetrien, 2005

Index
Abbildung
lineare, 65, 73
Abel, 30
abgeschlossene Menge, 95, 97
Ableitung, 12, 123
AbschluĂ, 95, 97
absolut konvergent, 99
absolute Konvergenz, 107
Additionstheorem, 113, 138
adjungierte Matrix, 67
affiner Raum, 46
aĚhnliche Matrizen, 83
allgemeine Potenz, 123
analytisch, 124
Anfangswertabbildung, 84
Approximationsproblem, 60
Arcusfunktionen, 144
Argument einer komplexen Zahl, 17
assoziativ, 29
Ausbreitungsgeschwindigkeit, 22
Austauschlemma, 54
Austauschsatz, 54
Banach, 97
Banach-scher Fixpunktsatz, 150
Banachraum, 97
Basis, 23, 50
duale, 90
reziproke, 90
BasisergaĚnzungssatz, 52
Basissatz, 52
Basistransformation, 81
Bernoulli, 131
beschraĚnkte Menge, 95, 97
bestimmte Divergenz, 117
bestimmtes Integral, 13
Betrag, 25
Betrag einer komplexen Zahl, 16
bijektiv, 75
Bild, 76, 115
Bildraum, 76
Binomialkoeffizient, 102, 135
binomische Reihe, 136
Blindwiderstand, 21
Bolzano, 104
C, 14
Cardano, 13
Cauchy, 25
Cauchy-Folge, 97
Cauchy-Produktreihe, 108, 111

cos, 144
cosh, 146
Defekt, 77
Definitionsbereich, 115
Delta-Distribution, 74
Determinante, 39
Differential, 124
Differentialgleichungssystem, 84
Differentialoperator, 84
differenzierbar, 12, 19, 123
stetig, 123
Dimension, 55
Dimensionsformel fuĚr Abbildungen, 77
Dimensionsformel fuĚr UnterraĚume, 56
Dirac, 74
direkte Summe, 57
Divergenz, 63
Divisionsregel, 127
Drehmatrix, 28, 45
Drehstreckung, 17
Dreiecksungleichung, 58
duale Basis, 90
Dualraum, 90
ebene Welle, 22
Effektivwert, 21
komplexer, 21
Einheitsmatrix, 29
einseitiger Grenzwert, 117
Element
inverses, 30
neutrales, 30
Eliminationsmatrix, 69
endlich erzeugt, 49
ÎľâUmgebung, 95, 97
Erzeugendensystem, 49
Euklid, 57
euklidischer Raum, 57
Euler, 112
Euler-sche Zahl e, 112
Exponentialfunktion, 18, 113
Familie, 40
Folge, 96
konvergente, 93, 97
Fourierreihe, 99
frei, 50
Funktion, 12, 115
Grenzwert, 116
Konvergenz, 116
157

158
Funktionale
lineare, 73
GauĂ, 14
GauĂ-Jordan-Form, 68
geometrische Reihe, 99, 109
gleichmaĚĂige Konvergenz, 109
Gleichungssystem, 67
Gradient, 63
Gram, 59
Grenze
obere, 103
untere, 103
Grenzwert
einseitiger, 117
uneigentlicher, 117
Grenzwert einer Folge, 93, 97
Grenzwert einer Funktion, 116
Gruppe, 30
lineare, 34
orthogonale, 34
spezielle lineare, 34
spezielle orthogonale, 34
Halbgruppe, 29
Hamilton, 14
harmonische Reihe, 99
Hauptsatz der Differential- und Integralrechnung, 13
Heisenberggruppe, 34
HelmholtzâProjektion, 63
Hermite, 57
Hilbertraum, 97
holomorph, 124
homogenes System, 84
Homomorphismus, 65, 73
de lâHospital, 131
Hyperbelfunktionen, 146
HaĚufungspunkt, 95, 97
ImaginaĚrteil, 14
Induktion, 54
Infimum, 103
inhomogenes System, 84
injektiv, 75, 121
innerer Punkt, 95, 97
Integral
bestimmtes, 13
unbestimmtes, 13
inverse Matrix, 68
invertierbare Matrix, 68
isomorph, 76
Isomorphismus, 76
Jordan
Camille, 71
Pascual, 71
Wilhelm, 71
Kern, 76
Kettenregel, 127

INDEX
kommutatives Diagramm, 16
kompakte Menge, 105
Komplement
orthogonales, 58
Komplementmenge, 95
komplexe Winkelfunktionen, 144
komplexe Zahl, 14
Komposition, 119
konjugiert komplexe Zahl, 16
Kontraposition, 75
kontravariant, 90
konvergent, 93, 97
Konvergenz
absolute, 99, 107
einer Reihe, 99
gleichmaĚĂige, 109
Konvergenz einer Funktion, 116
Konvergenzradius, 109
Koordinatensystem
kartesisches, 22
kovariant, 90
Kreisfrequenz, 22
Kreuzprodukt, 37
Kristall, 33
Kronecker, 41
KoĚrper, 11, 17
L2 , 98
Lagrange, 38
Landau-Symbole, 124
Lebesgue, 98
Leibniz, 107
Leibniz-Kriterium, 107
Levi-Civita-Tensor, 42
Limes, 93, 97
linear abhaĚngig, 38, 40, 50
linear unabhaĚngig, 40, 50
lineare Abbildung, 65, 73
lineare Funktionale, 73
lineare Gruppe, 34
lineare HuĚlle, 49
lineare Operatoren, 73
Linearkombination, 49
Linkssystem, 42
Logarithmus, 122, 136
Lorentz-Gruppe, 34
LaĚnge, 25
Majorantenkriterium, 100
Matrix, 27
adjungierte, 67
inverse, 29, 68
invertierbare, 29, 68
regulaĚre, 68
singulaĚre, 68
transponierte, 44, 67
Matrix-Differentialoperator, 84
Matrixprodukt, 28, 45, 66
Matrizen

159

INDEX
aĚhnliche, 83
Maximum, 103
lokales, 129
Menge
abgeschlossene, 95, 97
beschraĚnkt, 95, 97
kompakte, 105
offene, 95, 97
Metrik-Koeffizienten, 91
Minimum, 103
lokales, 129
MinkowskiâRaum, 34
Mittelwertsatz, 130
Verallgemeinerter, 130
Moivre, 17
monoton wachsend, 103, 121

Raum
affiner, 46
euklidischer, 57
normierter, 58, 96
unitaĚrer, 57
vollstaĚndiger normierter, 97
Realteil, 14
Rechtssystem, 42
reelle Zahlen, 97
regulaĚre Matrix, 68
Reihe, 99
binomische, 136
geometrische, 99, 109
harmonische, 99
Umordnung, 107
reziproke Basis, 90

Newtonverfahren, 152
Norm, 12, 16, 25, 58, 96
normierter Raum, 58, 96
Nullraum, 76

Sandwichprinzip, 105
Satz vom Maximum, 120
Satz von BolzanoâWeierstraĂ, 104
Satz von Rolle, 130
Scheinwiderstand
komplexer, 21
Schmidt, 59
Schranke
obere, 103
untere, 103
Schwarz, 25
Schwingungsgleichung, 86
sin, 144
singulaĚre Matrix, 68
sinh, 146
Skalarprodukt, 24, 37, 57
Spaltenrang, 83
Span, 49
Spatprodukt, 39
spezielle lineare Gruppe, 34
spezielle orthogonale Gruppe, 34
Stammfunktion, 13
Steinitz, 54
stetig, 12, 19, 118
stetig differenzierbar, 13
streng monoton wachsend, 121
Summe von VektorraĚumen, 56
Supremum, 103
surjektiv, 75
Symmetriegruppe, 33
System
homogenes, 84
inhomogenes, 84

obere Grenze, 103
obere Schranke, 103
offene Menge, 95, 97
Operatoren
lineare, 73
Ordnungsrelation, 11
Orientierung, 42
negative, 42
positive, 42
orthogonal, 58
orthogonale Gruppe, 34
Orthogonalisierungsverfahren
von Gram-Schmidt, 59
Orthogonalsystem, 41
Orthonormalbasis, 58
Orthonormalsystem, 41, 45, 58
Ortsvektor, 22
Parallelogrammgleichung, 58
Partialsumme, 99
Permutationsmatrix, 70
Ď, 143
Pivotelement, 71
Potenz
allgemeine, 123
Potenzreihe, 99, 108
der Logarithmusfunktion, 136
der Winkelfunktionen, 142
Produkt zweier P., 111
Produktregel, 126
Proximum, 60
Punkt
innerer, 95, 97
Quotientenkriterium, 101

tan, 144
tanh, 146
Taylor, 132
Teilfolge, 94, 97
Teilsumme, 99
transponierte Matrix, 44, 67

Randpunkt, 95, 97
Rang, 77

Umgebung, 95, 97
Umkehrfunktion, 121, 128

160
der Hyperbelfunktionen, 147
der Winkelfunktionen, 144
unbestimmtes Integral, 13
uneigentlicher Grenzwert, 117
Ungleichung von Cauchy-Schwarz, 25, 58
unitaĚrer Raum, 57
untere Grenze, 103
untere Schranke, 103
Unterraum, 49
Untervektorraum, 49
Urbild, 115
Vektorprodukt, 37
Vektorraum, 23, 47
euklidischer, 57
unitaĚrer, 57
Vergleichskriterium, 100
VerknuĚpfungstafel, 33
Verschiebungsgruppe, 33
vollstaĚndige Induktion, 54
vollstaĚndiger normierter Raum, 97
WeierstraĂ, 104
Wellenfront, 22
Wellenzahlvektor, 22, 90
Wert einer Reihe, 99
Wertebereich, 115
Widerstand
induktiver, 21
kapazitiver, 21
Ohmscher, 21
Winkelfunktionen, 19, 137, 141
komplexe, 144
Wirkwiderstand, 21
Wurzel
aus komplexer Zahl, 147
Wurzelfunktion, 121
Wurzelkriterium, 101
Zahlen
reelle, 97
Zeilenrang, 83
Zeilenstufenform, 68, 71
Zwischenwertsatz, 120

INDEX

Literaturhinweise
Die im Folgenden genannten BuĚcher sind mehrheitlich in der Lehrbuchsammlung der Bibliothek vorhanden.
Die empfohlene Einteilung in Lesergruppen ist rein subjektiv vorgenommen worden, was Sie aber keineswegs
davon abhalten soll, mit dem Buch zu arbeiten, mit dem Sie am Besten zurecht kommen.

Eher fuĚr Ingenieure
Papula, Mathematik fuĚr Ingenieure 1, 2
de Boer, Vektorâ und Tensorrechnung fuĚr Ingenieure
Burg, Haf, Wille, HoĚhere Mathematik fuĚr Ingenieure 1, 2, 3, 4, 5

Eher fuĚr Physiker
Fischer, Kaul, Mathematik fuĚr Physiker 1, 2, 3
JaĚnich, Mathematik 1, 2. Geschrieben fuĚr Physiker, (Sehr zu empfehlen)
JaĚnich, Analysis fuĚr Physiker und Ingenieure, (Sehr zu empfehlen)
Kuscer, Kodre, Mathematik in Physik und Technik
Fischer, Lineare Algebra
Barner, Flohr, Analysis 1, 2
Endl, Luh, Analysis 1, 2, 3
Madelung, Die mathematischen Hilfsmittel des Physikers, (Eher historisch interessant)

Eher fuĚr Mathematiker
Heuser, Analysis 1, 2
Walter, Analysis 1, 2
KoĚnigsberger, Analysis 1, 2
Hildebrandt, Analysis 1, 2
Blatter, Analysis 1, 2, 3
Koecher, Lineare Algebra und Analytische Geometrie
Courant, Vorlesungen uĚber Differentialâ und Integralrechnung, (Traditioneller Zugang)

161

