Mathematische Grundlagen fĂźr Informatiker
Eine EinfĂźhrung fĂźr StudienanfĂ¤nger

Stasys Jukna

Logik
Kombinatorik
Zahlentheorie
Algebra
Analysis
Stochastik
Lineare Algebra

Skript zur Vorlesung âMathematische Grundlagen der Informatikâ fĂźr StudienanfĂ¤nger
Johan Wolfgang Goethe UniversitĂ¤t
Frankfurt am Main, 2003

Prof. Dr. habil. S. Jukna
Institut fĂźr Mathematik und Informatik
Litauische Akademie der Wissenschaften
Vilnius, Litauen

Jetztige Adresse:
Lehrstuhl fĂźr Theoretische Informatik
Institut fĂźr Informatik
Fachbereich Informatik und Mathematik
J. W. Goethe UniversitĂ¤t Frankfurt
Frankfurt am Main

i

Vorwort
In der Informatik, wie auch in vielen anderen naturwissenschaftlichen FĂ¤chern, werden viele StudienanfĂ¤nger mit mathematischen Methoden und mathematischer Denkweise konfrontiert, auf die sie in
der Schule nicht vorbereitet wurden. Dieses Skript bietet SchulabgĂ¤ngern unterschiedlicher Qualifikation einen fairen Einschub aus der Mathematik, der die Einstieg ins Informatik-Studium erleichtern
sollte. Insbesondare sollte das Skript denjenigen Studierenden helfen, deren Studiengang â wie zum
Beispiel der Studiengang âBioinformatikâ an der UniversitĂ¤t Frankfurt â keine weiter Pflichtvorlesungen in der Mathematik vorsieht.
Das Skript ist aus der Vorlesung1 âMathematische Grundlagen der Informatikâ entstanden. Die Veranstaltung wurde fĂźr AnfĂ¤nger gedacht und darf nur ein Semester (mit 4 SWS) dauern. Es ist deshalb
klar, dass nur ein Teil der Mathematik besprochen werden kann. Deshalb habe ich die Themenauswahl sehr geziehlt und sehr pragmatisch getroffen: Nur die Sachen ausgewĂ¤hlt wĂźrden, die (nach der
Erfahrung) am hĂ¤ufigsten wĂ¤hrend des spĂ¤teren Informatik-Studiums gebraucht werden und die die
grĂśĂten Schwierigkeiten fĂźr Studierenden bereiten, wie zu Beispiel Beweisprinzipien (Logik, Induktion und Kombinatorik), asymptotische Analyse (Konvergenz von Folgen und Reihen, klein-o/grĂśĂ-O
Notation, Rekurrenzen) und insbesondare stochastische Analyse (diskrete Stochastik). Von der ganzen
Zahlentheorie werden wir nur die modulare Arithmetik betrachten, da sie am meisten in der Informatik Anwendungen findet und vollkom unbekannt fĂźr SchulabgĂ¤ngern ist. Die ganze Algebra ist mit
wenigen Ausnahmen (wie Eigenschaften von Gruppen) auf zwei wichtigen Themen â lineare Algebra
und Matrizenlgebra â die am hĂ¤ufigsten in der Informatik benutzt werden, reduziert.
Wir werden auf zu detalierten Formalisierungen wie auch auf zu abstrakten Verallgemeinerungen oft
verzichten. Stattdessen, werden wir uns mehr auf die Anwendbarkeit der Konzepte und insbesondere
auf die dahinter steckende Intuition konzentrieren. Bevor wir ein Konzept einfĂźhren werden wir oft
die Frage âWozu ist das gut?â stellen.
Da hier es um âMathematik fĂźr Anwenderâ handelt, sollte man auch eine Vorstellung haben, wie diese Gebiete der âreinenâ Mathematik zur Anwendungen im âwirklichen Lebenâ (Informatik inklusiv)
kommen. Deshalb werden wir wo mĂśglich die Anwendungen der Mathematik vorstellen: modulare
Arithmetik und RSA-Codes, Anwendugen der linearen Algebra in Kodierungstheorie und Kombinatorik (Fisherâs Ungleichung und Expandergraphen), Anwendungen der Matrizenalgebra in Graphentheorie und fĂźr die Analyse der Markov-Ketten, Anwendungen der Folgen und Reihen in Finanzmathematik, Anwendungen der Stochastik in der BĂśrse, usw. Ausser dass diese Anwendungen schon
selbst nicht trivial sind, sollen sie den Studierenden zeigen, dass auch in Anwendungen eine prĂ¤zise
mathematische Denkweise unverzichtbar ist.
Im Unterschied von vielen Mathematik-Skripten werden wir (mit nur ein paar Ausnahmen) alle angegebene SĂ¤tze auch beweisen. Die Studierende sollen sich gewohnen, an nichts in der Mathematik
zu glauben, bis sie es selbst nicht verifiziert haben. AuĂerdem, oft sind Beweise viel informative
als die SĂ¤tze selbst. Beherrscht man halbwegs die mathematische Denkweise und weiss man, was
die Konzepte eigentlich bedeuten, kann man (wenn nĂśtig) den Rest in anderen Mathematik-BĂźchern
nachlesen.
Das Skript enthĂ¤lt mehr Stoff, als in der Vorlesung tatsĂ¤hlich besprochen sein sollte. Insbesondare,
habe ich viele motivierende Beispiele, Anwendungen und Aufgaben ausgesucht, die den Stoff einwenig attraktiver fĂźr Studenten machen sollten. Diesen âgutte Nachtâ Stoff â der den grĂśĂten Teil des
1http://lovelace.thi.informatik.uni-frankfurt.de/âźjukna/Grundlagen/index.html
ÂŠ 2003 by S. Jukna

ii
Skripts ausmacht â sollten Studenten selber vor der Vorlesung nachlesen.
Zur Darstellung: Die ârelative Wichtigkeitâ der SĂ¤tze ist dĂźrch ihre Formatierung gut erkenbar.
Satz 0.1. Hier ist ein ânormallerâ Satz ...
und
Satz 0.2. Die âwichtigsteâ SĂ¤tze sind zusĂ¤tzlich im Rahmen gesetzt.
Die mit â markierte Abschnitte sind optional.
Ich danke Georg Schnitger, Gregor Gramlich, Maik Weinard, Markus Schmitz-Bonfigt, Uli Laube und
natĂźrlich meine Studenten fĂźr zahlreiche VerbesserungsvorschlĂ¤ge.
Stasys Jukna, Frankfurt a. M.

ÂŠ 2003 by S. Jukna

Inhaltsverzeichnis
1 Grundbegriffe und Beweismethoden
1.1

1

Mengen, Relationen und Funktionen . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.1.1

Relationen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

1.1.2

Graphen (binĂ¤re Relationen) . . . . . . . . . . . . . . . . . . . . . . . . . .

8

1.1.3

Abbildungen (Funktionen) . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.2

KardinalitĂ¤t unendlicher Mengen . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

1.3

Aussagenlogik und Beweismethoden . . . . . . . . . . . . . . . . . . . . . . . . . .

17

1.3.1

Aussageformen (PrĂ¤dikate) . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

1.3.2

Logische Beweisregeln . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

Mathematische Induktion: Beweis von Aussagen âx P(x) . . . . . . . . . . . . . . .

26

1.4.1

. . . . . . . . . . . . . . . . . . .

33

1.5

Das Taubenschlagprinzip: Beweis von Aussagen âx P(x) . . . . . . . . . . . . . . .

35

1.6

Kombinatorische AbzĂ¤hlargumente . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

1.6.1

Prinzip der doppelten AbzĂ¤hlung . . . . . . . . . . . . . . . . . . . . . . . .

40

1.6.2

Binomialkoeffizienten . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

1.6.3

Prinzip von Inklusion and Exklusion . . . . . . . . . . . . . . . . . . . . . .

47

Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

1.4

1.7

Induktion und Entwurf von Algorithmen

2 Algebra und Elementare Zahlentheorie

57

2.1

Division mit Rest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

58

2.2

Euklidischer Algorithmus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

64

2.3

Primzahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

2.4

Kleiner Satzt von Fermat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

Anwendung in der Krypthographie: RSA-Codesâ . . . . . . . . . . . . . . .

68

Chinesischer Restsatz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72

Anwendung: Schneller Gleichheitstestâ . . . . . . . . . . . . . . . . . . . .

74

Gruppen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

2.4.1
2.5

2.5.1
2.6

iii

INHALTSVERZEICHNIS

iv
2.6.1
2.7

Zyklische Gruppen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

Ringe und KĂśrper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

2.7.1

81

2.7.2

Polynomring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Komplexe

Zahlenâ

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

84

2.8

Allgemeine VektorrĂ¤ume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

85

2.9

Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86

3 Einschub aus der Analysis

89

3.1

Endliche Folgen und Reihen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

89

3.2

Unendlicher Folgen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

98

3.3

3.2.1

Konvergenzkriterien fĂźr Folgen . . . . . . . . . . . . . . . . . . . . . . . . 101

3.2.2

Bestimmung des Grenzwertes . . . . . . . . . . . . . . . . . . . . . . . . . 104

Unendliche Reihen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
3.3.1

Konvergenzkriterien fĂźr Reihen . . . . . . . . . . . . . . . . . . . . . . . . 112

3.3.2

Anwendung: Warum Familiennamen aussterben? . . . . . . . . . . . . . . . 117

3.3.3

Umordnungssatz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

3.4

Grenzwerte bei Funktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

3.5

Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

3.6

MittelwertsĂ¤tze der Differentialrechnung . . . . . . . . . . . . . . . . . . . . . . . . 126

3.7

Approximation durch Polynome: Taylorentwicklung . . . . . . . . . . . . . . . . . 129

3.8

Extremalstellen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

3.9

Die Bachmann-Landau-Notation: klein o und groĂ O . . . . . . . . . . . . . . . . . 132

3.10 Rekurrenzenâ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
3.10.1 Das Master Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
3.11 Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
4 Diskrete Stochastik

157

4.1

Intuition und Grundbegriffe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

4.2

Drei Modellierungsschritte . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
4.2.1

Das Geburtstagsproblem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162

4.3

Stochastische UnabhĂ¤ngigkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163

4.4

Bedingte Wahrscheinlichkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165

4.5

4.4.1

Multiplikationssatz fĂźr Wahrscheinlichkeiten . . . . . . . . . . . . . . . . . 168

4.4.2

Satz von der totalen Wahrscheinlichkeit . . . . . . . . . . . . . . . . . . . . 169

4.4.3

Satz von Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171

Stochastische Entscheidungsprozesse

. . . . . . . . . . . . . . . . . . . . . . . . . 174
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

v

4.5.1

Das âMonty Hall Problemâ . . . . . . . . . . . . . . . . . . . . . . . . . . . 177

4.5.2

Stichproben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178

4.5.3

Das âSekretĂ¤rinnen-Problemâ an der BĂśrse . . . . . . . . . . . . . . . . . . 179

4.6

Zufallsvariablen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184

4.7

Erwartungswert und Varianz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186

4.8

Analytische Berechnung von E [X ] und Var [X ] . . . . . . . . . . . . . . . . . . . . 190

4.9

Eigenschaften von E [X ] und Var [X ] . . . . . . . . . . . . . . . . . . . . . . . . . . 191

4.10 Verteilungen diskreter Zufallsvariablen . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.11 Abweichung vom Erwartungswert . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
4.11.1 Markov-Ungleichung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
4.11.2 Tschebyschev-Ungleichung . . . . . . . . . . . . . . . . . . . . . . . . . . 208
4.11.3 Chernoff-Ungleichungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
4.12 Das Urnenmodel â Hashingâ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
4.13 Bedingter Erwartungswertâ

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

4.14 Summen von zufĂ¤lliger LĂ¤nge â Waldâs Theorem . . . . . . . . . . . . . . . . . . . 228
4.15 Irrfahrten und MarkovâKetten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
4.16 Statistisches SchĂ¤tzen: Die Maximum-Likelihood-Methode
4.17 Die probabilistische

Methodeâ

â

. . . . . . . . . . . . . 240

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243

4.18 Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
5 Lineare Algebra

253

5.1

Lineare VektorrĂ¤ume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254

5.2

Basis und Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256

5.3

Skalarprodukt und Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258

5.4

Dimensionsschranke und ihre Anwendungenâ . . . . . . . . . . . . . . . . . . . . . 260

5.5

Matrizen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263

5.6

Rang einer Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270

5.7

LĂśsbarkeit der linearen Gleichungssysteme . . . . . . . . . . . . . . . . . . . . . . 272

5.8

GauĂ-Verfahren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275

5.9

Inversen von Matrizen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

5.10 OrthogonalitĂ¤t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
5.11 Determinanten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
5.12 Eigenwerte und Eigenvektoren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
5.13 Einige Anwendungen des MatrizenkalkĂźlsâ . . . . . . . . . . . . . . . . . . . . . . 292
5.13.1 MatrizenkalkĂźl unf komplexe Zahlen . . . . . . . . . . . . . . . . . . . . . 292
5.13.2 Diskrete Fourier-Transformation . . . . . . . . . . . . . . . . . . . . . . . . 293
ÂŠ 2003 by S. Jukna

vi

INHALTSVERZEICHNIS
5.13.3 Fehlerkorrigierende Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
5.13.4 Expandergraphen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
5.13.5 Expander-Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
5.13.6 Markov-Ketten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
5.14 Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

ÂŠ 2003 by S. Jukna

Schulstoff: Rechnen mit reellen Zahlen
In diesem Abschnitt stellen wir einige aus der Schule bekannten Fakten zusammen. Da nicht alle
diese Fakten in der Schule auch bewiesen worden sind, werden wir das im Kapitel 3 âEinschub aus
der Analysisâ tun.
Einige wichtige Mengen in der Mathematik sind:
Die Menge der natĂźrlichen Zahlen: N = {0, 1, 2, . . .}
Die Menge der natĂźrlichen Zahlen ohne Null: N+ = {1, 2, . . .}
Die Menge der ersten n positiven natĂźrlichen Zahlen: [n] = {1, 2, . . . , n}
Die Menge der ganzen Zahlen: Z = {. n
. . , â1, 0, 1, 2, . . .}
o
a
Die Menge der rationalen Zahlen: Q =
: a â Z und b â N+
b
Die Menge der reellen Zahlen: R
Summe, Differenz, Produkt und Quotient von zwei reellen Zahlen ist wieder eine reelle Zahl.



Ausnahme: Division durch 0 ist nicht erlaubt!

Anordnung der reellen Zahlen
Zwei beliebige reelle Zahlen x und y lassen sich vergleichen: d.h. es gilt entweder
x < y oder x = y oder x > y
|
{z
}
x6y

FĂźr beliebige reelle Zahlen x, y und z gilt:



x < y und y < z

=â

x<z

x<y

=â

x < y und z > 0

=â

xÂąz< yÂąz

x<y

=â

0<x<y

=â

xÂˇy>0

=â

Vorsicht: Aus x Âˇ y < z folgt x <

z
y

xÂˇz< yÂˇz

âx > ây
1
1
0< <
y
x
(x > 0, y > 0) oder (x < 0, y < 0)

im Allgemeinen nicht! Dies gilt nur wenn y > 0.
vii

INHALTSVERZEICHNIS

viii

Betrag
Ist x eine reelle Zahl, so ist der Betrag |x| vonx wie folgt definiert:

x fĂźr x > 0
|x| =
âx fĂźr x < 0
Anschauliche Bedeutung:
|x| = Abstand zwischen 0 und x auf der Zahlenlengeraden.
Es gilt:
|x| > 0

|x Âˇ y| = |x| Âˇ |y|
x
|x|
=
(y , 0)
y
|y|
|x + y| 6 |x| + |y|
(Dreicksungleichung)
HĂ¤ufige Form:
|a â b| = Abstand zwischen x und y auf der Zahlengeraden.

Summen- und Produktzeichen

X

und

Y

Zur AbkĂźrzung lĂ¤ngeren Summen vereinbart man
n
X

a i = a1 + a2 + a3 + . . . + a n

i=1

Pn
Zum Beispiel kĂźrzt man 1 + 2 + 3 + . . . + n als i=1
i ab. Dazu mĂźssen die Summanden mit einer
Nummer (Index) (oben ist das i) versehen sein. Mit den durch ein Summenzeichen ausgedrĂźckten
(endlichen) Summen wird genauso gerechnet wie mit ânormalenâ Summen auch.
Der Name des Indices spielt keine Rolle:
n
X

ai =

i=1

n
X

aj

j=1

Ist a0 , a1 , . . . eine Folge von Zahlen und I â {0, 1, . . .} eine endliche Teilmenge der Indices, so ist
X
ai
i âI

die Summe aller Zahlen ai mit i â I.
Man betrachtet auch Doppelsummen:
n X
m
X
i=1 j=1

ai j :=

m
X
j=1



a1 j +

m
X
j=1



a2 j + . . . +

m
X
j=1

an j


ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

ix

Mit den durch ein Summenzeichen ausgedrĂźckten (endlichen!) Summen wird genauso gerechnet wie
mit ânormalenâ Summen auch. So kann man zum Beispiel die Reihenfolge der Summen vertauschen
(eine solche Umformung nennt man auch das Prinzip der doppelten AbzĂ¤hlung):
n X
m
X
i=1 j=1

ai j =

m X
n
X

ai j

j=1 i=1

Analog vereinbart man
n
Y
i=1

a i = a1 Âˇ a2 Âˇ a3 Âˇ . . . Âˇ a n

Prominente Summen:
-

Arithmetische Summe:

n
X

k = 1 +2+3... + n =

k=1

-

Geometische Summe:
n
X

xk = 1 + x + x2 + . . . + x n =

k=0

-

n(n + 1)
.
2

1 â x n+1
fĂźr x , 1.
1âx

Harmonische Summe:
n
X
1
1 1
1
= 1 + + + . . . + = ln n + Î´ mit 0 < Î´ 6 1.
k
2 3
n
k=1

Potenzen und Wurzel
FĂźr a â R, n â N+ wird definiert:
Âˇ . . . Âˇ a}
a n = |a Âˇ a {z
n mal
0
a = 1
1
ân
a
=
(a , 0)
an
FĂźr a â R, a > 0, n â N+ gibt es genau eine nicht-negative reelle Zahl, die hoch n genommen a
â
ergibt. Diese Zahl wird mit n a bezeichnet, d.h.
â
n

Def.
a = x) ââ (x > 0) und (x n = a)
â
â
FĂźr ungerades n und a < 0 ist auch n a = â n âa definiert.
(

Gebrochene Exponenten: FĂźr a â R, a > 0, m, n â N+ wird definiert:
â
â
n
a m/n =
a m = ( n a) m
1
aâm/n =
m/n
a

ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

x

FĂźr alle a, b â R und p, q â Q, fĂźr die die folgenden AusdrĂźcke definiert sind, gilt:
a p Âˇ aq
ap
aq
p
a Âˇ bp
ap
bp
p q
(a )

= a p+q
= a pâq
= (a Âˇ b) p
 a p
=
b
= a p Âˇq

Die Rechenregeln fĂźr Wurzeln ergeben sich aus diesen Regeln durch den Ăbergang von

â
n

a zu a1/n .

Exponent und entsprechende Wurzel heben sich auf; d.h.
â
n



xn = (

â
n

Vorsicht mit negativem x: es gilt z.B.

x) n = x fĂźr alle x > 0.
â

x 2 = |x| fĂźr alle x â R;

â

x 2 = x gilt nur, wenn x > 0.

Die obigen Rechenregeln gelten auch fĂźr irrationalen Exponenten a x mit x â R \ Q.

Lineare und quadratische Gleichungen
Die LĂśsung fĂźr
mit a, b â R und a , 0

ax + b = 0
ist

x=â

b
a

Um die quadratische Gleichung
ax 2 + bx + c = 0

mit a, b, c â R, a , 0

zu lĂśsen, schreibt man zuerst sie um
x2 +

b
c
x=â
a
a

und stellt die linke Seite als Quadrat dar


b
x+
2a

2

c
b2
b2 â 4ac
=â + 2 =
a 4a
4a2

woraus
x=

bÂą

â

b2 â 4ac
2a

folgt.
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xi

Logarithmen
In der Gleichung
ax = r
sind a (Basis, a > 0, a , 1) und r (Numerus, r > 0) gegeben. Gesucht ist die Zahl x. Diese Zahl heiĂt
Logarithmus von r zur Basis a: schreibweise
x = loga r
Logarithmus loge r zur Basis e = 2, 7182818 . . . (Eulerâsche Zahl) bezeichnet man als ln r.

y = ax

y=x
y = log a x

1
a

Die Rechenregeln mit Logarithmen sind in folgendem Satz zusammen gefasst.
Satz 0.3. a, b, n > 1 seien reelle Zahlen. Dann gilt
(a) alog a n = n.
(b) logn (a Âˇ b) = logn a + log n b und logn
(c) Basisvertauschregel: loga n =

logb n
logb a

a
b



= logn a â logn b

(d) logb (a n ) = n Âˇ logb a
(e) (loga b) Âˇ (logb a) = 1
(f) blog a n = nlog a b

Beweis. (a) folgt aus der Definition.
(b):
nlog n a + logn b = nlog n a Âˇ nlog n b = a Âˇ b.
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xii
(c): Zu zeigen ist: logb n = (logb a)(loga n)
(a)
b(logb a)(loga n) = aloga n = n
(d):

(e):


n
n
Âˇ
log
a
log
a
b
b
b
= b
= an

(Definition von logb a)

(c) logb b
(loga b) Âˇ (logb a) =
Âˇ logb a = 1.
logb a
(f):
1/ log b a
(c) 
(e)
bloga n = blogb n
= n1/ log b a = nlog a b



FĂźr alle x â R, x , 0 gilt die folgende sehr nĂźtzliche Ungleichung:

1 + x < ex

(1)

1 + x > e x/(1+x)

(2)

und fĂźr alle x â (â1, 1) gilt2

y= ex

y =1 + x

1

â1

0

1

2Wir werden diese Ungleichungen spĂ¤ter beweisen (siehe Lemma 3.62).
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xiii

GauĂ-Klammern âxâ und âxâ
FĂźr eine reelle Zahl x â R ist
âxâ := max{b â Z : b 6 x}
âxâ := min{a â Z : x 6 a}

Eigenschaften:
x â 1 < âxâ 6 x 6 âxâ < x + 1
ââxâ = ââxâ; ââxâ = ââxâ
Sei n â N eine natĂźrliche Zahl, 2mâ1 6 n < 2m . Dann hat die binĂ¤re Darstellung von n genau
m = âlog2 nâ + 1 = âlog2 (n + 1)â
Bits.

Sinus und Cosinus

1
x

sin x

cos x

Symmetrie:
cos(âx) = cos x
sin(âx) = â sin x
Pythagoras:
cos2 x + sin2 x = 1
Additionstheoreme:
cos(x + y) = cos x Âˇ cos y â sin x Âˇ sin y
sin(x + y) = sin x Âˇ cos y + sin x Âˇ cos y
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xiv

Modulare Arithmetik
- Euklidâs Algorithmus zur Berechnung von ggT(a, b): ggT(a, b) = ggT(b, a mod b).
- Zâm = {a â Zm : a , 0 und ggT(a, m) = 1} ist eine multiplikative Gruppe.
- Kleiner Satz von Fermat: p prim â a pâ1 âĄ 1 mod p fĂźr alle a â N.

- Chinesischer Restsatz: Jedes Gleichhungssystem
x âĄ ai mod pi , i = 1, . . . , n
modulo Primzahlen pi hat genau eine LĂśsung x mit 0 6 x 6 p1 Âˇ p2 Âˇ Âˇ Âˇ pn . Insbesondare gilt
Z p1 Âˇp2 ÂˇÂˇÂˇp n
- Sind a, b <

Qn

i=1

Bi jek t ion

ââ

Z p1 Ă Z p2 Ă Âˇ Âˇ Âˇ Ă Z p n .

pi fĂźr Primzahlen p1 , . . . , pn so gilt:
ââ

a=b

a âĄ b mod pi fĂźr alle i = 1, . . . , n.

- Jede endliche Gruppe (G, âŚ), deren Ordnung p = |G| eine Primzahl ist, ist zyklisch: FĂźr alle a â G
gilt
G = {a, a2 , a3 , . . .}.

Prominente unendliche Reihen
-

Geometische Reihe: fĂźr |x| < 1
1 + x + x2 + x3 + x4 Âˇ Âˇ Âˇ + x n + Âˇ Âˇ Âˇ =

-

Verallgemeinerte geometrische Reihe: fĂźr |x| < 1
x + 2x 2 + 3x 3 + 4x 4 + Âˇ Âˇ Âˇ + nx n + Âˇ Âˇ Âˇ =

-

1
.
1âx

x
.
(1 â x) 2

Modifizierte harmonische Reihe: fĂźr r > 1
1+

1
1
1
1
+ r + r +ÂˇÂˇÂˇ+ r +ÂˇÂˇÂˇ = a
r
2
3
4
n

mit
1 < a 61+

1
2r â1

â1

.

Insbesondare gilt
1+

1
1
1
1
Ď2
+
+
+
Âˇ
Âˇ
Âˇ
+
+
Âˇ
Âˇ
Âˇ
=
.
22 32 42
n2
6
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xv

Ableitungen
Seien f , g : I â R differenzierbar in x â I.
-

( f + g) â˛ (x) = f â˛ (x) + g â˛ (x) (Summenregel)

-

( f g) â˛ (x) = f â˛ (x)g(x) + f (x)g â˛ (x) (Produkregel)

-

Ist f (x) , 0, so
 â˛
f
g â˛ (x) f (x) â g(x) f â˛ (x)
(x) =
(Quotientenregel)
g
( f (x)) 2

-

Sei f : I â R differenzierbar in x â I, sei J â f (I) und g : J â R differenzierbar in f (x). Dann
ist g âŚ f (x) = g( f (x)) differenzierbar in x â I und
(g âŚ f ) â˛ (x) = g â˛ ( f (x)) Âˇ f â˛ (x) (Kettenregel)

Ableitungen einiger Funktionen (fĂźr c â R und n â N):
-

f (x) = c â f â˛ (x) = 0

f (x) = x â f â˛ (x) = 1

-

f (x) = x n â f â˛ (x) = nx nâ1

-

f (x) = ln g(x) â f â˛ (x) =

-

1 â˛
g (x)
g(x)

1
1
â f â˛ (x) = â 2
x
x
g (x)
â˛
f (x) = c
â f (x) = (ln c)eg (x) g(x) â˛

Spezialfall: (ln x) â˛ =

1
x

f (x) =

f (x) = ecg (x) â f â˛ (x) = cecg (x) g â˛ (x)

Speczialfall: (c x ) â˛ = (ln c)e x
Spezialfall: (e x ) â˛ = e x

Einige Integrale:
Z

e x dx = e x ,

Z

1
dx = ln x,
x
Z
1 n+1
x n dx =
x
n+1

fĂźr n , â1

Extremstellen
Gegeben sei ein offenes Interval I in R, eine Funtion f : I â R und eine Stelle a â I. Sei f â˛ (a) = 0.
Gilt auĂerdem f â˛â˛ (a) > 0 (bzw. f â˛â˛ (a) < 0), so ist a lokale Minimalstelle (bzw. Maximalstelle) von f .
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xvi

Taylorreihen

e

x

â
X
x2 x3
xk
=1+x+
+
+...
=
k!
2
6
k=0

ln(1 + x) =

â
X

(â1) k

xk
x2 x3
=1âx+
â
Âą...
k
2
3

(â1) k

x 2k
x2 x4 x6
=1â
+
â
+...
(2k)!
2! 4! 6!

(â1) k

x 2k+1
x
x3 x5 x7
=
â
+
â
+...
(2k + 1)! 1! 3! 5! 7!

k=0

cos x =
sin x =

â
X
k=0
â
X
k=0

Taylorentwicklung von Funktionen ist im Appendix (Abschnit 3.7) skiziert.

Stochastik
- Satz von der totalen Wahrscheinlichkeit:

- Satz von Bayes:



Pr { A} = Pr {B} Âˇ Pr { A | B } + Pr B Âˇ Pr A B .
Pr { A | B } =

Pr { A}
Âˇ Pr {B | A }
Pr {B}

- Erwartunswert einer Zufallsvariable X : âŚ â I
X
E [X ] :=
a Âˇ Pr {X = a} .
a âI

- LinearitĂ¤t des Erwartungswertes: E [X + Y ] = E [X ] + E [Y ].

 

- Varianz: Var [X ] = E (X â E [X ]) 2 = E X 2 â E [X ]2 .

- Ist X die Indikatorvariable fĂźr ein Ereignis, so gilt


E [X A ] = Pr { A} und Var [X A ] = Pr { A} â Pr { A} 2 = Pr { A} Âˇ Pr A

- Geometrische Verteilung = âsolange bisâ Verteilung: UnabhĂ¤ngige Versuche mit der Erfolgswahrscheinlichkeit p , 0

 1
E Anzahl der Versuche bis zum ersten Erfolg = .
p

- Berechnung von E [X ] und Var [X ]: Setze

f (x) :=
und berechne

f â˛ (x)

wie auch

f â˛â˛ (x).

X
a âI

x i Pr {X = a}

Dann gilt

E [X ] = f â˛ (1) und Var [X ] = f â˛â˛ (1) + E [X ] â E [X ]2 .
ÂŠ 2003 by S. Jukna

INHALTSVERZEICHNIS

xvii

- Markov-Ungleichung fĂźr X > 0:
Pr {X > k} 6

E [X ]
.
k

 
- Tschebyschev-Ungleichung fĂźr X mit E X 2 < â:

Pr {|X â E [X ] | > k} 6

Var [X ]
.
k2

- Chernoff-Ungleichung fĂźr die Summe X = X1 + . . . + X n von unabhĂ¤ngigen Bernoulli-Variablen
mit Pr {X i = 1} = p:
Pr {X > (1 + Î´)np} 6 eâÎ´

2 n p/3

fĂźr jedes 0 < Î´ < 1.

ÂŠ 2003 by S. Jukna

xviii

INHALTSVERZEICHNIS

ÂŠ 2003 by S. Jukna

Kapitel 1

Grundbegriffe und Beweismethoden
Contents
1.1

Mengen, Relationen und Funktionen . . . . . . . . . . . . . . . . . . . . . . . .

1

1.1.1

Relationen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

1.1.2

Graphen (binĂ¤re Relationen) . . . . . . . . . . . . . . . . . . . . . . . . .

8

1.1.3

Abbildungen (Funktionen) . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.2

KardinalitĂ¤t unendlicher Mengen . . . . . . . . . . . . . . . . . . . . . . . . .

13

1.3

Aussagenlogik und Beweismethoden . . . . . . . . . . . . . . . . . . . . . . . .

17

1.3.1

Aussageformen (PrĂ¤dikate) . . . . . . . . . . . . . . . . . . . . . . . . . .

21

1.3.2

Logische Beweisregeln . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

Mathematische Induktion: Beweis von Aussagen âx P(x) . . . . . . . . . . . .

26

Induktion und Entwurf von Algorithmen . . . . . . . . . . . . . . . . . .

33

Das Taubenschlagprinzip: Beweis von Aussagen âx P(x) . . . . . . . . . . . .

35

Kombinatorische AbzĂ¤hlargumente . . . . . . . . . . . . . . . . . . . . . . . .

40

1.6.1

Prinzip der doppelten AbzĂ¤hlung . . . . . . . . . . . . . . . . . . . . . . .

40

1.6.2

Binomialkoeffizienten . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

1.6.3

Prinzip von Inklusion and Exklusion . . . . . . . . . . . . . . . . . . . . .

47

Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

1.4

1.4.1
1.5
1.6

1.7

1.1 Mengen, Relationen und Funktionen
Der (einzige) Barbier in Sevilla rasiert genau die MĂ¤nner der Stadt, die nicht sich selbst rasieren. Es
scheint nichts dagegen zu sprechen, die Menge M aller MĂ¤nner, die der Barbier rasiert, zu bilden:



M = {x : x ist ein mĂ¤nlicher Einwohner von Sevilla, den der Barbier rasiert}.
Ist der Barbier ein Element von M? Weder noch!
1

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

2

In Anbetracht solcher Paradoxe gibt die folgende, von Cantor 1895 gegebene ErklĂ¤rung eine zumindest fĂźr die praktische Arbeit ausreichend prĂ¤zise Fassung des Begriffs der Menge:
ErklĂ¤rung (keine Definition!) Eine Menge ist die Zusammenfassung
bestimmter, wohlunterschiedener Objekte unserer Anschauung oder unseres Denkens, wobei von jedem dieser Objekte eindeutig feststeht, ob
es zur Menge gehĂśrt oder nicht. Die Objekte der Menge heiĂen Elemente der Menge.
Diese Schwerigkeiten (den Begriff einer âMengeâ zu definieren) spielen in dieser Vorlesung keine
Rolle, man sollte aber davon wissen.
Wir schreiben âa â Aâ, wenn a ein Element der Menge A ist; âa < Aâ ist die Negation von a â A.
Wir schreiben âA â Bâ, wenn A eine Teilmenge von B ist, d.h. wenn jedes Element aus A auch in B
ist; wenn A â B und A , B, dann schreibt man auch A â B.
Die Potenzmenge P(A) einer Menge A ist die Menge aller Teilmengen von A (anstatt P(A) schreibt
man oft 2 A ). Beispiel: P({1, 2}) = {â, {1}, {2}, {1, 2}}.

Man stellt die Mengen dar entweder durch AufzĂ¤hlung der Elemente, z.B.
A = {1, 3, 4, 7}
oder durch Beschreibung der Eigenschaften der Elemente, z.B.
A = {a : a ist eine ganze Zahl mit a2 = a} = {0, 1}
A = {a : a ist ganze Zahl, 1 6 a 6 5, a , 2} = {1, 3, 4, 5}
Nach dem Zeichen â : â folgen die Bedingungen, die die Elemente erfĂźllen mĂźssen; eine Komma â,â
bedeutet hier ein âundâ. Die Anzahl der Elemente in einer endlichen Menge A, die MĂ¤chtigkeit von
A, wird mit | A| bezeichnet (manchmal auch #A).
Wichtige Regeln beim Umgang mit Mengen sind:
- Eine Menge enhĂ¤lt jedes Element nur einmal: a â A oder a < A. Es gibt also genau eine leere
Menge â = {}, die keine Elemente enthĂ¤lt.
- Eine Menge ist durch ihre Elemente bestimmt, d.h. zwei Mengen A und B sind genau dann gleich,
wenn sie die gleichen Elemente haben. Zwei Mengen A und B sind also genau dann gleich, wenn
A â B und B â A.



Diese Regel ist wichtig: So zeigt man Mengengleicheit! Um A = B zu beweisen, muss man
also folgendes zeigen:
fĂźr jedes x â A gilt x â B

und
fĂźr jedes x â B gilt x â A
- Elemente einer Menge kĂśnnen wieder Mengen sein: z.B. {N} ist eine Menge mit genau einem
Element, und {0, N} hat genau 2 Elemente.
ÂŠ 2003 by S. Jukna

1.1. MENGEN, RELATIONEN UND FUNKTIONEN

3

Man beachte den Unterschied zwischen â und â:
a â A, {a} â A und {a} â 2 A sind daher Ă¤quivalent.
VerknĂźpfungen von Mengen:
A âŠ B = {x : x â A und x â B} (Schnittmenge)
A âŞ B = {x : x â A oder x â B} (Vereinigungsmenge)
A \ B = {x : x â A und x < B} (Differenz)
A â B = {x : x â A \ B oder x â B \ A} (symmetrische Differenz)
Ist eine Menge U fixiert (ein âUniversumâ) und ist A â U, dann heiĂt A = U \ A das Komplement
von A (im Universum U).
A und B heiĂen disjunkt, falls A âŠ B = â gilt.
Die eingefĂźhrten Mengenoperationen lassen sich mit Hilfe der sogenanten Venn Diagramme graphisch
gut veranschaulichen.

Durchschitt

Vereinigung

Differenz

Symmentrische
Differenz

Das kartesische Produkt:1 ist definiert durch



A Ă B = {(a, b) : a â A und b â B}
Sind a , b, so gilt {a, b} = {b, a} aber (a, b) , (b, a).

c
b
a
1

Die n-te kartesische Potenz von A ist
Vektoren) (a1 , . . . , an ) mit ai â A.

An

2

3

4

n mal
z
}|
{
:= A Ă A Ă . . . Ă A. Die Elemente von An sind n-Tupel (oder

1Der Name âkartesischâ hat eine Geschichte: RenĂŠ Descartes, Philosoph und Mathematiker des 17. Jahrhunderts, hat
den systematischen Gebrauch von Koordinaten in der Geometrie eingefĂźhrt. Koordinaten im k-dimensionalen Raum sind
natĂźrlich k-Tupel (in der Ebene: Paare, im dreidimensionalen: Tripel) von Zahlen. Und weil es damals Ăźblich war, seinen
Namen zu latinisieren, gab sich Descartes den Namen Cartesius. Ihm zu Ehren spricht man von kartesischen Koordinaten
und vom kartesischen Produkt.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

4



Ist zum Beispiel A = {0, 1}, so ist An = {0, 1} n der n-dimensionale binĂ¤re WĂźrfel. Dieses
Objekt stellt eine wichtige Struktur mit Anwendungen in der Informatik dar. Ist die Reihenfolge
der Elemente in A = {a1 , a2 , . . . , an } fixiert, so kann man die Teilmengen S â A mit 0-1 Vektoren
x = (x 1 , x 2 , . . . , x n ) â {0, 1} n mit

1
falls ai â S
xi =
0
falls ai < S

kodieren. Ein solcher Vektor x heiĂt dann charakteristischer Vektor (oder Inzidenzvektor) von S. Sind
zum Beispiel A = {1, 2, 3, 4, 5} und S = {1, 3, 4}, so kann man den Vektor x = (1, 0, 1, 1, 0) als Code von
S betrachten. Diese Kodierung ist in der Informatik sehr wichtig: So stellt man Mengen im Computer
dar!

1.1.1 Relationen
Eine (binĂ¤re) Relation ist eine Beziehung zwischen Dingen. Zum Beispiel: zwei Menschen kĂśnnen
verwandt sein oder nicht. Ein Auto kann lĂ¤nger sein als ein anderes oder nicht. Zwei Mengen kĂśnnen
identisch sein oder nicht.
Will man verschiedene Mengen miteinander vergleichen, braucht man eine Beziehung zwischen diesen. Und auch eine einzelne Menge ist strukturlos, solange die einzelnen Elemente vĂśllig beziehungslos zueinander sind. Hat man aber eine Beziehung (Relation), so entsteht aus dem Chaos eine Strukur,
und die Untersuchung der Eigenschaften dieser Beziehungen ist eine der Hauptaufgaben der Mathematik.
Definition: Eine (binĂ¤re) Relation zwischen zwei Mengen A und B ist eine Teilmenge
R â A Ă B.
Im Falle A = B sprechen wir von einer Relation in A.

c
b
a
1

2

3

4

Abbildung 1.1: A = {a, b, c}, B = {1, 2, 3, 4} und R = {(a, 2), (a, 3), (b, 1), (b, 3), (c, 1), (c, 4)}
Eine Relation R ist also eine Menge geordneter Paare. Schreibweise: anstatt (a, b) â R schreibt man
oft a âź R b oder aRb.
Wenn wir irgendeine Beziehung als eine Relation abstrakt modelliert haben, kĂśnnen wir Ăźber einige
Eigenschaften dieser Relation sprechen.
Seien a, b, c â A beliebig. Eine binĂ¤re Relation R â A Ă A ist:
- reflexiv, wenn a âź R a
ÂŠ 2003 by S. Jukna

1.1. MENGEN, RELATIONEN UND FUNKTIONEN

5

- symmetrisch, wenn aus a âź R b stets b âź R a folgt
- antisymmetrisch, wenn aus a âź R b und b âź R a stets a = b folgt
- asymmetrisch, wenn aus a âź R b stets ÂŹ(b âź R a) folgt
- transitiv, wenn aus a âź R b und b âź R c stets a âź R c folgt
Der Unterschied zwischen antisymmetrischen und asymmetrischen Relationen ist, dass in antisymmetrischen Relationen auch a âź R a gelten kann, wĂ¤hrend diese Eigenschaft in asymmetrischen Relationen verboten ist.
Ăquivalenzrelationen sind deshalb so wichtig, weil man oft nur an bestimmten Eigenschaften der
untersuchten Objekte interessiert ist. Unterscheiden sich zwei Objekte nicht (bezĂźglich der Eigenschaften, die man gerade untersucht), so sagt man, sie sind Ă¤quivalent.
Eine Relation âźâ A Ă A heiĂt Ăquivalenzrelation, wenn sie die folgenden drei Eigenschaften hat:
(i)

a âź a fĂźr alle a â A (reflexiv)

(ii) Wenn a âź b, dann auch b âź a (symmetrisch)
(iii) Wenn a âź b und b âź c, dann gilt auch a âź c (transitiv)
reflexiv

symmetrisch

a

a

transitiv

a

b

b

c

Ist eine Ăquivalenzrelation âźâ A Ă A auf einer Menge A gegeben, so heiĂt eine Teilmenge S â A
Ăquivalenzklasse (bezĂźglich âź), falls gilt:
1. S , â,
2. x, y â S â x âź y,

3. x â S, y â A und x âź y â y â S.
â˛ Beispiel 1.1 : 1. Eine Ăquivalenzrelation in Z: a âź b ââ a und b den selben Rest r modulo
5 haben. Es gibt fĂźnf Ăquivalenzklassen: [r] = {x â Z : x = 5y + r}, r = 0, 1, 2, 3, 4
(Restklassen modulo 5):

ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ

â10
â9
â8
â7
â6

â5
â4
â3
â2
â1

0
1
2
3
4

5
6
7
8
9

10
11
12
13
14

15
16
17
18
19

20
21
22
23
24

ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ

0
4

1

2

3
m=5

2. Viele praktische Beispiele ..., z.B. Post: Briefe mit gleicher Postleitzahl gelten fĂźr die Sortierung in PostsĂ¤cken (Ăquivalenzklassen) als Ă¤quivalent.
Eine disjunkte Zerlegung einer Menge A besteht aus paarweise disjunkten Teilmengen A1 , . . . , An von
A, deren Vereinigung die ganze Menge A ergibt, d.h. es muĂ A = A1 âŞ A2 âŞ . . . âŞ An und Ai âŠ A j = â
fĂźr alle 1 6 i , j 6 n gelten.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

6

Satz 1.2. Ăquivalentzklassen bilden eine disjunkte Zerlegung.
Beweis. Sei âź eine Ăquivalenzrelation auf einer Menge A. Zu zeigen: (i) jedes Element a â A gehĂśrt zu genau einer Ăquivalenzklasse, und (ii) verschiedene Ăquivalenzklassen sind disjunkt. Dazu
definieren wir fĂźr ein festes gegebenes a â A die Menge 2
Sa := {x â A : x âź a}
aller Elemente, die Ă¤quivalent zu a sind. Wegen a âź a gilt a â Sa , und es folgt Sa , â. Wir zeigen,
dass Sa eine Ăquivalenzklasse ist. Sind x, y â Sa , so gilt x âź a und y âź a, also x âź y, da âź
symmetrisch und transitiv ist. Gilt nun x â Sa , y â A und x âź y, dann haben wir x âź a, also auch
y âź a (TransitivitĂ¤t) und daher y â Sa . Damit ist gezeigt, dass a in mindestens einer Ăquivalenzklasse
enthalten ist.
Es bleibt zu zeigen, dass zwei Ăquivalenzklassen S und S â˛ entweder gleich oder disjunkt sind. Angenommen, S und S â˛ sind nicht disjunkt und a â S âŠ S â˛ . Gilt nun x â S, so so gilt x âź a, und wegen
a â S â˛ folgt auch x â S â˛. Also ist S â S â˛ . Ebenso beweist man S â˛ â S, woraus S = S â˛ folgt.

Jede Ăquivalenzrelation âź auf einer Menge A liefert also eine Zerlegung von A in disjunkte Ăquivalenzklassen.
[c]
[a]
[b]

Diese Ăquivalenzklassen betrachtet man als Elemente einer neuen Menge, die man mit A/ âź bezeichnet. Man nennt sie die Quotientenmenge von A nach der Ăquivalenzrelation âź. Die Elemente von
A/ âź sind also spezielle Teilmengen von A. Indem man jedem Element a â A die Ăquivalenzklasse Sa zuordnet, in der es enthalten ist, erhĂ¤lt man eine kanonische (d.h. in der gegebenen Situation
eindeutig festgelegte) Abbildung
A â A/ âź

mit

a 7â Sa

Eine Relation < â A Ă A heiĂt Ordnung, wenn sie die folgenden drei Eigenschaften hat:
(i)

ÂŹ(a < a) fĂźr alle a â A (antireflexiv)

(ii)

Wenn a < b und a , b, dann ÂŹ(b < a) (antisymmetrisch)

(iii) Wenn a < b und b < c, dann ist auch a < c (transitiv)
antireflexiv

a

antisymmetrisch

a

b

transitiv

a

b

c

2So definierte Mengen Sa bezeichnet man oft auch mit [a] ; a ist dann ein ReprĂ¤sentant der Ăquivalenzklasse [a].
Beachte, dass aus a âź b immer [a] = [b] folgt. D.h. jedes Element aus [a] kann man als den Representanten der Klasse [a]
nehmen; bis auf die Ăquivalenzrelation âź sind sie alle âgleichâ.
ÂŠ 2003 by S. Jukna

1.1. MENGEN, RELATIONEN UND FUNKTIONEN

7

Einige Beispiele:
-

Die Potenzmenge P(S) mit der Ordnung: A < B genau dann, wenn A â B (siehe Abb. 1.2(a)).

-

Die Menge aller positiven natĂźrlichen Zahlen mit der Ordnung: a < b genau dann, wenn a kleiner
als b ist.

-

Die Menge aller positiven natĂźrlichen Zahlen mit der Ordnung: a < b genau dann, wenn b ohne
Rest durch a teiltbar ist (siehe Abb. 1.2(b)).

-

Die Menge aller Vektoren in Rn mit der Ordnung (a1 , . . . , an ) < (b1 , . . . , bn ) genau dann, wenn
ai 6 bi fĂźr alle i, und ai < bi fĂźr mindestens ein i gilt.

Kleine Ordungen kann man mittels sogenannter Hasse-Diagramme darstellen, wobei die Elemente als
Punkte und die Relationen als Verbindungen vom kleineren unteren Elementen zum grĂśĂeren oberen
Elementen dargestellt werden (siehe Abb. 1.2).
18

{a,b,c}

{b,c}
{a}

9

{a,c}

{a,b}

{b}

{c}

6
3
2

O

1

(a)

(b)
Abbildung 1.2:

Die Relation âkleinerâ (â<â) auf R hat eine weitere interessante Eigenschaft: sie ist vollstĂ¤ndig, d.h.
sie erfĂźllt die Foderung, dass je zwei Elemente vergleichbar sind. Eine Ordnung mit dieser Eigenschaft heiĂt vollstĂ¤ndige Ordnung (oder âtotale Ordnungâ oder âlineare Ordnungâ). Ist eine Ordnungsrelation nicht vollstĂ¤ndig, so nennt man sie manchmal partielle Ordnung. Welche von der oben
ausgelisteten Ordnungen sind vollstĂ¤ndig und welche nur partiel? (Ăbungsaufgabe!)
Da Relationen die Teilmengen R â A Ă B des kartesischen Produkts A Ă B sind (bei festen Mengen
A und B), sind Durchschnitt, Vereinigung, Differerenz und Komplement von Relationen erklĂ¤rt und
ergeben wieder eine Relation zwischen A und B; ebenso ist die Inklusion fĂźr Relationen definiert und
es gelten alle fĂźr Mengenoperationen Ăźblichen Regeln. Es gibt aber auch eine spezielle Operation
zwischen Relationen â ihre âVerknĂźpfungâ oder âKomposition.â
Sei R â A Ă B und S â B Ă C. Dann wird die Komposition R âŚ S als Relation zwischen A und C
definiert durch
R âŚ S := {(a, c) : es gibt ein b â B mit (a, b) â R und (b, c) â S}.
â˛ Beispiel 1.3 : Sei M eine Menge von Menschen,
R = {(x, y) : x ist Mutter von y}

S = {(y, z) : y ist verheiratet mit z}
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

8
Dann ist

R âŚ S = {(x, z) : x ist Schwiegermutter von z}
Eine wichtige Eigenschaft dieser Komposition von Relationen (und somit auch von Abbildungen) ist
ihre AssoziativitĂ¤t: Sind Ri â Ai Ă Ai+1 , i = 1, 2, 3 Relationen, dann gilt:
(R1 âŚ R2 ) âŚ R3 = R1 âŚ (R2 âŚ R3 ).
Man mache sich diese Aussage an Hand eines Diagramms klar:
R1 0 R2
A1

A2

A3

A4

R2 0 R3

Eine Sonderrolle, die dann besonders bei Abbildungen Bedeutung erlangt, spielt bezĂźglich der Komposition die identische Relation I A : Sie wird in einer beliebigen Menge A durch die Gleichheit definiert, also durch:
xI A y ââ x = y
Vor allem dann, wenn man sie als Teilmenge von AĂ A versteht (so haben wir Relationen ja definiert),
wird sie auch als Diagonale bezeichnet. Mit dieser identischen Relation gilt dann:
R âŚ I A = I A âŚ R = R.
Dreht man alle Paare (a, b) in R um, so bekommt man die inverse Relation Râ1 :



Râ1 := {(b, a) : (a, b) â R}

Ist R â A Ă B eine Abbildung, so ist dennoch Râ1 im Allgemeinen keine Abbildung. Im
Allgemeinen gilt auch weder R âŚ Râ1 = I A noch Râ1 âŚ R = IB . Sei zum Beispiel A = {1, 2} und
B = {a, b} sowie R = {(1, a), (1, b)}. Dann ist Râ1 = {(a, 1), (b, 1)} und man enthĂ¤lt R âŚ Râ1 = {(1, 1)}
und Râ1 âŚ R = B Ă B.

1.1.2 Graphen (binĂ¤re Relationen)
BinĂ¤re Relationen auf endlichen Mengen nennt man auch âGraphen.â Solche Relationen sind anschaulich und vielseitig anwendbar. Wir reden hier nicht von Graphen einer Funktion, fĂźr uns ist ein Graph
so etwas:

ÂŠ 2003 by S. Jukna

1.1. MENGEN, RELATIONEN UND FUNKTIONEN

9

Es gibt Punkte (Knoten) und Linien (Kanten) zwischen einigen dieser Punkte. Graphen sind so vielseitig, weil die Idee so einfach ist: es gibt eine Menge von Objekten (die Knoten), und zwei Knoten
sind entweder verbunden oder nicht.
Ein gerichteter Graph ist also ein Paar G = (V, E) mit E â V Ă V . Ist die Relation E symmetrisch
und anti-reflexiv, so ist G = (V, E) ein ungerichteter Graph (oder einfach ein Graph). Wir werden fast
ausschlieslich nur solche (ungerichtete) Graphen betrachten. Die Elemente v der Menge V werden
Knoten genannt, die Elemente e = {v, u} (oft schreibt man e = uv, um Klammern zu sparen) der
Menge E sind die Kanten des Graphen. Man sagt, dass die Kante e = {v, u} die Knoten v und u
verbindet; die Knoten u, v selbst sind Endknoten von e. Zwei Knoten, die in einem Graphen durch
eine Kante verbunden sind, heiĂen adjazent oder benachbart. Die Anzahl aller Nachbarn von u nennt
man als Grad von u und bezeichnet ihm mit d(u) (engl. Grad = degree).
â˛ Beispiel 1.4 : Der n-dimensionale binĂ¤re WĂźrfel ist ein Graph Q n = (V, E) dessen Knoten den 2n
binĂ¤ren Strings der LĂ¤nge n entsprechen. Zwei Konten (=Strings) sind genau dann adjazent, wenn
sie sich in genau einem Bit unterscheiden (siehe Abb. 1.3).
110

111

010

011

101

100

000

001

Abbildung 1.3: Der 3-dimensionale binĂ¤re WĂźrfel Q3 . Er hat 8 Knoten, je vom Grad 3, und 12 Kanten.

Graphen werden gewĂśhnlich mit Hilfe geometrischer Diagramme dargestellt. Oft aber wird zwischen
einem Graphen und einem diesen Graphen darstellenden Diagramm nicht deutlich unterschieden.
Es muss aber ausdrĂźcklich davor gewarnt werden, Graphen und Diagramme gleichzusetzen. Spezielle geometrische Darstellungen kĂśnnen das Vorhandensein von Eigenschaften suggerieren, die der
dargestellte Graph als eine Struktur, die lediglich aus einer Knotenmenge und einer Relation Ăźber
dieser Menge besteht, gar nicht besitzen kann. Zum Beispiel kann ein Kreis der LĂ¤nge 5 sowohl als
5-zackiger Stern als auch als 5-Eck dargestellt werden:
3

2

1

4

4

5

2

1

5

3

Um verschiedene Diagramme, die demselben Graph entsprechen, als ein Objekt zu betrachten, benutzt
man den Begriff der âIsomorphieâ. NĂ¤hmlich, man sagt, dass zwei Graphen G = (V, E) und G â˛ =
(V â˛, E â˛ ) isomorph sind, falls es eine bijektive Abbildung f : V â V â˛ mit der folgendem Eigenschaft
gibt: FĂźr jede zwei Knoten u , v in V
f (u) und f (v) sind Nachchbarn in G â˛ ââ u und v sind Nachchbarn in G.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

10

D.h. zwei Graphen sind isomorph, falls sie sich nur bis auf der Numerierung ihrer Knoten unterscheiden. Die folgende Abbildung gibt stellt einen sogenannten Petersenâs Graphen (links) dar und alle
diese Graphen sind isomorph:

Abbildung 1.4: Petersenâs Graph und seine 4 isomorphe Kopien.

Die zwei im nĂ¤chsten Abbildung dargestellte Graphen sind aber bereits verschieden (nicht isomorph),
da z.B. G1 zwei Knoten (a und e) von Grad 4 hat, wĂ¤hrend G2 nur einen solchen Knoten (Knoten 4)
hat.
c

3

b

2

e

a

f

4

1

5
G2

G1

Eine interessante Klasse von Graphen sind die bipartiten Graphen. Diese Graphen haben die Eigenschaft, dass es eine Zerlegung der Knotenmenge V in zwei disjunkte Teilmengen A und B gibt, so dass
von sĂ¤mtlichen Kanten der eine Endknoten zu A gehĂśrt und der andere zu B. Bipartite Graphen haben
eine groĂe Bedeutung, liefern sie doch unmittelbar eine Veranschaulichung der binĂ¤ren Relationen.
TatsĂ¤chlich kĂśnnen nĂ¤mlich die Elemente einer beliebigen Relation R â AĂ B als Kanten von Knoten
aus A nach Knoten aus B aufgefasst werden. Zum beispiel stellt der bipartite Graph

1

c

2

b

3

a

a
b
c
4

die binĂ¤re Relation

1

2

3

4

dar.

Wichtige Objekte in einem Graphen sind âWegeâ und âKreise.â Ein Weg (engl. walk) von u nach v ist
eine Folge u0 , u1 , . . . , ur von (nicht notwendig verschiedenen) jeweils benachbarten Knoten mit u = u0
und v = ur . Die LĂ¤nge dieses Weges ist die Anzahl l der Kanten, die er erhĂ¤lt; u und v sind seine Endknoten. Beachte, dass i.A. ein Weg sowohl einen Knoten wie auch eine Kante mehrmals durchlaufen
kann! Ein Weg ist einfach, falls er keinen Knoten mehr als einmal durchlĂ¤uft. Ein einfacher Weg mit
u0 = ur heiĂt Kreis oder Zyklus.
Definition: Ein Graph G = (V, E) heiĂt zusammenhĂ¤ngend, wenn es fĂźr zwei beliebige Knoten u, v â
V mindestens einen Weg von u nach v gibt.

ÂŠ 2003 by S. Jukna

1.1. MENGEN, RELATIONEN UND FUNKTIONEN

11

Ein Graph heiĂt zyklenfrei, wenn er keine Kreise (= geschlossene Wege) besitzt. Ein ungerichteter
Graph heiĂt Wald, wenn er zyklenfrei ist. Ein ungerichteter Graph heiĂt Baum, wenn er zyklenfrei
und zusammenhĂ¤ngend ist.

Alle BĂ¤ume (bis auf Isomorphie) mit 5 Knoten:

Ein wesentliches Charakteristikum von BĂ¤umen ist die Tatsache, dass jedes Paar von Knoten in einem
Baum durch genau einen Weg verbunden ist.
Wir beobachten weiter, dass das Streichen von Kanten oder Knoten schlimmstenfalls aus BĂ¤umen
WĂ¤lder machen kann, wĂ¤hrend das HinzufĂźgen nur einer Kante zu einem Baum, die Baumstruktur
sofort vollkommen zerstĂśrt.
Behauptung 1.5. Werden in einem Baum Kanten gestrichen, dann entsteht ein Wald. Wird in einem
Baum eine Kante zwischen zwei seiner Knoten hinzugefĂźgt, dann verliert man die Baumstruktur.
Beweis. Die Beobachtung, dass durch das Streichen von Kanten in einem Baum keine Zyklen entstehen kĂśnnen, beweist erste Aussage. Den Nachweis von der zweiten Aussage liefert folgende Ăberlegung: Sei T ein Baum, also ein zusammenhĂ¤ngender, zyklenfreier Graph. Aufgrund des Zusammenhangs von T sind zwei beliebige Knoten u, v durch einen Weg pu, v in T verbunden. Wird nun dem
Graph eine zusĂ¤tzliche Kante e = {u, v} hinzugefĂźgt, dann entsteht aus pu, v und e ein Kreis, der die
Baumstruktur von T zerstĂśrt.


1.1.3 Abbildungen (Funktionen)
Eine Relation f â A Ă B derart, dass fĂźr jedes a â A genau ein Element b â B mit (a, b) â f gibt,
heiĂt Abbildung (oder Funktion) von A nach B; A ist der Definitionsbereich der Abbildung und B ihr
Bildbereich (oder Wertebereich). Bei einer Abbildung wird also jedem Element a aus A in eindeutiger
Weise das Element b = f (a) in B zugeordnet.
Eine Abbildung kĂśnnen wir uns als âblack boxâ x 7â f (x) vorstellen, in die wir etwas hineinstecken
und dafĂźr etwas neues herausbekommen. Beispiel: x 7â x 2 ergibt das Quadrat einer Zahl, A 7â | A|
die MĂ¤chtigkeit einer Menge und x 7â |x| den Betrag einer Zahl. Bei einer Funktion ist es wichtig zu
wissen, was man hineinstecken darf und aus welchem Bereich das Ergebnis ist. Wir schreiben
f : AâB
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

12

um anzuzeigen, dass die Funktion f Eingaben aus der Menge A akzeptiert und die Ausgabe f (x) zu
der Menge B gehĂśhrt; A heiĂt Definitionsbereich und B Bildbereich von f . Also beschreibt f : A â B
den Typ der Funktion und x 7â f (x) ihr Ergebnis. Um beides in einem zu beschreiben, benutzt man
manchmal die Notation A â x 7â f (x) â B.
Die Menge aller Abbildungen von A nach B bezeichnet man mit B A , d.h.

B A := { f : f ist eine Abbildung von A nach B}.
FĂźr U â A heiĂt

f (U ) = { f (a) : a â U }

Bild von U unter f . FĂźr V â B heiĂt
f â1 (V ) = {a â A : f (a) â V }
Urbild von V unter f .
-1
f (V)

V

A
FĂźr b â B setzt man

B

f â1 (b) = {a â A : f (a) = b}.

Eine Funktion f : A â B heiĂt
â˘ surjektiv, falls f (A) = B, d.h. fĂźr jedes b â B ein a â A mit f (a) = b gibt.
â˘ injektiv, falls aus a1 , a2 â A stets f (a1 ) , f (a2 ) folgt,
â˘ bijektiv, falls f surjektiv und injektiv ist.
â˛ Beispiel 1.6 : Die Funtion f : N â N mit f (x) = x 2 ist injektiv, aber die Funtion g : Z â Z mit
g(x) = x 2 ist nicht â
injektiv: â1 , 1 aber (â1) 2 = 12 . Die letzte Funktion g(x) ist auch nicht
â1
surjektiv: g (2) = 2 gehĂśrt nicht zu Z.
Die Abbildung, die jedem lebenden Menschen sein momentanes Lebensalter zuordnet, ist eine Funktion in die Menge der natĂźrlichen Zahlen N. Sie ist nicht injektiv, da viele Menschen das gleiche Lebensalter haben. Diese Funktion ist nicht surjektiv, da es keine Menschen mit 4-stelligem Lebensalter
gibt.
Injektive Funktionen f : A â B kann man auf ihrem Bild B â˛ = f (A) umkehren und erhĂ¤lt dann die
Umkehrfunktion f â1 : B â˛ â A mit f â1 (y) = x ââ y = f (x). Man verwechsle die Umkehrfunktion
nicht mit der Funktion x 7â 1/ f (x).
ÂŠ 2003 by S. Jukna

1.2. KARDINALITĂT UNENDLICHER MENGEN

(a)

(b)

13

(c)

(d)

Abbildung 1.5: (a) weder injektiv noch surjektiv; (b) injektiv aber nicht surjektiv; (c) surjektiv aber
nicht injektiv; (d) sowohl injektiv wie auch surjektiv, also bijektiv.



Wenn f : A â A bijektiv und A endlich sind, dann heiĂt f eine Permutation von A, ein sehr
wichtiger mathematischer Begrif. Ist A = {1, 2, . . . , n} so bezeichnet man eine Permutation
f : A â A oft als


1
2
...
n
.
f (1) f (2) . . . f (n)

Sind f : A â B und g : B â C Funktionen, so definiert man die Komposition oder HintereinanderausfĂźhrung h := f âŚ g der Funktionen durch h(x) = f (g(x)).
Im Laufe der Vorlesung werden wir verschiedene mathematische Strukturen kennenlernen: Graphen,
Gruppen, Ringe, KĂśrper, VektorrĂ¤ume, usw. Jede Struktur besteht aus einer Menge A (dem âUniversumâ) und einer (oder mehreren) Relationen (oder Abbildungen) âź R â A Ă A. Hat man zwei solche
Strukturen (A, âź R ) und (B, âźS ), so heiĂen diese Strukturen isomorph, wenn es eine bijektive Abbildung f : A â B mit der Eigenschaft
âx, y â A :

f (x) âźS f (y) ââ x âź R y

gibt. Diese Eigenschaft bedeutet, dass die beide Strukturen im wesentlichen eine und dieselbe Struktur
darstellen.

1.2 KardinalitĂ¤t unendlicher Mengen
Wir wollen die Mengen gemĂ¤Ă ihrer âGrĂśĂeâ vergleichen. Mit endlichen Mengen haben wir kein
Problem: Die GrĂśĂe | A| einer solchen Menge A ist einfach die Anzahl ihrer Elemente
| A| = Anzahl der Elemente in A.
Was aber wenn wir unendliche Mengen haben?
Die Intuition hat in der Unendlichkeit keinen festen Platz, wie man an Hilberts Hotel feststellt: Hier
gibt es unendlich viele durchnummerierte Zimmer, die alle belegt sind. Ein NeuankĂśmmling erhĂ¤lt
dennoch ein freies Zimmer, ohne dass die anderen GĂ¤ste sich einen Raum teilen oder aus dem Hotel
verschwinden mĂźssen: Der neue Gast bekommt einfach das Zimmer 1, dessen ursprĂźnglicher Bewohner zieht nach Zimmer 2 um, wĂ¤hrend der Bewohner aus Zimmer 2 in das Zimmer 3 einzieht ... Es
kĂśnnen sogar unendlich viele neue GĂ¤ste unterkommen - die alten GĂ¤ste verlegt man von Zimmer n
nach Zimmer 2n und die neuen GĂ¤ste erhalten die Zimmer mit den ungeraden Nummern.
ÂŠ 2003 by S. Jukna

14

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

Deshalb vergleicht man die âMĂ¤chtigkeitenâ unendlichen Mengen nicht durch einen ZĂ¤hlvorgang,
sondern nach dem âOmnibus-Prinzipâ:
Das Omnibus-Prinzip: In einem Bus gibt es ebenso viele SitzplĂ¤tze wie FahrgĂ¤ste, wenn kein Fahrgast stehen muss und kein Sitz frei bleibt. Die Passagiere sitzen injektiv, also nicht gestapelt und
eindeutig (keine Person nimmt mehr als einen Platz ein).
Genau dann existiert eine bijektive (injektive und surjektive) Abbildung von der Menge aller FahrgĂ¤ste
auf die Menge aller SitzplĂ¤tze. âSurjektivâ bedeutet hier, dass alle Sitze besetzt sind.
Nach diesem Prinzip hat Cantor 1874 die folgende Definition eingefĂźhrt. Man sagt, dass eine Menge
A nicht grĂśĂer als eine andere Menge B ist, falls es eine injektive Abbildung f : A â B gibt. Ist f
bijektiv, so sagt man, dass A und B gleich groĂ sind (oder die gleiche KardinalitĂ¤t haben).



Sind die Mengen A und B endlich, so gibt es eine injektion f : A â B genau dann, wenn
| A| 6 |B| gilt. Deshalb ist fĂźr endlichen Mengen die GrĂśĂe genau die Anzahl der Elemente.
FĂźr unendlichen Mengen gilt das nicht mehr, da sie alle die gleiche Anzahl der Elemente haben â
nĂ¤hnlich â (unendlich viele).
â˛ Beispiel 1.7 : Sei N = {0, 1, 2, 3, 4, 5, . . .} und sei E = {0, 2, 4, 6, . . .} die Menge aller geraden Zahlen.
Es ist klar, dass E nicht grĂśĂer als N ist und, wenn man die Zahlen auf einer Linie dargestellt
vorstellt, hat E sehr viele âLĂźckenâ: jede zweite Zahl fehlt! Also sollte E echt kleiner als N sein?
Die Anwort ist: nein, die Mengen E und N sind gleich gross!
Beweis: f (n) = 2n ist eine bijektive Abbildung von N nach E (da x , y ââ 2x , 2y gilt).
Vom besondaren Interesse (insbesondare in der Informatik) sind Mengen, die nicht grĂśĂe als die
Menge N = {0, 1, 2, 3, 4, 5, . . .} aller natĂźrlichen Zahlen sind. Solche Mengen nennt man âabzĂ¤hlbarâ.
Eine Menge A heiĂt abzĂ¤hlbar, wenn es eine Injektion f : A â N gibt.
D.h. in diesem Fall kann man jedem Element a â A (Fahrgast) einen eindeutigen Nummer f (a) â N
(SitzplĂ¤tz) zuweisen. Die Menge A ist also abzĂ¤hlbar, wenn man ihre Elemente durchnummerieren
kann, A = {a0 , a1 , a2 , . . .}. Ist eine Menge nicht abzĂ¤hlbar, so heiĂt sie ĂźberzĂ¤hlbar.
Behauptung 1.8. Ist eine Menge A abzĂ¤hlbar, so ist jede Teilmenge S â A abzĂ¤hlbar.
Beweis. Wenn S nicht onehin endlich ist, kann man dies folgendermaĂen einsehen. Sei f : N â A
eine Bijektion. Wir definieren g : N â S, indem wir g(0), g(1), g(2), . . . jeweils auf das nĂ¤chste
Element von f (0), f (1), f (2), . . . setzen, das in S liegt. Zum Beispiel fĂźr A = {2n : n â N} und
S = {4n : n â N}:
f (0) = 0 â S â g(0) = f (0)
f (1) = 2 < S

f (2) = 4 â S â g(1) = f (2)
f (3) = 6 < S

f (4) = 8 â S â g(2) = f (4)
..
.

ÂŠ 2003 by S. Jukna

1.2. KARDINALITĂT UNENDLICHER MENGEN

15

Wir kennen auch andere unendliche Mengen:
N â Z â Q â R.
Sind diese Mengen grĂśĂer als N oder sind sie abzĂ¤hlbar, d.h. gleich gross wie N?
Auf dem ersten Blick scheinen diese Mengen viel grĂśĂer als N zu sein. Insbesondere die Menge Q,
da sie sehr âdichtâ ist: zwischen zwei beliebigen rationalen Zahlen liegt mindestens eine rationale
Zahl. Die Intuition ist aber trĂźgerich: Die ersten zwei Mengen N, Z und Q sind gleich gross (sind
abzĂ¤hlbar)! Nur die letzte Menge R ist âecht grĂśĂerâ als N (ist ĂźberzĂ¤hlbar).
â˛ Beispiel 1.9 : Z ist abzĂ¤hlbar. Die entsprechende Injektion f : Z â N kann man zum Beispiel
durch

2x â 1 falls x > 0
f (x) =
â2x falls x 6 0
definieren, d.h. positive Zahlen bekommen ungeraden und negative ungeraden Nummern.

â˛ Beispiel 1.10 : Ist N Ă N abzĂ¤hlbar? Ja:
8
7
6
5
4
3
2
1
0
0

1

2

3

4

5

6

7

8

...

Daraus folgt auch, dass Q abzĂ¤hlbar ist.
Welche Mengen dann sind ĂźberzĂ¤hlbar? Wenn wir uns einen unendlichen Bus mit den Sitzen 0, 1, 2, . . .
vorstellen, dann ist die Menge (der FahrgĂ¤ste) A abzĂ¤hlbar genau dann, wenn jeder Fahrgast a â
A einen eigenen Sitzplatz bekommt. Ist die Menge A der FahrgĂ¤ste unendlich, so muss auch jeder
Sitplatz auch tatsĂ¤chlich besetzt sein, D.h. es muss dann eine surjektive Abbildung f : N â A geben.
â˛ Beispiel 1.11 : (Cantorâs Diagonalisierungsmethode) Wir haben bereits gesehen, dass die Menge
Z der ganzen Zahlen wie auch die Menge Q der rationalen Zahlen abzĂ¤hlbar sind. Ist auch die
Menge R der reellen Zahlen abzĂ¤hlbar? Die Antwort ist nein â R ist ĂźberzĂ¤hbar. Es gibt bereits
Teilmengen der reellen Zahlen, die nicht abzĂ¤hbar sind: WĂ¤re etwa die Menge derjenigen Zahlen
abzĂ¤hlbar, die zwischen 0 und 1 liegen, dann kĂśnnte man sogar die Menge der abzĂ¤hlbar unendlichen Dezimalbruchentwicklungen der Form 0, x 1 x 2 . . . mit x i â {0, 1, . . . , 9} abzĂ¤hlen und wie
folgt auflisten:
0, a11 a12 a13 a14 . . .
0, a21 a22 a23 a24 . . .
0, a31 a32 a33 a34 . . .
..
..
..
..
.. ..
.
.
.
.
. .
ÂŠ 2003 by S. Jukna

16

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN
a11 wĂ¤re also die erste Kommastelle der Dezimalentwicklung, welche die kleinste Nummer in
der AbzĂ¤hlung erhĂ¤lt, a12 die zweite Kommastelle, usw. Wir konstruieren jetzt 0, b1 b2 b3 . . . wie
folgt:
b1 , a11 , b2 , a22 , . . . , bb , ak k , . . .
und erhalten damit eine Dezimalbruchentwicklung, die nicht in der obigen Liste vorkommen
kann, d.h. wir bekommen einen Fahrgast (Dezimalbruchentwicklung) der keinen Sitplaz in dem
Buss N bekommt. Aus diesem Widerspruch folgt, dass man auch die reellen Zahlen zwischen 0
und 1 nicht abzĂ¤hlen kann. Das Wort âDiagonaleâ erklĂ¤rt sich von selbst: Wir konstruieren die
Dezimalbruchentwicklung 0, b1 b2 b3 . . . indem wir die Diagonalelemente ak k vertauschen. D.h.
der Widerspruch befindet sich auf der Diagonale.

Satz 1.12. (Cantor) Sei A eine beliebige Menge und 2 A = {X : X â A} ihre Potenzmenge. Dann
existiert keine Surjektion f : A â 2 A . Insbesondere ist 2N ĂźberzĂ¤hlbar.
Beweis. Ein Widerspruchsbeweis. Angenommen f : A â 2 A ist surjektiv. Setze
D := {a â A : a < f (a)}.
Weil f surjektiv ist, existiert ein a0 â A mit f (a0 ) = D. Es gilt: entweder a0 â D oder a0 < D. Aber
nach der Definition von D gilt: a0 â D ââ a0 < f (a0 ) = D, ein Widerspruch.

WĂ¤re 2N abzĂ¤hlbar, so gĂ¤be es eine Bijektion N â 2N , aber (wie wir bereits gezeigt haben) es gibt
nicht einmal eine Surjektion.


Damit haben wir noch eine ĂźberzĂ¤lbare Menge P(N) = {X : X â N} gefunden! Beachte aber, dass
die Menge
E (N) = {X : X â N, X endlich}
bereits abzĂ¤hlbar ist! (Siehe Aufgabe 9)
Der grĂśĂter Unterschied zwischen der Mathematik und der Informatik ist, dass man sich in der Informatik nur mit solchen Funktionen beschĂ¤ftigt, deren Werte mit einer Program berechnet werden
kĂśnnen. Der folgende Korollar zeigt, dass damit sehr viele Funktionen ausser der Interesse von Informatikern bleiben.
Korollar 1.13. Es gibt (ĂźberzĂ¤hlbar viele) Abbildungen f : N â {0, 1}, die nicht durch ein Programm
berechnet werden kĂśnnen.
Beweis. Jedes Programm, egal in welcher Programmiersprache, ist eine endliche Folge von Symbolen aus einer endlichen Menge (Alphabet + Sonderzeichen). Daher ist die Menge aller Programme
abzĂ¤hlbar. Demnach ist die Menge B â {0, 1}N aller Abbildungen f : N â {0, 1}, welche von einem
Programm berechnet werden kĂśnnen, abzĂ¤hlbar. Nach dem Satz 1.12 ist {0, 1}N ĂźberzĂ¤hlbar. Deshalb
ist {0, 1}N \ B nicht leer, sogar ĂźberzĂ¤hlbar.

ÂŠ 2003 by S. Jukna

1.3. AUSSAGENLOGIK UND BEWEISMETHODEN

17

1.3 Aussagenlogik und Beweismethoden
Wir haben bereits einige Aussagen bewiesen, ohne die logische Struktur der Beweise zu betonen.
Diese Struktur in der BeweisfĂźhrung ist aber Ă¤userst wichtig, da jeder Schritt muss logisch korrekt
sein. Deswegen lĂśhnt es sich, die logische Struktur der Beweise genauer anzuschauen. ZunĂ¤chst mussen wir aber genauer die Struktur des Grundobjekts der mathematischen Beweise â der Aussage â
betrachten.

Aristoteles (384â322 vor Christus):
Eine Aussage ist ein spachliches Gebilde, von dem es sinnwoll ist, zu
sagen, es sei wahr oder falsch.

Eine Aussage also ist ein Satz, der entweder wahr oder falsch ist, aber nie beides zugleich. Wahre
Aussagen haben den Wahrheitswert 1 und falsche Aussagen den Wahrheitswert 0.
Zum Beispiel die Aussage
A = â15 ist durch 3 teilbarâ
ist wahr (also hat A den Wahrheitswert 1) aber die Aussage
B = â16 ist kleiner 12â
ist falsch (und deshalb hat B den Wahrheitswert 0).



Nicht jeder Satz ist eine Aussage! Um ein Beispiel fĂźr einen Satz vorzustellen, der weder wahr
noch falsch sein kann und deshalb keine Aussage ist, sehen wir uns Russells Paradoxon âDieser
Satz ist falsch.â an. Angenommen, der Satz wĂ¤re wahr, dann mĂźsste er falsch sein. Gehen wir aber
davon aus, dass der Satz falsch ist, dann mĂźsste er wahr sein. a
a Ăbrigens hat Russell mit diesem Paradoxon sehr prĂ¤gnant die tief verwurzelte Annahme der Mathematik, dass allen
SĂ¤tzen ein Wahrheitswert zugeordnet sei, erschĂźttert und eine tiefe Grundlagenkrise der Mathematik zu Beginn des 20. Jahrhunderts ausgelĂśst.

Als nĂ¤chstes schauen wir an, wie man einfachste (atomare) Aussagen in kompliziertere Aussagen
umwandeln kann.
Folgende VerknĂźpfungen von Aussagen werden hĂ¤ufig benutzt; dabei bezeichnen A und B zwei Aussagen:
A
0
0
1
1

B
0
1
0
1

Aâ§B
0
0
0
1

Aâ¨B
0
1
1
1

AâB
1
0
0
1

AâB
0
1
1
0

A
1
0

ÂŹA
0
1

â˘ A â§ B (lies A und B) ist genau dann wahr, wenn A und B beide wahr sind.
â˘ A â¨ B (lies A oder B) ist genau dann wahr, wenn mindestens eine der Aussagen A, B wahr ist.
ÂŠ 2003 by S. Jukna

18

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN
â˘ A â B (lies A und B sind Ă¤quivalent) ist genau dann wahr, wenn A und B beide den gleichen
Wahrheitswert haben.
â˘ A â B (lies entweder A oder B) ist genau dann wahr, wenn genau eine der Aussagen A, B wahr
ist. (Oft schreibt man AâB statt A â B.)
â˘ ÂŹA (lies nicht A) hat den umgekehrten Wahrheitswert wie A.

Eine wichtige VerknĂźpfung ist die Implikation A â B (lies wenn A, dann B; ist A wahr, dann auch
B):
A B
0 0
0 1
1 0
1 1

AâB
1
1
0
1

Insbesondere ist A â B immer wahr, wenn A falsch ist! Ist das sinnwoll? Dazu ein Beispiel von
Bertrand Russel.
âWenn 1 = 0 ist, bin ich der Papst!â
Beweis: Aus 1 = 0 folgt 2 = 1. Da der Papst und ich 2 Personen sind, sind wir 1 Person.





Die Implikation A â B sagt also nur, dass B wahr sein muss, falls die Aussage A richtig ist.
Sie sagt aber nicht, dass A auch tatsĂ¤chlich war ist! Die Implikation ist eine der wichtigsten logischen VerknĂźpfungen in der Mathematik: Sie erlaubt logisch konsistente Theorien bilden, ohne sich
um die (tatsĂ¤chliche) richtigkeit der ursprĂźnglichen Aussagen (sogenannten âAxiomen) zu kĂźmmern!
Um nicht so viele Klammern benutzen zu mĂźssen, legt man die âStĂ¤rkeâ der Bindung der Operationen
fest:
ÂŹ stĂ¤rker als â§
â§ stĂ¤rker als â¨
â¨ stĂ¤rker als â
â stĂ¤rker als â
Die Formel ÂŹ(A â§ B â¨ ÂŹC) â ÂŹC bedeutet also (ÂŹ((A â§ B) â¨ (ÂŹC))) â (ÂŹC)
Ordnet man den Aussagenvariablen â in diesem Kontext zusammen mit den Wahrheitswerten 1 und 0
auch atomare Formeln genannt â Wahrheitswerte zu, so erhĂ¤lt man aus der aussagenlogischen Formel
eine Aussage. Zur Ermittlung des Wahrheitswerteverlaufs der Formel kann eine Wahrheitstafel aufgestellt werden, in der sich fĂźr alle mĂśglichen Kombinationen von Wahrheitswerten, die die einzelnen
Aussagenvariablen der Formel annehmen kĂśnnen, der Wahrheitswert der Formel ablesen lĂ¤sst.
â˛ Beispiel 1.14 : FĂźr die Formel ÂŹ(A â§ B â¨ ÂŹC) â ÂŹC ergibt sich die folgende Wahrheitstafel.
ÂŠ 2003 by S. Jukna

1.3. AUSSAGENLOGIK UND BEWEISMETHODEN
A
1
1
1
1
0
0
0
0

B C
1 1
1 0
0 1
0 0
1 1
1 0
0 1
0 0

A â§ B ÂŹC
1
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1

A â§ B â¨ ÂŹC
1
1
0
1
0
1
0
1

19

ÂŹ(A â§ B â¨ ÂŹC)
0
0
1
0
1
0
1
0

ÂŹ(A â§ B â¨ ÂŹC) â ÂŹC
1
1
0
1
0
1
0
1

Jede Zeile enthĂ¤lt eine Belegung der beteiligten Aussagenvariablen mit einem der beiden Wahrheitswerte 1 und 0. Dabei kommt jede mĂśgliche Belegung genau einmal vor. Die letzte Spalte
der Wahrheitstafel gibt den Wahrheitswerteverlauf der Formel an. Man sieht, dass der Wahrheitswerteverlauf eindeutig durch die Struktur der Formel und ihren Aufbau aus den verknĂźpften
Teilformeln festgelegt wird.

Aussagenlogische Formeln kann man als Schaltkreise (oder SchaltplĂ¤ne) darstellen:

A
V
B
C

&
V

Abbildung 1.6: Darstellung von (A â¨ ÂŹB) â§ (A â¨ C)

Aussagenlogische Formeln kĂśnnen erfĂźllbar sein (d.h. wahr bei geeigneter Wahl der Wahreitswerte
der atomaren Aussagen) aber auch nicht, z.B. A â§ ÂŹA; dann heiĂen sie Kontradiktionen (Widerspruche). Tautologien sind immer wahr, z.B. A â¨ ÂŹA.

Zwei aussagelogischen Formeln F und H heiĂen (logisch) Ă¤quivalent (Bezeichnung F ââ H),
wenn sie bei jeder Belegung der in ihnen vorkommenden Aussagenvariablen denselben Wert annehmen, d.h. wenn F â H eine Tautologie ist.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

20
Doppelnegation
deMorgans Regeln

Kontrapositionsgesetz
Prinzip des indirekten Beweises
oder Widerspruchbeweis
DistributivitĂ¤t
Prinzip vom ausgeschlossenen Dritten
Idempotenz
Kontradiktionsregeln

ÂŹ(ÂŹA)
ÂŹ(A â¨ B)
ÂŹ(A â§ B)
AâB
(A â B)
(A â B)
(A â B)
A â§ (B â¨ C)
A â¨ (B â§ C)
A â¨ ÂŹA
A â§ ÂŹA
Aâ§ A
Aâ¨ A
Aâ¨0
Aâ§0

ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ
ââ

A
ÂŹA â§ ÂŹB
ÂŹA â¨ ÂŹB
ÂŹB â ÂŹA
(A â§ ÂŹB) â ÂŹA
(A â§ ÂŹB) â B
(A â§ ÂŹB) â 0
(A â§ B) â¨ (A â§ C)
(A â¨ B) â§ (A â¨ C)
1
0
A
A
A
0

Eine nĂźtziliche Merkregel ist, dass man die logische Implikation A â B mittels ÂŹ und â¨ auffassen
kann:
AâB

ââ

ÂŹA â¨ B

â˛ Beispiel 1.15 : Um an einem kleinen Beispiel zu demonstrieren, wie man mit Hilfe der aufgelisteten
logischen Ăquivalenzen tatsĂ¤chlich zu Vereinfachungen kommen kann, betrachten wir die Formel
ÂŹ(ÂŹA â§ B) â§ (A â¨ B).
ÂŹ(ÂŹA â§ B) â§ (A â¨ B)
(ÂŹ(ÂŹA) â¨ (ÂŹB)) â§ (A â¨ B)
(A â¨ ÂŹB) â§ (A â¨ B)
A â¨ (ÂŹB â§ B)
Aâ¨0
A

(deMorgans Regel)
(Doppelnegation)
(DistributivitĂ¤t)
(Prinzip vom ausgeschlossenen Widerspr.)
(Kontradiktionsregel)

Aufgrund der mit dieser Umformung nachgewiesenen logischen Ăquivalenz der beiden Formeln
kann nun anstelle der komplizierten Formel ÂŹ(ÂŹA â§ B) â§ (A â¨ B) stets die einfacher strukturierte
atomare Formel A zur logischen Beschreibung des Sachverhalts benutzt werden.
Mit Mengen kann man genauso ârechnenâ wie mit Aussagen!
Mengen Aussagen
AâŞB
Aâ¨B
AâŠB
Aâ§B
AâB
AâB
A
ÂŹA
So ist zum Beispiel A âŞ B = A âŠ B (deMorgans Regel), usw.
ÂŠ 2003 by S. Jukna

1.3. AUSSAGENLOGIK UND BEWEISMETHODEN

21

1.3.1 Aussageformen (PrĂ¤dikate)
Die bischer betrachtete Aussagenlogik erweist sich als nicht ausreichend, um allgemeine logische
Ausagen zu treffen. So wollen wir beispielsweise Aussagen Ăźber Elemente von Mengen treffen. Dazu
benutzt man sogenannte âPrĂ¤dikateâ.
Ein n-stelliges PrĂ¤dikat Ăźber M ist einfach eine n-stellige Abbildung P : M n â {0, 1}. Ein PrĂ¤dikat
P(x 1 , . . . , x n ) nimmt also in AbhĂ¤ngigkeit von der Belegung der x i mit Elementen aus M den Wert
âwahrâ oder âfalschâ an.
Um PrĂ¤dikate in Aussagen umzuwandeln, benutzt man so genannte Quantoren: den Allquantor âx
(fĂźr alle x â M) und den Existenzquantor âx (es gibt ein x â M). Jeder Quantor bindet das freie
Vorkommen der Variablen, die er quantifiziert. Die mit dem Allquantor gebildeten Aussagen werden
Allaussagen genannt, und die mit dem Existenzquantor gebildeten Aussagen heiĂen Existenzaussagen.
Sei P(x) eine Aussageform Ăźber dem Universum M.

-

Die Aussage âx P(x) ist genau dann wahr, wenn es mindestens ein a in M existiert, so dass
P(a) wahr ist.

-

Die Aussage âx P(x) ist genau dann wahr, wenn P(a) fĂźr jedes a aus M wahr ist.

â˛ Beispiel 1.16 : Sei S(x) die Aussageform âx 6 x + 1)â Ăźber N. Dann stellt âx : S(x) die Aussage
âFĂźr jedes n in N gilt n 6 n + 1â dar. Diese Aussage ist wahr, da S(n) fĂźr jede natĂźrliche Zahl n
eine Aussage mit dem Wahrheitswert 1 liefert.
Sei S(x) wie oben. Dann ist âx : S(x) die Aussage âEs gibt ein n in N, fĂźr das (n 6 n + 1) giltâ.
Diese Aussage ist ebenfalls wahr, da z.B. S(5) wahr ist.
Sei P(x) die Aussageform âx ist eine Primzahlâ Ăźber N. Dann stellt âx : P(x) die Aussage âFĂźr
jedes n aus N gilt: n ist eine Primzahlâ dar. Diese Aussage ist falsch, da z.B. 4 keine Primzahl
ist. Also ist P(4) nicht wahr, und P(n) damit nicht fĂźr jedes n aus N wahr.
Sei P(x) wie in oben definiert. Dann ist âx : P(x) die Aussage âEs gibt eine natĂźrliche Zahl n,
die eine Primzahl istâ. Diese Aussage ist wahr, da z.B. 3 eine Primzahl ist und damit P(3) eine
wahre Aussage.

Die Quantoren â und â kĂśnnen nun auf mehrstellige PrĂ¤dikate angewendet werden: Sei P ein nstelliges PrĂ¤dikat, dann ist âx i P(x 1 , . . . , x n ) fĂźr x i , 1 6 i 6 n, wieder ein PrĂ¤dikat, bei dem eine
Variable, die Variable x i , âgebundenâ ist. âx i P(x 1 , . . . , x n ) ist also nun ein (n â 1)-stelliges PrĂ¤dikat,
auf das wieder ein Quantor angewandt werden kann. Analoges gilt fĂźr âx i P(x 1 , . . . , x n ). Dabei heiĂt
x i gebundene Variable, alle anderen heiĂen freie Variablen.
Um aus einer Aussageform mit mehreren Variablen durch Quantifizierung Aussagen (d.h. nullstellige
PrĂ¤dikate) zu erhalten, muss jede freie Variable durch einen gesonderten Quantor gebunden werden!
Alle innerhalb der Aussagenlogik gĂźltigen Ăquivalenzen gelten auch in der PrĂ¤dikatenlogik. DarĂźber
hinaus existieren noch weitere Ăquivalenzen, welche die Quantoren miteinbeziehen:
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

22

âx : P(x)

ââ

ÂŹ (âx : ÂŹP(x))

ÂŹ (âx : P(x))

ââ

âx : ÂŹP(x)

ÂŹ (âx : P(x))

ââ

âx : ÂŹP(x)

(âx : P(x)) â§ (âx : Q(x))

ââ

âx : P(x) â§ Q(x)

(âx : P(x)) â¨ (âx : Q(x))

ââ

âx : P(x) â¨ Q(x)

âxây : P(x, y)

ââ

âyâx : P(x, y)

âxây : P(x, y)

ââ

âyâx : P(x, y)

N egationsregeln :

Ausklammerungsregeln :

V ertauschungsregel :

Zur Schreibweise: Man schreibt
âx â M P(x)

âx â M P(x)

anstatt
anstatt

âx(x â M â P(x))
âx(x â M â§ P(x))

Man scheibt auch âx! P(x) fĂźr âes gibt genau ein x mit P(x) = 1â.
Hier sind ein paar hĂ¤ufiger Fehlern, die man mit dem Umgang mit Quantoren macht.



Bei verschiedenen Quantoren kommt es auf die Reihenfolge der Quantoren an! Z.B. bezeichne
P(x, y) die Aussageform âx > yâ, x und y seien Variablen Ăźber den Universum N. Dann ist
âyâx P(x, y) wahr. Aber âxây P(x, y) ist falsch.



Die folgenden Formelpaare sind nicht Ă¤quivalent, obwohl sie den Ausklammerungsregeln sehr
Ă¤hnlich sind:
(âx : P(x)) â¨ (âx : Q(x)) mit âx : P(x) â¨ Q(x)



(âx : P(x)) â§ (âx : Q(x)) mit âx : P(x) â§ Q(x)

Falsche Ăbersetzung von umgangssprachlichen Implikationen. Wenn A = âich bin mit meiner
Hausaufgaben fertigâ und B=âich werde ins Kino gehenâ, dann besagt A â B âich werde ins
Kino gehen, wenn ich mit meiner Hausaufgaben fertig bin â, wobei B â A besagt: âich werde ins
Kino gehen, nur wenn ich mit meiner Hausaufgaben fertig bin â.



Falsche Negierung von Aussagen ohne deMorgans Regeln zu benutzen: ÂŹ(Aâ¨ B) und ÂŹAâ¨ÂŹB
sind nicht Ă¤quivalent! Das Gleiche gilt fĂźr Mengenoperationen: so ist z.B. A âŞ B = AâŠ B, nicht
aber A âŞ B = A âŞ B.
ÂŠ 2003 by S. Jukna

1.3. AUSSAGENLOGIK UND BEWEISMETHODEN



23

Falsche Beschreibung einer Existensaussage als âx(A(x) â B(x)) anstatt âx(A(x) â§ B(x)).
Zum Beispiel die Aussage âEs gibt eine ungerade Zahl, die prim istâ hat die Form âx(U (x) â§
P(x)), nicht âx(U (x) â P(x)). Allgemeine Regel: Nach einem â-Quantor folgt normalerweise ein
UND, nicht die Implikation.



Falsche Beschreibung einer âFĂźr-alle-Aussageâ als âx(A(x)â§B(x)) anstatt âx(A(x) â B(x)).
Zum Beispiel die (falsche!) Aussage âJede gerade Zahl ist primâ hat die Form âx(G(x) â
P(x)), nicht âx(G(x) â§ P(x)). Allgemeine Regel: Nach einem â-Quantor folgt normalerweise eine
Implikation, nicht UND.



Negation von Aussagen mit Quantoren, insbesondere in Umgangssprache. Zum Beispiel, Negation von âmanche Katzen magen Wurstâ ist nicht die Aussage, dass âmanche Katzen hassen
Wurstâ, sondern âkeine Katzen mĂśgen Wurstâ oder âalle Katzen hassen Wurstâ.

1.3.2 Logische Beweisregeln
Es gibt ein Paar grundlegender logischer Regeln, wie man eine neue wahre Aussage aus bereits als
wahr bekannten Aussagen ableiten kann. Diese Regeln haben die Form
A1 , A2 , . . . , An
B
und ihre Bedeutung ist: falls alle Aussagen A1 , A2 , . . . , An wahr sind, dann ist auch die Aussage B
wahr.
Hier sind die wichtigsten Regeln.

Modus ponens:
A, A â B
B
Ist A wahr und folgt B aus A, dann ist auch B wahr.

Logische Schlusskette:
A â B, B â C
AâC
Folgt B aus A und C aus B, dann folgt auch C aus A.

Kontrapositionsregel:
ÂŹB â ÂŹA
AâB
Folgt ÂŹA aus ÂŹB, dann muss auch B aus A folgen.
ÂŠ 2003 by S. Jukna

24

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

Reductio ad absurdum - Widerspruchsregel:
ÂŹB, ÂŹA â B
A
Wir wissen, dass B falsch ist und dass aus ÂŹA das Gegenteil folgen wĂźrde. Also ist die Aussage A
wahr.
â˛ Beispiel 1.17 : (Widerspruchsregel) Eine falsche Behauptung: 1 ist die grĂśĂte reelle Zahl.
Beweis: Angenommen, es gibt eine andere grĂśĂte Zahl y. Es ist nun 1 < y, also ist y insbesondere
eine positive Zahl, d.h wir kĂśnnen die Ungleichung mit y multiplizieren und erhalten y < y2 . Das
ist aber ein Widerspruch zur Annahme, dass y die grĂśsste reelle Zahl ist, also folgt die Behauptug
und somit ist 1 die grĂśsste reelle Zahl.
Wo liegt der Fehler? In der Negation der Behauptung! Die richtige Negation mĂźsste lauten: 1 ist
nicht die grĂśsste reelle Zahl.
â˛ Beispiel 1.18 : (Widerspruchsregel) Wir wollen die Aussage A = âes gibt unendlich viele Primzahlenâ beweisen. Dazu nehmen wir das Gegenteil (also
QnÂŹA ist war) an und sei etwa {p1 , p2 , . . . , pn }
die endliche Menge der Primzahlen und sei P = i=1
pi . Dann ist P + 1 keine Primzahl, wird
also von einer Primzahl, etwa pi 0 echt geteilt. Also sollte die Aussage B:
P+1 Y
1
=
pi +
pi 0
p
i0
i=1

ist eine natĂźrliche Zahl

i,i 0

wahr sein, was offensichtlich Unsinn ist. Also muss die Aussage A richtig sein (nach der Widerspruchsregel).
â˛ Beispiel 1.19 : (Kontrapositionsregel) Sei a eine beliebige ganze Zahl. Wir wollen die Aussage
wenn a2 eine ungerade Zahl ist, dann ist a ungerade

(â)

beweisen. Diese Aussage hat die Form A â B, wobei A = âa2 ist ungeradeâ und B = âa ist
ungeradeâ. Wir fĂźhren einen Beweis durch Kontraposition. Sei also angenommen, dass a gerade
ist (ÂŹB gilt). Dann ist a = 2 Âˇ k fĂźr eine ganze Zahl k. Deshalb ist a2 = (2 Âˇ k) Âˇ a = 2 Âˇ (k Âˇ a). Weil
k Âˇ a eine ganze Zahl ist, folgt schlieĂlich a2 = 2 Âˇ k â˛ fĂźr eine ganze Zahl k â˛. Also ist a2 gerade
(ÂŹA gilt). Damit haben wir ÂŹB â ÂŹA gezeigt und damit auch, dass die ursprĂźngliche Aussage
A â B gelten muss (Kontrapositionsregel). Genauso beweist man die Aussage
wenn a2 eine gerade Zahl ist, dann ist a gerade

(ââ)

Wir zeigen die kontrapositive Aussage: wenn a ungerade ist, dann ist auch a2 ungerade. Ist a
ungerade, so ist a = 2k + 1 fĂźr eine ganze Zahl k. Da x = y =â x 2 = y2 , haben wir, dass
a2 = (2k + 1) 2 = 4k 2 + 4k + 1 = 2(2k 2 + 2k) + 1
Da k eine ganze Zahl ist, ist auch (2k 2 +2k) eine ganze Zahl. Also ist a2 ungerade, wie behauptet.
Die beiden Aussagen (â) und (ââ) zusammen liefen die Aussage
a2 ist gerade ââ a ist gerade

(1.1)
ÂŠ 2003 by S. Jukna

1.3. AUSSAGENLOGIK UND BEWEISMETHODEN

25

â˛ Beispiel 1.20 : (Widerspruchsregel) Eine reelle Zahl x genau dann rational ist, wenn es zwei ganze
Zahlen a und b mit der Eigenschaft
x=

a
und die Zahlen a, b haben3 keinen gemeinsamen Teiler k > 2
b

Wir wollen die Aussage

â
A = â 2 ist irrationalâ
â
beweisen. Dazu nehmen wir an, dass 2 eine rationale Zahl ist. Dann kann man diese Zahl als
Quotient a/b zweier ganzen Zahlen a und b darstellen, wobei a und b keinen gemeinsamen Teiler
k > 2 haben. Sei nun B die Aussage
B=

âa und b haben einen gemeinsamen Teiler k > 2â

Wir wissen (nach Annahme), dass B falsch ist. Also, um die Aussage A zu beweisen, reicht es zu
zeigen, dass ÂŹA â B wahr ist. Dazu nehmen wir an, dass ÂŹA wahr ist. Dann gilt:
â
1. 2 = a/b.
2. FĂźr beliebige Zahlen x, y mit x = y gilt x 2 = y2 . Also gilt 2 = a2 /b2 .
3. Multiplizieren wir beide Seiten mit b2 so erhalten wir a2 = 2b2 .
4. Da a2 gleich zwei mal eine ganze Zahl ist, ist a2 gerade.
5. Nach (1.1) ist auch a gerade.
6. Nach der Definition von geraden Zahlen, muss es eine ganze Zahl c mit a = 2c geben.
7. Aus 2b2 = a2 und a = 2c folgt 2b2 = 4c2 und damit folgt b2 = 2c2 .
8. Also ist b2 eine gerade Zahl und, wieder nach (1.1), ist auch b gerade.
9. Da beide Zahlen a und b gerade sind, ist 2 ein gemeinsamer Teiler von a und b, d.h. die
Aussage B ist wahr.



Da B falsch ist und (wie wir gerade gezeigt
â haben) ÂŹA â B gilt, muss (nach der Widerspruchsregel) die Aussage A falsch sein. Also ist 2 eine irrationale Zahl, wie behauptet.
Jeder einzelner Schritt im Beweis muss korrekt begrĂźndet werden! Zum Beispiel schreibt man
1=

â

1=

p

?

(â1)(â1) =

als âBeweisâ fĂźr 1 = â1. Aber die Aussage, dass
ist falsch!

â
â

â
â
â1 â1 = ( â1) 2 = â1

xy =

â â
x y fĂźr alle Zahlen x und y gelten muss,



Man muss vorsichtig sein, wenn man beide Seiten einer Gleichung durch einer Variable dividiert ax = xb =â a = b nur wenn x , 0. Um x , 0 nachzuweisen, muss man zusĂ¤tzliche
Arbeit leisten.



Man behauptet: aus ax < bx folgt a < b. Das ist noch schlimmer! Die Folgerung a < b ist
einfach falsch, wenn x < 0 ist, und ist unbewiesen, wenn x = 0 ist.
ÂŠ 2003 by S. Jukna

26

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

1.4 Mathematische Induktion: Beweis von Aussagen âx P(x)
Als nĂ¤chstes werden wir ein allgemeines (und Ăźberraschend mĂ¤chtiges) Prinzip â das Induktionsprinzip â kennen lernen. Dieses Prinzip erlaubt, Aussagen der Form âân â N : P(n)â zu beweisen.4
Nicht immer lĂ¤sst sich solche Aussagen einfach dadurch beweisen, dass man eine beliebige natĂźrliche
Zahl a wĂ¤hlt und dann einen Beweis fĂźr P(a) fĂźhrt. Zum Beispiel ist ein solcher Beweis fĂźr die
Behauptung

Jeder Geldbetrag von mindestens 4 Pfennigen lĂ¤sst sich allein mit Zwei- und FĂźnfpfennigstĂźcken
bezahlen

etwas umstĂ¤ndlich, falls wir a Pfennige bezahlen mĂźssen und sonst nichts Ăźber a wissen. Wenn wir
jedoch wissen, in welcher StĂźckelung a Pfennige zu bezahlen sind, kĂśnnen wir daraus recht leicht
schlieĂen, wie eine StĂźckelung fĂźr a + 1 Pfennige aussehen kann: Man stelle sich dazu den MĂźnzenhaufen fĂźr a Pfennige vor.
1. Wenn er zwei ZweipfennigstĂźcke enthĂ¤lt, nehmen wir sie fort und legen dafĂźr ein FĂźnfpfennigstĂźck hinzu
2. Wenn er ein FĂźnfpfennigstĂźck enthĂ¤lt, nehmen wir es fort und legen dafĂźr drei ZweipfennigstĂźcke
hinzu.
Im ersten Fall enthĂ¤lt der MĂźnzenhaufen a â 2 Âˇ 2 + 5 = a + 1 Pfennige, und im zweiten Fall a â
5 + 3 Âˇ 2 = a + 1 Pfennige. Da jeder solcher MĂźnzenhaufen von mindestens 4 Pfennigen entweder
ein FĂźnfpfennigstĂźck oder zwei ZweipfennigstĂźcke enthalten muss, ist stets eine der beiden Regeln
anwendbar.
Da sich 4 Pfennige mit zwei ZweipfennigstĂźcken bezahlen lassen, kĂśnnen wir jetzt mit Hilfe der
beiden Umtauschregeln fĂźr jeden grĂśĂeren Betrag eine StĂźckelung in Zwei- und FĂźnfpfennigstĂźcken
angeben.
In der Beweisargumentation haben wir Ăźbrigens nicht gezeigt, wie man einen beliebigen Betrag
stĂźckeln kann. Wir haben âlediglichâ gezeigt, dass man 4 Pfennige stĂźckeln kann, und wie man aus
der StĂźckelung eines Betrages von a Pfennigen â fĂźr ein beliebiges a > 4 â die StĂźckelung von a + 1
Pfennigen ableiten kann. Da jede natĂźrliche Zahle a > 4 durch wiederholte Addition von 1 erhalten werden kann, erhalten wir fĂźr jedes a auch eine StĂźckelung. Diese Vorgehensweise nennt man
(mathematische) Induktion. 5
4FĂźr den Beweis der Existenzaussagen âx â M : P(x) gibt es das sogenannte Taubenschlagprinzip, dass wir spĂ¤ter
kennenlernen werden. Dieses Prinzip hat auch eine weitgehende (und in der Mathematik wie auch in der Informatik hĂ¤ufig
anwendbare) Erweiterung â die sogenannte probabilistische Methode. Aus ZeitgrĂźnden kĂśnnen wir diese Methode nicht
ausfĂźhrlich betrachten und werden diese Methode spĂ¤ter nur auf einigen wenigen Beispielen veranschaulichen.
5In deutschsprachiger Literatur wird of die Name âvollstĂ¤ndige Induktionâ benutzt. Ich sehe aber keinen Grund, warum
man hier noch das Wort âvollstĂ¤ndigeâ benutzen sollte â es ist doch keine Induktion bekannt, die ânicht vollstĂ¤ndigâ ist!
Das Wort âmathematischeâ kann man notfalls benutzen, wenn man unbedingt den Unterschied zur Induktion in der Elektrotechnik unterstreichen will. Wir werden dies aber nicht tun.
ÂŠ 2003 by S. Jukna

1.4. MATHEMATISCHE INDUKTION: BEWEIS VON AUSSAGEN âX P(X )

27

Das Induktionsprinzip
Die Grundidee der Induktion6 beruht auf dem axiomatischen Aufbau der natĂźrlichen Zahlen nach
Peano: Man kann jede natĂźrliche Zahl dadurch erhalten, indem man, beginnend mit der 0, wiederholt
1 addiert. Entsprechend beweist man eine Eigenschaft P(n) fĂźr jede natĂźrliche Zahl n, indem man
zuerst die Eigenschaft P(0) â die so genannte Induktionsbasis oder Verankerung â beweist, und anschlieĂend zeigt, dass fĂźr beliebige natĂźrliche Zahlen a aus P(a) auch P(a +1) folgt â der so genannte
Induktionsschritt.
Definition: (Das Induktionsprinzip)
Sei P(n) ein PrĂ¤dikat Ăźber dem Universum N.
1. Basis: Zeige, dass P(0) wahr ist.
2. Induktionsschritt n 7â n + 1: FĂźr beliebiges n â N zeige, dass
P(n) â P(n + 1) wahr ist.
Dann gilt auch die Aussage ân â N : P(n).
Beweis. Durch Kontraposition. Nehmen wir an, dass die Aussage ân â N : P(n) gilt nicht. D.h. es
gibt ein n0 â N mit ÂŹP(n0 ). Da aber P(n0 â 1) â P(n0 ), impliziert das ÂŹP(n0 â 1); hier haben wir
die Widerspruchsregel
ÂŹP(n0 ), P(n0 â 1) â P(n0 )
ÂŹP(n0 â 1)
benutzt. Damit bekommen wir nach n0 Schritten, dass ÂŹP(0) wahr sein muss, ein Widerspruch.

Man muss nicht unbedingt von Null starten! Will man eine Aussage von der Form ân > n0 : P(n)
beweisen, so ist P(n0 ) das Induktionsbasis.
Nicht immer ist es im Induktionsschritt einfach, alleine von P(n) auf P(n + 1) zu schlieĂen. Betrachtet man den Induktionsschritt genauer, so sieht man, dass man eigentlich sogar die GĂźltigkeit von
P(0) â§ Âˇ Âˇ Âˇ â§ P(n) als Voraussetzung nutzen kann. Damit ergibt sich das Prinzip der verallgemeinerten
Induktion:
Gelten die beiden Aussagen P(0) und ân â N : (P(0) â§ Âˇ Âˇ Âˇ â§ P(n)) â P(n + 1), dann gilt die
Aussage ân â N : P(n).

Einige falsche Anwendungen
Zuerst geben wir ein Paar falsche(!) Anwendungen.7 Die erste (und nicht so ganz ernst gemeinte)
Behauptung ist:
In einen Koffer passen unendlich viele Paare von Socken.
âBeweisâ mit Induktion: Induktionsbasis: n = 1. Ein Paar Socken passt in einen leeren Koffer.
Induktionsschritt n 7â n + 1: In einem Koffer sind n Paar Socken. Ein Paar Socken passt immer noch
rein, dies ist eine allgemeingĂźltige Erfahrung. Also sind nun n + 1 Paar Socken in dem Koffer.

6Wer hat die Induktion erfunden? Das ist nicht klar. Klar ist nur, dass Francesco Maurolico die Induktion in seinem Buch
Arithmeticorum Libri Due (1575) benutzt hat (um zu zeigen, dass die Summe der ersten n ungeraden Zahlen gleich n2 ist;
siehe Aufgabe 21).
7Am besten lernt man aus den Fehlern ...
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

28

Wo ist der Fehler? Die Induktion ist ein konstruktives Beweisverfahren und solche Beweise erfordern
auch konstruktive Argumente. Im Sockenbeispiel war das Argument âdie Erfahrung sagt, dass immer
noch ein Paar Socken mehr in den Koffer paĂtâ nicht konstruktiv. Ein konstruktives Argument sollte
genau sagen wo die LĂźcke fĂźr das weitere Paar Socken sein wird!
Noch eine falsche Behauptung:
Alle natĂźrliche Zahlen sind gleich.
âBeweisâ mit Induktion. Es reicht zu zeigen, dass a = b fĂźr alle a, b â N gilt. Wir âbeweisenâ das mit
Induktion Ăźber n = max{a, b}. Dazu betrachten wir die Aussage
P(n) = âfĂźr alle a, b, â N mit max{a, b} = n gilt a = bâ
Basis n = 0 ist richtig, denn aus a, b â N und max{a, b} = 0 folgt a = 0 und b = 0.

Induktionsschritt: n 7â n + 1. Nehmen wir an, dass P(n) gilt und betrachten ein beliebiges Paar von
Zahlen a, b â N mit max{a, b} = n+1. Dann ist max{aâ1, bâ1} = n und, nach der Induktionsannahme,
gilt a â 1 = b â 1 und damit auch a = b. Fertig.




Wo ist der Fehler? Aus max{a, b} = n + 1 folgt zwar immer, dass dann auch max{a â 1, b â 1} =
n gelten muss. Aber die Induktionsvoraussetzung wird damit nicht unbedingt erfĂźlt: Ist z.B.
a = 0, dann gehĂśhrt a â 1 = â1 nicht mehr zum N, und die Aussage P(n) spricht nur Ăźber natĂźrlichen
Zahlen!



Man vergisst oft, die Induktionsbasis zu verifizieren. Z.B. sei P(n) die Aussage âân â N : n =
n + 1â. Man wĂ¤hlt ein beliebiges n â N und zeigt, dass P(n) â P(n + 1), was richtig ist, da
n = n + 1 â (n + 1) = (n + 1) + 1. Aber P(0) ist falsch, da 0 = 1 nicht gilt.

Noch eine falsche Behauptung:
Wenn sich unter n Tieren ein Elefant befindet, dann sind alle diese Tiere Elefanten.
âBeweisâ mit Induktion. Induktionsanfang: n = 1 : Wenn von einem Tier eines ein Elefant ist, dann
sind alle diese Tiere Elefanten.
Induktionsvorausetzung: Die Behauptung sei richtig fĂźr alle natĂźrlichen Zahlen kleiner oder gleich n.
InduktionsschluĂ: Sei unter n + 1 Tieren eines ein Elefant. Wir stellen die Tiere so in eine Reihe, dass
sich dieser Elefant unter den ersten n Tieren befindet. Nach Induktionsannahme sind dann alle diese
ersten n Tiere Elefanten. Damit befindet sich aber auch unter den letzten n Tieren ein Elefant, womit
diese auch alle Elefanten sein mĂźssen. Also sind alle n + 1 Tiere Elefanten.

Wo ist das Argument falsch? Im Fall n + 1 = 2 kann man den Elefanten zwar so stellen, dass er bei
den ersten n = 1 Tieren steht. Folglich sind alle Tiere unter den ersten n = 1 Tieren Elefanten. Aber
deshalb befinden sich unter den âletztenâ n Tieren nicht notwendig Elefanten.

Einige richtige Anwendungen
Nun (endlich) folgen einige Beispiele fĂźr SĂ¤tze, die man gut (und richtig!) mittels Induktion beweisen
kann.8
8Im Laufe der Vorlesung werden wir auch andere Induktionsbeweise sehen.
ÂŠ 2003 by S. Jukna

1.4. MATHEMATISCHE INDUKTION: BEWEIS VON AUSSAGEN âX P(X )

29

Satz 1.21. (Bernoulli-Ungleichung) Ist x â R und x > â1, so gilt fĂźr alle n â N+
(1 + x) n > 1 + nx
Beweis. Wir fĂźhren den Beweis mittels Induktion Ăźber n.
Basis: FĂźr n = 1 gilt 1 + x > 1 + x.
Induktionsschritt n â n + 1:
(1 + x) n+1

=

(1 + x) n Âˇ (1 + x) (Bemerkung: 1 + x > 0)

> (1 + nx) Âˇ (1 + x) (nach Induktionsvoraussetzung)
= 1 + nx + x + nx 2

= 1 + (n + 1)x + nx 2
> 1 + (n + 1)x.

Wir zeigen nun mittels verallgemeinerter Induktion, dass jede natĂźrliche Zahl, die grĂśĂer oder gleich
2 ist, als Produkt von Primzahlen dargestellt werden kann. 9 Es gilt zum Beispiel
1815 = 3 Âˇ 5 Âˇ 11 Âˇ 11.
Aus dieser Faktorisierung von 1815 lĂ¤sst sich nicht unmittelbar auf die Primzahlzerlegung von 1815 +
1 = 1816 schlieĂen, wie es bei der bisher betrachteten Induktion notwendig gewesen wĂ¤re.
Satz 1.22. (Primzahldarstellung) Sei n eine natĂźrliche Zahl, und n > 2. Dann ist n Produkt von
Primzahlen.
Beweis. Wir fĂźhren den Beweis mittels verallgemeinerter Induktion.
Basis: Da 2 eine Primzahl ist, ist 2 triviales Produkt von sich selbst, also Produkt einer Primzahl.
Schritt n â n + 1: Sei n beliebig und nehmen wir an, dass alle Zahlen von 2 bis n sich als Produkte
von Primzahlen schreiben lassen. Wir zeigen, dass dann n + 1 ebenfalls ein Produkt von Primzahlen
ist. Dazu machen wir eine Fallunterscheidung.
Fall 1: n + 1 ist eine Primzahl. Dann ist n + 1 die einzige Primzahl, aus der das Produkt n + 1 besteht.
Fall 2: n + 1 ist keine Primzahl. Dann gibt es zwei echte Teiler von n + 1, also natĂźrliche Zahlen a
und b mit 2 6 a, b < n + 1, so dass n + 1 = ab . Da a und b beide kleiner als n + 1 sind, kĂśnnen wir
die Induktionsvoraussetzung nutzen und die Zahlen a und b als Produkte von Primzahlen schreiben.
Dann ist n + 1 = ab auch ein Produkt von Primzahlen.

Nun benutzen wir die Induktion, um noch eine nĂźtzliche Ungleichung zu beweisen.
Eine reellwertige Funktion f (x) heiĂt konvex, falls fĂźr alle reelle Zahlen Îť zwischen 0 und 1 gilt:
f (Îť x + (1 â Îť)y) 6 Îť f (x) + (1 â Îť) f (y).
9Zur Erinerrung: p â N is eine Primzahl genau dann, wenn p > 2 und p ist nur durch 1 und p teilbar. Achtung: 1 ist also
keine Primzahl!
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

30

Geometrisch gesehen, ist f konvex, falls die Gerade â, die die Punkte (x, f (x)) und (y, f (y)) verbindet, oberhalb der Kurve f (z) liegt. Ein einfaches KonvexitĂ¤tstkriterium ist f â˛â˛ (x) > 0 (siehe
Abschnitt 3.8).

l(z)

y

x

Abbildung 1.7: Eine konvexe Funktion

Satz 1.23. (Jensenâs Ungleichung) Seien 0 6 Îť i 6 1 mit
f

X
r
i=1

Îťi xi



6

r
X

Pr

i=1

Îť i = 1. Ist f konvex, so gilt:

Îť i f (x i ).

i=1

Beweis. Induktion Ăźber r. FĂźr r = 1 ist die Ungleichung offensichtlich richtig, da dann Îť 1 = 1 gelten
muss. FĂźr r = 2 ist die Ungleichung auch richtig wegen der KonvexitĂ¤t. Nehmen wir also an, dass die
Ungleichung fĂźr r Summanden richtig ist, und zeigen, dass sie dann auch fĂźr r + 1 Summanden richtig
bleibt. Dazu reicht es, die Summe der ersten zwei Terme in Îť 1 x 1 + Îť 2 x 2 + . . . + Îť r +1 x r +1 durch den
Term Âľy, wobei
Îť1
Îť2
Âľ = Îť 1 + Îť 2 und y =
x1 +
x2 .
Îť1 + Îť2
Îť1 + Îť2
Pr
Pr
Da 0 6 Âľ 6 1 und Âľ + i=3 = i=1 Îť i = 1, kĂśnnen wir die Induktionsannahme anwenden, woraus
X



r +1
r +1
r +1
X
X
f
Îť i x i = f Âľy +
Îť i x i 6 Âľ f (y) +
Îť i f (x i )
i=1

i=3

i=3

folgt. Nun benutzen wir wieder die KovexitĂ¤t von f und erhalten


Îť1
Îť2
Âľ f (y) = (Îť 1 + Îť 2 ) f
x1 +
x2
Îť1 + Îť2
Îť1 + Îť2


Îť1
Îť2
6 (Îť 1 + Îť 2 )
f (x 1 ) +
f (x 2 )
Îť1 + Îť2
Îť1 + Îť2
= Îť 1 f (x 1 ) + Îť 2 f (x 2 ).


Obwohl einfach, ist Jensenâs Ungleichung10 in vielen FĂ¤llen sejr nĂźtzlich. Sie leifert uns sofort, zum
Beispiel, die folgende Ungleichung zwischen arithmetischem und geometrischem Mittel.
10Genau wie sogenannte Cauchy-Schwarz Ungleichung
X
2  X
 X

n
n
n
x i yi
6
x 2i
yi2 ,
i=1

i=1

i=1

ÂŠ 2003 by S. Jukna

1.4. MATHEMATISCHE INDUKTION: BEWEIS VON AUSSAGEN âX P(X )

31

Satz 1.24. (Geometrisches und arithmetisches Mittel) Seien a1 , . . . , an nicht-negative Zahlen.
Dann gilt
â
a1 + a2 + . . . + a n
n
a1 Âˇ a2 Âˇ . . . Âˇ a n 6
.
(1.2)
n
Beweis. Sei f (x) = e x , Îť 1 = . . . = Îť n = 1/n und x i = ln ai , fĂźr alle i = 1, . . . , n. Nach der Jensenâs
Ungleichung gilt:
!
!1/n
!1/n
n
n
n
n
n
Pn
X
X
Y
Y
1X
x
/n
ln
a
)
(
ai =
Îť i f (x i ) > f
Îť i x i = e i=1 i
=
e i
=
ai
.
n
i=1

i=1

i=1

i=1

i=1


Ist Induktion nur etwas fĂźr Zahlen? Nein, in P(n) ist n nur ein Parameter, die Aussage âP(n) ist
wahrâ muss nicht unbedingt eine Aussage Ăźber die Zahl n sein. Z.B. kann man auch solche Aussagen
P(n) nehmen: âJedes einfache Polygon mit n Ecken lĂ¤Ăt sich durch n â 3 Diagonalen triangulieren.â
Das allgemeine Schema ist wie folgt. Man hat eine Menge S und ein PrĂ¤dikat Q(s) Ăźber S. Man will
zeigen, dass âs â S : Q(s) gilt. DafĂźr wĂ¤hlt man eine passende Abbildung
S â s 7â L(s) â N,
die zu jedem Element s â S seine âLĂ¤ngeâ L(s) zuweist.11 Dann betrachtet man das PrĂ¤dikat P(n) =
âfĂźr alle s â S mit L(s) = n gilt Q(s)â und zeigt mittels Induktion, dass ân â N P(n) gilt.
Um diese Verallgemeinerung zu demonstrieren, betrachten wir die Anzahl der Elemente in dem kartesischen Produkt zweier Mengen.
Behauptung 1.25. FĂźr endliche Mengen A, B gilt: | A Ă B| = | A| Âˇ |B|.
Beweis. Durch Induktion nach n := | A| (dabei sei B beliebig aber fest).

Induktionsanfang: fĂźr n = 0 ist A = â, also auch A Ă B = â.

Induktionsschritt: n 7â n + 1. Sei A eine beliebige Menge mit | A| = n + 1 Elementen. Wegen n + 1 > 1
existiert ein Element a â A. Wir betrachten die Menge X := A \ {a}. Dann hat X nur n Elemente,
erfĂźhlt also |X Ă B| = n Âˇ |B| nach Induktionsvoraussetzung. Wegen
A Ă B = ({a} Ă B) âŞ (X Ă B) und ({a} Ă B) âŠ (X Ă B) = â
folgt
| A Ă B| = |{a} Ă B| + |X Ă B| = |B| + n Âˇ |B| = (n + 1) Âˇ |B| = | A| Âˇ |B|.

Ein Geradenarrangement besteht aus endlich vielen Geraden in der Ebene. Dabei habe eine Gerade
kein Ende und keinen Anfang. Die Geraden unterteilen die Ebene in verschiedene FlĂ¤chen.
die wir erst spĂ¤ter beweisen werden (siehe Satz 5.6).
11Dieser Schritt, den man als Parametrisierungsschritt bezeichnen kann, ist wichtig. Hat ein Objekt s zwei (oder mehrere)
Merkmale a, b â N, so kann man L(s) = a + b aber auch L(s) = a Âˇ b oder auch L(s) = max{a, b} usw. wĂ¤hlen. Nicht
jede Auswahl wird jedoch zum Erfolg fĂźhren. Deshalb muss man an dieser Stelle genauer Ăźberlegen, mit welcher Funktion
L(s) = f (a, b) es am einfachsten wird, den Induktionsschritt durchzufĂźhren.
ÂŠ 2003 by S. Jukna

32

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

Satz 1.26. (FĂ¤rbungen von FlĂ¤chen) Bei beliebiger Anzahl der Geraden ist es mĂśglich die entstehenden FlĂ¤chen mit nur zwei Farben so zu fĂ¤rben, dass keine zwei benachbarte FlĂ¤chen dieselbe Farbe
tragen werden.
Beweis. Induktion Ăźber die Anzahl der Geraden n. Basis n = 1 ist richtig, da wir dann nur zwei
FlĂ¤chen haben kĂśnnen.
Induktionsschritt: n â 1 7â n. Sei die Aussage nun fĂźr n â 1 Geraden richtig. Wir wollen zeigen, dass
dann die Aussage auch fĂźr n Geraden richtig ist. Dazu nehmen wir ein beliebiges Geradenarrangement
mit n Geraden und entfernen eine (auch beliebige) Gerade g. Nach der Induktionsannahme, muss eine
ZweifĂ¤rbung der FlĂ¤chen in dem verbleibenden Arrangement von n â 1 Geraden mĂśglich sein. Nun
nehmen wir die Gerade g wieder dazu. Nach der Hinzunahme von g werden manche der alten FlĂ¤chen
in zwei neue FlĂ¤chen geteilt. Nun kommt der Trick: Wir behalten die Farben fĂźr (neue) FlĂ¤chen, die
auf einer Seite von g liegen, und kippen die Farben der verbleibenden FlĂ¤chen um.

In vielen Anwendungen ist ein Knoten des Baumes B als Startknoten ausgezeichnet; dann spricht
man Ăźber einen Wurzelbaum. In einem solchen Baum kann das VerhĂ¤ltnis der einzelnen Knoten des
Baumes zueinander begrifflich gut beschrieben werden. Dazu benutzt man den Begriff der Tiefe eines
Knotens.
Als Tiefe eines Knotens v von B wird der Abstand von v zur Wurzel (d.h. die Anzahl der Kanten
in dem einzigen Weg von v zur Wurzel) bezeichnet. Die Tiefe von B ist die maximale Tiefe eines
Knotens von T. Alle Knoten gleicher Tiefe bilden ein Knotenniveau. Als Kinder eines Knotens v von
B werden sĂ¤mtliche Knoten bezeichnet, die zu v benachbart sind und deren Tiefe die von v um eins
Ăźbersteigt. v heiĂt Vater seiner Kinder. In einem binĂ¤ren Baum hat jeder innere Knoten hĂśchstens
zwei Kinder. Knoten ohne Kinder heiĂen BlĂ¤tter (siehe Abb. 1.8).
r

v

a
b
u

w

Abbildung 1.8: Dieser Wurzelbaum mit der Wurzel r hat die Tiefe 3. u und w sind Kinder des Knotens
v, und v besitzt die Tiefe 1. a, b und v bilden ein Knotenniveau.
FĂźr ein Baum B sei t(B) seine Tiefe und |B| die Anzahl der BlĂ¤tter. Weiss man |B|, was kann man
dann Ăźber die Tiefe t(B) sagen? Mann kann aber zeigen, dass log2 |B| bereits die untere Grenze fĂźr
die Tiefe ist: Kein binĂ¤rer Baum B kann kleinere Tiefe als log2 |B| haben.
Satz 1.27. FĂźr jeden binĂ¤ren Baum B gilt: t(B) > log2 |B|, d.h.
Tiefe(B) > log2 (Anzahl der BlĂ¤tter).
Beweis. Wir fĂźhren den Beweis mittels Induktion Ăźber die Tiefe t = t(B).
ÂŠ 2003 by S. Jukna

1.4. MATHEMATISCHE INDUKTION: BEWEIS VON AUSSAGEN âX P(X )

33

r
r

Abbildung 1.9: Diese zwei Beipiele zeigen, dass es sowohl BĂ¤ume mit t(B) = |B| â1 wie auch BĂ¤ume
mit t(B) = log2 |B| geben kann.

Basis t = 0: In diesem Fall besteht B nur aus einem Knoten. Dieser Knoten ist ein Blatt, es gilt also
|B| = 1. Da 1 6 20 bzw. log2 1 6 0 ist die Behauptung fĂźr d = 0 wahr.

Induktionsschritt t â 1 7â t: Sei die Behauptung bereits fĂźr alle binĂ¤ren BĂ¤ume der Tiefe 6 t â 1
bewiesen und sei B ein beliebiger binĂ¤rer Baum der Tiefe t = t(B). Wir wollen zeigen, dass |B| 6
2t (B) gilt (was Ă¤quivalent zu t(B) > log2 |B| ist).
Sei r die Wurzel von B und seien (r, u) und (r, v) die beiden mit r inzidenten Kanten. Wir betrachten
die beiden in u und v wurzelnden UnterbĂ¤ume Bu und Bv von B. Beide diese TeilbĂ¤ume haben Tiefe
hĂśchstens t â 1 und fĂźr die Anzahl der BlĂ¤tter gilt: |Bu | + |Bv | = |B|. Nach Induktionsannahme gilt
also
|Bu | 6 2t (Bu ) und |Bv | 6 2t (B v ) .
Wir erhalten damit
|B| = |Bu | + |Bv | 6 2t (Bu ) + 2t (B v ) 6 2 Âˇ 2t â1 6 2t .


1.4.1 Induktion und Entwurf von Algorithmen
Anwendungen der Induktion findet man in allen mathematischen Gebieten, von Mengenlehre bis
Geometrie, von Differentialrechnung bis Zahlentheorie. Sogar der Beweis des groĂen Fermatschen
Satzes12 (Andrew Wiles, 1993) verwendet die Induktion (neben vielen anderen Argumenten). Die Induktion ist auch in der Informatik wichtig, denn rekursive Algorithmen sind induktive Beschreibungen
von Objekten.
Viele Probleme, Modelle oder PhĂ¤nomen haben eine sich selbst referenzierende Form, bei der die
eigene Struktur immer wieder in unterschiedlichen Varianten enthalten ist. Wenn diese Strukturen
in eine mathematische Definition, einen Algorithmus oder eine Datenstruktur Ăźbernommen werden,
wird von Rekursion gesprochen. Rekursive Definitionen sind jedoch nur sinnvoll, wenn etwas immer
durch einfachere Versionen seiner selbst definiert wird, wobei im Grenzfall ein Trivialfall gegeben
ist, der keine Rekursion benĂśtigt. Rekursion hat also was mit der Induktion zu tun â man kann die
Rekursion als Induktion âin umgekehrter Richtungâ betrachten.
Deshalb ist Induktion nicht nur fĂźr den Beweis, dass ein gegebener Algorithmus korrekt ist, geeignet â man kann sie auch fĂźr den Entwurf der Algorithmen benutzen! Das allgemeine Schema â als
dynamisches Programmieren bekannt â ist folgendes:
12Dieser Satzt besagt, dass es keine natĂźrlichen Zahlen x, y, z gibt, so dass fĂźr n grĂśĂer als 2 die folgende Relation gilt:
x n + yn = zn .
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

34

Um ein Problem zu lĂśsen, lĂśst man zuerst Teilprobleme und die LĂśsungen der Teilprobleme zu
einer LĂśsung des usprĂźnglichen Problems kombiniert.
Rekursive Programme sind ein Spezialfall dynamischen Programme: Die LĂśsungen der Teilprobleme
mĂźssen nicht abgeschpeichert werden.
Das folgende Problem wurde 1883 von Edouard Lucas erfunden.
â˘ Gegeben sind drei PlĂ¤tze zum Stapeln und n Scheiben, die zu Beginn alle auf dem 1. Stapel liegen.
â˘ Alle Scheiben sind unterschiedlich groĂ und sie mĂźssen auf einem Stapel in geordneter Weise
liegen, so daĂ nicht grĂśĂere Scheiben auf kleineren liegen.
â˘ Jede Scheibe ist beweglich und kann von einem Stapel auf einen anderen getragen werden. Es
dĂźrfen jedoch nicht mehrere Scheiben auf einmal bewegt werden.
â˘ Aufgabenstellung: Alle Scheiben sind von dem 1. Stapel auf den 2. Stapel zu befĂśrdern.

3

2

1

Man kann dieses Problem mittels Induktion lĂśsen. Sei P(n) die Aussage: âWir haben einen Algorithmus, der das Problem fĂźr n Scheiben lĂśsstâ.
Induktionsbasis: P(0) ist wahr, da wir keine Scheiben Ăźberhaupt haben.
Induktionsschritt: Wir wissen wie man das Problem fĂźr n â 1 Scheiben lĂśsen kann und wollen einen
Algorithmus fĂźr n Scheiben entwerfen. Dazu beobachten wir, dass das Problem, n Scheiben vom
Stapel 1 zum Stapel 2 zu befĂśrdern, lĂ¤sst sich lĂśsen, wenn
1. zunĂ¤chst die obersten n â 1 Scheiben von Stapel 1 zum als Hilfsstapel genutzten Stapel 3 verlegt
werden,

1

2

3

2. dann die jetzt zuoberst liegende Scheibe auf Stapel 1 nach Stapel 2 verlegt wird

1

2

3

und abschlieĂend
3. wieder n â 1 Scheiben vom Hilfsstapel 3 nach Stapel 2 verlagert werden.
ÂŠ 2003 by S. Jukna

1.5. DAS TAUBENSCHLAGPRINZIP: BEWEIS VON AUSSAGEN âX P(X )

1

2

35

3

Zu beachten ist dabei, dass die Rollen der drei Stapel (Ausgangs-, Ziel- und Hilfsstapel) stĂ¤ndig
wechseln.
Damit kĂśnnen wir ein rekursives Programm Hanoi(n; 1, 2, 3), dass die gegebenen n Scheiben von 1.
Stapel auf 3. Stapel befĂśrdert, wie folgt beschreiben:

Algorithmus Hanoi(n; 1, 2, 3)
While n > 0 do
Rufe Hanoi(n â 1; 1, 3, 2) auf [die obersten n â 1 Scheiben von Stapel 1 zum Stapel 2]
Verlege die zuoberst liegende Scheibe auf 1 nach 3
Rufe Hanoi(n â 1; 2, 1, 3) auf [verlege n â 1 Scheiben vom Hilfsstapel 2 nach 3]

1.5 Das Taubenschlagprinzip: Beweis von Aussagen âx P(x)
Das sogenannte Taubenschlagprinzip, in der englischsprachiger Literatur auch als Pigeonhol Principle
bezeichnet, geht auf den Mathematiker G. L. Dirichlet zurĂźck. Dieses Prinzip erlaubt Existenzaussagen âx P(x) Ăźber endliche Universen M zu beweisen, ohne ein konkretes Element a â M, fĂźr das
P(a) gilt, anzugeben! Das Prinzip selbst basiert sich auf folgender einfacher Beobachtung:
Halten sich r + 1 Tauben in r TaubenlĂścher auf, so gibt es mindestens einen TaubenlĂśch, in dem
sich wenigstens zwei Tauben befinden.
Ist das VerhĂ¤ltnis von TaubenlĂścher zu Tauben nicht nur k + 1 zu k sondern zum Beispiel 2k + 1 zu
k, so kann man sogar schlieĂen, dass in einem der TaubenlĂścher mindestens 3 Tauben sitzen mĂźssen.
Taubenschlagprinzip:
Halten sich sr + 1 Tauben in r TaubenlĂścher auf, so gibt es mindestens
einen Taubenloch, in dem sich wenigstens s + 1 Taube befinden.
WĂ¤re das nĂ¤hmlich nicht der Fall, so hĂ¤tten wir hĂśchstens sr Tauben insgesamt.
â˛ Beispiel 1.28 : In einer Gruppe von 8 Leuten haben (mindestens) zwei am gleichen Wochentag
Geburtstag. Warum? Seien die Leute âTaubenâ und die Wochentage âTaubenlĂścherâ. Wir haben
also r = 7 TaubenlĂścher und r + 1 Taube. Das Taubenschlagprinzip (mit s = 1) garantiert in
dieser Situation die Existenz eines Wochentages, an dem also mindestens s + 1 = 2 Leute der
Gruppe Geburtstag haben.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

36

â˛ Beispiel 1.29 : Wir sagen, dass zwei Leute befeindet sind, falls sie nicht befreundet sind. Behauptung:
(â)

In jeder Gruppe von 6 Leuten gibt es 3 Leute, die paarweise befreundet oder paarweise
verfeindet sind. 13

Dieses Problem lĂ¤sst sich auch als Graphenproblem stellen. Die Gruppe von Leuten steht fĂźr eine
Menge von Knoten. Sind zwei Leute befreundet, so wird eine Kante zwischen den beiden entsprechenden Knoten erstellt, d.h. die beiden Knoten sind benachbart; sind zwei Leute verfeindet,
so liegt keine Kante zwischen den entsprechenden Knoten. Damit ist die Kantenmenge bestimmt.
Die Behauptung liest sich dann folgendermaĂen: jeder Graph mit 6 Knoten enthĂ¤lt 3 Knoten, die
entweder paarweise benachbart oder die paarweise nicht benachbart sind. Den Beweis kann man
leicht aus der Abbildung 1.10 ablesen.

e

Abbildung 1.10: Ist das Paar e befreundet oder befeinded?

Frage: Gilt die Behauptung (â) mit 5 statt 6 Leuten in der Gruppe?
â˛ Beispiel 1.30 : Ein Sandkasten hat die Form eines gleichseitigen Dreiecks mit Seiten der LĂ¤nge 2
Meter (Abb. 1.11(A)).
1

1
2

2

1

1
2
A

1

1
B

Abbildung 1.11: Die Sandkasten und ihre Aufteilung.

In diesem Sandkasten wollen 5 Kinder spielen. Das Problem ist nur, dass die Kinder um mehr als
einem Meter von einander entfernt sein sollen, da sonst werden sie krĂ¤ftig streiten. Ist es mĂśglich,
die 5 Kinder in den Sandkasten so zu verteilen, dass endlich Ruhe herrschen wird?
Antwort ist nein. Dazu teile die Sandkasten in 4 Teilen auf, wie in Abb. 1.11(B) gezeigt ist
(das sind unsere âTaubenlĂścherâ). Da wir mehr als 4 Kinder haben, werden mindestens 2 Kinder
in einem Taubenloch (=kleinem Dreieck) sitzen, und die Abstand zwischen den Kindern wird
hĂśchstens 1 Meter sein.
13Das ist die einfachste Form (âKindergartenformâ) des in 1930 bewiesenen berĂźhmten Satzes von Frank Plumpton
Ramsey, der zur Geburt der Ramseytheorie â einem Gebiet der diskreter Mathematik â gefĂźhrt hat.
ÂŠ 2003 by S. Jukna

1.5. DAS TAUBENSCHLAGPRINZIP: BEWEIS VON AUSSAGEN âX P(X )

37

â˛ Beispiel 1.31 : Behauptung: In jeder Menge S â Z von |S| = 1000 ganzen Zahlen gibt es zwei
Zahlen x , y, so dass x â y durch 573 teilbar ist.
Auf erstem Blick sieht das Problem sehr schwer aus â diese 1000 Zahlen kĂśnnen doch beliebig
sein! Auch unsere alte Freundinnen â die Induktion â kann hier nur wenig helfen. Andererseits,
kĂśnnen wir die Behauptung mit dem Taubenschlagsprinzip in ein paar Zeilen beweisen.
Beweis: Als Tauben nehmen wir die Elemente von S und als TaubenlĂścher die Elementen von
R = {0, 1, . . . , 572}. Wir setzen die Taube x â S in das Taubenloch r â R genau dann, wenn x
geteilt durch 573 den Rest r ergibt. Da |S| > |R|, mĂźssen mindestens zwei Tauben x und y in
einem Taubenloch r aufhalten. Das bedeutet, dass x und y geteilt durch 573 den selben Rest r
ergeben, und damit muss x â y durch 573 teilbar sein.
Beachte, dass die Wahl der Zahlen (1000 und 573) hier unwichtig ist â wichtig ist nur, dass
|S| > |R| gilt. Ist n > m, so muss jede Menge aus n Zahlen zwei Zahlen x und y enthalten, deren
Differenz x â y durch m teilbar ist.
Auf dem ersten Blick scheint das Taubenschlagprinzip nur einfache Schlussfolgerungen zuzulassen
kann. Wie die folgenden Anwendungen zeigen, trĂźgt der Schein! Man kann nĂ¤hmlich mit diesem
Prinzip einige klassische Resultate beweisen. Hier beschrĂ¤nken wir uns mit einigen Beispielen.
In folgenden Satz geht es um die Existenz rationaler Approximationen von irrationalen Zahlen. Der
Beweis war die erste nicht triviale Anwendung des Taubenschlagsprinzips in der Mathematik Ăźberhaubt! Diese Anwendung hat Dirichlet gemacht, deshalb nennt man das Prinzip oft Dirichletâs Prinzip.
Dirichlet 1879:
Sei x eine reelle Zahl und nicht rational. FĂźr jede natĂźrliche Zahl n existiert eine rationale Zahl p/q mit 1 6 q 6 n und
xâ

1
1
p
<
6 2.
q
nq
q

Beweis. Ist x eine reelle Zahl, so definieren wir L(x) als die Abstand zwischen x und der ersten
ganzen Zahl, die links von x steht: L(x) := x â âxâ. Es ist klar, dass dieser Abstand zwischen 0 und 1
liegen muss.
Sei x â R \ Q eine reelle aber irrationale Zahl. Als âTaubenâ nehmen wir n + 1 Zahlen L(ax) fĂźr
a = 1, 2, . . . , n + 1 und sortieren sie in folgende n Intervalle ein, die die âTaubenlĂścherâ darstellen:

 



1
1 2
nâ1
0,
,
,
, ... ,
,1 .
n
n n
n
Die RĂ¤nder der Intervalle werden nicht angenommen, da x reell und nicht rational ist. Da wir mehr
Tauben als TaubenlĂścher haben, mussen nach dem Taubenschlagsprinzip mindestens zwei Zahlen â
dies seien L(ax) und L(bx) â in einem Interval liegen. Da jedes der Intervale kĂźrze als 1/n ist, muss
der Abstand zwischen diesen zwei Zahlen kleiner als 1/n sein:
q

p

z
}|
{
z }| {
1
|L(ax) â L(bx)| = |(ax â âaxâ) â (bx â âbxâ)| = | (a â b) x â (âaxâ â âbxâ) | < .
n
Damit haben wir zwei ganze Zahlen p und q gefunden, fĂźr die die Ungleichung |qx â p| < 1/n und
damit auch die Ungleichung |x â p/q| 6 n1q gilt. Da q die Differenz zweier ganzen Zahlen ist, welche
sich im Bereich 1, 2, ... , n + 1 bewegen, gilt auch q 6 n.

ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

38

Als nĂ¤chstes betrachten wir ein (auch klassisches) Resultat Ăźber Teilfolgen einer Zahlenfolge. FĂźr eine
Folge a1 , a2 , . . . , an von Zahlen ist ai 1 , ai 2 , . . . , ai r eine Teilfolge der LĂ¤nge r, falls 1 6 i1 < i2 < . . . <
ir 6 n gilt. D.h. Eine Teilfolge der LĂ¤nge r einer Folge entsteht, wenn wir alle ausser r Folgenglieder
weg lassen. Die Teilfolge ist monoton steigend bzw. monoton fallend falls ai 1 < ai 2 < . . . < ai r bzw.
ai 1 > ai 2 > . . . > ai r gilt. Die Teilfolge ist monoton, falls sie monoton steigend oder monoton fallend
ist.
Zum Beispiel besitzt die Folge
7, 6, 11, 13, 5, 2, 4, 1, 9, 8
der LĂ¤nge n = 10 die monoton fallende Teilfolge
7, 6, 2, 1
der LĂ¤nge r = 4.
ErdoĚsa âSzekeres 1935:
Jede Folge von n verschiedenen Zahlen enthĂ¤lt eine monotone Teilfolge
â
der LĂ¤nge mindestens n.
a Paul

ErdoĚs 1913â1996. Einer der grĂśĂten Mathematikern des letzten Jahrhunderts.

Bemerkung 1.32. Wichtig hier ist, dass das Ergebnis fĂźr alle Folgen gilt. D.h. egal aus welchen
â
Zahlen eine beliebige Folge der LĂ¤nge n besteht, muss sie eine monotone Teilfolge der LĂ¤nge n
enthalten.
Beweis. Um die Notation zu vereinfachen, nehmen wir an, dass n ein Quadrat ist, d.h. n = r 2 fĂźr
eine natĂźrliche Zahl r gilt. Sei nun a1 , . . . , an eine beliebige Folge von n verschiedenen Zahlen. Wir
ordnen ai das Paar (Fi , Wi ) zu, wobei
Fi = die LĂ¤nge der lĂ¤ngsten monoton fallenden Teilfolge steht, die in ai endet,
Wi = die LĂ¤nge der lĂ¤ngsten monoton wachsenden Teilfolge, die in ai startet.
Angenommen, a1 , . . . , an besitzt weder eine monoton wachsende noch eine monoton fallende Teilfolge der LĂ¤nge r. Dann gilt Fi , Wi < r fĂźr alle i = 1, . . . , n. Die Zahlen Fi und Wi liegen also zwischen
1 und r â 1 und es kann deshalb hĂśchstens (r â 1) 2 verschiedene Paare (Fi , Wi ), i = 1, . . . , n geben.

Wir betrachten die Zahlen a1 , . . . , an als âTaubenâ und die Paare (x, y) mit x, y â {1, . . . , r â 1} als
âTaubenlĂścherâ. Wir setzen die i-te Taube in Taubenloch (x, y) genau dann, wenn Fi = x und Wi = y
gilt. Da wir n Tauben und nur (r â 1) 2 < r 2 = n TaubenlĂścher haben, mĂźssen zwei Tauben, seien es ai
und a j mit i < j, in einem Taubenloch (x, y) sitzen, d.h. es muss Fi = Fj = x und Wi = W j = y gelten.
Das ist jedoch unmĂśglich, da ai > a j ein Widerspruch zu Fi = Fj und ai < a j einen Widerspruch
zu Wi = W j liefert. In erstem Fall (ai > a j ) gilt Fj > Fi + 1 (die fallende Folge zu ai kann man um
einen Element a j erweitern) und in zweiten Fall (ai < a j ) gilt Wi > W j + 1 (die steigende Folge zu
a j kann man um einen Element ai erweitern).

Um kompliziertere Existenzaussagen von der Form
âx(P(x) â âyQ(x, y))
ÂŠ 2003 by S. Jukna

1.5. DAS TAUBENSCHLAGPRINZIP: BEWEIS VON AUSSAGEN âX P(X )

39

zu beweisen, lĂśhnt es sich oft die beide mĂ¤chtige Beweisprinzipien â Induktion und das Taubenschlagprinzip â zu kombinieren. Wir beschrĂ¤nken uns nur auf einem Beispiel. In diesem Beispiel geht es um
Eigenschaften von Graphen.
Ein Dreieck in einem ungerichteten Graphen besteht aus drei benachbarten knoten. Uns interessiert
nun die Frage: Wieviel Kanten kann ein Graph haben ohne dass er einen Dreieck enthĂ¤lt?
In Bild unten erkennt man einen vollstĂ¤ndigen bipartiten Graphen mit 2n Knoten fĂźr n = 3, also
insgesamt 6 Knoten.
Dieser Graph enthĂ¤lt keine Dreiecken. Wie man unschwer erkennen kann, wĂźrde aber die Hinzunahme einer beliebigen der verbleibenden mĂśglichen 6 Kanten jeweils 3 Dreiecke schlieĂen. Somit scheint bei einer Knotenanzahl von 2n, n2 der hĂśchste Wert zu
sein, fĂźr den sich ein dreiecksfreier Graph noch konstruieren lĂ¤sst.
Diese ganzen Ăberlegungen mĂźnden nun in den
Satz 1.33. (Mantel 1907) Wenn ein Graph G mit 2n Knoten mindestens n2 + 1 Kanten besitzt, dann
besitzt G ein Dreieck.
Beweis. Der Beweis erfolgt per Induktion nach n. Induktionsbasis n = 1: In diesem Fall ist die Behauptung wahr, denn beide Seiten der Implikation sind falsch, (ein Graph mit 2 Knoten keine 2 Kanten
besitzen kann).
Induktionsschritt n 7â n + 1: Wir nehmen also an, dass die Behauptung fĂźr n gilt und betrachten jetzt
einen beliebigen Graphen G = (V, E) mit |V | = 2(n + 1) Knoten und |E| = (n + 1) 2 + 1 Kanten. Die
beiden Knoten x und y seien adjazent in G, d.h. xy â E. Mit H wird der, durch die Wahl von x und y
definierte Teilgraph, bezeichnet: 14
y

x

z

H

Der Graph H besitzt folglich 2n Knoten. Sollte H mehr als n2 Kanten haben, gilt der Sastz aufgrund
unserer Induktionsvoraussetzung, denn dann gĂ¤be es ein Dreieck im Teilgraphen H und damit auch
in G. Also wird nun angenommen, dass der Teilgraph H hĂśchstens n2 Kanten hat. Sei F die Menge
aller Kanten, die von den Knoten x und y zu Knoten in H fĂźhren. Insgesamt haben wir15
|F | > |E| â n2 â 1 = ((n + 1) 2 + 1) â n2 â 1 = 2n + 1
solchen Kanten. Wir haben also 2n + 1 Kanten die von x bzw. y in den Teilgraphen H mit 2n Knoten
fĂźhren. Diese Kanten betrachten wir nun als âTaubenâ und Knoten von H als âTaubenlĂścherâ. Nach
dem Taubenschalgprinzip muss in H ein Knoten z existieren, der mit x und y jeweils eine Kante hat.
Folglich besitzt G das Dreieck {x, y, z}.

14Um H zu bekommen, entfernen wir also aus G die Knoten x und y mit der entsprechenden Kanten.
15Wir mĂźssen aus |E| â n2 noch 1 abziehen, da x y zu E gehĂśhrt.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

40

1.6 Kombinatorische AbzĂ¤hlargumente
Kombinatorik (wie die Diskrete Mathematik) beschĂ¤ftigt sich vor allem mit endlichen Mengen. In
klassicher Kombinatork geht es hauptsĂ¤chlich um Fragen vom Typ: âAuf wie viele Arten kann man
...â, was im Extremfall heiĂen soll âKann man ... Ăźberhaupt?â Um vernĂźnftig Ăźber solche Fragen reden
zu kĂśnnen, bilden wir die Menge der Objekte, die uns interessieren, und fragen nach ihrer MĂ¤chtigkeit.
In Kombinatorik haben sich einige spezielle Regeln herausgebildet, die alle ganz klar sind, sobald man
sie einmal gesehen hat, auf die man aber erst einmal kommen muss.
Satz 1.34. Kombinatorische AbzĂ¤hlregeln:
1. Gleichheitsregel: Wenn zwischen Mengen A und B eine bijektive Abbildung existiert, dann ist
| A| = |B|.
2. Summenregel: Ist A die Vereinigung von k paarweise disjunkten, endlichen Mengen A1 , . . . , Ak ,
dann ist | A| = | A1 | + . . . + | Ak |.
3. Produktregel: Ist A = A1 Ă A2 Ă . . . Ă Ak das kartesische Produkt endlicher Mengen A1 , . . . , Ak ,
dann gilt | A| = | A1 | Âˇ | A2 | Âˇ . . . Âˇ | Ak |.
4. Zerlegungsregel: Ist f : A â B eine Abbildung, dann gilt:
X
| A| =
| f â1 (b)|
b âB

Beweis. Zu (3): siehe Behauptung 1.25. Zu (4): Sind b1 , b2 â B und b1 , b2 , so gilt f â1 (b1 ) âŠ

f â1 (b2 ) = â.
â˛ Beispiel 1.35 : (Produktregel) Wenn man ein Objekt in k Schritten konstruiert, und man im iten Schritt die Wahl zwischen si MĂśglichkeiten hat, dann kann man das Objekt insgesamt auf
s1 Âˇ s2 Âˇ . . . Âˇ sk Arten konstruieren. Man kann sich diese Regel sehr schĂśn mit einem Baumdiagramm veranschaulichen. Das folgende Diagramm veranschaulicht die Konstruktion der sechs
Permutationen von {a, b, c}:

a

b

c

b

c

a

c

a

b

c

b

c

a

b

a

1.6.1 Prinzip der doppelten AbzĂ¤hlung
Das sogenannte Prinzip der doppelten AbzĂ¤hlung ist die folgende âoffensichtlicheâ Aussage: Wenn
wir die Elemente einer Menge in zwei verschiedenen Reihenfolgen abzĂ¤hlen (und dabei jeweils keine
Fehler machen), dann werden wir die gleichen Anworten bekommen.
ÂŠ 2003 by S. Jukna

1.6. KOMBINATORISCHE ABZĂHLARGUMENTE

41

Prinzip der doppelten AbzĂ¤hlung:
Sei A eine Tabelle mit m Zeilen, n Spalten und mit Elementen 0 und 1. Sei zi die Anzahl der Einsen
in der i-ten Zeile, und s j die Anzahl der Einsen in der j-ten Spalte. Dann gilt
m
X
i=1

zi =

n
X

s j = Gesamtzahl der Einsen in A

j=1

Anschaulich:

=

â˛ Beispiel 1.36 : In einer Vorlesung sind 32 der HĂśrer mĂ¤nnlich. Dabei ist jeder Student mit 5 Studentinnen und jede Studentin mit 8 Studenten befreundet.
Frage: Wie viele Studentinnen besuchen die Vorlesung?
Diese Frage lĂ¤sst sich mit dem Prinzip der doppelten AbzĂ¤hlung beantworten. Sei n die (bis jetzt
unbekannte) Anzahl der Studentinnen in der Vorlesung, und betrachte die âFreundschaftstabelleâ
A = (ai j ) wobei ai j = 1 falls der i-te Student mit der j-ten Studentinnen befreundet ist, und
ai j = 0 sonst. Die Tabelle hat also m = 32 Zeilen und n Spalten. Jede Zeile hat 5 EinseintrĂ¤ge
(zi = 5) und jede Spalte hat 8 EinseintrĂ¤ge (s j = 8). Also folgt durch zeilen- und spaltenweises
AbzĂ¤hlen
32
n
X
X
32 Âˇ 5 =
zi =
sj = 8 Âˇ n
i=1

j=1

Also besuchen n = 20 Studentinnen die Vorlesung.
Im folgenden Satz ist d(u) = |{e â E : u â e}| der Grad von Knoten u (=die Anzahl der zu u
inzidenten Kanten).
Satz 1.37. (Euler 1736) Sei G = (V, E) ein ungerichteter Graph. Dann gilt
X
d(u) = 2 Âˇ |E|.
u âV

Beweis. Betrachte die Tabelle M, die aus n = |V | Zeilen und m = |E| Spalten besteht und deren
EintrĂ¤ge wie folgt definiert sind:

1
falls u â e
M (u, e) =
0
sonst.
Dann hat die Zeile zu Knoten u genau d(u) Einsen und die Spalte zu Kante e genau 2 Einsen. Die
Behauptung folgt also direkt aus dem Prinzip der doppelten AbzĂ¤hlung.

ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

42

1.6.2 Binomialkoeffizienten
Sei X eine endliche Menge mit n = |X | Elementen und k 6 n.

Eine k-Teilmenge von X ist eine Menge {x 1 , x 2 , . . . , x k } aus k verschiedenen Elementen von X (Ordnung hier ist unwichtig!). Die Anzahl C (n, k) solchen Teilmengen bezeichnet man mit
 
n
k

und nennt diese Zahl binomischer Koeffizient (oder Binomialkoeffizient). Beachte, dass nk genau die
Anzahl der 0-1 Folgen der LĂ¤nge n mit k Einsen ist; eine Eins bzw. Null in der Position i sagt uns, ob
das i-te Element von X gewĂ¤hlt bzw. nicht gewĂ¤hlt wird. Also gilt:
 
n
= Anzahl der k-elementigen Teilmengen einer Menge aus n Elementen
k
= Anzahl der 0-1 Folgen der LĂ¤nge n mit genau k Einsen




Das ist die Definition von nk ! Nicht (wie man Ăźblicherweise behauptet) die Gleichung
n!
k!(nâk )! . Diese Gleichung leitet man aus der Definition ab! (Siehe Satz 1.39.)

n
k



=

Eine k-Permutation von X ist eine geordnete Folge (x 1 , x 2 , . . . , x k ) aus k verschiedenen Elementen
von X . (Ordnung ist hier wichtig!) Die Anzahl P(n, k) solcher Folgen bezeichnet man mit
(n)k
FĂźr k = n schreibt man n! statt (n)n (gesprochen: n FakultĂ¤t); man setzt auch 0! = 1 (nur eine
Vereinbarung).
Viele einfache kombinatorische Fragestellungen lassen sich auf gewisse Grundaufgaben zurĂźckzufĂźhren, nĂ¤mlich aus einer gegebenen endlichen Menge bestimmte Wahlen zu treffen. Je nachdem, ob
wir es zulassen, dass dabei ein und dasselbe Element mehrfach ausgewĂ¤hlt wird (ohne oder mit Wiederholungen), und ob wir die Auswahl als geordnet betrachten (1. Element, 2. Element, . . .) oder als
ungeordnet, ergeben sich vier verschiedene Anzahlen:
Satz 1.38. Sei X eine endliche Menge mit n Elementen und sei k eine natĂźrliche Zahl. Dann wird
die Anzahl der MĂśglichkeiten fĂźr die Auswahl von k Elementen aus dieser n-elementigen Menge X
gegeben durch:
ungeordnet
 
n
k

ohne Wiederholungen

mit Wiederholungen




n+k â1
k

geordnet

(n)k =

 
n
Âˇ k!
k
nk

ÂŠ 2003 by S. Jukna

1.6. KOMBINATORISCHE ABZĂHLARGUMENTE

43

Beweis. Die EintrĂ¤ge der ersten Zeile in dieser Tabelle entsprechem einfach den Definitionen. Es
bleibt also die beide EintrĂ¤ge in der zweiten Zeile zu begrĂźnden.
Fall 1: mit Wiederholungen + geordnet. Es gibt n MĂśglichkeiten das erste Element x 1 auszuwĂ¤hlen.
Da wir Wiedeholungen von Elementen in der Auswahl erlauben, gibt es danach immer noch n MĂśgk mal

z
}|
{
lichkeiten das zweite Element x 2 auszuwĂ¤hlen, usw. Nach der Produktregel gibt es also n Âˇ n Âˇ . . . Âˇ n =
nk MĂśglichkeiten eine geordnete Folge (x 1 , x 2 , . . . , x k ) aus k Elementen von X auszuwĂ¤hlen.

Fall 2: mit Wiederholungen + ungeordnet. In diesem Fall ist die gesucht Zahl S(n, k) die Anzahl der
ganzzahligen LĂśsungen (a1 , . . . , an ) fĂźr die Gleichung
a1 + Âˇ Âˇ Âˇ + a n = k

(â)

unter der Bedignung, dass alle ai > 0 sind: es reicht jedes ai als die Anzahl der Vorkommen des
i-ten Elements von M in der gewĂ¤hlten Teilmenge zu interpretieren. Jede LĂśsung (a1 , . . . , an ) der
Gleichung (â) entspricht genau einer Folge aus Nullen und Einsen der LĂ¤nge k + (n â 1) = n + k â 1
mit n â 1 Einsen:
0
. . 0} 1 |0 .{z
. . 0} 1 |0 .{z
. . 0} 1 . . . . . . 1 |0 .{z
. . 0}
| .{z
a1

Da es genau16

n+kâ1
nâ1



=

n+kâ1
k



a2

a3

an

solche Folgen gibt, sind wir fertig.



Nummerisch lassen sich die Binomialkoeffizienten und FakultĂ¤ten wie folgt berechnen:
Satz 1.39. Es gilt:
(n)k = n(n â 1) Âˇ Âˇ Âˇ (n â k + 1)
und

 
n
(n)k
n!
n nâ1 nâ2
nâk +1
=
=
= Âˇ
Âˇ
ÂˇÂˇÂˇ
.
k
k!
k!(n â k)! 1
2
3
k

Beweis. Die erste Gleichheit ist einfach: es gibt |X | = n MĂśglichkeiten das erste Element x 1 auszuwĂ¤hlen; danach gibt es |X \ {x 1 }| = n â 1 MĂśglichkeiten das zweite Element x 2 auszuwĂ¤hlen, usw.
Letztendlich gibt es |X \ {x 1 , . . . , x kâ1 }| = n â (k â 1) = n â k + 1 MĂśglichkeiten das k-te Element
x k auszuwĂ¤hlen. Insgesamt gibt es also n(n â 1) Âˇ Âˇ Âˇ (n â k + 1) MĂśglichkeiten eine geordnete Folge
(x 1 , x 2 , . . . , x k ) aus k verschiedenen Elementen von X auszuwĂ¤hlen.
Nun beweisen wir die zweite Gleichung. Wir kĂśnnen alle geordnete Folgen (x 1 , x 2 , . . . , x k ) aus k verschiedenen Elementen von X wie folgt erzeugen: zuerst wĂ¤hlenwir eine k-Teilmenge {x 1 , x 2 , . . . , x k }
aus k verschiedenen Elementen von X ; hier haben wir genau nk MĂśglichkeiten. Wenn die Teilmenge
{x 1 , x 2 , . . . , x k } bereits gewĂ¤hlt
 ist, bleiben genau k! MĂśglichkeiten diese Teilmenge zu permutieren.
Also haben wir insgesamt nk Âˇ k! geordneten Folgen (x 1 , x 2 , . . . , x k ). Da nach Definition diese Zahl
gleich (n)k ist, es folgt
 
n
Âˇ k! = (n)k
k
oder Ă¤quivalent
 
n
(n)k
n!
=
=
k
k!
k!(n â k)!


n 
16Hier haben wir die Formel nâr
= nr benutzt (siehe 1.3).
ÂŠ 2003 by S. Jukna

44

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

Es gibt viele nĂźtzliche Gleichungen, die die Arbeit mit Binomialkoeffizienten erleichtern. Um solche
Gleichungen zu erhalten, reicht es in den meisten FĂ¤llen die kombinatorische (nicht die numerische,
wie vom Satz 1.39 gegeben) Natur der Binomialkoeffizienten auszunutzen. Um das zu demonstrieren,
beachten wir, dass eine Teilmenge durch sein Komplement eindeutig bestimmt ist. Diese einfache
Boebachtung liefert uns sofort die Gleichung:


n
nâk



 
n
=
.
k

(1.3)

In Ă¤hnlicher Weise kann man auch andere Gleichungen beweisen.
Satz 1.40. (Pascalâscher Rekurrenzsatz fĂźr Binomialkoeffizienten)
  
 

n
nâ1
nâ1
=
+
.
k
kâ1
k
Beweis. Sei X eine Menge mit n = |X | Elementen. Fixiere
 ein beliebiges x â X . Die Anzahl der
k-Teilmengen von X , die das Element x enthalten, ist nâ1
kâ1 und die Anzahl der k-Teilmengen von X ,
die das Element x vermeiden (d.h. nicht enthalten), ist nâ1
k . Da es keine anderen k-Teilmengen in X
gibt, folgt die Behauptung.

Der folgender einfacher (aber sehr nĂźtzlicher) Satz ist vom Sir Isaac
Newton in ca. 1666 bewiesen worden. Dieser Satz erklĂ¤rt den Namen:
Binomialkoeffizienten sind die Koeffizienten in der Berechnung des âbinomischen Ausdrucksâ (x + y) n .

Satz 1.41. (Binomischer Lehrsatz) Sei n eine positive ganze Zahl. Dann gilt fĂźr alle reelle Zahlen x
and y:
n  
X
n k nâk
n
(x + y) =
x y .
k
k=0

Beweis. (Kombinatorischer Beweis) Wenn wir die Terme
(x + y) n = (x + y) Âˇ (x + y) Âˇ . . . Âˇ (x + y)
|
{z
}
n -mal
n
ausmultiplizieren, dann gibt es genau k MĂśglichkeiten den Term x k y nâk zu erhalten. Warum? Wenn
wir die Terme multiplizieren, wĂ¤hlen wir aus jedem Term entweder x oder y. Sei a = (a1 , . . . , an ) die
(diese Auswahl) entsprechende 0-1 Folge mit ai = 1 genau dann, wenn aus dem i-ten Term (x + y)
die erste Zahl x ausgewĂ¤hlt wĂźrde. Sei N (k) die Anzahl der Vorkommen von x k y nâk in dem Produkt
(x + y) n . Dann ist
 
n
N (k) = Anzahl der 0-1 Folgen der LĂ¤nge n mit genau k Einsen =
.
k

ÂŠ 2003 by S. Jukna

1.6. KOMBINATORISCHE ABZĂHLARGUMENTE

45

Beweis.
(Induktiver Beweis) FĂźr n = 0 ist die linke Seite gleich (x + y) 0 = 1 und die rechte ist auch
n  0 0â0
= 1.
0 x y
Induktionsschritt: n 7â n + 1.

(x + y) n+1 = (x + y) Âˇ (x + y) n
n  
X
n k nâk
= (x + y) Âˇ
x y
Induktionsannahme
k
k=0
n  
n  
X
X
n k nâk
n k nâk
=xÂˇ
x y
+yÂˇ
x y
k
k
k=0
k=0
n  
n  
X
X
n k+1 nâk
n k nâk+1
=
x y
+
x y
k
k
k=0
k=0

n 
n  
X
X
n
n k nâk+1
r nâr +1
=
x y
+
x y
(r := k + 1)
r â1
k
r =1
k=0
  
 
 
n 
X
n
n
n 0 n+1
n n+1 0
r nâr +1
+
x y
+
x y
+
x y
=
r â1
r
0
n
r =1





n 
X
n + 1 r nâr +1
n + 1 0 n+1
n + 1 n+1 0
=
x y
+
x y
+
x y
(Satz 1.40)
r
0
n+1
r =1

n+1 
X
n + 1 r (n+1)âr
=
x y
.
r
r =0


â˛ Beispiel 1.42 : Mit binomischem Lehrsatz kann man viele nĂźtzliche Sachen beweisen. Zum beispiel
so kann man die folgende Eigenschaft der ganzen Zahlen zeigen. FĂźr jede ganze Zahl m und fĂźr
jede natĂźrliche Zahl k > 1 gilt:
m k ist ungerade ââ m ist ungerade.
Beweis: Die Richtung â ist trivial: wĂ¤hre nĂ¤hmlich m gerade d.h. m = 2 j fĂźr eine ganze Zahl
j, so wĂ¤re auch m k = 2k ( j k ) eine gerade Zahl. Um die andere Richtung â zu beweise, nehmen
wir an, dass m ungerade ist, d.h. m hat die Form m = 2 j + 1 fĂźr eine ganze Zahl j. Setze x = 2 j
und y = 1 und wende den binomischen Satz an:
 
 
 
k
k
1 k
2 k
k k
m = (2 j + 1) = 1 + (2 j)
+ (2 j)
+ Âˇ Âˇ Âˇ + (2 j)
.
1
2
k
Also hat unswere Zahl m k die Form 1 plus eine gerade Zahl, und muss deshalb ungerade sein.




FĂźr wachsenden n und k ist es schwer, den Binomialkoeffizient nk exakt auszurechnen. Andererseits
reicht es fĂźr viele Anwendungen, nur die Zuwachsrate (wie schell oder wie langsam nk wĂ¤chst) zu
wissen. Oft reicht bereits die folgende AbschĂ¤tzung:
Lemma 1.43.

 n k
k

   
n
en k
6
<
.
k
k
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

46
Beweis. Untere Schranke:
 n k
k

n n
n
n nâ1
nâk +1
= Âˇ ÂˇÂˇÂˇ 6 Âˇ
ÂˇÂˇÂˇ
=
k k
k
k kâ1
1

 
n
.
k

Obere Schranke. Nach (1) und binomischem Lehrsatz,

e

nt

 
n  
X
n i
n k
> (1 + t) =
t >
t .
i
k
n

i=0

FĂźr t = k/n bekommt man
   k
n
k
e >
,
k
n
k

wie erwĂźnscht.



FĂźr FakultĂ¤ten oft reichen die folgenden triviale AbschĂ¤tzungen:
 n n/2
2

6 n! 6 nn

Viel bessere AbschĂ¤tzung ist durch die berĂźhmte Stirling-Formel17 gegeben:
â

2Ďn

 n n
e

6 n! 6

â

2Ďn

 n n
e

Âˇ e1/12n .

(1.4)

Mit dieser AschĂ¤tzung von n! kann man die folgende AschĂ¤tzung fĂźr Binomialkoeffizienten zeigen
(Ăbungsaufgabe 18) Sei 0 < Îą < 1 und sei Îąn eine ganze Zahl. Dann gilt:


n
Îąn



= â

1 + o(1)
Âˇ 2n ÂˇH (Îą) ,
2ĎÎą(1 â Îą)n

(1.5)

wobei
H (Îą) := â(Îą log2 Îą + (1 â Îą) log2 (1 â Îą))
die sogenannte binĂ¤re Entropie-Funktion ist. 19 Graphisch sieht diese Funktion so aus:
17James Strirling Methodus Differentialis, 1730.
18Hinweis: H (Îą) = log2 h(Îą) mit h(Îą) = ÎąâÎą (1 â Îą) â(1âÎą) .
19Der Term âo(1)â hier ist eine funktion, die fĂźr wachsendes n gegen 0 strebt. Wir werden diese âklein-oâ Notation in
Abschnit 3.9 genauer betrachten.
ÂŠ 2003 by S. Jukna

1.6. KOMBINATORISCHE ABZĂHLARGUMENTE

47

1.6.3 Prinzip von Inklusion and Exklusion
Das Prinzip von Inklusion und Exklusion (bekannt auch als das Sieb von Eratosthenes) ist ein mĂ¤chtiges kombinatorisches AbzĂ¤hlprinzip.
FĂźr zwei beliebige endlichen Mengen A und B gilt
| A âŞ B| = | A| + |B| â | A âŠ B|.
FĂźr drei Mengen A, B und C gilt (siehe Abb. 1.12):
| A âŞ B âŞ C | = | A| + |B| + |C | â | A âŠ B| â | A âŠ C | â |B âŠ C | + | A âŠ B âŠ C |
Im Allgemeinen wollen wir fĂźr n gegebene Teilmengen A1 , . . . , An von X die Anzahl | A1 âŞ Âˇ Âˇ Âˇ âŞ An |
der Elemente in der Vereinigung bestimmen. Als die erste Approximation kĂśnnen wir die Summe
| A1 | + Âˇ Âˇ Âˇ + | An |

(1.6)

nehmen. Jedoch wird diese Zahl im allgemeinen zu groĂ sein: Wenn Ai âŠ A j , â, dann wird jedes
Element von Ai âŠ A j in (1.6) zweimal gezĂ¤hlt, einmal in | Ai | und ein zweites Mal in | A j |. Wir kĂśnnen
die Situation korrigieren, indem wir aus (1.6) die Summe
X
| Ai âŠ A j |.
(1.7)
16i< j6n

subtrahieren. Aber dann wird die Zahl zu klein sein, da jedes Element von Ai âŠ A j âŠ Al , â drei Mal
in (1.7) gezĂ¤hlt ist: einmal in | Ai âŠ A j |, ein zweites Mal in | A j âŠ Ak |, und ein drittes Mal in | Ai âŠ Ak |.
Wir probieren noch mal die Situation zu korrigieren, indem wir die Summe
X
| Ai âŠ A j âŠ Ak |,
16i< j <k6n

ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

48

B

A
1

2

11111
00000
00000
11111
00000
11111
3
00000
2 11111
000002
11111

1

1
C
Abbildung 1.12: In | A| + |B| + |C | wird jedes Element aus A \ (B âŞ C), B \ (A âŞ C) und C \ (A âŞ B)
nur einmal gezĂ¤hlt, aber jedes Element aus A âŠ B, B âŠ C und A âŠ C wird zweimal gezĂ¤hlt; deshalb
muss man diese Zahlen abziehen. Aber dann wird jedes Element aus A âŠ B âŠ C zweimal abgezogen
(anstatt einmal); also mĂźssen wir fĂźr jedes solches Element noch 1 zu addieren.

dazu addieren. Dann wird aber die Zahl wieder zu groĂ sein, usw. Denoch werden wir nach n Schritten
bereits die richtige Zahl finden.
Satz 1.44. (Prinzip von Inklusion and Exklusion, Siebformel) Seien A1 , . . . , An endliche Teilmengen eines gemeinsamen Universums X . Dann gilt:
n
[

Ai

=

i=1

X

16i6n

| Ai | â

X

16i< j6n

| Ai âŠ A j | +

X

16i< j <k6n

| Ai âŠ A j âŠ Ak | + . . .

. . . + (â1) n+1 | A1 âŠ A2 âŠ . . . âŠ An |

Beweis. (mittels AbzĂ¤hlen): FĂźr eine Indexmenge I â {1, . . . , n} und ein Element x â X sei
f I (x) :=



(â1) | I |+1
0

falls x â
sonst.

T

i âI

Ai

Dann kĂśnnen wir die rechte Summe so umschreiben:
X
\
XX
(â1) | I |+1
Ai =
f I (x)
I ,â

i âI

I ,â x âX

:=S (x)

X zX }| {
=
f I (x)

(doppeltes ZĂ¤hlen)

x âX I ,â

=

X

S(x).

x âX

Es reicht deshalb zu zeigen, dass S(x) = 1 fĂźr jedes x â A1 âŞ . . . âŞ An , und S(x) = 0 fĂźr alle anderen
Elemente x gilt. Um das zu zeigen, nehmen wir ein beliebiges (aber festes) x â X .
Fall 1: x < A1 âŞ . . . âŞ An . Dann sind alle f I (x) = 0 und damit ist auch S(x) = 0.

Fall 2: x â A1 âŞ . . . âŞ An . Dann ist die Menge J := {i : x â Ai } nicht leer, und f I (x) , 0 genau
ÂŠ 2003 by S. Jukna

1.6. KOMBINATORISCHE ABZĂHLARGUMENTE

49

dann, wenn I â J und I , â gilt. Sei m := | J |. Dann gilt
S(x) =

X

f I (x) =

â,I âJ

X

(â1)

| I |+1

â,I âJ

=

m  
X
m
k=1

k

(â1) k+1

oder Ă¤quivalent
âS(x) =

m  
X
m

k

k=1

(â1) k =

m  
X
m

|k=0

k
{z

(â1) k â

=(1â1) m =0

Also haben wir in diesem Fall S(x) =

m
0



= 1, wie erwĂźnscht.

}

 
m
0



â˛ Beispiel 1.45 : Eine SekretĂ¤rin hat n Briefe an n verschiedene EmpfĂ¤nger geschrieben und die Kuverts adressiert. Nun steckt sie die Briefe blindlings in die Kuverts. Wie wahrscheinlich ist es,
dass kein Brief im richtig adressierten Kuvert steckt?
An der Theatergarderobe gaben n Herrn ihre HĂźte ab. Nach der Vorstellung gibt die Garderobefrau die HĂźte wahllos und zufĂ¤llig zurĂźck. Wie wahrscheinlich ist es, dass kein einziger Herr
seinen eigenen Hut erhĂ¤lt?
Mit dem Prinzip von Inklusion and Exklusion lĂ¤sst sich zeigen, dass diese Wahrscheinlichkeit
Ăźberraschen groĂ ist: sie liegt sehr nah zu eâ1 = 0.3678....
Das Problem lĂ¤sst sich wie folgt formalisieren. Die Menge [n] = {1, 2, 3, 4, . . . , n} wird bijektiv
auf sich selbst abgebildet. Wie wahrscheinlich ist eine fixpunktfreie Permutation von [n]? Eine
Permutation f : [n] â [n] ist fixpunktfrei, wenn f (x) , x fĂźr alle x â [n].
Behauptung: Die Anzahl der fixpunktfreien Permutationen von {1, . . . , n} ist gleich
n
X
i=0

 
n
X
(â1) i
n
(â1)
(n â i)! = n!
.
i
i!
i

i=0

Pn (â1) i
â1
Die Summe i=0
i! ist der Anfangsterm der Taylor-Reihe20 fĂźr e . Die Anzahl der fixpunktfreien Permutationen stimmt also asymptotisch mit n!/e Ăźberein.
Beweis. Sei X die Menge aller n! Permutationen f : [n] â [n], und sei Ax â X die T
Menge aller
Permutationen f fĂźr die gilt: f (x) =Tx. Dann ist | Ax | = (n â 1)!, und allgemeiner,
x âI A x =
(n â |I |)!, da die Permutationen in i âI Ax alle Punkte x â I fixieren mĂźssen, und den Rest
permutieren mĂźssen. Eine Permutation ist also fixpunktfrei genau dann, wenn sie in keiner der
Mengen A1 , . . . , An liegt. Nach dem Prinzip von Inklusion and Exklusion gilt:
|X \ (A1 âŞ . . . âŞ An )| =

X

I â {1, ..., n }

(â1)

|I |

(n â |I |)! =

n
X
i=0

 
n
(â1)
(n â i)!
i
i


20Mehr Ăźber Taylorenwicklung von Funktionen kann man in Abschnitt 3.7 finden.
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

50

Eine direkte (aber oft sehr nĂźtzliche) Folgerung aus dem Prinzip von Inklusion and Exklusion ist die
folgende AbschĂ¤tzung.
Korollar 1.46. (Boole-Ungleichungen) Seien A1 , . . . , An endliche Teilmengen. Dann gilt:
X

16i6n

| Ai | â

X

16i< j6n

| Ai âŠ A j | 6

n
[

Ai 6

i=1

X

16i6n

| Ai |

1.7 Aufgaben
1. 1. Vereinfache folgenden AusdrĂźcke durch Betrachten eines Mengendiagrams:
(a) A âŞ ( A \ B), (b) A âŠ ( A \ B), (c) A \ ( A âŞ B), (d) B âŞ ( A \ B), (e) A \ (B \ A),
(f) A \ ( A \ B).
1. 2. Zeichne Mengendiagramme fĂźr folgenden Mengen: (a) A âŠ B, (b) A âŞ B, (c) A âŠ B, (d) A âŞ B.

1. 3. Stelle fest, welche der folgenden Relationen R auf der Menge der Menschen reflexiv, symmetrisch, antisymmetrisch, asymmetrisch oder transitiv sind, wobei (a, b) â R genau dann, wenn
1. a ist grĂśĂer als b
2. a und b wurden am selben Tag geboren
3. a hat denselben Vornamen wie b
4. a und b haben eine gemeinsame GroĂmutter.
Stelle fest, ob die folgenden Relationen R auf der Menge der ganzen Zahlen reflexiv, symmetrisch, antisymmetrisch, asymmetrisch oder transitiv sind, wobei (x, y) â R genau dann, wenn
(a) x , y
(b) xy > 1
(c) x = y + 1 oder x = y â 1
(d) x âĄ y mod 7 (geteilt durch 7 ergeben beide Zahlen den selben Rest)
(e) x ist ein Vielfaches von y
(f) x und y sind beide negativ oder beide nicht-negativ
(g) x = y 2
(h) x > y 2
1. 4. Seien f : A â B und g : B â C Abbildungen. Man zeige
(a) Sind f und g injektiv, so auch f âŚ g.
(b) Sind f und g surjektiv, so auch f âŚ g.
(b) Sind f und g bijektiv, so auch f âŚ g.

1. 5. FĂźr zwei ganzen Zahlen x und y schreibt man x|y genau dann, wenn x die Zahl y ohne Rest teilt, d.h.
wenn es ein z â Z mit y = x Âˇ z gibt. Seien R und S die folgenden beiden Relationen Ăźber Z:
R = {(x, y) â Z2 : x|y} und

S = {(y, z) â Z2 : 2| (y + z)}.
ÂŠ 2003 by S. Jukna

1.7. AUFGABEN

51

Zeige, dass dann
R âŚ S = {(x, z) â Z2 : x ist ungerade oder z ist gerade}.
1. 6. Sei A eine beliebige Menge, und seien R und S Relationen Ăźber A. Zeige, dass dann:
(R âŚ S) â1 = S â1 âŚ Râ1 .
1. 7. Seien R und S zwei Ăquivalenzrelationen Ăźber A. Zeige, dass R âŚ S genau dann eine Ăquivalenzrelation
Ăźber A ist, wenn R âŚ S = S âŚ R gilt.
1. 8. Interessanterweise sind die Eigenschaften injektiv, surjektiv und bijektiv bei Abbildungen endlicher Mengen aus kombinatorischen GrĂźnden gleichwertig. Sei A eine endliche Menge und f : A â A eine Abbildung.
Zeige, dass die folgenden Aussagen Ă¤quivalent sind:
(1) f ist surjektiv.
(2) f ist injektiv.
(3) f ist bijektiv.
P
Hinweis: | A| = a â A | f â1 (a)|.

1. 9. Sei E (N) = {X : X â N, X endlich} die Menge aller endlichen Teilmengen von N. Wir definieren
P
f : E â N durch f (â) = 0 und f (X ) = x âX 2 x fĂźr â , X â E. Zeige, dass f eine Bijektion von E (N) auf
N ist. Hinweis: Warum ist die BinĂ¤rdarstellung einer natĂźrlichen Zahl eindeutig?
1. 10. Zeige, dass der n-dimensionale WĂźrfel Q n bipartit ist. Hinweis: Sei U die Menge aller Strings, die eine
ungerade Anzahl von Einsen haben.
1. 11. Von den folgenden drei Aussagen ist genau eine richtig:
(a) Fritz hat Ăźber tausend BĂźcher.
(b) Fritz hat weniger als tausend BĂźcher.
(c) Fritz hat mindestens ein BĂźch.
Wieviele BĂźcher hat Fritz?
1. 12. Auf einem Bauernhof in der NĂ¤he von Frankfurt gibt es sowohl gefrĂ¤Ăige als auch nicht gefrĂ¤Ăige
Schweine. Es ist bekannt, dass jedes alte Schwein gefrĂ¤Ăig ist und dass jedes gesunde Schwein gefrĂ¤Ăig ist.
Welche der nachstehenden Folgerungen sind dann zulĂ¤ssig?
(a) Es gibt sowohl alte als auch junge Schweine auf dem Hof.
(b) Es gibt junge Schweine auf dem Hof.
(c) Alle nicht gefrĂ¤Ăigen Schweine sind jung.
(d) Einige junge Schweine sind krank.
(e) Alle junge Schweine sind krank.
1. 13.
(a) Gilt die Aussage: A â B â A âŠ B = A?
(b) Beweise: A âŠ B = A \ ( A \ B).

(c) Das kartesische Produkt A Ă A einer Menge A mit sich selbst lĂ¤sst sich formal auch mit Hilfe der Potenzmenge P ( A) definieren. Seien a, b Elemente von A. Dann soll das geordnete Paar (a, b) definiert sein als
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

52

die Teilmenge von P ( A), die genau die Mengen {a} und {a, b} enthĂ¤lt, also kurz geschrieben:
n
o
(a, b) := {a}, {a, b}

Seien a, b, c, d â A. Zeige, dass folgende Ăquivalenz gilt:

(a, b) = (c, d) ââ a = c und b = d
1. 14. FĂźr f : A â B und g : B â A sei die Komposition (g âŚ f )(x) = g( f (x)) die identische Abbildung, d.h.
(g âŚ f )(x) = x fĂźr alle x â A. Zeige, dass dann f injektiv und g surjektiv sind. Gebe ein Beipiel an, in dem f
nicht surjektiv und g nicht injektiv ist.
1. 15. Wir betrachten die Abbildung f : A â B. Seien U,V â B. Zeige: Es gilt
f â1 (B \ U) = A \ f â1 (U)
und
f â1 (U âŠ V ) = f â1 (U) âŠ f â1 (V ).
Hinweis: Um die Gleichheit zweier Mengen X und Y zu beweisen, muss man folgendes zeigen: FĂźr jedes x â X
gilt x â Y und fĂźr jedes y â Y gilt y â X.
1. 16. Wir betrachten die Abbildung f : X â Y . Seien A, B â X bzw. U,V â Y .

(a) Zeige: Es ist f ( A âŠ B) â ( f ( A) âŠ f (B)). Gilt die umgekehrte Richtung?

(b) Zeige: Es gilt f â1 ( f ( A)) â A und f ( f â1 (U)) â U. Zeige, dass i.A. keine Gleichheit gilt.
1. 17.
(a) Zeige die logische Ăquivalenz von A â B und ( A â§ B) â¨ (ÂŹA â§ ÂŹB).
(b) Zeige, dass die folgenden Formelpaare nicht Ă¤quivalent sind:
(âx : P(x)) â¨ (âx : Q(x)) mit âx : P(x) â¨ Q(x)
(âx : P(x)) â§ (âx : Q(x)) mit âx : P(x) â§ Q(x)
Hinweis: Um zu zeigen, dass zwei Formelpaare A und B nicht Ă¤quivalent sind, reicht es ein Gegenbeispiel
anzugeben. D.h. es reicht ein Universum M und die PrĂ¤dikate P(x) und Q(x) auf M anzugeben, fĂźr die die
Ăquivalenz A ââ B nicht gilt.
1. 18. Gebe einen Widerspruchs-Beweis fĂźr die folgende Aussage an. FĂźr jede natĂźrliche Zahl t und fĂźr jede
natĂźrliche Zahl n gilt:
Wenn t > 2 und t ein Teiler von n ist, dann ist t kein Teiler von n + 1.
1. 19. Eine Schnecke kriecht tagsĂźber an einer Mauer nach oben, rutscht aber nachts um die HĂ¤lfte der erreichten HĂśche wieder nach unten. Bezeichnen wir mit h n die am n-ten Abend erreichte HĂśche, so ist h n+1 = 21 h n +1.
Zeige, dass
1
h n = 2 â nâ1
2
fĂźr alle n = 1, 2, . . . gilt.
1. 20. Zeige, dass eine Menge von n Elementen 2n Teilmengen besitzt. Hinweis: Induktion Ăźber n. FĂźr ein
festes Element a und jede Teilmenge S entweder a â S oder a < S gilt.
1. 21. Zeige, dass die Summe 1 + 3 + 5 + . . . + (2n â 1) der ersten n ungeraden natĂźrlichen Zahlen gleich n2 ist.
Hinweis: Induktion.
1. 22. Beweise die folgenden Aussagen durch Induktion:
ÂŠ 2003 by S. Jukna

1.7. AUFGABEN

53

1. FĂźr jede natĂźrliche Zahl n ist sind n3 + 2n und n3 â n durch 3 teilbar.
Pn
2
2
2. FĂźr jede natĂźrliche Zahl n gilt k=0
k 3 = n (n+1)
.
4
â
Pn
3. FĂźr jede natĂźrliche Zahl n > 1 gilt k=1 â1 > 2( n + 1 â 1). Hinweis: Nach Anwendung der Induktik
onsvoraussetzung reduziert sich das Problem auf eine Ungleichung, deren GĂźltigkeit man durch Quadrieren
nachweisen kann.
n
X
4. FĂźr jede natĂźrliche Zahl n gilt:
2i = 2n+1 â 1.
i=0

5. Die Folge der so genannten harmonischen Zahlen H1 , H2 , H3 , . . . ist definiert durch
Hn = 1 +

1 1
1
+ +... + .
2 3
n

Zeige, dass fĂźr jede natĂźrliche Zahlen n gilt:
H2 n > 1 +

n
.
2

6. FĂźr alle natĂźrlichen Zahlen n, k > 1 gilt: 1 + k/n 6 (1 + 1/n) k .
1. 23. Ein Polygon ist eine geschlossene zusammenhĂ¤ngende Folge von Strecken, wobei keine zwei Strecken
auf einer Gerade liegen. Ein Polygon wird einfach genannt, wenn sich keine Strecken schneiden. Die Treffpunkte der Strecken heiĂen Ecken. Ein einfaches Polygon mit n Ecken wird auch als ein n-Eck genannt. D.h.
ein einfaches Polygon ist ein von einem geschlossenen, sich nicht schneidenden, Streckenzug begrenztes, ebenes geometrisches Objekt. Eine Diagonale ist eine Strecke, die zwei nicht benachbarte Ecken verbindet. Ein
einfaches Polygon S heiĂt konvex, wenn fĂźr alle nicht benachbarte Ecken a, b â S gilt, dass alle Punkte der
Diagonale zwischen a und b ebenfalls in S liegen.
Zeige folgendes: Die Zahl der Diagonalen in jedem ebenen, konvexen n-Eck mit n > 3 ist gleich
D(n) =

n(n â 3)
2

Hinweis: Induktion Ăźber die Anzahl der Ecken kĂśnnte hilfreich sein.
1. 24. Wir betrachten das folgende Maximierungsproblem. Gegeben sind eine endliche Menge A und eine
Abbildung f : A â A. Das Ziel ist, eine grĂśĂtmĂśgliche Teilmenge S â A zu finden, so dass die EinschrĂ¤nkung
f S von f auf S eine Bijektion ist, d.h. es muss folgendes gelten:
(a) f (S) â S,
(b) fĂźr alle x â S gibt es genau ein y â S mit f (x) = y.
Zeige, wie man dieses Problem mit Hilfe der Induktion effizient lĂśsen kann. Hinweis: Reduziere das Problem
fĂźr Mengen A mit n Elementen auf dasselbe Problem fĂźr Mengen mit n â 1 Elementen. DafĂźr enferne aus A ein
Element x 0 mit f â1 (x 0 ) = â, falls es ein solches Element gibt. Was passiert, wenn es kein solches Element x 0
gibt?
1. 25. (Das âBerĂźhmtheits-Problemâ) In einer Party nehmen n Personen teil. Eine BerĂźmtheit ist eine Person
X, die keine andere der n â1 Teilnehmer kennt, aber die allen anderen Personen bekannt ist. Sie kommen in eine
solche Party und wollen wissen, ob es eine BerĂźhmtheit gibt und, falls ja, diese BerĂźhmtheit auch herausfinden.
Sie kĂśnnen nur die Fragen stellen, ob eine Person eine andere Person kennt oder nicht. Insgesamt gibt es also
n(n â 1)/2 mĂśgliche Fragen.
Zeige, wie man dieses Problem mit nur 3(n â 1) Fragen lĂśsen kann. Hinweis: Benutze Induktion, d.h. reduziere
das Problem fĂźr n Personen auf ein Problem fĂźr weniger Personen.
1. 26. Ein Geradenarrangement in allgemeiner Lage (engl. lines in general position) ist ein Arrangement aus
endlich vielen, paarweise nicht parallelen Geraden in der Ebene, von denen keine drei einen Punkt gemeinsam
ÂŠ 2003 by S. Jukna

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

54

haben. Dabei habe eine Gerade kein Ende und keinen Anfang. Ein solches Arrangement unterteilt die Ebene in
verschiedene FlĂ¤chen.
Zeige: Jedes Geradenarrangement in allgemeiner Lage mit mindestens 3 Geraden besitzt stets ein Dreieck als
FlĂ¤che.
1. 27. Zeige mit Hilfe des binomischen Lehrsatzes, dass
â
â
(a) der Wert (1 â 5) n + (1 + 5) n fĂźr jedes n â N ganzzahlig ist.
â
â
â
â
(b) der Wert ( 2 + 3) n + ( 2 â 3) n fĂźr jedes gerade n â N ganzzahlig ist.
1. 28. Wie viele MĂśglichkeiten gibt es, k Kugeln in n (n > k) Kisten zu verteilen, so dass keine Kiste mehr als
eine Kugel enhĂ¤lt?

  
n
n nâk
gilt.
1. 29. Zeige, dass
=
k+1
k k+1
1. 30. Zeige, dass fĂźr jedes k das Produkt von k aufeinanderfolgenden natĂźrlichen Zahlen durch k! teilbar ist.

Hinweis: Betrachte n+k
k .
1. 31. Zeige die folgende Rekursionsgleichung:
 


n
n nâ1
=
.
k
k kâ1



Hinweis: Benutze das Prinzip der doppelten AbzĂ¤hlung um k Âˇ nk = n Âˇ nâ1
kâ1 zu zeigen. DafĂźr zĂ¤hle, wie viele
Paare (x, M) es gibt, wobei M eine k-elementige Teilmenge von {1, . . . , n} ist und x â M.

1. 32. Seien 0 6 l 6 k 6 n. Zeige, dass

    

n
k
n
nâl
=
.
k
l
l
k âl
Hinweis: Benutze das Prinzip der doppelten AbzĂ¤hlung, um die Anzahl aller Paare (L, K ) von Teilmengen aus
{1, . . . , n} mit L â K, |L| = l und |K | = k zu bestimmen.


 n 
Pn n 2
Pn n  n 
n 2
1. 33. Beweise die Gleichung: i=0
= 2n
= ni nâi
. Zeige, dass es genau i=0
i
n . Hinweis: i
i nâi
MĂśglichkeiten gibt, n-elementige Teilmenge aus {1, 2, . . . , 2n} auszuwĂ¤hlen. Alternativ: Wende den binomischen Lehrsatz auf (a + x) 2n an.
1. 34. Berechne die Anzahl W (n, k) der kĂźrzesten Wege im 2-dimensionalen Gitter vom Punkt A = (0, 0) zum
Punkt B = (n, k). Hinweis: Jeden Weg kann man als eine Folge der Buchstaben h (fĂźr âhochâ) und r (fĂźr
ârechtsâ) beschreiben.
y

B=(13,7)
7

A= (0,0)

13

x

Abbildung 1.13: Ein kĂźrzester Weg von A nach B
n  
X
n i
1. 35. Gebe eine geschlossene Form fĂźr die Summe
2 an.
i
i=0

ÂŠ 2003 by S. Jukna

1.7. AUFGABEN

55

1. 36. Sei K das durchschnittliche Kapital (in Euro) eines deutchen BĂźrgers. Betrachte eine Person als âreichâ,
falls sie mindestens K/2 Euro besitzt. Zeige, dass die reichen Personen mindestens die HĂ¤lfte aller Gelder
haben.
1. 37. Ein Affe wurde erfolgreich darauf dressiert, KlĂśtzchen aus einer Urne (ohne ZurĂźcklegen) zu ziehen und
sie in einer Reihe von links nach rechts aufzustellen.
In die Urne wurden nun sechs KlĂśtzchen gelegt, wovon drei mit dem Buchstaben A, zwei mit dem Buchstaben
N und eins mit dem Buchstaben B markiert wurden.
Wie groĂ ist die Wahrscheinlichkeit dafĂźr, dass die vom Affen aufgestellte Reihe die Buchstabenfolge âBANANAâ ergibt?
Hinweis: Der Affe kann nicht lesen. Da der Affe die KlĂśtzchen rein zufĂ¤llig zieht, ist die Wahrscheinlichkeit
gleich:
Anzahl der gĂźnstigen Versuche
Anzahl aller Versuche
1. 38. Wir haben eine Menge X der zu erledigenden Jobs, die wir auf m Prozessoren beliebig verteilen: X =
X1 âŞ X2 âŞ . . . âŞ X m , wobei X i die Menge der dem Prozessor i zugewiesenen Jobs ist. Sei
L :=

m
1 X
|X i |
m
i=1

die durchschnittliche Belastung eines Prozessors. Wir sagen, dass ein Prozessor i belastet ist, falls |X i | > L/2
gilt. Zeige, dass (egal wie wir die Jobs verteilen!) mindestens die HĂ¤lfte aller Jobs von belasteten Prozessoren
ausgefĂźhrt wird.
1. 39. Zeige, dass in jedem endlichen ungerichteten Graphen mindestens zwei Knoten denselben Grad haben
mĂźssen. Hinweis: Taubenschlagprinzip.
1. 40. Sei S â {1, 2, . . . , 2n} mit |S| = n + 1. Zeige:
(a) Es gibt zwei Zahlen a, b in S, so dass b = a + 1. Hinweis: Betrachte die TaubenlĂścher (i,i + 1).
(b) Es gibt zwei Zahlen a, b in S, so dass a + b = 2n + 1. Hinweis: Betrachte die TaubenlĂścher (i, 2n â i + 1),
i = 1, 2, . . . , n + 1.
(c) Es gibt zwei Zahlen a , b in S, so dass a ein Teiler von b ist. Hinweis: Jede Zahl x â {1, 2, . . . , 2n} lĂ¤sst
sich als Produkt x = k(x) Âˇ 2a eindeutig darstellen, wobei k(x) eine ungerade Zahl zwischen 1 und 2n â1 ist.
Betrachte nun die Zahlen x â S als Tauben und nimm die TaubenlĂścher 1, 3, 5, . . . , 2n â 1. Setze die Taube
x ins Taubenloch k(x). Zeige, dass dann mindestens ein Taubenloch zwei Tauben (Zahlen) x < y enthalten
muss.
1. 41. Ein Matching ist eine Menge paarweise disjunkter Kanten. Ein Stern ist ein Menge von Kanten, die einen
Knoten gemeinsam haben. Zeige: Jeder ungerichtete Graph mit 2(k â 1) 2 + 1 Kanten enthĂ¤lt ein Matching oder
einen Stern mit k Kanten.
1. 42. Ein vollstĂ¤ndiger binĂ¤rer Baum ist ein binĂ¤rer Baum bei dem alle BlĂ¤tter die gleiche Tiefe haben. Offenbar sind vollstĂ¤ndige binĂ¤re BĂ¤ume besonders kompakt gebaute BĂ¤ume.
Zeige: jeder vollstĂ¤ndiger binĂ¤rer Baum der Tiefe d besitzt insgesamt 2d+1 â 1 Knoten.

ÂŠ 2003 by S. Jukna

56

KAPITEL 1. GRUNDBEGRIFFE UND BEWEISMETHODEN

ÂŠ 2003 by S. Jukna

Kapitel 2

Algebra und Elementare Zahlentheorie
Contents
2.1

Division mit Rest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

58

2.2

Euklidischer Algorithmus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

64

2.3

Primzahlen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

2.4

Kleiner Satzt von Fermat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

Anwendung in der Krypthographie: RSA-Codesâ . . . . . . . . . . . . . .

68

Chinesischer Restsatz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72

2.4.1
2.5

2.5.1
2.6
2.7

Anwendung: Schneller

Gleichheitstestâ

. . . . . . . . . . . . . . . . . . .

74

Gruppen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

2.6.1

Zyklische Gruppen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

Ringe und KĂśrper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

2.7.1

Polynomring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

2.7.2

Komplexe Zahlenâ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

84

2.8

Allgemeine VektorrĂ¤ume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

85

2.9

Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86

In der Algebra geht es um Rechnen mit Zahlen und Veralgemeinerungen von Zahlen. Wir wissen
aus der Schule, wie man in manchen unendlichen Mengen, wie N, Q oder R, rechnen kann. Jeder
(klassischer) Computer kann aber nur in einer endlichen Menge der Zahlen
Zn = {0, 1, . . . , n â 1}
rechnen, wobei n durch den SpeicherkapazitĂ¤t beschrĂ¤nkt ist. Wie kann er nun in einer solchen Menge vernĂźnftig addierien/subtrahieren oder multiplizieren/dividieren? Die Antwort ist einfach: Rechne
zyklisch! Im tĂ¤glichen Leben rechnen wir stĂ¤ndig âzyklischâ, besonders offensichtlich in der Zeitrechnung. Addierien und subtrahieren kann man hier sehr einfach. Es ist 9 Uhr. Wie spĂ¤t ist es 5 Stunden
spĂ¤ter? Wie spĂ¤t war es vor 3 Stunden? Die Frage also ist, wie soll man zyklisch multiplizieren und
dividieren? Dies ist die wichtigste Frage der sogenannten modularen Arithmetik, und wir werden in
diesem Kapitel diese Frage beantworten.
57

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

58

2.1 Division mit Rest
In diesem Abschnitt betrachten wir nur die ganze Zahlen.
Sind a â Z und n â N+ , so kann man a durch n teilen (durch mehrfaches Abziehen von n aus a) bis
ein Rest 0 6 r < n bleibt. Diesen Rest r bezeichnet man mit
a mod n
und nennt den Rest von a modulo n. D.h.1
a mod n = a â n Âˇ

jak
n

Die Zahl n heiĂt Teiler von a, in Zeichen m | a, wenn es ein q â Z mit a = qn gibt. D.h.
n|a

ââ

âq : a = qn

ââ

a mod n = 0.

Lemma 2.1. Teilbarkeits-Regeln:
1. Aus a | b folgt a | bc fĂźr alle c.
2. Aus a | b und b | c folgt a | c (TransitivitĂ¤t).
3. Aus a | b und a | c folgt a | sb + tb fĂźr alle s und t.
4. Ist c , 0, so gilt a | b ââ ac | bc.
Beweis. Wir beweisen nur (2) (alle andere Behauptungen kann man analog zeigen). Aus a | b und
b | c folgt b = sa und c = tb fĂźr bestimmte s und t. Daraus c = tb = t(sa) = (ts)a und damit auch
a | c folgt.

Zwei ganze Zahlen a und b, die den gleichen Rest bezĂźglich n haben, werden kongruent modulo n
genannt. Man schreibe
a âĄ b mod n.



In modularer Arithmetik muss man klar zwischen Bezeichnungen âa âĄ b mod nâ und âa =
b mod nâ unterscheiden. Die erste Bezeichnung sagt, dass die Reste a mod n und b mod n
gleich sind, wĂ¤hrend die zweite sagt, dass a der Rest von b modulo n ist (und damit auch zwangsweise
0 6 a 6 n â 1 gelten muss):
a âĄ b mod n
a = b mod n

ââ
ââ

geteilt durch n, beide a und b denselben Rest besitzen
a ist der Rest von a geteilt durch n

Die folgende Eigenschaft der Kongruenzen werden werden wir oft (ohne das explizit zu erwĂ¤hnen)
benutzen.
Lemma 2.2. FĂźr a, b â Z gilt genau dann a âĄ b mod n, wenn n | (a â b).
1Zur Erinnerung: âxâ ist die grĂśĂte ganze Zahl, die nicht grĂśĂer als x ist. So ist z.B. â3/2â = 1 und ââ3/2â = â2.
ÂŠ 2003 by S. Jukna

2.1. DIVISION MIT REST

59

Beweis. Wir schreiben a = q1 n + r1 und b = q2 n + r2 mit 0 6 r1 , r2 < n. Das Lemma sagt nun:
r1 = r2 ââ n | (q1 â q2 )m + (r1 â r2 ) Die Richtung â ist vĂśllig klar. Die andere Richtung â gilt,
weil |r1 â r2 | < n gilt.

Lemma 2.3. Seien x âĄ y mod n und a âĄ b mod n. Dann gilt:
1. x + a âĄ y + b mod n.
2. x â a âĄ y â b mod n.
3. xa âĄ yb mod n.

4. x d âĄ y d mod n.
Beweis. Ăbungsaufgabe.



Man fasst alle ganzen Zahlen, die bei Divison durch m denselben Rest haben, d.h. die paarweise
kongruent modulo m sind, zu einer sogenannten Restklasse zusammen.
Ist m > 0, so heiĂt fĂźr alle a â Z
[a]m := {n â Z : n âĄ a mod m}
die Restklasse von a modulo m. Die Menge aller Restklassen bezeichnet man mit
Z/Zm = {[a]m : a â Z}.
Zum Beispiel fĂźr m = 5 bildet jede der fĂźnf Zeilen eine Restklasse modulo 5:
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ

â10
â9
â8
â7
â6

â5
â4
â3
â2
â1

0
1
2
3
4

5
6
7
8
9

10
11
12
13
14

15
16
17
18
19

20
21
22
23
24

ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ
ÂˇÂˇÂˇ

Jede Restklasse [a]m enthĂ¤lt genau eine Zahl r â {0, 1, . . . , m â 1} mit [r]m = [a]m und diese Zahl
nennt man den ReprĂ¤sentanten dieser Klasse. Die Menge dieser ReprĂ¤sentanten bezeichnet man mit
Zm , d.h.
Zm := {0, 1, . . . , m â 1}.
In Zm kann man genauso wie in Z addieren und multiplizieren:
a + b := (a + b) mod m
a Âˇ b := (a Âˇ b) mod m
D.h.
a + b in Zm = Rest von a + b geteilt durch m in Z
a Âˇ b in Zm = Rest von a Âˇ b geteilt durch m in Z
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

60

Die zwei anderen Operationenâdie Differenz und die Divisionâsind in allen algebraischen Strukturen durch die Addition und die Multiplikation definiert:
x â a := x + y wobei y die LĂśsung von y + a = 0 ist
x/a := x Âˇ z wobei z die LĂśsung von z Âˇ a = 1 ist.

Die Zahlen y und z nennt man dann entsprechend die additive und multiplikative Inversen von a und
sind durch y = âa und z = aâ1 bezeichnet. In der Struktur (Zm , +, Âˇ) mĂźssen also diese Inversen die
folgenden Gleichungen erfĂźllen:
a + (âa) âĄ 0 mod m

a Âˇ aâ1 âĄ 1 mod m.

0

3

1

2

Die Menge Zm kann man als einen Kreis vorstellen. Dann sind die Operationen a + b und a â b besonders einfach auszufĂźhren. Will man a + b
(bzw. a â b) berechnen, so startet man im Punkt a und lĂ¤uft den Kreis
b Schritte vorwĂ¤rts (bzw. rĂźckwĂ¤rts). So bekommt man z.B. in Z4 , dass
1 + 3 = 0 und 1 â 3 = 2 gilt.

Aber vorsichtig: Dividieren in Zm kann man ohne weiteres nicht! FĂźr die Division muss folgendes
gelten:
x/a = b ââ x = ab.
D.h. x/a bezeichnet die einzige(!) Zahl b, fĂźr die die Gleichheit x = ab gilt. Insbesondare muss
0/a = 0 fĂźr alle a , 0 gelten. Diese Eigenschaft ist bereits in Z4 verletzt: Hier gilt 0 = 2 Âˇ 2 und damit
auch 0/2 = 2 , 0.
Um durch a â Zm dividieren zu kĂśnnen, muss also Zm die multiplikative Inverse enthalten, d.h. es
muss eine Zahl x = aâ1 â {1, 2, . . . , m â 1} mit der Eigenschaft ax âĄ 1 mod m geben. Dann ist die
Division x/a von x durch a modulo m nichts anderes als die Multiplikation von x mit aâ1 . D.h.
x/a := xaâ1
Deshalb kann man in Zm nur durch solche Zahlen a â Zm \ {0} dividieren, die eine multiplikative
Inverse aâ1 modulo m besitzen.
â˛ Beispiel 2.4 : Wir betrachten die Strukturen (Zm , +, Âˇ) fĂźr m = 2, 3, 4, 5, wobei die Operationen +
und Âˇ modulo m definiert sind.
1. Z2 = {0, 1}

+ 0 1
0 0 1
1 1 0

Âˇ 0 1
0 0 0
1 0 1

1 hat die Inverse â division mĂśglich!
2. Z3 = {0, 1, 2}

+
0
1
2

0
0
1
2

1
1
2
0

2
2
0
1

Âˇ
0
1
2

0
0
0
0

1
0
1
2

2
0
2
1

1 und 2 sind die Inversen von sich selbst â division mĂśglich!
ÂŠ 2003 by S. Jukna

2.1. DIVISION MIT REST

61

3. Z4 = {0, 1, 2, 3}

+
0
1
2
3

0
0
1
2
3

1
1
2
3
0

2
2
3
0
1

3
3
0
1
2

Âˇ
0
1
2
3

0
0
0
0
0

1
0
1
2
3

2
0
2
0
2

3
0
3
2
1

2
0
2
4
1
3

3
0
3
1
4
2

a = 2 hat keine Inverse aâ1 â Division durch 2 unmĂśglich!
4. Z5 = {0, 1, 2, 3, 4}
+
0
1
2
3
4

0
0
1
2
3
4

1
1
2
3
4
0

2
2
3
4
0
1

3
3
4
0
1
2

4
4
0
1
2
3

Âˇ
0
1
2
3
4

0
0
0
0
0
0

1
0
1
2
3
4

4
0
4
3
2
1

Alle a â Z5 haben Inversen â division mĂśglich! Was ist z.B. die multiplikative Inverse von
2? Antwort: Wenn wir die Zeile zu 2 in der Multiplikationstabelle anschauen, finden wir eine
1 in der Spalte zu 3. Also ist 2 Âˇ 3 âĄ 1 mod 5 und damit ist 3 die multiplikative Inverse von 2
modulo 5. So ist
Division durch 2 = Multiplikation mit 3
Division durch 3 = Multiplikation mit 2
Division durch 4 = Multiplikation mit 4
5. Sei a = 8 und m = 15. Dann ist 2a = 16 âĄ 1 mod 15. Also ist x = 2 eine multiplikative
Inverse von 8 modulo m.
6. Sei a = 12 und m = 15. Dann ist die Folge (ax mod m : x = 0, 1, 2, . . .) periodisch und
nimmt die Werte aus {0, 12, 9, 6, 3} (nachrechnen!). Also hat die Zahl 12 keine multiplikative
Inverse modulo 15.
Eine natĂźrliche Frage deshalb ist
Durch welche Zahlen a â Zm kann man dividieren?
Oder Ă¤quivalent
Welche Zahlen a â Zm besitzen ihre multiplikative Inversen aâ1 ?
Diese Fragen haben eine sehr elegante Antwort. Zwei Zahlen a und b heiĂen teilerfremd (oder relativ
prim), falls sie keinen gemeinsamen Teiler (auĂer 1) haben, d.h. aus x | a und x | b folgt x = 1.
Satz 2.5. (Existenz von multiplikativen Inversen)
a â Zm hat die multiplikative Inverse aâ1 ââ a und m teilerfremd sind.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

62

Damit haben genau die Zahlen aus
Zâm := {a â Zm : a , 0 und ggT(a, m) = 1}
ihre multiplikative Inversen. D.h. wir kĂśnnen jede Zahl in Zm durch jede der Zahlen aus Zâm dividieren.
Um Satz 2.5 zu beweisen, brauchen wir das Konzept des âgrĂśĂten gemeinsammen Teilersâ.
Der grĂśĂter gemeinsamer Teiler von a und b ist definiert als
ggT(a, b) = max{d : d teilt a und b};
manchmal ist diese Zahl mit ggT (a, b) bezeichnet. Die Zahlen a und b sind also teilerfremd (oder
relativ prim), falls ggT(a, b) = 1 gilt.
Linearkombinationen von zwei Zahlen a, b â Z sind alle Zahlen von der Form ax + by mit x, y â Z.
Eine Linearkombination ax + by ist positiv, falls ax + by > 1 gilt.
Satz 2.6. ggT(a, b) = die kleinste positive Linearkombination von a und b.
Beweis. Sei d = ggT(a, b) und sei t die kleinste Zahl2 in der Menge
A = {ax + by : x, y â Z} âŠ N+ .
Aus d|a und d|b folgt d|t. Wir wollen zeigen, dass auch t|a und t|b gilt, woraus t | d und damit auch
t = d folgt.
Um t|a zu zeigen, schreiben wir a = qt + r mit q â Z und 0 6 r < t, woraus r = a â qt folgt. Wir
wissen, dass t die Form ax + by mit x, y â Z hat. Delshalb ist die Zahl
r = a â qt = a â q(ax + by) = a(1 â qx) + b(âqy)
auch eine nicht negative (denn r > 0) Linearkombination von a und b. Da 0 6 r < t und t als die
kleinste positive Linearkombination von a und b gewĂ¤hlt war, ist das nur dann mĂśglich, wenn r = 0
gilt.
Die Teilbarkeit von b durch t folgt mit dem selben Argument.



Satz 2.6 liefert uns die folgenden wichtigsten Eigenschaften des grĂśĂten gemeinsammen Teilers.
Lemma 2.7. FĂźr alle ganze Zahlen a, b â Z und n > 1 gilt: ggT(an, bn) = n Âˇ ggT(a, b).
Beweis. Nach Satz 2.6 gilt:
ggT(an, bn) = min{anx + bny : x, y â Z}

= n Âˇ min{ax + by : x, y â Z}

(Minimum in N+ )
(Minimum in N+ )

= n Âˇ ggT(a, b).


2Beachte, dass A nicht leer ist.
ÂŠ 2003 by S. Jukna

2.1. DIVISION MIT REST

63

Der folgende Fakt gibt uns eine der wichtigsten Eigenschaten der teilerfremden Zahlen.
Lemma 2.8. Aus m | ab und ggT(a, m) = 1 folgt: m | b.
Beweis. Nach dem Satz 2.6 kĂśnnen wir ggT(m, a) als Linearkombination 1 = ggT(m, a) = mx + ay
darstellen. Dann gilt auch b = bmx + bay. Da m beide Summanden bmx und bay teilt, muss m auch
ihre Summe b teilen.

FĂźr a â Zm sei

aZm := {ax mod m : x = 0, 1, 2, . . . , m â 1} â Zm

die Menge aller verschiedenen Zahlen 0, a, 2a, . . . , (m â 1)a modulo m. Im Allgemeinen kann es
passieren, dass ax âĄ ay mod m fĂźr einige Zahlen x , y â Zm gilt (z.B. 2 Âˇ 1 âĄ 2 Âˇ 3 mod 4);
dann ist aZm eine echte Teilmenge von Zm . Ist aber a relativ prim zu m, dann sagt der folgender
Satz, dass aZm = Zm gelten muss, d.h. in diesem Fall muss aZm einfach eine Permutation von
Zm = {0, 1, . . . , m} sein.
Satz 2.9. Ist ggT(a, m) = 1, so gilt
aZm = Zm .
Insbesondare hat dann die Gleichung ax âĄ b mod m genau eine LĂśsung in Zm .
Beweis. Um die Behauptung zu verifizieren, nehmen wir an, dass es zwei verschiedene Zahlen 0 6
x , y 6 m â 1 mit ax âĄ ay mod m gibt. Dann muss auch ax â ay = a(x â y) durch m teilbar sein,
d.h. es muss ein q â Z mit (x â y)a = qm geben. Da aber a und m relativ prim sind, muss x â y
durch m teilbar sein (Lemma 2.8). Das ist aber unmĂśglich, da beide Zahlen x und y nicht negativ und
kleiner als m sind. Ein Widerspruch.
Nun betrachten wir die (modulare) Gleichung ax âĄ b mod m. Da der Rest r = b mod m in Zm liegt
und Zm = aZm gilt, muss es ein einziges x â Zm mit xa mod m = r geben.

Damit haben wir eine Richung des Satzes 2.5 bewiesen: Ist ggT(a, m) = 1, so hat die Gleichung
ax âĄ 1mod m genau eine LĂśsung x â Zm , und diese LĂśsung ist genau die multiplikative Inverse
x = aâ1 von a modulo m. Die andere Richtung des Satzes 2.5 (a â Zm hat keine multiplikative
Inverse in Zm , wenn ggT(a, m) , 1) folgt unmittelbar aus dem folgenden Fakt.
Lemma 2.10.
âx â Zm : ax âĄ b mod m

ââ

ggT(a, m) | b.

Beweis. (â) Sei d = ggT(a, m). Hat ax âĄ b mod m eine LĂśsung x, so muss (nach Lemma 2.2) die
Zahl m (und damit auch die Zahl d) die Differenz ax â b teilen. Da aber a durch d teilbar ist, muss
auch b durch d telbar sein.
(â) Wenn d | b, dann sind a, b und m durch d teilbar und wir kĂśnnen die Gleichung ad x âĄ bd mod m
d
betrachten. Da ad und m
teilerfremd
sein
sind
(d
=
ggT(a,
m)
ist
der
grĂśĂter
gemeinsammer
Teiler
d
von a und m), hat diese Gleichung nach Lemma 2.9 eine LĂśsung x â Zm . Da m | (ax â b) genau
dann, wenn ad x â bd durch m

d teilbar ist, ist muss x auch die LĂśsung von ax âĄ b mod m sein.
ÂŠ 2003 by S. Jukna

64

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

In Z kann man die Gleichung a Âˇ c = b Âˇ c mit c , 0 duch c kĂźrzen:



xÂˇA=yÂˇA â x=y

(falls a , 0).

In Zm kann man dies ohne weiteres nicht tun:
2Âˇ 6 3 âĄ 4Âˇ 6 3 mod 6 â 2 âĄ 4 mod 6

â das ist falsch!

Nichtdestotrotz, kann man auch die modulare Gleichung x Âˇ a = y Âˇ a mod m durch a kĂźrzen, falls a
und m teilerfremd sind.
Lemma 2.11. (KĂźrzungsregel) Ist ggT(a, m) = 1, so kann man die beiden Seiten der Gleichung
ax âĄ ay mod m durch a kĂźrzen:
ax âĄ ay mod m â x âĄ y mod m.
Beweis. Da ax âĄ ay mod m, ist ax â ay = (x â y)a durch m teilbar. Da aber ggT(a, m) = 1 gilt, muss
dann (laut Lemma 2.8) x â y durch m teilbar sein, d.h. muss x âĄ y mod m gelten.


2.2 Euklidischer Algorithmus
Wie kann man den grĂśĂter gemeinsamer Teiler ggT(a, b) berechnet, ohne zuvor mĂźhsam aller Teiler
von a und b zu bestimmen? DafĂźr gibt es ein gutes altes Verfahren â der Euklidischer Algorithmus. Die Idee des Euklidischen Algorithmus ist es, aus zwei Zahlen den grĂśĂten gemeinsamen Teiler
schrittweise herauszudividieren.
Der Algorithmus bassiert sich auf den folgenden zwei einfachen Beobachtungen:
1. When b|a, dann ggT(a, b) = b.
2. When a = bt + r, dann ggT(a, b) = ggT(b, r):
Beweis: Jeder gemeinsammer Teiler von a und b muss auch r = a â bt teilen, woraus ggT(a, b) 6
ggT(b, r) folgt. Die andere Richtung ggT(b, r) 6 ggT(a, b) ist auch richtig, da jeder Teiler von b
und r muss auch a = bt + r teilen.

Euklid aus Alexandria, ca. 325 - 265 J. vor Christus:
Euklid(a, b) (braucht a > b > 0)
If b = 0
gib a aus
else

gib Euklid(b, a mod b) aus

ÂŠ 2003 by S. Jukna

2.3. PRIMZAHLEN

65

â˛ Beispiel 2.12 : Berechne ggT(348, 124):
Euklid(348, 124) â 346 = 2 Âˇ 124 + 100
Euklid(124, 100) â 124 = 1 Âˇ 100 + 24
Euklid(100, 24) â 100 = 4 Âˇ 24 + 4
Euklid(24, 4) â 24 = 6 Âˇ 4 + 0

Euklid(4, 0) â gib 4 als ggT(348, 124) aus

Wir wissen bereits (Satz 2.5), dass man in Zm durch alle Zahlen a â Zm , a , 0 dividieren kann, die
relativ prim zu m sind. Dazu mĂźssen wir aber die multiplikativen Inversen aâ1 mod m auch finden
kĂśnnen. Und das kann man mit Hilfe von Euklidischem Algorithmus tun. Der Algorithmus liefert uns
nĂ¤hmlich die LĂśsung fĂźr ax + my = ggT(a, m) = 1, was Ă¤quivalent zu ax âĄ 1 mod m ist.
â˛ Beispiel 2.13 : Finde die multiplikative Inverse von 17 modulo 64. Zuerst wenden wir den Euklidischen Algorithmus, um ggT(17, 64) zu bestimmen:
(a)
(b)
(c)

64 = 3 Âˇ 17 + 13
17 = 1 Âˇ 13 + 4
13 = 3 Âˇ 4 + 1
4 =4Âˇ1+0

â r2 = 13
â r3 = 4
â r4 = 1
â r5 = 0

Nun rechnen wir rĂźckwĂ¤rts:
(c)
(b)
1 = 13 â 3 Âˇ 4 = 13 â 3 Âˇ (17 â 1 Âˇ 13) = 4 Âˇ 13 â 3 Âˇ 17
(a)
= 4 Âˇ (64 â 3 Âˇ 17) â 3 Âˇ 17
=

4 Âˇ |{z}
64 â |{z}
15 Âˇ |{z}
17
|{z}
x

=
âĄ

a

y

4 Âˇ 64 + (â15) Âˇ 17
49 Âˇ 17 mod 64

b

(da â15 âĄ 49 mod 64)

Also, 49 ist die gesuchte multiplikative Inverse von 17 modulo 64, d.h. 17â1 mod m = 49 gilt.

2.3 Primzahlen
Eine Primzahl ist eine natĂźrliche Zahl p > 2, die nur durch 1 und sich selbst teilbar ist.



Achtung: 1 ist also keine Primzahl!

Satz 2.14. (Euklidischer Hilfsatz) Ist p prim, so gilt: p | ab â p | a oder p | b.
Beweis. Angenommen, p teilt a nicht. Da p eine Primzahl ist, muss dann ggT(a, p) = 1 gelten. Dann
muss aber nach Lemma 2.8 p | b gelten.

ÂŠ 2003 by S. Jukna

66

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

Satz 2.15. (Fundamentalsatz der Arithmetik) Jede natĂźrliche Zahl n > 2 lĂ¤sst sich bis auf die
Reihenfolge der Faktoren auf genau einer Weise als Produkt von Primzahlen schreiben:



n = p1 Âˇ p2 Âˇ Âˇ Âˇ pk .
Achtung: In der Produktzerlegung n = p1 p2 Âˇ Âˇ Âˇ pk kann eine Primzahl mehrmals vorkommen!

Beweis. Die Existenz einer solchen Primzahlzerlegung haben wir bereits in Kapitel 1 mittels Induktion bewiesen (siehe Satz 1.22). Zur Eindeutigkeit: Sind
n = p1 p2 Âˇ Âˇ Âˇ ps = q1 q2 Âˇ Âˇ Âˇ qr
zwei Primzahlzerlegungen von n, dann teilt p1 das Produkt links. Nach Satz 2.14 muss p1 mindestens
einen der Terme qi teilen, woraus p1 = pi folgt. Durch Umnummerierung der qi âs wird p1 = q1
erreicht. Dann bleibt p2 Âˇ Âˇ Âˇ ps = q2 Âˇ Âˇ Âˇ qr und durch Wiederholung desselben Schlusses ergibt sich
schlieĂlich r = s und pi = qi fĂźr alle i nach eventueller Umnummerierung der qi âs.

Mit diesem Satz kĂśnnen wir zum Beispiel eine weitere (nicht trivialle!) Eigenschaft der teilerfremden
Zahlen zeigen.
Lemma 2.16. Aus a | m und b | m und ggT(a, b) = 1 folgt: ab | m.
Beweis. FĂźr jede natĂźrliche Zahl x > 2 sei P(x) die Menge aller verschiedenen Primzahlen in Primzahldarstellung von x. Aus dem Euklidischen Hilfsatz (Satz 2.14) folgt
x|y

ââ

P(x) â P(y),

denn jede Primzahl p â P(x) muss (wegen x|y) nach Satz 2.14 mindestens eine Primzahl aus P(y)
teilen, was nur dann mĂśglich ist, wenn p selbts zu P(y) gehĂśhrt.
Aus a | m folgt P(a) â P(m), und aus b | m folgt P(b) â P(m). Damit muss P(a) âŞ P(b) â
P(m) gelten. Ausserdem, muss P(a) âŠ P(b) = â gelten, da a und b teilerfremd sind. Somit gilt auch
P(a Âˇ b) â P(m), woraus ab | m folgt. .

Da Primzahlen den âSkeletâ aller Zahlen darstellen, haben sie die KĂśpfe von Menschen seit Ewigkeit
beschĂ¤ftigt. Es ist zum Beispiel bekannt, dass es âungefĂ¤hrâ n/ ln n Primzahlen in Interval {2, 3, . . . , n}
gibt. Viele Fragen aber bleiben immer noch offen. Die bekanntesten davon sind die folgenden zwei
Vermutungen.
Vermutung 2.17. (Goldbach Conjecture) Jede natĂźrliche Zahl n > 4 ist die Summe zweier Primzahlen.
Zum Beispiel, 4 = 2 + 2, 6 = 3 + 3, 8 = 3 + 5, usw. Es war bereits gezeigt, dass die Vermutung fĂźr
alle Zahlen n 6 1016 gilt. In 1939 hat Schnirelman gezeigt, dass jede gerade Zahl eine Summe von
hĂśchstens 300000 Primzahlen ist. Das war nur ein Anfang â heute wissen wir bereits, dass jede gerade
Zahl die Summe von 6 Primzahlen ist.
ÂŠ 2003 by S. Jukna

2.4. KLEINER SATZT VON FERMAT

67

Vermutung 2.18. (Twin Prime Conjecture) Es gibt unendlich viele Zahlen p, so dass beide p und
p + 2 prim sind.
In 1966 hat Chen gezeigt, dass es unendlich viele Primzahlen p gibt, so dass p + 2 ein Produkt von
hĂśchstens zwei Primzahlen ist. Also ist diese Vermutung âfastâ richtig!

2.4 Kleiner Satzt von Fermat
Nun beweisen wir den sogenannten âkleinen Satzâ von Fermat3, der sich als sehr nĂźtzlich â insbesondare in der Kryptographie â erwiesen hat.
Kleiner Satz von Fermat
Ist p eine Primzahl und a â N, dann
a p âĄ a mod p
Insbesondere, falls p kein Teiler von a ist, dann
a pâ1 âĄ 1 mod p
Beweis. (Direkter Beweis) Wenn wir modulo p rechnen, so sind nach Satz 2.9 alle Zahlen
a, 2a, 3a, . . . , (p â 1)a
verschieden und keiner kann gleich 0 sein: WĂ¤re es nĂ¤mlich ka âĄ 0a mod p, so kĂśnnten wir beide
Seiten kĂźrzen, was k âĄ 0 mod p liefern wĂźrde; aber die Zahl k 6 p â 1 zu klein dafĂźr ist. Wenn wir
also diese Zahlen modulo p nehmen, so bekommen wir genau die Zahlen 1, 2, . . . , p â 1 (vielleicht in
einer andreren Reihenfoolge). Deshalb muss auch das Produkt
a Âˇ 2a Âˇ 3a Âˇ Âˇ Âˇ (p â 1)a = a pâ1 Âˇ (p â 1)!
modulo p dem Produkt
1 Âˇ 2 Âˇ 3 Âˇ Âˇ Âˇ (p â 1) = (p â 1)!
gleich sein, d.h.
a pâ1 Âˇ (p â 1)! âĄ (p â 1)! mod p
gelten muss. Da p und (p â 1)! offenbar teilerfremd sind, kĂśnnen wir (nach Lemma 2.11) diese modulare Gleichung durch (p â 1)! kĂźrzen, was die gewĂźnschte Kongruenz a p âĄ a mod p liefert.

Beweis. (Induktiver Beweis) Induktion Ăźber a. Induktionsbasis a = 0 ist trivial.
Induktionsschritt: a 7â a + 1. Wir wenden den binomischen Lehrsatz an und erhalten:
p  
p
X
X
p
p!
p
k
(a + 1) =
Âˇa =
ak Âˇ
âĄ a p + 1 mod p
k
k!(p â k)!
k=0

k=0

3Nicht verwecheln das mit dem berĂźhmten âLetzten Satz von Fermatâ: Die Gleichung x n + y n = z n fĂźr n > 2 hat keine
nicht trivialle ganzzahlige LĂśsungen. Dieser Satz war nur in 1994 (nach mehr als 300 Jahren!) von Andrew John Weils
bewiesen worden.
ÂŠ 2003 by S. Jukna

68

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

da in der letzten Summe fĂźr k < {0, p} der k-the Term durch p teilbar ist. Da nach der Indultionsvoraussetzung a p âĄ a mod p, folgt die Behauptung:
(a + 1) p âĄ a p + 1 âĄ a + 1 mod p.
Ist nun p kein Teiler von a, so kann man nach Lemma 2.11 (KĂźrzungsregel) beide Seiten der Kongruenz a p âĄ a mod p durch a dividieren, um die gewĂźnschte Kongruenz a pâ1 âĄ 1 mod p zu erhalten. 



Ist p prim, so kann man die multiplikative Inversen aâ1 in Z p sehr leicht berechnen: Nimm
einfach
aâ1 = a pâ2 .

Nun so weit so gut ... aber wie soll man denn die Potenzen a m modulo n schnell berechnen? Ein
trivialler Algorithmus a m = a Âˇ a Âˇ Âˇ Âˇ a braucht fast m Multiplikationen. Es gibt aber viel schneller
Algorthmus, der nur logarithmisch viele (anstatt satten m) Multiplikationen braucht. um die Potenzen
a m mod n auszurechen.
Potenzierungs-Algorithmus: Zuerst P
bestimme die 0-1 Bits (ÇŤ 0 , ÇŤ 1 , . . . , ÇŤ r ) mit r 6 log2 (m + 1) in
der BinĂ¤rdarstellung von m, d.h. m = ri=0 ÇŤ i 2i . Dieser Schritt ist einfach:

0 falls m gerade
ÇŤ 0 :=
1 sonst
 
Danach ersetze m durch m2 und bestimme das nĂ¤chste Bit

0 falls m gerade
ÇŤ 1 :=
1 sonst
usw. Nach r 6 log2 (m + 1) Schritten sind wir fertig. Nun ist
P
Y i
i
a m = a i:ÇŤ i =1 2 =
a2
i:ÇŤ i =1

Also reicht uns nur die r + 1 Zahlen
2

3

2

a, a2 , a2 = (a2 ) 2 , a2 = (a2 ) 2 , . . . , a2

r

modulo n auszurechnen, wobei jede nĂ¤chste Zahl einfach Quadrat der vorigen ist!
Der kleiner Satz von Fermat sieht ziemlich einfach aus, hat aber bereits viele Abwendungen gefunden.
Im nĂ¤chsten Abschnitt betrachten eine Anwendung in der Kryptographie.

2.4.1 Anwendung in der Krypthographie: RSA-Codesâ
Nachrichten so zu verschlĂźsseln, dass sie kein Unbefugter versteht, ist nicht nur der Traum von kleinen
Jungs oder von Spionen â es ist mittlereweile unser Alltag geworden. Das allgemeine Model ist das
folgende: Eine Nachricht besteht aus einer Zahl a â Zn (n groĂ genug), die der Sender Bob (der
Bankkunde) der AnfĂ¤ngerin Alice (einer Bankangestellten) so mitteilen will, dass kein Lauscher die
Nachricht versteht. Dazu wendet Bob eine Bijektion f : Zn â Zn auf a und sendet f (a) an Alice.
Sie kennt eine Funktion g mit der Eigenschaft
g( f (a)) = a
ÂŠ 2003 by S. Jukna

2.4. KLEINER SATZT VON FERMAT

69

(die Inverse g(x) = f â1 (x) von f ) und kann also die Nachricht a rekonstruieren.
Eine der Schwierigkeiten von verschlĂźsselter Kommunikation ist die Tatsache, dass man vor dem
Senden der Nachricht eine VerschlĂźsselungsmethode (d.h. Funktionen f und g = f â1 ) verabreden
muss, damit die EmpfĂ¤nger die Nachricht versteht.
1976 Ăźberlegten sich Rivest, Shamir und Adleman, dass die VerschlĂźsselungsfunktion f eigentlich
nicht geheim zu sein braucht; wichtig ist nur, dass kein Unbefugter die EntschlĂźsselungsfunktion
g = f â1 kennt. (Lange ging man davon aus, dass deshalb auch f geheim sein muss.) Die RSA-Codes
von Rivest, Shamir und Adleman sind so genannte Public-Key Verfahren.
Das wichtigste in diesem (und manchen Ă¤hnlichen) Verfahren ist, dass die Bijektion f : Zn â Zn die
folgende âSichercheits-Bedingungâ erfĂźhlt:
(â) Ohne die Unkehrfunktion g = f â1 zu wissen, ist es sehr schwer aus b = f (a) die Zahl a zu
bestimmen.
Hat Alice eine solche (schwer umkehrbare) Funktion f , so kann sie f bekannt machen. Zum Besipiel
kann sie diese Funktion auf ihrer Web-Seite angeben. Diese Funktion f ist also das âpublic-keyâ. Die
Umkehrfunktion g = f â1 (das âsecret-keyâ) behaltet Alice fĂźr sich selbst streng geheim.
Nun kann Bob (der Kunde) seine Nachricht a â Zn (z.B. ein Ăberweisungsantrag) Alice so mitteilen:
- Zuerst hollt er sich das Public-Key f .
- Danach berechnet er die verschlĂźsselte Nachricht b = f (a) und verschickt b an Alice.
- Alice benutzt ihr secret-key g = f â1 , um die Nachricht zu entschlĂźsseln: g(b) = f â1 ( f (a)) = a.
Mit einem Ă¤hnlichen Verfahren kann Alice (eine Bankangestellte) ihre Nachrichten auch unterschreiben (âdigital signatureâ). Will nĂ¤hmlich Bob sicher sein, dass eine (nicht verschlĂźsselte) Nachricht a
auch wirklich von Alice stammt, mĂźssen sie beide sich so verhalten:
- Zuerst berechnet Alice Ď = f â1 (a), ihre âDigitale-Unterschriftâ.
- Dann verschickt sie (a, Ď) = (Nachricht, Unterschrift) an Bob.
- Bob berechnet dann a â˛ = f (Ď). Gilt a â˛ = a, so weiss Bob, das die Nachricht a von Alice stammt,
da f (Ď) = f ( f â1 (a)) = a gilt.
Das alles klingt sehr gut. Aber wie sollte man die Funktionen f : Zn â Zn mit der Eigenschaft (â)
wĂ¤hlen? DafĂźr kann man die modulare Arithmetik benutzen! NĂ¤hmlich kann Alice (die Bankangestelte) den folgenden Algorithmus anwenden.
Die Zahlen d und e erfĂźhlen also die Gleichung 4
de = 1 + k (p â 1)(q â 1)
fĂźr ein k â Z. FĂźr uns wichtig wird nur, dass
Die Zahl de â 1 durch beide Zahlen p â 1 und q â 1 teilbar ist.
4Als notwendige Bedingung wird heute empfohlen, p und q jeweils als 256-Bit Zahl zu wĂ¤hlen: der Faktorisierungsweltrekord liegt bei 512 Bit, d.h. bei Zahlen, die aus zwei Primfaktoren von je 256 Bit zusammengesetzt sind.
ÂŠ 2003 by S. Jukna

70

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE
Tabelle 2.1: RSA-Algorithmus

1. WĂ¤hle rein zufĂ¤llig zwei grĂśĂe Primzahlen p, q und berechne
n = pq wie auch Ď(n) = (p â 1)(q â 1).
Die Nachrichten sind dann natĂźrliche Zahlen aus Zn = {0, 1, . . . , n â 1}.
2. WĂ¤hle eine kleine Zahl e, die teilerfremd zu Ď(n) ist (Public Key).
3. Berechne die multiplikative Inverse d = eâ1 mod Ď(n) (Secrtet Key).
4. Mache das Paar (n, e) der Zahlen n und e bekannt.

VerschlĂźsselungs- und EntschlĂźsselungsfunktionen f und g sind dann definiert durch
f (x) := x e mod n
g(x) := x d mod n
Die Sichercheit des Verfahrens beruht sich darauf, dass es sehr schwer fĂźr den Lauscher (ohne die
Zahl d zu wissen) aus b = a d mod n die Nachricht a hierauszukriegen ist. Warum? Da der Lauscher
die Primzahlen p und q mit p Âˇ q = n finden muss5 und bisher keine effizienten (Polynomialzeit)
Algorithmen fĂźr die Primzahlzerlegung bekannt sind.6
Und wie mit der Korrektheit des Verfahrens? Um die Korrektheit zu beweisen, mĂźssen wir zeigen,
dass g( f (a)) = a, d.h.
a ed âĄ a mod n,

fĂźr alle a â Zn gilt. Da nach der Auswahl ed â 1 durch p â 1 wie auch durch q â 1 teilbar ist, folgt
diese Behauptung direkt aus folgendem Fakt.
Lemma 2.19. Sei n ein Produkt von verschiedenen Primzahlen und sei b eine Zahl, so dass b â 1
durch p â 1 fĂźr jeden Primteiler p von n teilbar ist. Dann gilt fĂźr alle a â Z:
a b âĄ a mod n.
Insbesondere gilt dann a b mod n = a fĂźr alle a â Zn .
Beweis.
Q Sei P die Menge aller Primzahlen, die n teilen. Nach unsere Annahme ist n das Produkt
n = p âP p dieser Primzahlen. Aus Lemma 2.16 folgt, dass eine Zahl x genau dann durch n teilbar
sein kann, wenn x durch alle Primzahlen p â P teilbar ist. Da wir n | a b â a zeigen wollen, reicht es
uns also zu zeigen, dass a b âĄ a mod p fĂźr jede Primzahl p â P gilt.7 Da p eine Primzahl ist, gibt es
5Obwohl das nicht Ăśffensichtlich ist, man kann aber folgendes zeigen: Hat man die Zahl d > 1 mit a d âĄ 1 mod n fĂźr
alle a â Zn , so kann man mit grĂśĂer Wahrscheinlichkeit eine Zerlegung n = pq von n in Primfaktoren schnell finden.
6Also ist das RSA-Verfahren nicht 100% sicher! Deshalb bemĂźhen sich viele Mathematiker einen mathematischen
Beweis zu finden, dass ein solcher (Polynomialzeit) Primzahlzerlegungalgorithmus wirklich nicht existiert. Das ist aber eines
der schwierigsten Problemen der Mathematik und Theoretischen Informatik Ăźberhaupt â das berĂźhmte P =NP? Problem.
7Zur Erinennerung: n | (x â y) ââ x âĄ y mod n (siehe Lemma 2.2).
ÂŠ 2003 by S. Jukna

2.4. KLEINER SATZT VON FERMAT

71

nur zwei mĂśgliche FĂ¤lle: entweder ggT(a, p) = p oder ggT(a, p) = 1.

Fall 1: ggT(a, p) = p. In diesem Fall ist a durch p teilbar, woraus a âĄ 0 mod p und damit auch
a b âĄ a mod p folgt.

Fall 2: ggT(a, p) = 1. In diesem Fall sagt uns der kleiner Satz von Fermat, dass es a pâ1 âĄ 1 mod p
gelten muss. Wir wissen, dass b â 1 durch p â 1 teilbar ist, d.h. b â 1 = k (p â 1) fĂźr eine Zahl k â Z
gilt. Nach Lemma 2.3(4) muss (a pâ1 ) k âĄ 1k mod p und damit auch a bâ1 âĄ 1 mod p gelten. Es bleibt
also beide Seiten mit a zu multiplizieren (Lemma 2.3(3) erlaubt das) um die gewĂźnschte Kongruenz
a b âĄ a mod p zu erhalten.


Der Lauscher hat verschlĂźsselte Nachricht â die Zahl b = f (a) â gesehen. Genau wie Bob, kennt
er die Zahlen n und e und die VerschlĂźsselungsfunktion f (x) := x e mod n. Er weiss auch, dass
f : Zn â Zn eine Bijektion ist (sonnst hĂ¤tte das ganze Verfachren gar nicht fuktioniert!). Also kann
er âeinfachâ f (x) fĂźr alle x â Zn berechnen bis er das einzige x mit f (x) = b (= f (a)) findet; dann
muss ja x = a gelten, und er hat die Nachricht geknackt! Wirklich? Ganz und gar nicht! Ein solcher
Vorgang von Lauscher ist absolut hoffnungslos, da die Menge Zn zu groĂ ist: Da n eine mindestens
512-Bit Zahl ist, hat Zn mehr als 2512 > 10500 Elemente!



Wenn Alice ihre geheime Primzahlen p und q gewĂ¤hlt hat, dann macht sie das Produkt n = p Âˇ q
fĂźr alle bekannt. Kann sie auch das Produkt Ď(n) = (pâ1) Âˇ (qâ1) bekannt geben? Die Antwort
ist: Nein! Um das zu sehen, beachte, dass
Ď(n) = pq â (p + q) + 1 = n â (p + q) + 1,
also
p + q = n â Ď(n) + 1.
Aber dann
(p â q) 2 = (p + q) 2 â 4pq = (n â Ď(n) + 1) 2 â 4n.
Waren die beide Zahlen n und Ď(n) bekannt, so kĂśnnte man leicht die beide Zahlen p + q und p â q
berechnen, und damit auch die beide Primzahlen p und q leicht bestimmen.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

72



Es ist zu empfĂ¤hlen, die Primzahlen p, q (p > q) so zu wĂ¤hlen, dass die Differenz p â q groĂ
ist. Warum? Angenommen p â q ist klein. Ist n = pq, so gilt
n=

Da p und q nah zu einander liegen, ist

 p + q 2
2

â

 p â q 2

s=

pâq
2

t=

p+q
2

klein, und

2

â
nicht viel grĂśĂer als n sein kann. Ausserdem wissen wir, dass t 2 â n = s2 das Quadrat einer Zahl
(diesmal s) sein muss. Also kann der Lauscher einfach alle Zahlen
â
â
â
t = â nâ, t = â nâ + 1, t = â nâ + 2, . . .
ausprobieren, bis t 2 â n = s2 fĂźr ein s gilt. Dann hat er die Primzahlen p und q bereits gefunden:
p = t + s und q = t â s.
Wie wir bereits erwĂ¤hnt haben, berĂźht die Sichercheit des RSA-Verfahrens auf die Schwierigkeit,
eine Zahl n als Produkt n = pq zweier Primzahlen darzustellen (Faktorisierungsproblem). Ein anderes
Verfahren, das Diffie-Helman public-key Verfahren, benutzt statdessen die Schwierigkeit, Logarithmen
modulo n zu berechnen, d.h. fĂźr gegebene a, b â Zân eine Zahl x mit bx âĄ a mod n zu finden.

2.5 Chinesischer Restsatz
In vielen MathematikbĂźchern aus alten Zeiten, angefangen bei Ăźber 2000 Jahre alten chinesischen
MathematikbĂźchern (Handbuch der Arithmetik von Sun-Tzun Suan-Ching), aber auch in berĂźhmten
âLiber abaciâ von Leonardo von Pisa (Fibonacci), finden sich Aufgaben, in denen Zahlen gesucht
werden, die bei Division durch verschiedenen andere Zahlen vorgegebene Reste lassen. Fangen wir
zur Demonstration mit einem Beispiel an:
âWie alt bist Du?â wird Daisy von Donald gefragt. âSo was fragt man eine Dame doch nichtâ
antwortet diese. âAber wenn Du mein Alter durch drei teilst, bleibt der Rest zwei.â âUnd wenn
man es durch fĂźnf teilt?â âDann bleibt wieder der Rest zwei. Und jetzt sage ich Dir auch noch,
dass bei Division durch sieben der Rest fĂźnf bleibt. Nun mĂźsstest Du aber wissen, wie alt ich
bin.â
Ăbersetzt in heutige mathematische Sprache lautet diese Aufgabe so: Man finde eine Zahl, die bei
Division durch 3,5,7 die Reste 2,3,2 lĂ¤sst. Zu lĂśsen ist also das modulare Gleichungssystem

x âĄ 2 mod 3
x âĄ 2 mod 5
x âĄ 5 mod 7
ÂŠ 2003 by S. Jukna

2.5. CHINESISCHER RESTSATZ

73

Ăhnliche Aufgabe stammt von Sun-Tzun Suan-Ching (zwishen 280 und 473 vor Christus).
Den folgende allegemeine Satz hat Châin-Chiu-Shao 1247 bewiesen.
Satz 2.20. (Chinesischer Restsatz) Seien m1 , m2 , . . . , mr paarweise teilerfremde, positive Zahlen
und M = m1 Âˇ m2 Âˇ Âˇ Âˇ mr . Dann gibt es fĂźr beliebig gewĂ¤hlte Zahlen a1 , . . . , ar genau eine Zahl x mit
0 6 x < M, die alle Kongruenzen x âĄ ai mod mi , i = 1, . . . , r simultan erfĂźllt. Die LĂśsung ist durch
x := a1 M1 s1 + a2 M2 s2 + Âˇ Âˇ Âˇ + ar Mr sr
gegeben, wobei Mi := M/mi und si = Miâ1 mod mi die multiplikative Inverse von Mi modulo mi ist.
Beweis. Setze Mi := M/mi und beachte, dass ggT(mi , Mi ) = 1 fĂźr alle i = 1, . . . , r gilt.8 Also hat
(nach dem Satz von BĂŠzout) jedes Mi die multiplikative Inverse
si = Miâ1 mod mi
modulo mi . Wir setzen
x := a1 M1 s1 + a2 M2 s2 + Âˇ Âˇ Âˇ + ar Mr sr
und ĂźberprĂźfen, ob es die obigen Kongruenzen erfĂźllt. Zuerst stellen wir fest, dass fĂźr i , j stets mi
ein Teiler von M j ist, d.h. M j âĄ 0 mod mi . Daraus folgt fĂźr alle i
x âĄ 0 + Âˇ Âˇ Âˇ + 0 + ai Mi si + 0 Âˇ Âˇ Âˇ + 0 âĄ ai Âˇ 1 âĄ ai mod mi .
Zum Beweis der Eindeutigkeit nimmt man an, dass es zwei LĂśsungen 0 6 x < y < M gibt. Dann
sind mit Lemma 2.2 alle mi Teiler von y â x. Nach Voraussetzung (paarweise teilerfremd) ist auch M
Teiler von y â x, und folglich ist y = x.

â˛ Beispiel 2.21 : Gesucht ist eine LĂśsung des Gleichungssystems:

x âĄ 2 mod 3
x âĄ 3 mod 5
x âĄ 2 mod 7
Wir haben M = 3 Âˇ 5 Âˇ 7 = 105 und
M1 = 105/3 = 35 âĄ 2 mod 3

M2 = 105/5 = 21 âĄ 1 mod 5
M3 = 105/7 = 15 âĄ 1 mod 7
s1 = 2â1 mod 3 = 2
s2 = 1â1 mod 5 = 1
s3 = 1â1 mod 7 = 1
8Das folgt aus Lemma 2.8: Ist ggT(m, a) = ggT(m, b) = 1, so gilt ggT(m, ab) = 1. Die Zahl d = ggT(m, ab) muss m
wie auch ab teilen. Aus ggT(m, a) = ggT(m, b) = 1 und d | m folgt, dass ggT(d, a) = ggT(d, b) = 1 gelten muss. Da nach
Lemma 2.8 d beide Zahlen a und b teilen muss, kann d nicht grĂśĂer als 1 sein.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

74
Also ist die LĂśsung

a 1 M1 s 1

a 2 M2 s 2

a 3 M3 s 3

z }| { z }| { z }| {
x = 2 Âˇ 35 Âˇ 2 + 3 Âˇ 21 Âˇ 1 + 2 Âˇ 15 Âˇ 1 = 233 âĄ 23 mod 105.
In Anwendungen ist das folgende Korollar von Chinesischem Restsatz oft sehr nĂźtzlich.
Q
Korollar 2.22. Seien p1 , . . . , pr Primzahlen und M = ri=1 pi . Weiterhin seien a, b < M beliebige
ganze Zahlen. Gilt a âĄ b mod pi fĂźr alle i = 1, . . . , r, so gilt a = b.
Beweis. Da a < M ist, kann nach dem Chinesischem Restsatz nur eine Zahl x mit 0 6 x < M alle
Kongruenzen x âĄ a mod pi simultan erfĂźllen, nĂ¤hmlich die Zahl x = a selbst.

Bemerkung 2.23. Warum ist der Chinesischer Restsatz interessant? Einfach, weil man mit ihm groĂe
Zahlen mittels viel kleineren Zahlen eindeutig kodieren kann. Sind z.B. p, q teilerfremd und n = p Âˇ q,
dann ist

Zn â x 7â x mod p, x mod q â Z p Ă Zq

eine bijektive Abbildung. D.h. keine zwei Zahlen in Zn kĂśnnen kongruent modulo p und modulo q
sein!

2.5.1 Anwendung: Schneller Gleichheitstestâ
Zwei Personen an den Enden eines Nachrichtenskanals wollen zwei natĂźrliche Zahlen a, b 6 210.000
auf Gleichheit hin ĂźberprĂźfen. Um Ăbertragungsfehler zu vermeiden, mĂśchten sie die Zahlen nicht
vollstĂ¤ndig Ăźbermitteln.
Das folgende Verfahren erlaubt einen Vergleich der beiden Zahlen, dabei werden anstelle der 10.000
Bits einer Zahl nur k Âˇ 202 Bits gesendet. Wir werden sehen, dass schon fĂźr k = 1 das Verfahren
hĂśchste Sicherheit garantiert.
Algorithmus (Probabilistischer Gleichheitstest)
1. WĂ¤hle zufĂ¤llig Primzahlen p1 , . . . , pk zwischen 2100 und 2101 .
2. Ăbertrage die Zahlen pi und a mod pi fĂźr alle i = 1, . . . , k.
3. Falls a . b mod pi fĂźr ein i, gib âa , bâ aus.
4. Anderfalls treffe die Entscheidung âa = bâ.
Wie in RSA-Codes, setzt das Verfahren voraus, dass man sich die benĂśtigten Primzahlen leicht verschaffen kann. Darauf werden wir nicht angehen.
Wir wollen nun die Wahrscheinlichkeit schĂ¤tzen, dass das Verfahren zu einer Fehlentscheidung fĂźhrt.
Die Fehlentscheidung kann nur dann auftreten, wenn die Zahlen a und b verschieden sind und trotzdem a âĄ b mod pi fĂźr alle i = 1, . . . , k gilt. Sei P die Menge aller Primzahlen zwischen 2100 und
2101 :
P := {q : q ist eine Primzahl und 2100 < q 6 2101 }
ÂŠ 2003 by S. Jukna

2.6. GRUPPEN

75

Nach dem berĂźhmten Primzahlensatz gilt fĂźr die Anzahl Ď(x) aller Primzahlen q < x die asymptotische Formel Ď(x) âź x/ ln x. Zwischen 2100 und 2101 gibt es daher approximativ
|P| = Ď(2101 ) â Ď(2100 ) â

2101
2100
2100
â
â
â 99 Âˇ 1026
ln 2101 ln 2100 100 ln 2

solchen Primzahlen. Sei auch
P(a, b) := {q â P : a âĄ b mod q}.
Behauptung: Sind a, b 6 210.000 und a , b, so gilt: |P(a, b)| < 100.
Um die Behauptung zu beweisen, nehmen wir an, dass P(a, b) mindestens 100 Primzahlen q1 , . . . , q100
enthĂ¤lt. Aus dem Chinesischen Restsatz (siehe Korollar 2.22) folgt a âĄ b mod M, mit M = q1 Âˇ . . . Âˇ
q100 . Es gilt M > (2100 ) 100 = 210.000 , nach der Annahme (dass a, b 6 210.000 ) folgt daher a = b.
Eine Fehlentscheidung ist also nur mĂśglich, falls a , b und das Verfahren zufĂ¤lligerweise nur Primzahlen aus P(a, b) auswĂ¤hlt. Bei k-facher unabhĂ¤ngiger Wahl einer Primzahl ist die Fehlerwarscheinlichkeit also hĂśchstens


k

99
|P(a, b)| k
â
= 10â26Âˇk .
|P|
99 Âˇ 1026
Schon fĂźr k = 1 ist dies ein verschwindend kleiner Wert.

2.6 Gruppen
Wir haben bereits gesehen, dass man in Z p fĂźr beliebige Primzahlen p vernĂźnftig addieren/substrahieren
wie auch multiplizieren/dividieren kann. In diesem Fall sagt man, dass Z p ein âKĂśrperâ ist. Das Wort
âvernĂźnftigâ bedeutet hier, dass die Addition wie auch Multiplikation in Z p sogenannte âGruppenEigenschaftenâ haben.
Sei G eine nichtleere Menge und âŚ eine binĂ¤re Operation (so etwas wie eine Addition + oder Multiplikation Âˇ). Man sagt, dass âŚ eine binĂ¤re Operation auf G ist (oder, dass G abgeschlossen unter âŚ ist),
falls x âŚ y â G fĂźr alle x, y â G gilt. Die Operation âŚ ist:
-

assoziativ, falls (x âŚ y) âŚ z = x âŚ (y âŚ z);

-

kommutativ, falls x âŚ y = y âŚ x.

Ist G unter einer assoziativen Operation âŚ abgeschlossen, so nennt man (G, âŚ) Halbgruppe.

In vielen Strukturen gibt es ein sogennantes âneutralesâ Element e â M, das im gewissen Sinne
âmacht nichtsâ:
-

e â M ist ein neutrales Element, falls x âŚ e = e âŚ x = x.

Zum Beispiel in (N, +) und (Z, +) ist e = 0, wĂ¤hrend in (N, Âˇ) und (Z, Âˇ) ist e = 1.

Hat man ein neutrales Element e â M, so fragt man, ob es fĂźr jedes x â M seine âInverseâ gibt.
-

x â1 â M ist eine Inverse von x â M, falls x âŚ x â1 = x â1 âŚ x = e.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

76

Zum Beispiel in (Z, +) ist x â1 = âx und in (Q, Âˇ) ist x â1 = 1/x.
Definition:
Ist âŚ eine associative binĂ¤re Operation auf einer Menge G, so ist (G, âŚ)
eine Gruppe, falls es ein neutrales Element e gibt und jedes Element
x â G eine Inverse aâ1 â G hat. Ist die Operation âŚ auch kommutativ,
so nennt man die Gruppe kommutativ oder abelischa
a Niels Henrik

Abel, 1802â1829

â˛ Beispiel 2.24 : Sei n â Z, n > 1 und Zn = {0, 1, . . . , n â 1}, und seien + und Âˇ die Addition und die
Multiplikation modulo n.
- Ist (Zn , +) eine Gruppe? Ja, das ist eine abelische (d.h. kommutative) Gruppe mit neutralem
Element 0 und mit Inversen aâ1 = n â a.
- Ist (Zn , Âˇ) eine Gruppe? Nein, da z.B. 0 keine Inverse hat.
- Ist dann (Zn \ {0}, Âˇ) eine Gruppe? Nicht unbedingt! FĂźr n = 2 und n = 3 ist das eine abelische
Gruppe, aber fĂźr n = 4 nicht mehr:
Âˇ
1
2
3

1
1
2
3

2
2
0
2

3
3
2
1

Die Menge {1, 2, 3} ist nicht durch Âˇ abgeschlossen (2 Âˇ 2 mod 4 = 0 < Z4 \ {0}) und 2 hat keine
Inverse.
- (Z5 \ {0}, Âˇ) ist wiederum eine Gruppe:
Âˇ
1
2
3
4

1
1
2
3
4

2
2
4
1
3

3
3
1
4
2

4
4
3
2
1

Beachte, dass jede Zeile und jede Spalte in der Multiplikationstabelle eine Permutation der
Elemente Zn \ {0} = {1, 2, 3, 4} ist. Ist das ein Zufall? Man kann zeigen (tue das!), dass dies in
jeder endlichen Gruppe gilt.
Ist n > 1 keine Primzahl, also n = ab fĂźr a, b > 2, so ist (Zn \ {0}, Âˇ) mit der Multiplikation Âˇ modulo n
keine Gruppe mehr! Warum? Da aÂˇb = 0 mod n gilt und 0 nicht in Zn \{0} liegt. Trotzdem auch fĂźr zusammengesetzten9 Zahlen n kann man in Zn eine Teilmenge finden, die bereits eine Gruppenstruktur
hat. Diese Teilmenge ist uns bereits bekannt:
Zân := {a â Zn : a , 0 und ggT(a, n) = 1}
Satz 2.25. FĂźr jedes n > 1 ist (Zân , Âˇ) mit der Multiplikation modulo n eine kommutative Gruppe.
9Zusammengesetzt = nicht prim.
ÂŠ 2003 by S. Jukna

2.6. GRUPPEN

77

Beweis. Die Multiplikation Âˇ ist Ăśffensichtlich kommutativ, und 1 â Zân ist ein neutrales Element in
(Zân , Âˇ). Ausserdem, hat nach dem Satz von BĂŠzout jedes Element a â Zân seine multiplikative Inverse
aâ1 , die auch ein Element von Zân ist.10
Es bleibt also zu zeigen, dass die Menge Zân Ăźberhaupt unter der Multiplikation modulo n abgeschlossen ist.
Wir fĂźhren einen Widerspruchsbeweis durch. Seien a, b â Zân und nehmen wir an, dass a Âˇ b < Zân ,
d.h. ggT(a Âˇ b, n) > 1 ist. Dann gibt es eine Primzahl p mit p | n und p | a Âˇ b. Ausserdem muss
ggT(p, a) = 1 gelten, da andererfalls p die Zahl a teilen wĂźrde, was zusammen mit p | n einen
Widerspruch mit ggT(a, n) = 1 liefern wĂźrde. Da nun p | ab und ggT(p, a) = 1, so muss p | b gelten
(siehe Lemma 2.8). Aber zusammen mit p | n wiederum einen Widerspruch mit ggT(b, n) = 1.

Sei (G, âŚ) eine beliebige Gruppe. Eine Teilmenge U â G heiĂt Untergruppe von G, wenn (U, âŚ) eine
Gruppe ist.
â˛ Beispiel 2.26 : - FĂźr jede Gruppe G sind U = G und U = {e} Untergruppen.
- Sei m â N, dann bildet die Menge mZ = {x â Z : m|x} aller durch m teilbaren Zahlen eine
Untergruppe von (Z, +).
- Z und Q sind Untergruppen von (R, +).
- (Q \ {0}, Âˇ) ist Untergruppe von (R \ {0}, Âˇ).
- Betrachte die Gruppe Z10 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} mit der Addition + modulo 10. Dann ist
H = {0, 2, 4, 6, 8} eine Untergruppe von Z10 .
FĂźr jede Teilmenge U â G und fĂźr jedes Gruppenelement a â G wird die Menge
aU = {a âŚ u : u â U }
Nebenklasse (oder Linksnebenklasse) von a bezĂźglich U genannt.11
â˛ Beispiel 2.27 : Wir betrachten die Untergruppe U = 5Z := {5x : x â Z} der Gruppe (Z, +). Die
Nebenklassen 2+U = {. . . , â8, â3, 2, 7, 12, . . .} und 7+U sind gleich. Es gibt genau 5 voneinander
verschiedene Nebenklassen, nĂ¤mlich die Ăquivalenzklassen der Relation âĄ mod 5.
â˛ Beispiel 2.28 : In der Untergruppe U = {x â R : x > 0} der Gruppe (R \ {0}, Âˇ) gibt es genau zwei
voneinander verschiedene Nebenklassen, die Menge der positiven und die Menge der negativen
Zahlen.
Die wichtigste Eigenschaft der Nebenklassen ist, dass sie eine disjunkte Zerlegung von Gruppen darstellen.
Satz 2.29. Sei U eine Untergruppe der Gruppe (G, âŚ). Dann gehĂśrt jedes Element x â G zu genau
einer Nebenklasse von U.
10Warum?! Sei d = ggT(aâ1 , n). Aus a âŚ aâ1 = 1 + kn, d | aâ1 und d | n folgt d | 1, also d = 1 gelten muss.
11Die Rechtsnebenklasse von U ist als U a = {u âŚ a : u â U } definiert.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

78

Beweis. Da e â U, gehĂśrt jedes x â G zu xU. Es bleibt also zu zeigen, dass kein x zu zwei verschiedenen Nebenklassen gehĂśren kann.
Nehmen wir deshalb an, dass aU âŠ bU , â. Dann gilt aw = bv fĂźr geeignete w, v â U; also ist
a = bvw â1 und damit auch folgt
liegt in U
z }| {
a âŚ u = b âŚ v âŚ w â1 âŚ u

fĂźr alle u â G. Deswegen gilt aU â bU, und analog bU â aU.



Nun kĂśnnen wir einen der HauptsĂ¤tze der Gruppentheorie beweisen. Die Ordnung einer endlichen
Gruppe G ist die Anzahl |G| seiner Elemente. Der folgender Satz sagt, dass die Ordnung jeder(!)
Untergruppe ein Teiler der Gruppenordnung ist.
Satz von LaGrange:
Ist G eine endliche Gruppe und U eine Untergruppe von G, dann gilt
|G| = n Âˇ |U | wobei n die Anzahl der Nebenklassen von U ist.

Beweis. Seien a1U, . . . , an U alle verschiedene Nebenklassen von U. Nach Satz 2.29 sind je zwei
verschiedene Nebenklassen disjunkt und ihre Vereinigung a1U âŞ a2U âŞ . . . âŞ an U is gleich G. Nach
der KĂźrzungsregel12 gilt |aU | = |U | fĂźr jedes a â G, d.h. die Nebenklassen besitzen die gleiche
MĂ¤chtigkeit. Damit ist
|G| = |a1U âŞ a2U âŞ . . . âŞ an U | = |a1U | + |a2U | + . . . + |an U | = n Âˇ |U |.

Warum ist dieser Satz auch in der Praxis nĂźtzlich? Nur ein Beispiel: Angenomen, wir haben eine
Gruppe G und wollen ein âgutesâ Element in G finden. Nehmen wir auch an, dass die âschlechtenâ
Elemente eine Untergruppe U von G bilden. Dann wissen wir sofort, dass entweder U = G (keine
âgutenâ Elemente vorhanden sind) oder |U | 6 |G|/2 (mindestens die hĂ¤lfte der Elemente âgutâ sind!).
In diesem letzten Fall kĂśnnen wir einfach die Elemente aus G rein zufĂ¤llig (je mit Wahrscheinlichkeit
1/2) auswĂ¤hlen. Da die Menge der âgutenâ Elemente sehr dicht ist, werden wir uns ziemlich schnell
auf einen âgutenâ Element stoĂen.

2.6.1 Zyklische Gruppen
Sei (G, âŚ) eine Gruppe und a â G, dann ist a k die AbkĂźrzung fĂźr
kâmal
z
}|
{
a := a âŚ a âŚ . . . âŚ a
k

12a âŚ u1 = a âŚ u2 =â u1 = u2

ÂŠ 2003 by S. Jukna

2.6. GRUPPEN

79

Lemma 2.30. Ist (G, âŚ) eine endliche Gruppe und a â G, dann ist
Ha = {a, a2 , a3 , . . .}
eine Untergruppe von G.
Beweis. Weil es nur endlich viele Gruppenelemente gibt, muss in der Folge a, a2 , a3 , . . . ein Glied
mehrfach vorkommen, z.B. a i = a j mit i < j. Wegen der Gruppenregeln haben wir sofort e =
a i Âˇ (a i ) â1 = a i Âˇ (aâ1 ) i = a j Âˇ (aâ1 ) i = a jâi . AuĂerdem gilt: aâ1 = a jâiâ1 .

Die Anzahl |Ha | der Elemente in dieser Untergruppe heiĂt die Ordnung des Elements a. Die Gruppe
G heiĂt zyklisch, wenn es ein Element a â G mit Ha = G gibt; jedes derartige a heiĂt erzeugendes
Element (oder Erzeugendes) von G.
Satz 2.31. Jede endliche Gruppe (G, âŚ), deren Ordnung p = |G| eine Primzahl ist, ist zyklisch.
Beweis. Nimm ein beliebiges Element13 a â G, a , e, und betrachte die von a erzeugte zyklische
Untergruppe Ha . Nach Satz von Lagrange, muss |Ha | ein Teiler von p = |G| sein. Da aber p prim ist,
kann das nur dann sein, wenn |Ha | gleich 1 oder p ist. Aus a , e folgt |Ha | > 2, und damit auch
|Ha | = p. Da aber Ha â G, folgt die Behauptung: G = Ha .

Lemma 2.32. Ist (G, âŚ) eine endliche Gruppe und a â G, dann gilt: a |G | = e.
Beweis. Sei k = |Ha | die Ordnung von a â G. Da Ha eine Untergruppe von G ist, sagt uns der Satz
von Lagrange, dass |G|/k eine ganze Zahl sein muss. Dann gilt auch: a |G | = (a k ) |G |/k = e |G |/k =
e.

Sei
Zân := {a â Zn : a , 0 und ggT(a, n) = 1}.

Die Menge Zân bildet eine multiplikative Gruppe (siehe Satz 2.25) und sein Ordnung |Zân | ist die
berĂźhmte Euler-Funktion Ď(n), d.h.
Ď(n) = Anzahl der Zahlen in 1, 2, . . . , n â 1, die relativ prim zu n sind
Korollar 2.32 liefert uns sofort den folgenden Satz von Euler, der in Kryptographie benutzt wird (siehe
Abschnitt 2.4.1). Ein Spezialfall dieses Satzes mit m = p eine Primzahl, den kleinen Satz von Fermat,
haben wir bereits direkt bewiesen.
Korollar 2.33. (Euler) FĂźr jedes a â Zân gilt:
aĎ(n) âĄ 1 mod n
Zwei Strukturen (G, âŚ) und (H, â) heiĂen isomorph, falls es eine Bijektion f : G â H mit f (a âŚ b) =
f (a) â f (b) fĂźr alle a, b â G gibt.
13Warum ist G , {e}?
ÂŠ 2003 by S. Jukna

80

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

Satz 2.34. (Cayley) Zwei zyklische Gruppen sind genau dann isomorph, wenn sie den gleichen Ordnung besitzen.
Beweis. âââ Aus AnzahlgrĂźnden klar.
âââ: Es seien (G1 , âŚ) und (G2 , â) zwei zyklische Gruppen mit erzeugenden Elementen a â G1 und
b â G2 . Sei |G1 | = |G2 |. Wir definieren die Bijektion f : G1 â G2 mit f (a k ) := bk (k â Z). Seien
x, y â G1 . Dann existieren j, k â N mit a = a j und y = a k . Dann ist
f (x âŚ y) =
=

f (a j âŚ a k ) = f (a j+k ) = b j+k = b j â bk

f (a j ) â f (a k ) = f (x) â f (y).

Also sind G1 und G2 isomorph.





Als Korollar bekommen wir, dass (bis auf Isomorphie) die Gruppen (Zm , +) und (Z, +) die
einzigen zyklischen Gruppen sind!

2.7 Ringe und KĂśrper
Eine Menge F mit zwei binĂ¤ren Operationen + und Âˇ heiĂt KĂśrper, wenn
1. (F, +) und (F \ {0}, Âˇ) sind kommutative Gruppen und

2. Es gelten die Distributivgesetze: a Âˇ (b + c) = (a Âˇ b) + (a Âˇ c) und (a + b) Âˇ c = (a Âˇ b) + (a Âˇ c).
Zum Beispiel, (R, +, Âˇ) und (Q, +, Âˇ) sind KĂśrper. Aber beide diese KĂśrper sind unendlich. Gibt es
KĂśrpern mit endlich vielen Elementen? Ja, und sogar sehr viele!
Satz 2.35. Ist p prim, so ist (Z p , +, Âˇ) ein KĂśrper.
Beweis. (Z p , +) ist eine kommutative Gruppe: 0 â Z p ist das neutrale Element und die additive
Inverse von a â Z p ist p â a und.

Z p \ {0} ist auch eine kommutative Gruppe: 1 â Z p \ {0} ist das neutrale Element und aâ1 := a pâ2 ist
die multiplikative Inverse von a â aâ1 := a pâ2 . Warum? Da laut kleinem Satz von Fermat fĂźr jedes
a â Z p , a , 0 die Kongruenz a pâ1 âĄ 1 mod p gilt.

Ist q = pm eine Potenz einer Primzahl p, so gibt es ein KĂśrper mit q
Elementen. Dieser KĂśrper heiĂt Galois KĂśrper (engl. Galoisa field) und
ist mit GF (q) bezeichnet.
a Evariste Galois

1811â1832

Sei (K, +, Âˇ) ein KĂśrper. Falls es eine natĂźrliche Zahl n gibt, fĂźr die
1| + 1 +{zÂˇ Âˇ Âˇ + 1} = 0
n-mal

ÂŠ 2003 by S. Jukna

2.7. RINGE UND KĂRPER

81

gilt, heiĂt die kleinste solche Zahl die Charakteristik von K und wird mit char(K ) bezeichnet. Gibt es
kein solches n, so definiert man char(K ) = 0.
Nach dem Satz 2.35 kommt jede Primzahl als Charakteristik eines KĂśrpers vor. Inresanterweise gilt
auch die Umkehrung:
Satz 2.36. Die Charakteristik eines KĂśrpers ist stets 0 oder eine Primzahl.
Beweis. Angenommen, char(K ) = pq mit p, q , 1 und p, q â N. Dann gilt
0= 1
| + 1 +{zÂˇ Âˇ Âˇ + 1} = |(1 + 1 +{zÂˇ Âˇ Âˇ + 1)} Âˇ |(1 + 1 +{zÂˇ Âˇ Âˇ + 1)}
pq-Summanden
p-Summanden q-Summanden

Wir haben also, dass ab = 0 fĂźr zwei Elementen a, b â F \ {0} mit

a = (1 + 1 + Âˇ Âˇ Âˇ + 1) und b = (1 + 1 + Âˇ Âˇ Âˇ + 1)
|
{z
}
|
{z
}
p-Summanden
q-Summanden

gilt. Das bedeutet aber, dass die Menge F \ {0} nicht unter der Multiplikation abgeschlossen ist, d.h.
(F \ {0}, Âˇ) keine Gruppe ist. Ein Widerspruch.

Es gibt viele Strukturen (R, +, Âˇ), die âsehr Ă¤hnlichâ zu KĂśrpern sind mit einer Ausnahme: man kann
da nicht vernĂźnftig dividieren, d.h. (R\{0}, Âˇ) keine Gruppe ist. Ist aber die Multiplikation Âˇ mindestens
associativ, so nennt man dann (R, +, Âˇ) Ring. Ist Âˇ auch kommutativ, so heiĂt auch der Ring kommutativ.
Ein Ring, in dem die Regel a Âˇ b = 0 =â (a = 0 â¨ b = 0) gilt, heiĂt nullteilerfrei.14
â˛ Beispiel 2.37 : -

(Z, +, .) ist nullteilerfrei Ring, aber kein KĂśrper.

Sei m = ab eine natĂźrliche Zahl, die keine Primzahl ist, dann ist (Zm , +, Âˇ) ein kommutativer
Ring mit Eins, aber nicht nullteilerfrei: ab âĄ 0 mod m.

Sei (R, +, Âˇ) ein Ring und R R die Menge aller Funktionen von R nach R. Durch punktweise
Definition der Addition ( f â g)(x) = f (x) + g(x) und Multiplikation ( f â g)(x) = f (x) Âˇ g(x)
von Funktionen f , g â R R erhĂ¤lt (R R , â, â) die Struktur eines Rings, der Funktionenring
genannt wird.

Bemerkung 2.38. Ein kommutativer und nullteilerfreier Ring R ist ein âhĂźbscher Ringâ, er kann
zu einem KĂśrper (seinem QuotientenkĂśrper) erweitert werden (ganz analog wie man den Ring der
ganzen Zahlen Z zum KĂśrper der rationalen Zahlen Q erweitert): man betrachtet die Menge aller
geordneten Paare (a, b) â R Ă R mit b , 0 und schreibt sie formal als BrĂźche ab ; zwei BrĂźche ba und
c
d werden als gleich angesehen, wenn ad = bc.

2.7.1 Polynomring
Wir betrachten nun Polynome in der Variablen x mit rationalen Koeffizienten (oder allgemeiner mit
Koeffizienten in einem KĂśrper F). Ein Polynom f = f (x) Ăźber F ist gegeben durch einen Ausdruck
der Gestalt
f (x) = an x n + anâ1 x nâ1 + Âˇ Âˇ Âˇ + a1 x + Âˇ Âˇ Âˇ + a0
14Ein kommutativer (bezĂźglich der Multiplikation) nullteilerfreier Ring mit mindestens zwei Elementen heiĂt IntegritĂ¤tsbereich.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

82

mit n â N und ai â F, i = 0, 1, . . . , n.

Glieder ai x i mit dem Koeffizienten ai = 0 dĂźrfen aus der Summe weglassen bzw. zur Summe beliebig hinzufĂźgen werden. Wir betrachten also zwei Polynome als identisch, falls sie dieselben Glieder
haben, abgesehen von Summanden mit dem Koeffizient 0.
Der Grad grad( f ) eines Polynoms f (x) = an x n + anâ1 x nâ1 + Âˇ Âˇ Âˇ + a1 x + Âˇ Âˇ Âˇ + a0 ist die grĂśĂte
Zahl i, so dass ai , 0. Das zugehĂśrige ai bezeichnen wir als den Anfangskoeffizienten oder hĂśchsten
Koeffizienten von f .
Mit Polynomen kann man rechnen wie mit ganzen Zahlen. Zwei polynome f (x) = an x n + anâ1 x nâ1 +
Âˇ Âˇ Âˇ + a1 x + Âˇ Âˇ Âˇ + a0 und g(x) = bm x m + bmâ1 x mâ1 + Âˇ Âˇ Âˇ + b1 x + Âˇ Âˇ Âˇ + b0 lassen sich addieren
( f + g)(x) := (an + bn )x n + Âˇ Âˇ Âˇ (a1 + b1 )x + (a0 + b0 )
(o.B.d.A. kĂśnnen wir m = n annehmen) und multiplizieren
( f g)(x) := cn+m x n+m + cn+mâ1 x n+mâ1 + Âˇ Âˇ Âˇ + c1 x + Âˇ Âˇ Âˇ + c0

mit

ci :=

X

a j bk .

j+k=i

Damit bilden die Polynomen Ăźber F ein Polynomring, der mit F[x] bezeichnet ist.
Die Polynomringe F[x] haben viele Gemeinsamkeiten mit dem Ring Z der ganzen Zahlen. Insbesondere kann man ein Polynom p(x) des Grades n durch ein Polynom q(x) des Grades m 6 n mit Rest
dividieren.
â˛ Beispiel 2.39 : Seien f (x) = 3x 5 â 2x 4 + x 2 â 3x + 5 und g(x) = x 2 + 1. Dann ist:
3x5 â 2x4 + 0x3 + x2 â3x + 5
â3x5

â3x 3

x2 + 1
3x3 â 2x2 â 3 x + 3

â 2x4 â 3x3 + x2 â3x +5
+ 2x4

+2x 2
â 3x3+3x2 â3x +5
+ 3x3

+3x
3x 2

+5

â 3x 2

â3
2

Damit ist f (x) = q(x)g(x) + r (x) mit q(x) = 3x 2 â 2x 2 â 3x + 3 und dem Rest r (x) = 2.
Die Polynome kann man also genau wie ganze Zahlen mit dem Rest dividieren.
Satz 2.40. (Division mit Rest) Zu Polynomen f (x), g(x) , 0 existieren Polynome q(x), r (x), so
dass
f (x) = q(x)g(x) + r (x), mit grad(r) < grad(g) oder r (x) = 0.

ÂŠ 2003 by S. Jukna

2.7. RINGE UND KĂRPER

83

Beweis. Seien f , g , 0 Polynome vom Grade n und m, mit Anfangskoeffizienten an und bm . Falls
n > m, kĂśnnen wir g aus f hinausdividieren, also das Polynom f 1 (x) = f (x) â p(x) mit p(x) =
nâm g(x) bilden. Offenbar hat f einen kleineren Grad als f , da p(x) den Term a x n
f (x) â an bâ1
1
n
m x
enthĂ¤llt. Gilt grad( f 1 ) > m, so kann g aus f 1 ein weiteres Mal hinausdividiert werden. Dies lĂ¤Ăt sich
fortsetzen, bis ein Polynom r vom Grad kleiner m oder aber das Nullpolynom Ăźbrigbleibt.

Jedes Polynom p(x) bestimmt in natĂźrlicher Weise eine Funktion von F nach F, indem fĂźr x ein a â F
eingesetzt und der Ausdruck in F ausgewertet wird. Das Ergebnis dieser Auswertung wird mit p(a)
bezeichnet und Wert von p(x) an der Stelle a genannt.



Man beachte, dass zwischen Polynomen und Polynomfunktionen im Allgemeinen streng unterschieden werden muss, da verschiedene Polynome (z.B. x und x 3 Ăźber Z3 ) dieselbe Polynomfunktion darstellen kĂśnnen. Betrachtet man jedoch Polynome Ăźber unendlichen KĂśrpern, dann
stellen verschiedene Polynome auch immer verschiedene Polynomfunktionen dar.
Ein a â F heiĂt Nullstelle des Polynoms p(x), falls p(a) = 0.
Lemma 2.41. Genau dann ist a â F eine Nullstelle von p(x), wenn es ein Polynom q(x) mit p(x) =
q(x) Âˇ (x â a) gibt.
Beweis. Nach Satz 2.40 ist p(x) = q(x)(x â a) + r (x), wobei r = 0 oder grad(r) < grad(x â a) = 1
ist. In jedem Fall ist r = b mit b â F. Einsetzen x := a liefert r (a) = p(a) â q(a)(a â a) = 0, d.h.
b = 0 (der Rest r (x) ist ein Nullpolynom).


Korollar 2.42. Jedes Polynom p(x) , 0 vom Grad n kann hĂśchstens n verschiedene Nullstellen
haben.

Der Euklidische Algorithmus lĂ¤Ăt sich nun auch auf rationale Polynome anwenden. Da sich der Grad
der Restpolynome bei jeder Division verkleinert, bricht er nach endlich vielen Schritten ab, seine
Laufzeit ist durch den Grad des Polynoms g beschrĂ¤nkt. Wie fĂźr ganze Zahlen kĂśnnen wir also feststellen:
Zwei rationale Polynome f (x), g(x) , 0 besitzen einen grĂśĂten gemeinsamen Teiler d(x), und
es gibt rationale Polynome s(x) und t(x), so dass
d(x) = s(x) f (x) + t(x)g(x).
Eine wichtige Aufgabenstellung ist die sogenannte Interpolation von Polynomen, d.h. es soll ein Polynom gefunden werden, das an gegebenen Stellen gegebene Werte annimmt. Die wichtigste Anwendung besteht in der Approximation (AnnĂ¤hrung) von Funktionen durch Polynome. DarĂźber hinaus ist
die Interpoliation ein wichtiger Bestandteil fĂźr schnelle Algorithmen zur Polynommultiplikation.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

84

Satz 2.43. (Interpolationsformel von Lagrange) Seien n verschiedene Stellen a1 , a2 , . . . , an â F
und n Werte b1 , b2 , . . . , bn gegeben. Dann erfĂźllt das Polynom
p(x) :=

n
X
i=1

n
Y
x â aj
bi
ai â a j
j=1
j,i

die Bedingung p(ai ) = bi fĂźr alle 1 6 i 6 n. AuĂerdem ist p(x) das einzige Polynom vom Grad
6 n â 1, das diese Bedingung erfĂźllt.
Beweis. Die erste Aussage ist trivial. Es bleibt also nur die zweite Aussage (Eindeutigkeit) zu beweisen. Angenommen, es gibt ein polynom pâ˛ (x) vom Grad 6 n â 1 mit pâ˛ (ai ) = bi fĂźr alle 1 6 i 6 n.
Dann hat das Polynom q(x) := p(x) â pâ˛ (x) mindestens n verschiedenen Nullstellen a1 , . . . , an . Da
aber grad(q) 6 n â 1, kann das (laut Korolar 2.42) nur dann sein, wenn q(x) = 0 ein Nullpolynom
ist, d.h. nur wenn die Polynome p(x) und pâ˛ (x) gleich sind.


2.7.2 Komplexe Zahlenâ
Beim Rechnen mit reellen Zahlen stĂśĂt man auf das Problem, dass gewisse algebraische Gleichungen
â die einfachste solche Gleichung ist x 2 + 1 = 0 â in R keine LĂśsungen besitzten (das Quadrat
einer reellen Zahl kann nicht negativ sein). Anderseits gibt es Gleichungen mit rationalen, ja sogar
mit ganzzahligen Koeffizienten, die zwar keine LĂśsung in Q, wohl aber eine in R besitzen, wie zum
Beispiel x 2 â 2 = 0. Diese in Q nicht lĂśsbare Gleichung wird also lĂśsbar, indem man Q zu R erweitert.
Das legt den Gedanken nahe, dass vielleicht eine nochmalige Erweiterung von R zu einem noch
grĂśĂeren KĂśrper auch eine LĂśsung von x 2 + 1 = 0 (und mĂśglicherweise von weiteren, in R unlĂśsbaren
Gleichungen) fĂźhrt.
Man kann die Erweiterung von R bis C genauso konstruieren, wie die Erweiterung von Z bis Q
konstruiert ist. NĂ¤hmlich, man kann Q als die Menge aller Paare (a, b) mit a, b â Z und b , 0
betrachten mit der Adition
(a, b) + (a â˛, bâ˛ ) = (abâ˛ + a â˛ b, bbâ˛ )

oder

a a â˛ abâ˛ + a â˛ b
=
+
b bâ˛
bbâ˛

oder

a a â˛ aa â˛
Âˇ
= â˛
b bâ˛
bb

und der Multiplication
(a, b) Âˇ (a â˛, bâ˛ ) = (aa â˛, bbâ˛ )

Analog kann man C als die Menge R Ă R mit der VerknĂźpfungen
(a, b) + (a â˛, bâ˛ ) = (a + a â˛, b + bâ˛ )
(a, b) Âˇ (a â˛, bâ˛ ) = (aa â˛ â bbâ˛ , abâ˛ + a â˛ b)
definieren. Man kann dann zeigen, dass C ein KĂśrper ist mit
1 = (1, 0)
als neutralem Element hinsichtlich Multiplikation: (1, 0) Âˇ (a, b) = (a, b). Da (0, 1) Âˇ (0, 1) = (â1, 0) =
â1, ist
i = (0, 1)
ÂŠ 2003 by S. Jukna

2.8. ALLGEMEINE VEKTORRĂUME

85

die LĂśsung von x 2 = 1 in C. D.h. das Paar i = (0, 1) bezeichnet die âZahlâ
â
i = â1.
Komplexe Zahlen sind also Paare z = (a, b) von reelen Zahlen. Ein solches Paar schreibt man auch
in der Form
z = a + bi
mit
i = (0, 1)
und nennt dies die Normalform von z. Jede komplexe Zahl z = a + bi wird also eindeutig durch die
beiden reellen Zahlen a und b beschreiben. Man nennt a den Realteil von z und b den ImaginĂ¤rteil,
kurz a = Re z und b = Im z.
Der Grund, warum die komplexen Zahlen gut sind, erklĂ¤rt der folgender Satz.
Satz 2.44. (Fundamentalsatz der Algebra) Sind a0 , a1 , . . . , an komplexe Zahlen, an , 0, so ist die
Polynomgleichung a0 + a1 x + a2 x 2 + . . . + an x n = 0 in C stets lĂśsbar.
Ein Beweis fĂźr den Fundamentalsatz war schon GauĂ bekannt. Der Beweis ist aber nicht einfach, und
wir verzichten auf ihm.

2.8 Allgemeine VektorrĂ¤ume
In Kapitel 5 werden wir sehen, wie man mit Vektoren Ăźber einem KĂśrper F rechnen kann. Es gibt
aber auch andere Strukturen, die nicht Teilmengen von Fn sind und trotzdem dieselben Eigenschaften,
wie VektorrĂ¤ume V â Rn , haben. Deshalb lĂśhnt es, einen allgemeineren Begriff des âVektorrĂ¤umsâ
kennenzulernen.
Sei F = (F, +, Âˇ, 0, 1) ein KĂśrper, wobei 0 das neutrale Element der Gruppe (F, +) und 1 das neutrale
Element der Gruppe (Fâ , Âˇ) ist.

Ein Vektorraum Ăźber den KĂśrper F ist eine additive abelische Gruppe G = (V, +, 0), die mit einer
Operation F Ă V â (Îť, v) â Îťv â V (âMultiplikation mit Skalarâ) versehen ist, fĂźr die folgende
Axiome fĂźr alle Îť, Âľ â F und u, v â V gelten:
(a) (Îť Âˇ Âľ)v = Îť(Âľv)
(b) Îť(u+v) = Îťu+Îťv (beide Additionen in G)
(c) (Îť + Âľ)v = Îťu+Îťv (die erste Addition in F, die zweite n G)
(d) 1v = v
Die Elemente von V heiĂen dann Vektoren.
In anderen WĂśrtern, ein Vektorraum ist eine Menge, deren Elemente sich addieren und mit Skalar
multiplizieren lassen, wobei die Summe von Vektoren und das Vielfache eines Vektors wieder Elemente der Menge sind. Die Elemente so eines Vektorraumes sind (heiĂen) Vektoren.
Hier sind drei Standardbeispiele fĂźr VektorrĂ¤ume.
1. Der Ăźblicher Vektorraum Fn von (richtigen) Vektoren mit komponentenweise Addition und Skalarmultiplikation, wie oben.
ÂŠ 2003 by S. Jukna

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

86

2. Es sei Pn die Menge aller reellen Polynome vom Grade hĂśchstens n. Jedes Polynom p mit
p(x) = an x n + anâ1 x nâ1 + . . . + a1 x + a0
wird durch seine Koeffizienten an , anâ1 , . . . , a0 reprĂ¤sentiert, also durch ein (n + 1)-Tupel
(an , anâ1 , . . . , a0 ) von reellen Zahlen. Umgekehrt entspricht jedem solchen Tupel von reellen Zahlen genau ein Polynom vom Grade hĂśchstens n. Zwei Polynome aus Pn kann man addieren und
mit einer reeler Zahl nultiplizieren. Damit bildet Pn einen Vektorraum Ăźber dem KĂśrper der reellen
Zahlen.
3. Es sei F ein KĂśrper, X eine (beliebige) Menge und sei F X die Menge aller Funktionen f : X â F
mit der Addition
( f + g)(x) := f (x) + f (y)
und der Skalarmultiplikation mit einer reellen Zahl Îť gemĂ¤Ă
Îť f (x) := Îť Âˇ f (x)

âx â X.

Man Ăźberzeugt sich sofort, dass F X Vektorraum Ăźber dem KĂśrper F ist.
Alle Konzepte â wie lineare UnabhĂ¤ngigkeit, Basis und Dimension, die wir im Kapitel 5 nur fĂźr VektorrĂ¤ume V â Fn betrachten werden â kann man auch auf allgemeine VektrorĂ¤ume direkt Ăźbertragen!

2.9 Aufgaben
2. 1. Zeige: FĂźr beliebige ganze Zahlen a, b, c gilt ggT(a, b â ca) = ggT(a, b).

2. 2. Zeige, dass n4 + 4 fĂźr n > 1 keine Primzahl ist. Hinweis: Stelle n4 + 4 als ein Produkt von zwei Summen
dar.
P
âi ist durch 3 teilbar genau dann,
2. 3. Zeige: Eine ganze Zahl n â Z in Dezimaldarstellung n = â
i=0 a i 10
Pâ
wenn i=0 ai durch 3 teilbar ist.
2. 4. Beweise oder widerlege: Ist s âĄ r mod m, so gilt a s âĄ a r mod m.
2. 5. Gilt Lemma 2.11 auch wenn ggT(a, m) > 1?
2. 6. Zeige folgendes: Sind die Zahlen m und n durch d teilbar, so ist auch ggT(m, n) durch d teilbar. Fazit:
der grĂśĂter gemeinsamer Teiler ggT(m, n) ist nicht nur der âgrĂśĂterâ im BezĂźg seiner GrĂśĂe; er ist auch der
âgrĂśĂterâ in dem Sinne, dass jeder gemeinsamer Teiler d von m, n muss auch ggT(m, n) teilen! Hinweis:
Satz 2.6.
2. 7. Zeige, dass auch die Umkehrung von Lemma 2.11 (KĂźrzungsregel) gilt: ax âĄ ay mod m â x âĄ y mod m
fĂźr alle x, y â Zm gilt, so mĂźssen a und m teilerfremd sein.
2. 8. Zeige, dass es in (Z6 , +, Âˇ) (modulo 6) Elemente a , 0 gibt, die keine multiplikative Inverse haben.
2. 9. Zeige, dass a â Zm ein Nullteiler15 genau dann ist, wenn ggT(a, m) > 1 gilt.

2. 10. Fibonacci-Zahlen f 0 , f 1 , f 3 , . . . sind rekursiv wie folgt definiert: f 0 = 0, f 1 = 1 und f n+2 = f n+1 + f n fĂźr
n > 0. Zeige, dass fĂźr n > 2 jede zwei aufeinander folgende Fibonacci-Zahlen f n und f n+1 teilerfremd sind,
d.h. ggT( f n , f n+1 ) = 1 gilt.
2. 11. Leite den Satz von BĂŠzout aus dem Satz 2.6 ab.
15a â Zm ist ein Nulteiler, wenn a , 0 und es ein x â Zm mit x , 0 und a Âˇ x âĄ 0 mod m gibt.
ÂŠ 2003 by S. Jukna

2.9. AUFGABEN

87

2. 12. Sei Ď(n) die Eulersche Funktion, d.h. Ď(n) = |Zân | wobei
Zân = {a â Zn : a , 0 und ggT(a, n) = 1}
Zeige:
(i) Ist p prim, so gilt Ď(p) = p â 1.

(ii) FĂźr jede Primzahl p und alle k â N gilt Ď(pk ) = pkâ1 (p â 1).

(iii) Sind p, q verschiedene Primzahlen, so gilt Ď(pq) = Ď(p)Ď(q). Hinweis: Um Ď(n) zu bestimmen, lĂśhnt es
sich oft zuerst die Anzahl der Zahlen in Zn , die nicht teilerfremd mit n sind, zu bestimmen.
2. 13. Sei p > 2 eine ungerade Primzahl. Zeige:
pâ1
X
i=1

i pâ1 âĄ â1 mod p und

pâ1
X
i=1

i p âĄ 0 mod p.

2. 14. (Wie alt Daisy von Donald eigentlich ist?) LĂśse das folgende modulare Gleichungssystem:
x
x
x

âĄ 2 mod 3
âĄ 2 mod 5
âĄ 5 mod 7

2. 15. Seien 1 6 a , b 6 N zwei ganze Zahlen. Wieviele Primzahlen p mit p > M mit a âĄ b( mod p) kann
dann es geben? Hinweis: Chinesicher Restsatz.
2. 16. Zeige folgendes:
1. In einer Halbgruppe (H, âŚ) gibt es hĂśchstens ein neutrales Element e. (Also ist das neutrale Element einer
Gruppe eindeutig.)
2. In einem Monoid (M, âŚ) hat jedes Element a â M hĂśchstens ein Inverses. (D.h. in einer Gruppe sind die
Inversen eindeutig.)
3. In einer Gruppe (G, âŚ) ist fĂźr alle a, b â G das Inverse (a âŚ b) â1 = a â1 âŚ bâ1 .
4. In einer Gruppe gilt die KĂźrzungsregel, d.h. man kann von links und von rechts âkĂźrzenâ.
5. In einer Halbgruppe (H, âŚ) gilt das allgemeine AssoziativitĂ¤tsgesetz: Seien a1 , a2 , . . . , a n â H. Dann liefert
jede sinvolle Beklammereung von a1 âŚ a2 âŚ . . . âŚ a n denselben Wert. Hinweis: Zeige mit Induktion Ăźber n,
dass die Menge Pn aller Werte der verschiedenen Beklammerungen von a1 âŚ a2 âŚ . . . âŚ a n genau ein Element
enhĂ¤lt.
2. 17. (Putnam Exam, 1971) Sei M eine Menge und âŚ eine binĂ¤re Operation auf M mit folgenden zwei Eigenschaften
(x âŚ y) âŚ z = (y âŚ z) âŚ x
(â)
und
xâŚx=x

(ââ)

fĂźr alle x, y, z â M. Zeige, dass dann âŚ auch kommutativ ist.

2. 18. Sei (G, âŚ) eine multiplikative Gruppe mit a 2 = e fĂźr alle a â G. Zeige, dass dann âŚ auch kommutativ
sein muss.
2. 19. Zwei Gruppen (G, âŚ) und (H, â) sind isomorph, falls es eine bijektive Abbildung f : G â H mit
f (a âŚ b) = f (a) â f (b) fĂźr alle a, b â G gibt.
Zeige, dass die Gruppen (R+ , Âˇ) und (R, +) isomorph sind. Hinweis: Betrachte die Logarithmus-Funktion ln x.
2. 20.

ÂŠ 2003 by S. Jukna

88

KAPITEL 2. ALGEBRA UND ELEMENTARE ZAHLENTHEORIE

1. Zeige: (N, âŚ) mit a âŚ b = a b ist keine Halbgruppe.
2. Ist die Potenzmenge einer Menge eine Gruppe bezĂźglich Vereinigung (oder bezĂźglich Schnitt)?
3. Sei M eine endliche Menge, SM die Menge aller bijektiven Abbildungen von M auf M und bezeichne âŚ
die Komposition von Abbildungen. Dann ist (SM , âŚ) eine Gruppe mit der identischen Abbildung id M als
neutralem Element. Diese Gruppe ist genau dann abelisch, wenn M nicht meht als 2 Elemente hat.
2. 21. Zeige, dass in einem Ring (R, +, Âˇ) gilt: 0 Âˇ a = 0 = a Âˇ 0 und (âa) Âˇ b = â(a Âˇ b) = a Âˇ (âb). Hinweis:
0 + 0 = 0.
2. 22. Zeige, dass jeder KĂśrper nullteilerfrei ist, d.h. ab = 0 â a = 0 â¨ b = 0.
â
â
2. 23. Zeige, dass {a + b 2 : a, b â Q} ein KĂśrper ist. Hinweis: a 2 â 2b2 , 0, da 2 irrational ist.
2. 24. Sei (R, +, Âˇ) ein kommutativer Ring (d.h. a Âˇ b = b Âˇ a fĂźr alle a, b â R) mit |R| > 2, in dem die Gleichung
a Âˇ x = b fĂźr alle a, b â R, a , 0 eine LĂśsung hat. Zeige, dass dann R ein KĂśrper ist.

2. 25. Man kann Ringe bis zum KĂśrper erweitern, wie auch KĂśrper bis zum anderen (grĂśĂeren) KĂśrper erweitern.
1. Erweiterung von Z bis Q. Betrachte Q als die Menge aller Paare (a, b) mit a, b â Z und b , 0 mit der
Adition
a a â˛ abâ˛ + a â˛ b
(a, b) + (a â˛, bâ˛ ) = (abâ˛ + a â˛ b, bbâ˛ )
oder
+
=
b bâ˛
bbâ˛
und der Multiplikation
(a, b) Âˇ (a â˛, bâ˛ ) = (aa â˛, bbâ˛ )

oder

a a â˛ aa â˛
Âˇ
= â˛
b bâ˛
bb

Zeige, dass dann Q ein KĂśrper ist.
2. Erweiterung von R bis C. Betrachte C als die Menge R Ă R mit der VerknĂźpfungen
(a, b) + (a â˛, bâ˛ )
(a, b) Âˇ (a â˛, bâ˛ )

=
=

(a + a â˛, b + bâ˛ )
(aa â˛ â bbâ˛, abâ˛ + a â˛ b).

Zeige, dass dann C ein KĂśrper ist mit 1 = (1, 0) als neutralem Element hinsichtlich Multiplikation ist.
Wie sieht dann die LĂśsung von x 2 = 1 in C aus?

ÂŠ 2003 by S. Jukna

Kapitel 3

Einschub aus der Analysis
Contents
3.1
3.2

Endliche Folgen und Reihen . . . . . . . . . . . . . .
Unendlicher Folgen . . . . . . . . . . . . . . . . . . .
3.2.1 Konvergenzkriterien fĂźr Folgen . . . . . . . . .
3.2.2 Bestimmung des Grenzwertes . . . . . . . . . .
3.3 Unendliche Reihen . . . . . . . . . . . . . . . . . . . .
3.3.1 Konvergenzkriterien fĂźr Reihen . . . . . . . . .
3.3.2 Anwendung: Warum Familiennamen aussterben?
3.3.3 Umordnungssatz . . . . . . . . . . . . . . . . .
3.4 Grenzwerte bei Funktionen . . . . . . . . . . . . . . .
3.5 Differentiation . . . . . . . . . . . . . . . . . . . . . .
3.6 MittelwertsĂ¤tze der Differentialrechnung . . . . . . .
3.7 Approximation durch Polynome: Taylorentwicklung .
3.8 Extremalstellen . . . . . . . . . . . . . . . . . . . . .
3.9 Die Bachmann-Landau-Notation: klein o und groĂ O
3.10 Rekurrenzenâ . . . . . . . . . . . . . . . . . . . . . .
3.10.1 Das Master Theorem . . . . . . . . . . . . . . .
3.11 Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

89
98
101
104
106
112
117
119
123
124
126
129
131
132
140
149
151

3.1 Endliche Folgen und Reihen
Eine Folge (engl. progression) ist eine Funktion f : N â R, deren Definitionsbereich gleich der
Menge der natĂźrlichen Zahlen N ist. D.h. eine Folge ist eine (potentiel unendliche) Folge reeller
Zahlen
f (0), f (1), f (2), . . . , f (n), . . .
GewĂśhnlich wird eine Folge f in der Form
<an > = a0 , a1 , a2 , . . .
89

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

90

einfach aufgeschrieben, also als Abfolge der Folgenglieder an = f (n). Der Funktionswert an heiĂt
in diesem Zusammenhang auch das n-te Folgenglied der Folge. Man kann eine bestimmte Folge auf
zwei Arten beschreiben:
1. Durch eine explizite Definition: Man gibt eine Formel an, aus der man jedes Glied sofort berechnen
kann, z.B. an = n2 .
2. Durch eine rekursive Definition: Zuerst gibt man das erste Glied a0 (oder a1 ) der Folge an, dann
gibt man zusĂ¤tzlich eine Formel an, mit der man das Glied an+1 aus dem Glied an (oder aus den
Gliedern a0 , a1 , . . . , an ) berechnen kann.
â˛ Beispiel 3.1 : Das Rad des Theodorus (griechischer Gelehrter, 465 v.Chr.) ist eines der ersten Beispiele einer Rekursion.
â â â Die Konstruktion trĂ¤gt seinen Namen, weil er mit ihrer Hilfe erstmals
bewies, dass 3, 5, 7, . . . irrationale Zahlen sind.
Dieses Rad kann durch einen rekursiven Algorithmus gebildet werden:

Die Bildungsregel lautet folgendermaĂen:
D1 Rechtwinkliges Dreieck mit SeitenlĂ¤nge 1
D2 Die Hypotenuse von D1 ist ein Schenkel. Der andere Schenkel besitzt die LĂ¤nge 1.
D3 Die Hypotenuse von D2 ist ein Schenkel. Der andere Schenkel besitzt die LĂ¤nge 1.
D4 Die Hypotenuse von D3 ist ein Schenkel. Der andere Schenkel besitzt die LĂ¤nge 1.
...
Dn Die Hypotenuse von Dnâ1 ist ein Schenkel. Der andere Schenkel besitzt die LĂ¤nge 1.
In mathematischer Notation sieht die rekursive Bildungsregel so aus:
â
a1 =
2
q
an+1 =
a2n + 1
Gegeben sei eine Folge <an > = a1 , a2 , a3 , . . .. Dann nennt man die Folge <Sn > = S1 , S2 , S3 , . . ., deren
Elemente Sn nach der Vorschrift
Sn = a1 + a2 + a3 + . . . + an =

n
X

ai

i=1

ÂŠ 2003 by S. Jukna

3.1. ENDLICHE FOLGEN UND REIHEN

91

gebildet werden, die Reihe <Sn > der Folge <an >. Prominente Reihen sind:
Arithmetische Reihe:
n
X

k = 1 + 2 + 3 . . . + n.

k=1

Geometische Reihe: 1

n
X

xk = 1 + x + x2 + . . . + x n .

k=0

Harmonische Reihe:
n
X
1
1 1
1
= 1+ + +...+ .
k
2 3
n
k=1

Pn
Wie kann man eine âgeschlosseneâ Form fĂźr die Funktion Sn : N â R mit Sn = k=0
ak finden?
Es gibt zwar eine sogenannte Theorie der erzeugenden Funktionen, die in vielen FĂ¤llen (aber nicht
immer!) eine solche geschlossene Form fĂźr Sn finden lĂ¤sst, das ist aber ein zeitaufwĂ¤ndiges Thema
und wir werden diese Theorie nicht betrachten. Stattdessen werden wir uns auf ein Paar wichtiger
Regeln beschrĂ¤nken.

Arithmetische Reihe
Zuerst betrachten wir die arithmetische Reihe Sn = 1 + 2 + . . . + n. Eine einfache (aber sehr kluge)
Idee2 ist die Reihenfolge von Zahlen umzukehren und die beiden Reihen aufzuaddieren:
Sn
+
Sn

=

1

+

2

+

3

+ ... +

nâ1

+

n

=

n

+

nâ1

+

nâ2

+ ... +

2

+

1

2Sn

= (n + 1) + (n + 1) + (n + 1) + . . . + (n + 1) + (n + 1)

Dies ergibt 2 Âˇ Sn = n(n + 1) und wir haben eine nĂźtzliche Formel bewiesen:
Satz 3.2. (Arithmetische Reihe)
Sn =

n
X

k=

k=1

n(n + 1)
.
2

(3.1)

1Der Grund, warum die Folge ak = x k eine âgeometrische Folgeâ heiĂt, ist dass der Betrag jedes Folgengliedes (fĂźr
n > 2) das geometrische Mittel der beiden Nachbarn ist:
a n+1
an
=
an
a nâ1

Âˇ a n a nâ1

â
â a n+1 a nâ1 = a2n â |a n | = a n+1 a nâ1 .
2Dieser Trick hat GauĂ als Kind erfunden. Um eine Ruhepause in der Klasse zu bekommen, hat der Lehrer die folgende
Aufgabe gestellt: Wie groĂ ist die Summe S = 1 + 2 + . . . + 100 der ersten 100 Zahlen. Leider hat der Lehrer Pech gehabt:
Bereits nach wenigen Minuten hat der kleine GauĂ die Antwort 5050 gegeben.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

92

Geometrische Reihe
Der folgender Trick ist als Verschiebungs-Trick bekannt: Ist Sn =
Summe (â) in der Gleichung
(â)

Sn + an+1

Pn

k=0

ak , dann versuche die rechte

z }| {
n
X
= a0 +
ak+1
k=0

mittels Sn darstellen.
Um diesen Trick zu demonstrieren, betrachten wir die geometrische Reihe:
Sn =

n
X

xk

k=0

Dann ist
Sn

Sn + x n+1 = 1 +

n
X

x k+1

k=0

z }| {
n
X
=1+xÂˇ
x k = 1 + x Âˇ Sn
k=0

und wir erhalten:
Sn Âˇ (x â 1) = x n+1 â 1
Damit haben wir eine weitere nĂźtzliche Formel bewiesen: 3
Satz 3.3. (Geometrische Reihe)
n
X

xk =

k=0

x n+1 â 1 1 â x n+1
=
xâ1
1âx

fĂźr x , 1

(3.2)

Manchmal lohnt es sich zu probieren, das n-te Folgenglied an als die Differenz bn+1 â bn von zwei
aufeinanderefolgenden Folgenglieder einer anderen Folge bn darzustellen.
Teleskop-Trick: Gegeben sei Sn =

t
X

ak mit s 6 t. Bestimme eine Folge <bk >, so dass fĂźr alle k

k=s

gilt: ak = bk+1 â bk . Dann gilt: Sn = bt +1 â bs .
Beweis.
Sn = as + as+1 + as+2 + . . . + at â1 + at

= (bs+1 â bs ) + (bs+2 â bs+1 ) + (bs+3 â bs+2 ) + . . . + (bt â bt â1 )

= âbs + (bs+1 â bs+1 ) + (bs+2 â bs+2 ) + (bs+3 â bs+3 ) + . . . + (bt â bt ) + bt +1
= bt +1 â bs


3Sie sollten sich diese Formel gut merken â sie taucht in vielen Situationen immer wieder auf!
ÂŠ 2003 by S. Jukna

3.1. ENDLICHE FOLGEN UND REIHEN

93

â˛ Beispiel 3.4 : Wir wenden den Teleskop-Trick auf
Sn :=

n
X
k=1

an. Da

1
k (k + 1)

1
1
1
= â
,
k (k + 1)
k k+1

erhalten wir
Teleskopsumme
  }| 

{
1 1
1 1
1 1
1
1
1
Sn =
â
+
â
+
â
+...+
â
=1â
.
1 2
2 3
3 4
n n+1
n+1
Pn
x k mit x , 0 an. Gesucht sind bk mit
â˛ Beispiel 3.5 : Wir wenden den Teleskop-Trick auf k=0
k
k
x = bk+1 â bk fĂźr alle k > 0. Ansatz: bk = f (k)x . Dann gilt:
z





x k = f (k + 1)x k+1 â f (k)x k ââ 1 = f (k + 1)x â f (k)
Der Ansatz f (k) = f (k + 1) ergibt f (k) =
n
X
k=0

1
xâ1

x k = bn+1 â b0 =

und damit bk =

xk
xâ1 .

Also gilt:

x n+1
1
x n+1 â 1
â
=
.
xâ1 xâ1
xâ1

â˛ Beispiel 3.6 : (Ratenzahlung) Im tĂ¤glichen Leben hat man meist nicht nur Einmalzahlungen zu
leisten, sondern hĂ¤ufig werden gleiche BeitrĂ¤ge R in regelmĂ¤Ăigen AbstĂ¤nden (Zeiteinheiten wie
Monat, Vierteljahr, Jahr, usw) ein- oder ausgezahlt (Raten, Renten, Pensionen, usw.). Das eingezahlte Geld wird wieder mit x% verzinst.
Bei der nachschĂźssigen Rente erfolgt die Zahlung R am Ende des Zeiteinheits, bei der vorschĂźssigen Rente dagegen zu Beginn der Zeiteinheit. In Zusammenhang mit Renten interessiert man
sich vor allem fĂźr den Gesamtwert, unter BerĂźcksichtigung von Zinseszins, den eine Rente am
Anfang bzw. Ende der Ratenzahlungen hat. Hierbei kommt es darauf an, ob die Rente nach- oder
vorschĂźssig ist. Denn bei nachschĂźssiger Zahlung einer n-maligen Rente wird die erste Rentenzahlung nur (n â 1)-mal verzinst, bei vorschĂźssiger aber n-mal.
Nehmen wir an, Sie zahlen monatlich am Anfang des Monats (vorschĂźssige Zahlung) den Betrag
R auf ein Konto ein und das Geld werde wieder mit x% verzinst (z.B. ein Festgeldkonto). So
haben Sie am Ende des ersten Monats
x
K1 = R Âˇ q
mit
q := 1 +
.
100
Am Ende des zweiten Monats
K2 =

RÂˇq
|{z}

Geld von diesem Monat

+

Geld vom Vormonat

und allgemeiner am Ende der n-ten Monats
Kn

R Âˇ q2
| {z }

= Rq + Rq2 + . . . + Rq n
= R Âˇ (q + q2 + . . . + q n )

= Rq(1 + q + q2 + . . . + q nâ1 )

mit q := 1 +

x
100
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

94
Bei der nachschĂźssigen Zahlungen ist
Kn

= R + Rq + Rq2 . . . + Rq nâ1
= R(1 + q + q2 + . . . + q nâ1 )

Falls zu Beginn des Zeitraums auch noch ein Anfangskapital K0 vorliegt (wie oft bei RatensparvertrĂ¤gen), so ergeben sich als Endwerte entsprechend
Kn
Kn

qn â 1
qâ1
n
q â1
= K0 q n + R
qâ1
= K0 q n + Rq

(vorschĂźssiger Zahlung)
(nachschĂźssiger Zahlung)

Angenommen, Sie schlieĂen mit Ihrer Bank einen Ratensparvertrag Ăźber 10 Jahre und zu einem
ZinsfuĂ von x = 6% ab. Zu begin des ersten Jahres zahlen Sie einen Einmalbetrag von 1500 e ein
und anschlieĂend jeweils am Ende des Jahres eine Rate von 200 e. Welchen Wert hat das Kapital
nach 10 Jahren?
Da es sich um nachschĂźssige Ratenzahlung handelt, ergibt sich als Endwert
K10 = 1500 Âˇ 1, 0610 + 200 Âˇ

1, 0610 â 1
â 5.322 e
1, 06 â 1

Das von Ihnen eingezahlte Kapital betrĂ¤gt 1500 + 10 Âˇ 200 = 3.500 e. Der Rest stammt aus den
Zinseszinsen.
LĂśst man die Rentenendwertformeln (ohne Anfangskapital) nach R auf, so kann man bestimmen,
welche jĂ¤hrliche Rate zu zahlen ist, um bei einem ZinsfuĂ von x% nach n Jahren ein gewĂźnschtes
x
Kapital K n zu erhalten. Mit dem Aufzinsungsfaktor q = 1 + 100
ergeben sich bei nach- und
vorschĂźssiger Zahlung:
R = (K n â K0 q n ) Âˇ

qâ1
K n â K0 q n q â 1
und
R
=
Âˇ n
qn â 1
q
q â1

â˛ Beispiel 3.7 : FĂźr ihre Wohnung haben Sie sich endlich den neuen Fernseher mit Videorekorder fĂźr
A e gekauft. Bei einem monatlichen Zinssatz von x% zahlen Sie fĂźr n Monate pro Monat R e
zurĂźck und haben dann alles getilgt.
Frage: Wieviel mĂźssen Sie monatlich bezahlen, d.h. wie groĂ ist R?
Um dieses Problem zu behandeln, machen wir eine Art doppelte BuchfĂźhrung, nĂ¤mlich das verzinste Darlehen bei der Bank sowie die Gesamtsumme der gezahlten BetrĂ¤ge. Stichtag ist jeweis
der 1. Tag im Monat nach der Einzahlung.

wobei wieder q = 1 +

1. Monat
2. Monat
3. Monat
..
.

Darlehen
A
Aq
Aq2
..
.

eingezahlte BetrĂ¤ge
R
Rq + R
Rq2 + Rq + R
..
.

k-ter Monat

Aqkâ1

Rqkâ1 + . . . + Rq + R

x
100 .
ÂŠ 2003 by S. Jukna

3.1. ENDLICHE FOLGEN UND REIHEN

95

Wenn nach n Monaten alles bezahlt ist, muss
Aq nâ1 = R(q nâ1 + . . . + Rq + R) = R

qn â 1
qâ1

gelten. Also ist
R= AÂˇ
ihr monatlich zu bezahlender Betrag.

q n â q nâ1
qn â 1

6
x
Als Beispiel betrachten wir A = 1000 e und n = 24 bei x = 12
= 0, 5; also q = 1 + 100
= 1, 005.
Dann mĂźssen Sie monatlich R = 44, 10 â e einzahlen. In dieser Zeit haben Sie also R Âˇ 24 =
1058, â e gezahlt.

â˛ Beispiel 3.8 : (Jahresrenten) Sie gewinnen in einem Lotto 600.000 e. Der Lotterieinhaber macht
Ihnen einen Vorschlag: statt diesen Betrag bereits jetzt bar auszuzahlen, kĂśnnen Sie 20 Jahre
lang jedes Jahr 50.000 e ausgezahlt bekommen. In 20 Jahren hĂ¤tten Sie also auf Ihrem Konto
1.000.000 e (eine Million!) WĂźrden Sie einen solchen Vorschlag annehmen, wenn eine Ihnen
bekannte Bank4 8% Zinsen fĂźr Ihr Kapital jĂ¤hrlich (mit Garante!) bezahlen bereit ist?
Der Grund, warum der Zinssatz p hier wichtig ist, ist folgender. Wenn wir 10 e heute mit Zinssatz
p anlegen, so werden wir im nĂ¤chsten Jahr (1 + p) Âˇ 10 = 10, 80 e haben, (1 + p) 2 Âˇ 10 â 11, 66 e
in zwei Jahren, usw. Oder anders gesagt, 10 e , die sie im nĂ¤chsten Jahr bekommen, sind heute
nur 1/(1 + p) Âˇ 10 â 9, 26 e wert. Der Grund: wenn wir heute 9, 26 e anlegen, so haben wir in
einem Jahr (1 + p) Âˇ 9, 26 = 10 e.
Die Auszahlung m = 50.000 e am Anfang des ersten Jahres ist natĂźrlich auch m Euro wert. Aber
die Anzahlung am Anfang des nĂ¤chsten Jahres ist nur m/(1 + p) wert, die am Anfang des dritten
Jahres ist nur m/(1 + p) 2 wert, usw. und die am Anfang des n-Jahres ist nur m/(1 + p) nâ1 wert.
Die gesamte Auszahlung ist also in Wirklichkeit (betrachtet von heute) nur
V (n) =

n
X
i=1

m
(1 + p) iâ1

(3.3)

wert mit p = 0, 08. Setzen wir x := 1/(1 + p), so erhalten wir
V (n) = m Âˇ

nâ1
X

xj

j=0

1 â xn
= mÂˇ
1 âx
= mÂˇ

= mÂˇ

1â
1

(nach (3.2))
n

1
1+p
1
â 1+p

1+pâ



1
1+p

p

nâ1

.

FĂźr m = 50.000 e, n = 20 und p = 0, 08 ergibt sich V â 530.180, â e.
4Um der RealitĂ¤t nahe zu kommen, sollte man einen Zinssatz p mit 3% 6 p 6 5% annehmen. Und mit der Garantie ist
auch so eine Sache ...
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

96

Harmonische Reihe
Pn
Im Allgemeinen sind die Summen Sn =
k=0 ak sehr schwer exakt zu bestimmen. Andererseits
reicht es in vielen FĂ¤llen insbesondere in der Laufzeitanalyse von Algorithmen, nur eine vernĂźnftige
AbschĂ¤tzung fĂźr Sn zu finden.
Als Beispiel betrachten wir die Teilsummen der harmonischen Reihe:
n

1 1
1 X1
.
Hn := 1 + + + . . . + =
2 3
n
k

(3.4)

k=1

Es ist fĂźr die Summe Hn keine geschlossenen Form bekannt, die sie vereinfacht. Wir schĂ¤tzen nun
Hn gegen den Logarithmus ab. DafĂźr teilen wir die Summanden in PĂ¤ckchen auf und zwar setzen wir
Pk :=

n 1
1
1
1 o
,
,
.
.
.
,
,
2kâ1 2kâ1 + 1 2kâ1 + 2
2k â 1

D.h.
P1 = {1}, P2 =

n1 1o
n1 1 1 1o
, , P3 =
, , , , ...
2 3
4 5 6 7

Ist K die Anzahl der vollen PĂ¤ckchen, so muss 2K â1 6 n < 2K â 1 gelten, woraus
log2 (n + 1) < K 6 log2 n + 1
folgt. Ein volles PĂ¤ckchen Pk enthĂ¤lt |Pk | = 2k â 2kâ1 = 2kâ1 Zahlen. Die grĂśĂte Zahl in Pk ist
die kleinste ist 2k1â1 und |Pk | = 2kâ1 . Hieraus schlieĂen wir (fĂźr jedes volles PĂ¤ckchen)

1
,
2k â1

X
1
1
1
= |Pk | Âˇ k <
x 6 |Pk | Âˇ kâ1 = 1
2
2
2
x âP
k

Aufsummiert erhalten wir
K
K
+1
X
X
1
1
Âˇ log2 (n + 1) =
< Hn 6
1 = log2 n + 2.
2
2
k=1

k=1

Genauer kann man sogar zeigen, dass
ln n < Hn 6 ln n + 1.
In gewissem Sinne ist diese AbschĂ¤tzung nicht wesentlich schĂ¤rfer als die oben angegebene. Der
natĂźrliche Logarithmus ist ein konstantes Vielfaches des Zweierlogarithmus und beide AbschĂ¤tzungen
sagen aus, dass die Summen Hn âasymptotischâ wie ein Logarithmus wachsen:
Satz 3.9. (Harmonische Reihe)
ln n < Hn =

n
X
1
6 ln n + 1
k

(3.5)

k=1

ÂŠ 2003 by S. Jukna

3.1. ENDLICHE FOLGEN UND REIHEN

97

â˛ Beispiel 3.10 : (Laufzeit vom Quicksort Algorithmus) Die (erwartete) Laufzeit von Quicksort Algorithmus kann man durch
nâ1 X
n
X
2
T (n) =
j âi +1
i=1 j=i+1

darstellen. Nun wollen wir diesen Ausdruck vereinfachen.

T (n)

=

n X
n
X

i=1 j=i+1
n
X

6 2Âˇ

n X
n
n nâi+1
X
X
X 1
2
1
=2Âˇ
=2Âˇ
j âi +1
j âi +1
j
i=1 j=i+1

Hn

i=1

j=2

(Hn die harmonische Reihe)

i=1

= 2n Âˇ Hn 6 3(n ln n)
Der folgender Satz ermĂśglicht es, endliche Summen durch Integrale abzuschĂ¤tzen. Eine Funktion
F (x) heiĂt Stammfunktion der Funktion f (x), wenn F â˛ (x) = f (x) fĂźr alle x gilt. Der Hauptsatz der
Rb
Differential- und Integralrechnung besagt, dass dann a f (t)dt = F (b) â F (a) gilt.
Satz 3.11. (Integral-Kriterium) Sei F (x) eine Stammfunktion von f : R â R.
(a) Wenn f monoton wachsend ist, dann gilt
F (n) â F (m â 1) 6

n
X

f (i) 6 F (n + 1) â F (m).

n
X

f (i) 6 F (n) â F (m â 1).

i=m

(b) Wenn f monoton fallend ist, dann gilt
F (n + 1) â F (m) 6

i=m

Beweis. Wir verifizieren nur Teil (a). Teil (b) folgt mit analogem Argument. Da f monoton wachsend
Z i+1
ist, gilt
f (x) dx 6 f (i + 1), denn die FlĂ¤che unter der Kurve f (x), zwischen i und i + 1, ist
i

nach oben beschrĂ¤nkt durch die FlĂ¤che des Rechtecks zwischen i und i + 1 mit der HĂśhe f (i + 1). Die
FlĂ¤che dieses Rechtecks ist f (i + 1) Âˇ 1 = f (i + 1).

Es ist also
F (n) â F (m â 1) =

Z

n

f (x) dx =
mâ1

nâ1 Z
X

i=mâ1

i+1

f (x) dx 6
i

nâ1
X

i=mâ1

f (i + 1) =

n
X

f (i).

i=m

Analog erhalten wir
f (i) 6

Z

i+1

f (x) dx,
i
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

98

1

1

i

i+1

j

(a)

Abbildung 3.1: Fall (a)

R i+1
i

j+1

(b)

f (x) dx 6 f (i + 1) und Fall (b) f ( j) 6

R j+1
j

f (x) dx

denn f ist monoton wachsend und damit
n
X
i=m

f (i) 6

n Z
X
i=m

i+1

f (x) dx =
i

Z

n+1
m

f (x) dx = F (n + 1) â F (m).


Pn
a
â˛ Beispiel 3.12 : Wir betrachten die Reihe Sn :=
k=1 k fĂźr a , â1. (FĂźr a = â1 das ist die
harmonische Reihe und wir haben bereits sie duch das Logarithmus abgeschĂ¤tzt.) Sei F (x) =
x a+1 /(a + 1). Da F (x) â˛ = x a , ist F eine Stammfunktion von x a und wir erhalten als Konsequenz:
na+1 6 Sn Âˇ (a + 1) 6 (n + 1) a+1

3.2 Unendlicher Folgen
Wir wollen das Verhalten einer unendlichen Folge
<an > = a0 , a1 , a2 , . . .
betrachten. Insbesondere wollen wir wissen, ob sich die Folge irgendwan stabilisiert, d.h. ob es eine
Zahl a â R gibt, so dass fĂźr jede (beliebig kleine) Zahl ÇŤ > 0 âfast alleâ Glieder an um hĂśchstens ÂąÇŤ
von a entfernt sind. âFast alle Gliederâ heiĂt hier âalle auĂer vielleicht endlich vielen ersten Gliedernâ.
Gilt dies, so sagt man, dass die Folge gegen a konvergiert (oder strebt), und nennt diese Zahl den
Grenzwert von <an >. D.h. die Folge <an > strebt gegen a, wenn die folgende Aussage gilt:
âÇŤ > 0 âN â N ân > N :

|an â a| < ÇŤ

(3.6)
ÂŠ 2003 by S. Jukna

3.2. UNENDLICHER FOLGEN

99

Îľ

a

N

Um die sprachlichen Formulierungen zu vereinfachen, vereinbaren wir nun, dass die Aussage
fĂźr fast alle n gilt P(n)
bedeutet, dass
Eigenschaft P(n) fĂźr alle, bis auf endlich viele n gilt
Also konvergiert eine Folge <an > gegen a genau dann, wenn fast alle Folgenglieder in der ÇŤ-Umgebung
UÇŤ (a) := {x â R : |x â a| < ÇŤ } von a liegen und ÇŤ > 0 kann beliebig klein gemacht werden. FĂźr die
Konvergenz einer Folge sind also nur die âhinteren Gliederâ verantwortlich, was am Anfang passiert
ist egal.
Wiederum anders formuliert: Eine Folge konvergiert gegen einen Grenzwert, falls es
zu jeder Toleranzgrenze ÇŤ > 0 einen Schwellenwert N â N gibt, ab dem die Folge die vorgegebebe
Genauigkeit |an â a| < ÇŤ erzielt.
Die Negation der obigen Aussage (3.6) ist
âÇŤ > 0 âN â N ân > N :

|an â a| > ÇŤ

D.h. die Folge <an > strebt nicht gegen a, wenn es eine Ungebung UÇŤ (a) gibt, so dass unendlich viele
Folgenglieder auĂerhalb dieser Umgebung liegen.
Diese Begriffsbestimmung (Konvergenz) ist die Grundlage der Analysis! Seine Bedeutung beruht
darauf, dass viele GrĂśĂen nicht durch einen in endlich vielen Schritten exakt berechenbaren Ausdruck
gegeben, sondern nur mit beliebiger Genauigkeit approximiert werden kĂśnnen.
Eine wichtige Eigenschaft konvergierenden Folgen ist, dass der Grenzwert eindeutig bestimmt ist.
Behauptung 3.13. Jeder konvergente Folge <an > hat genau einen Grenzwert a. Dieser Grenzwert
wird mit a = lim an bezeichnet.
nââ

Beweis. Sind a und a â˛ Grenzwerte ein und derselben Folge <an > und a , a â˛, dann wĂ¤hlen wir
â˛|
ÇŤ := |aâa
2 . Ab einem Schwellenwert N â N liegen alle Folgenglieder a n mit n > N in der
ÇŤ-Umgebung von a. Es gibt auch einen Schwellenwert N â˛, ab dem alle Folgenglieder in der ÇŤUmgebung von a â˛ liegen. Ab dem grĂśĂeren der beiden Schwellenwerte mĂźssen die Folgenglieder
also im Durchschnitt der beiden ÇŤ-Umgebungen liegen. Der Durchschnitt ist aber leer: Gebe es ein x
in beiden Umgebungen, so kĂ¤me man zu der unsinnigen AbschĂ¤tzung:
|a â a â˛ | = |a â x + x â a â˛ | 6 |x â a| + |x â a â˛ | < ÇŤ + ÇŤ = |a â a â˛ |.

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

100

Im Umgang mit dem Begriff der Konvergenz benutzt man ein wichtiges Prinzip, das als das Archimedische Prinzip bekannt ist:
Zu jeder reellen Zahl x â R gibt es ein n â N mit x < n.
Insbesondere erlaubt dieses Prinzip zu zeigen, dass eine Folge <an > eine Nullfolge ist, d.h. gegen 0
konvergiert. Der Grund, warum Nullfolgen wichtig sind, ist offensichtlich: lim an = a genau dann,
nââ
wenn <an â a> eine Nullfolge ist.
â˛ Beispiel 3.14 : (Das fundamentale Beispiel) Die Folge an = 1/n, n > 1, ist eine Nullfolge:
lim (1/n) = 0.
nââ

Beweis: Sei ÇŤ > 0. Dann gibt es eine natĂźrliche Zahl N mit N > 1/ÇŤ. Wir wĂ¤hlen ein solches N
und erhalten fĂźr alle n > N ebenfalls n > 1/ÇŤ und daher |1/n â 0| = 1/n < ÇŤ.
â˛ Beispiel 3.15 : Die Folge an = x n mit x â R und |x| < 1 ist eine Nullfolge, d.h. lim x n = 0 fĂźr
nââ
|x| < 1.

Beweis: Sei ÇŤ > 0 und q = 1/|x|. Da q > 1, gibt es es eine natĂźrliche Zahl N mit q N > 1ÇŤ .
Wir wĂ¤hlen eine solche Zahl N und erhalten fĂźr alle n > N ebenfalls q n > 1/ÇŤ und daher
|1/q n â 0| = 1/q n < ÇŤ.

Eine divergente Folge <an > ist âbĂśsartigâ, da sie keinen Grezwert besitzt, d.h. die Folge keinen
einzelnen âHĂ¤ufungspunktâ hat. Soll man sofort solche Folgen wegwerfen? Nicht unbedingt â es kann
sein, dass die Folge trotzdem âstabil genugâ ist, wenn sie nur wenige verschiedene âHĂ¤ufungspunkteâ
hat.
Ist <an > eine Folge und 0 6 n0 < n1 < n2 . . . eine unendliche Folge von natĂźrlichen Zahlen, dann
wird (an k ) = an0 , an1 , an2 , . . . eine Teilfolge von (an )n âN genannt.
Zahl a â R heiĂt HĂ¤ufungspunkt von <an >, wenn es eine Teilfolge <an k > gibt mit lim an k = a.
kââ

In anderen Worten, a ist ein HĂ¤ufungspunkt von <an >, wenn in jeder Umgebung von a unendlich
viele Folgenglieder liegen. 5 Der grĂśĂte HĂ¤ufungspunkt heiĂt Limes-Superior und wird als
lim sup an

oder

lim an

bezeichnet. Der kleinster HĂ¤ufungspunkt heiĂt Limes-Inferior und wird mit
lim inf an

oder

lim an

bezeichnet. Bei konvergenten Folgen stimmen beide Ăźberein: lim sup an = lim inf an .
â˛ Beispiel 3.16 : Betrachte die folge <an > mit an = (â1) n + n1 . Dann strebt die âgeradeâ Teilfolge
<a2k > gegen 1, wĂ¤hrend die âungeradeâ Teilfolge <a2k+1 > gegen â1 strebt. Die HĂ¤ufungspunkte
sind also lim sup an = 1 und lim inf an = â1.
Satz 3.17. (BolzanoâWeierstraĂ) Jede beschrĂ¤nkte Folge reeller Zahlen hat mindestens einen HĂ¤ufungspunkt.
5Zur Erinnerung: a ist ein Grenzwert von <a n >, wenn in jeder Umgebung von a fast alle (d.h. alle, bis auf endlich
viele) Folgenglieder liegen.
ÂŠ 2003 by S. Jukna

3.2. UNENDLICHER FOLGEN

101

Beweis. Ist die Folge <an > beschrĂ¤nkt, so gibt es zwei Zahlen a0 6 b0 mit der Eigenschaft, dass
fast alle Folgenglieder in dem Intervall [a0 , b0 ] liegen. Wir halbieren das Intervall [a0 , b0 ]. In einer der
beiden HĂ¤lften sind unendlich viele Folgenglieder, die eine Teilfolge von <an > bilden. Diese HĂ¤lfte
sei [a1 , b1 ]. Setzen wir das fort, so erhalten wir eine Intervallschachtelung mit gemeinsamen Punkt
h. Dies h ist HĂ¤ufungspunkt. Ist nĂ¤mlich ÇŤ > 0 gegeben, so gibt es ein N mit |a N â bN | < ÇŤ und
h â [a N , bN ]. Nach Konstruktion sind in [a N , bN ] â (h â ÇŤ, h + ÇŤ unendlich viele Folgenglieder. 

3.2.1 Konvergenzkriterien fĂźr Folgen
NatĂźrlich mĂśchten wir den Grenzwert einer Folge bestimmen. Aber zuerst ist zu klĂ¤ren, ob die Folge
Ăźberhaupt konvergiert und hier helfen uns sogenannte Konvergenzkriterien.
Die Folge <an > heiĂt beschĂ¤nkt, falls es ein L â R mit |an | 6 L fĂźr alle n gibt.
Es ist klar, dass unbeschrĂ¤nkte Folgen keinen (endlichen) Grenzwert haben kĂśnnen. Also sind solche
Folgen besonders âbĂśsartigâ und man kann solche Folgen sofort weg werfen. Was aber wenn eine Folge beschrĂ¤nkt ist? Dann muss sie auch nicht unbedingt einen Grenzwert haben: Nimm zum Beispiel
an = (â1) n .
Ist die Folge monoton fallend (an > an+1 fĂźr alle n) oder monoton wachsend (an 6 an+1 fĂźr alle n),
so kĂśnnen die Folgenglieder nicht hin und hier springen â sie kĂśnnen sich nur in einer Richtung (nur
nach oben oder nur nach unten) bewegen. NatĂźrlich, reicht die Monotonie alleine noch nicht: z.B. ist
die Folge an = n (streng) monoton wachsend aber lim an = â. Der Grund hier ist, dass diese Folge
unbeschrĂ¤nkt wĂ¤chst.
Beide Eigenschaften â Monotonie und BeschrĂ¤nkung â haben allein keinen bzw. nur einen geringen
Bezug zur Konvergenz, ihre Kombination ist aber Ăźberraschenderweise sehr mĂ¤chtig und liefert ein
oft benutztes Konvergenzkriterium.
Satz 3.18. (Monotonie-Kriterium) Ist die Folge <an > monoton, so gilt:
<an > ist kovergent

ââ

<an > ist beschrĂ¤nkt

Beweis. (â): Sei a0 6 a1 6 a2 6 . . . eine monoton wachsende und beschrĂ¤nkte Folge (der Fall
einer monoton fallender Folge ist analog). Dann gibt es eine obere Schranke L â R mit an 6 L fĂźr
alle n und sogar eine kleinste obere Schranke a mit der Eigenschaft: 6 fĂźr jedes ÇŤ > 0 gibt es ein
N = N (ÇŤ ) mit a â a N < ÇŤ. Damit gilt fĂźr alle n > N:
<ÇŤ

>0

z }| { z }| {
|a â an | = a â an = (a â a N ) â (an â a N ) < ÇŤ.

Da dies fĂźr beliebig kleines ÇŤ > 0 gilt, strebt <an â a> gegen 0 und deshalb muss an gegen a streben.
(â) Dieser Richtung gilt fĂźr beliebige (nicht nur fĂźr monotone) Folgen: Ist die Folge <an > konvergent, so ist sie beschrĂ¤nkt. Sei lim an = a. Dann gibt es N mit |an â a| < 1, d.h. a â 1 < an < a + 1
fĂźr alle n > N. Setzen wir b = max{a â 1, a + 1}, so ist |an | 6 b fĂźr alle n > N. Damit ist aber auch

|an | 6 L fĂźr alle n, wenn wir L als L = b + |a0 | + . . . + |a N | nehmen.
6D.h. die Folgenglieder beliebig nahe zu a kommen.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

102

Es gibt auch ein paar anderen nĂźtzlichen Konvergenzkriterien.
Satz 3.19.
1. Majorantenkriterium fĂźr Nullfolgen: Ist lim an = 0 und |bn | 6 L Âˇ |an | fĂźr L > 0 und fast
nââ
alle n, dann gilt lim bn = 0. Die Folge <an > heiĂt dann die Majorante fĂźr die Folge <bn >.
nââ

2. Ist <an > beschrĂ¤nkt und lim bn = 0, dann gilt lim an Âˇ bn = 0.
nââ

nââ

3. Vergleichskriterium: Seien <bn > und <cn > zwei Folgen mit demselben Grenzwert b. Gilt
bn 6 an 6 cn fĂźr fast alle n, so gilt lim an = b.
nââ

Beweis. Zu 1: Sei ÇŤ > 0 beliebig. Da <|an |> eine Nullfolge ist, gibt es einen Schwellenwert N, so
dass |an | < ÇŤ/L fĂźr alle n > N. Dann gilt auch fĂźr alle n > N: |bn | 6 L Âˇ |an | < L Âˇ (ÇŤ/L) = ÇŤ.

Zu 2: Da <an > beschrĂ¤nkt ist, gibt es ein L > 0, so dass |an | 6 L gilt. Sei nun ÇŤ > 0 beliebig. Da
<bn > eine Nullfolge ist, gibt es einen Schwellenwert N, so dass |bn | < ÇŤ/L fĂźr alle n > N gilt. Dann
gilt auch fĂźr alle n > N: |an Âˇ bn | = |an | Âˇ |bn | < L Âˇ (ÇŤ/L) = ÇŤ.
Den Beweis des Vergleichskriteriums lassen wir als Ăbungsaufgabe.
â˛ Beispiel 3.20 : Zu zeigen:
lim

nââ

Dazu betrachten wir die Folge bn =
Satz 1.41) gilt fĂźr jedes n > 2:
n

n = (1 + bn ) =

â
n

n = 1.

(3.7)

n â 1 mit n > 1. Nach dem binomischen Lehrsatz (siehe

n  
X
n
i=0

â
n



i

bin

 
n 2 n(n â 1) 2
Âˇ bn
>
b =
2
2 n

und daraus durch Umstellung
b2n <

2
1
64Âˇ
nâ1
n

Da 1/n eine Nullfolge ist, ist auch lim bn = 0, woraus lim
Genauso kann man
lim

nââ

â
n

â
n

n = lim(1 + bn ) = 1 folgt.

nc = 1

(3.8)

fĂźr jedes c â R, c > 0 zeigen.
â
â
â˛ Beispiel 3.21 : Sei c > 1 eine ganze Zahl und betrachten die Folge an = n c. Dann gilt 1 6 n c 6
â
â
n
n, wenn n > c ist. Der Grenzwert von n n ist 1 (siehe das vorige Beispiel). Auf der linken Seite
â
haben wir eine 1, also ist der Grenzwert auch 1. Nach dem Vergleichskriterium gilt: lim n c = 1.
nââ

â˛ Beispiel 3.22 : Dieses Beispiel soll zeigen, dass man unter UmstĂ¤nden die Glieder einer Nullfolge
mit dem Gliedern einer unbeschrĂ¤nkten Folge multiplizieren darf, ohne die Nullfolgeneigenschaft
zu verlieren.
ÂŠ 2003 by S. Jukna

3.2. UNENDLICHER FOLGEN

103

Wir betrachten die beiden Zahlenfolgen <x n > = 1, x, x 2 , x 3 , . . . und <n> = 0, 1, 2, 3, . . . mit
1
0 < |x| < 1, und bilden daraus die Folge <an > mit an = nx n . Indem wir |x| als 1+q
fĂźr ein
q â R schreiben, folgt fĂźr n > 2:
|nx n | =
<

n
n


=
(1 + q) n 1 + n1 q + n2 q2 + . . . + q n
n
2
2
1

= 2 Âˇ
.
n 2 =
2
(n â 1)q
q nâ1
2 q
|{z}
L

1
Damit haben wir gezeigt, dass die Nullfolge ( nâ1
) eine Majorante fĂźr die Folge an = nx n darstellt, und das Majorantenkriterium fĂźr Nullfolgen sagt uns, dass auch <an > eine Nullfolge ist.
1
Die Folge < nâ1
> ist erst recht eine Majorante fĂźr die Folge <x n >, was uns das Nebenergebnis
liefert, dass <x n > fĂźr |x| < 1 eine Nullfolge darstellt.

Wenn man nicht zeigen kann, dass eine Folge konvergent ist, wie kann man dann zeigen, dass diese
Folge divergiert? Dazu kann man das folgende Kriterium benutzen.
Satz 3.23. (Teilfolgenkriterium) Wenn eine Folge gegen a konvergiert, dann konvergiert jede ihre
Teilfolge gegen a.
Beweis. Sei (an )n âN eine Folge und (an k )k âN ihre Teilfolge. Sei an â a und ÇŤ > 0 beliebig klein.
Dann gibt es ein m0 , so dass |an â a| < ÇŤ fĂźr alle n > m0 . WĂ¤hle k0 so, dass nk0 > m0 . Dann ist
nk > nk0 > m0 fĂźr alle k > k0 und damit auch |an k â a| < ÇŤ.

Wie kann man dieses Kriterium benutzen, um divergenz einer Folge <an > zu zeigen? In vielen FĂ¤len
reicht es eine monotone und unbeschrĂ¤nkte Teilfolge von <an > zu finden. Dann muss auch nach
Monotonie-Kriterium auch die Folge <an > selbts divergieren!
Konvergente Folgen nĂ¤hren sich ihrem Grenzwert beliebig nahe an. Das hat Konsequenzen fĂźr die
Beziehung der Folgenglieder untereinander: Auch sie mĂźssen beliebig nahe aneinander rĂźcken. Diese
Eigenschaft nennt man auch das Cauchy-Kriterium.
Eine Folge <an > reeller Zahlen heiĂt Cauchy-Folge, wenn
âÇŤ > 0 âN ân, m > N : |an â am | < ÇŤ.
Man nennt diese Eigenschaft âKonvergenz in sichâ. Informell: Egal wie klein die (von ÇŤ bestimmte)
Umgebung ist, kommt die Folge irgendwann mal hinein und bleibt fĂźr immer da.

Îľ

N

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

104

Cauchy-Kriterium
Eine Folge reeller Zahlen ist genau dann konvergent, wenn sie eine
Cauchy-Folge ist.



Die Behauptung âkonvergiert â Cauchy-Folgeâ ist trivial. Weniger trivial ist die andere
Richtung âCauchy-Folge â konvergiertâ: Es ist nicht sofort klar, warum sich eine beliebige,
hin und hier springende Cauchy-Folge irgendwann mal doch stabilisieren muss.
Beweis. (=â): Sei <an > konvergent mit Grenzwert a. Dann gibt es zu ÇŤ > 0 ein N = N (ÇŤ ) mit
|an â a| < ÇŤ fĂźr alle n > N. Es folgt
|an â am | 6 |an â a| + |a â am | < 2ÇŤ

fĂźr alle n, m > N (Dreiecksungleichung). Das is âCauchyâ mit 2ÇŤ fĂźr ÇŤ.
(â=): Sei <an > eine Cauchy-Folge. Dann gibt es N â N mit |an â am | < 1 fĂźr alle n, m > N. Damit
ist fĂźr alle n > N
|an | = |an â a N + a N | 6 |an â a N | + |a N | < 1 + |a N |
Setze s = max{|a0 |, |a1 |, . . . , |a N â1 |, 1 + |a N |}. Dann gilt |ak | 6 s fĂźr alle k â N, d.h. die Folge <an >
ist beschrĂ¤nkt. Damit hat die Folge <an > einen HĂ¤ufungspunkt b (nach dem Satz von Bolzanoâ
WeierstraĂ). Wir werden zeigen, dass b der Grenzwert von <an > ist.
Sei ÇŤ > 0 gegeben. Es gibt N â N mit |an âam | < ÇŤ/2 fĂźr alle n, m > N (Cauchy-Folge). Andererseits
(da b ein HĂ¤ufungspunkt ist) gibt es fĂźr jedes n m > n mit |am â b| < ÇŤ/2. Damit gilt fĂźr jedes n > N
|an â b| = |an â am + am â b| 6 |an â am | + |am â b| <

ÇŤ ÇŤ
+ = ÇŤ.
2 2




Vorsicht mit dem Cauchy-Kriterium: Aus der schwĂ¤cheren Eigenschaft |an+1 â an | < ÇŤ fĂźr
alle
Pn n >1 N (ÇŤ ) kann nicht auf die1Konvergenz von <an > geschlossen werden! So gilt etwa fĂźr
Hn = k=1
lim (Hn+1 â Hn ) = 0. Aber wir haben bereits
k wegen H n+1 â H n = n+1 sicherlich nââ
gezeigt (siehe 3.5), dass Hn > ln n gilt. D.h. die Folge Hn ist nicht beschrĂ¤nkt und somit (nach
Monotonie-Kriterium) ist sie divergent.

3.2.2 Bestimmung des Grenzwertes
Soll der Grenzwert einer Folge <an > bestimmt werden, so muss zunĂ¤chst die Konvergenz der Folge
gezeigt werden. Dazu benutzt man eins der oben genannten Konvergenzkriterien. Angenommen, a =
lim an existiert. Man muss nun a bestimmen. Wir beschreiben eine allgemeine Vorgehensweise.
nââ

Definition: Eine reelle Funktion f heiĂt stetig in a â R, falls fĂźr jede Folge <an > mit lim an = a
nââ
gilt: lim f (an ) = f (a), d.h. lim f (an ) = f ( lim an ).
nââ

nââ

nââ

ÂŠ 2003 by S. Jukna

3.2. UNENDLICHER FOLGEN

105

Intuitiv bedeutet die stetigkeit von f , dass die Funktion keinen Sprung in a macht. So ist zum Beispiel
die Funktion f : [0, 1] â {0, 1} mit

0 falls x â [0, 1)
f (x) =
1 falls x = 1
nicht stetig in x = 1. Die Funktion f (x) = x12 ist Ăźberall stetig, auĂer im Punkt x = 0. Zum Beispiel
stetig Ăźberall sind:
â
Polynome, e x , x, x a mit a > 0, n x sin(x), cos(x), usw.
Satz 3.24. Sei die Folge <an > rekursiv als an+1 = f (an ) gegeben, wobei f : R â R eine stetige
Funktion ist. Existiert der Grenzwert a = lim an , so gilt
nââ

f (a) = a.
D.h. der Grenzwert a ein Fixpunkt von f (x) ist.
Beweis.
stetigkeit

a = lim an = lim an+1 = lim f (an )
nââ

nââ

nââ

z}|{
=


lim an = f (a).

f

nââ



â
â˛ Beispiel 3.25 : Sei <an > durch a0 = 2 und an+1 = an gegeben. Die Folge ist beschrĂ¤nkt: 1 6
â
an 6 2 (Induktion). Da an > 1, gilt an+1 = an 6 an ; damit ist die Folge auch monoton
fallend. Mit dem Monotoniekriterium existiert a = lim an . Dann ist
nââ

a = lim an = lim an+1 = lim
nââ

nââ

nââ

â

stetigkeit

an

z}|{
=

q

lim an =

nââ

â

a,

also muss der Grenzwert entweder 0 oder 1 sein, da dies die einzigen LĂśsungen der Gleichung
â
a = a sind. Da alle Folgenglieder > 1 sind, kann 0 nicht Grenzwert sein, also gilt lim an = 1.
nââ

â˛ Beispiel 3.26 : (Existenz der Wurzel) Sei b â R, b > 0 fest. Wir betrachten die Folge <an > mit
a0 > 0 und


1
b
an+1 =
an +
2
an
â
Zu zeigen: lim an = b. Die Folge ist monoton fallend7 an > an+1 und beschrĂ¤nkt: Aus
nââ
a0 > 0 folgt an > 0 fĂźr alle n. Nach dem Monotoniekriterium existiert a = lim an . Dann ist mit
nââ





7Sei f (x) = 12 x + bx . Dann gilt f (x) 2 > b ââ x 4 + 2x 2 b + b2 > 4x 2 b ââ x 4 â 2x 2 b + b2 > 0 ââ
(x 2 â b) 2 > 0. Also ist f (x) 2 > b fĂźr alle x und somit auch a2n > b fĂźr alle n > 1. Deshalb gilt:
60

a n+1 â a n =

1
2



an +

b
an



z }| {
b â a2n
â an =
60
2a n

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

106
f (x) :=

1
2

x+

b
x



a

=

lim an = lim an+1 = lim f (an )

nââ

nââ

nââ

stetigkeit

z}|{
=
=


lim an = f (a)


1
b
a+
2
a
f

nââ

und damit gilt a2 = b.
HĂ¤ufig benutzt man bei der Bestimmung der Grenzwerte von Folgen nicht direkt die Definition, sondert fĂźhrt die Konvergenz nach gewissen Regeln auf schon bekannte Folgen zurĂźck. Dazu dienen die
folgenden Regeln (deren Ableitung Regel ist als Ăbungsaufgabe gestellt).
Grenzwertregeln:
Seien <an > und (bn ) konvergente Folgen mit lim an = a und lim bn = b, dann gilt
nââ

1.
2.
3.
4.
5.

nââ

lim (an + bn ) = a + b

nââ

lim (an Âˇ bn ) = a Âˇ b

Spezialfall: lim (c Âˇ an ) = c Âˇ a

nââ

nââ

an
a
lim
=
(falls b , 0 und bn , 0 fĂźr n > n0 )
nââ bn
b
lim |an | = |a|
nââ
â
â
lim |an | = |a|
nââ

3.3 Unendliche Reihen
Die zu einer unendlichen Folge <an > = a0 , a1 , a2 , . . . gehĂśrende Reihe ist dieP
Folge <Sn > mit Sn =
P
n
â
i=0 ai . Den Grenzwert der Teilsummen-Folge <Sn > bezeichnet man oft als
n=0 a n .



P
Diese (am hĂ¤ufigsten benutzte) Bezeichnung â â
n=0 a n â ist nur eine Verabredung â unendliche
Summen
als
solche
sind
nicht
definiert!
Hat
aber
die Folge <Sn > einen Grenzwert S, so kann
Pâ
man
bereits
a
=
S
schreiben.
Damit
versteht
man
dann, dass âdie Folge <Sn > mit Sn =
n=0 n
Pn
i=0 ai gegen S strebtâ. Also ist
â
X

an = die Bezeichnumg fĂźr lim

nââ

n=0

n
X

an .

n=0

â˛ Beispiel 3.27 : Betrachtet man die (alternierende) Folge an = (â1) n , so P
kann man nicht ohne weiteres die enstprechende Reihe als âunendliche Summeâ hinschreiben: â
n=0 a n = 1 â 1 + 1 â 1 +
1 â 1 + . . ., da man sonst âzeigenâ kĂśnnte, dass die Reihe gegen 0 konvergiert
(1 â 1) + (1 â 1) + (1 â 1) + . . . = 0
| {z } | {z } | {z }
0

0

0

ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

107

wie auch gegen 1
1 + (â1 + 1) + (â1 + 1) + (â1 + 1) + . . . = 1,
| {z } | {z } | {z }
0

0

0

was natĂźrlich
ist. Deshalb muss man die entsprechende Reihe als die Folge (Sn )
Pn ein Schwachsinn
n
mit Sn = n=0 (â1) betrachten. Dann haben wir: S0 = +1, S1 = 0, S2 = +1, S3 = 0, S4 = +1,
usw. â eine Folge, die offensichtlich divergiert: sie springt immer zwischen 1 und 0.
Aus dem Monotonie-Kriterium fĂźr Folgen (Satz 3.18) ergibt sich das folgende Konvergenzkriterium
fĂźr Reihen.
Satz 3.28.
Gilt 0 6 ak 6 bk fĂźr fast alle k und konvergiert die
Pâ
Pâ Majorantenkriterium fĂźr Reihen:
Reihe k=0 bk , so kovergiert auch die Reihe k=0 ak .
Beweis. Nach dem Monotonie-Kriterium fĂźr Folgen (Satz 3.18) ist die konvergente
Folge Bn =
P
Pn
n
b
beschrĂ¤nkt
(b
>
0).
Folglich
(da
a
6
b
)
ist
auch
die
Folge
A
=
a
k
k
k
n
k=0 k
k=0 k beschrĂ¤nkt
und damit muss sie auch konvergieren (widerum nach dem Monotonie-Kriterium fĂźr Folgen).


Geometrische Reihe
Wir wissen bereits, dass

n
X

xk =

k=0

1 â x n+1
1
x n+1
=
â
1âx
1âx 1âx

fĂźr beliebiges x , 1 gilt. Da fĂźr |x| < 1 die Folge an = x n eine Nullfolge ist (siehe Beispiel 3.15),
erhalten wir (mit der Benutzung der Grenzwertregeln):

Unendliche geometrische Reihe
â
X

xk =

k=0

1
1â x

falls |x| < 1

(3.9)

â˛ Beispiel 3.29 :
0, 999999999 . . . = 0, 9

â
X
k=0

(1/10) k = 0, 9 Âˇ

1
10
= 0, 9 Âˇ
= 1.
1 â (1/10)
9

â˛ Beispiel 3.30 :
â

X
1 1 1
1
1+ + + +... =
(1/2) k =
= 2.
2 4 8
1 â (1/2)
k=0

1â

1 1 1
+ â +... =
2 4 8

â
X
k=0

(â1/2) k =

1
= 2/3.
1 â (â1/2)
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

108
Wir betrachten die folgende Reihe:
n
X
i=1

i Âˇ x i = x + 2x 2 + 3x 3 + . . . + nx n .

Das ist keine geometrische Reihe, da der Quotient ai+1 /ai nicht konstant ist. In solchen FĂ¤llen kann
uns eine unserer alten Freundinnen â die Differenzierung â helfen. In unserem Fall kĂśnnen wir zum
Beispiel beide Seiten der uns schon bekannten Gleichung (3.2)
n
X

xi =

1 â x n+1
1âx

â˛

 1 â x n+1 â˛

i=0

differenzieren und erhalten

n
X
i=0

xi

=

1âx

Also folgt 8
n
X

ix iâ1 =

i=1

=
=

â(n + 1)x n (1 â x) â (â1)(1 â x n+1 )
(1 â x) 2
â(n + 1)x n + (n + 1)x n+1 + 1 â x n+1
(1 â x) 2
n
1 â (n + 1)x + nx n+1
.
(1 â x) 2

Die Differenzierung verĂ¤ndert die
von x in
Term. In unserem Fall haben wir eiPExponenten
Pjedem
n
n
i
iâ1
ne geschlossene Form nicht fĂźr i=1 ix sondern fĂźr i=1 ix
ausgerechnet (der i-te Term in der
letzten Summe ist x iâ1 , nicht x i ). Nichtdestotrotz ist die LĂśsung (mindestens in diesem Fall) ziemlich
einfach: multipliziere beide Seiten mit x. Dies ergibt:
n
X

ix i =

i=1

x â (n + 1)x n+1 + nx n+2
.
(1 â x) 2

(3.10)

Obwohl dieser Ausdruck auf dem ersten Blick nicht viel einfacher aussieht, hat er aber eine schĂśne
Eigenschaft: fĂźr beliebiges (aber festes) x â R mit |x| < 1 âstrebtâ die rechte Seite gegen x/(1 â x) 2 ,
wenn n â â (da fĂźr |x| < 1 gilt lim x n = 0). Damit haben wir noch eine nĂźtzliche Gleichung
nââ
erhalten:

Verallgemeinerte geometrische Reihe
â
X
k=0

8Zur Erinnerung: (x n ) â˛ = nx nâ1 und



k xk =

x
,
(1 â x) 2

falls |x| < 1

(3.11)


f (x) â˛
f (x) â˛ Âˇ g(x) â f (x) Âˇ g(x) â˛
=
.
g(x)
g(x) 2
ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

109

â˛ Beispiel 3.31 : (Unendliche Jahresrenten) Ein junger Familienvater hat einen Jackpot von
10.000.000 e gewonnen. Das ist schon eine Menge Geld, man kann damit was anfangen. Wir
nehmen wieder an, dass der Zinsenzatz bei p = 8% liegt (und zwar garantiert). Der Mann ist aber
ein guter Familienvater und will, dass von diesem Geld nicht nur er, sondern seine Kinder, Enkelkinder usw. profitieren kĂśnten. Der Lottoinhaber weiĂ das und macht einen Vorschlag: statt diesen
Betrag bereits jetzt bar auszuzahlen, will er (und spĂ¤ter seine Nachfolger) fĂźr alle k = 1, 2, . . . (bis
zu unendlich!) am Ende k-tes Jahres einen Betrag von k Âˇ m Euro mit m = 50.000 e zahlen. D.h.
im 1. Jahr bekomt der Vater 50.000 e, im 2. Jahr bereits 100.000 e, im 3. Jahr 150.000 e usw. Ab
dem 10. Jahr soll der Familienvater bereits eine halbe Million und weiter noch mehr (jĂ¤hrlich!)
bekommen, ab dem 20. Jahr bereits eine runde Million und mehr (wiederum jĂ¤hrlich) bekommen
usw. Falls dem Vater etwas pasieren sollte, werden die Kinder jedes Jahr mehr als eine Million
Euro pro Jahr bekommen, und spĂ¤tere Enkelkinder sollten mehrere Millionen jĂ¤hrlich ausbezahlt
bekommen. So rechnet der Mann: schon die Enkelkinder sollten viel mehr jĂ¤hrlich bekommen,
als er von heutigem Gewinn fĂźr Zukunft verteilen kĂśnnte. Klingt sehr attraktiv: der Betrag kann
unbeschrĂ¤nkt wachsen â ein Paradies fĂźr seine Nachfolger! Es ist doch schwer zu glauben, dass
der Familienvater besser die heutigen 10.000.000 e + Zinseszins wĂ¤hlen sollte. Ist das so?
Die Antwort ist: Der Familienvater sollte das Angebot besser ablehnen. In unserem Fall hat die
gesamte Auszahlung in Wirklichkeit (betrachtet von heute) nur den Wert (siehe (3.3)):
V

=

â
X
k=1

= mÂˇ
= mÂˇ

kÂˇm
(1 + p) k
1
1+p
1 2
(1 â 1+p
)

1+p
p2

(nach (3.11) mit x =

1
1+p )

(multipliziere mit (1 + p) 2 ).

FĂźr m = 50.000 e und p = 8% = 0, 08 ist V = 8.437.500 e. Und eine (intuitive) ErklĂ¤rung
ist sehr einfach: Obwohl die Auszahlungen jedes Jahr wachsen, ist das Wachstum mit der Zeit
nur additiv, wĂ¤hrend in Zukunft bezahlte Euros exponentiell schnell ihren Wert (vom heutigen
Standpunkt) verlieren. Die Auszahlungen in der weiteren Zukunft sind (im wesentlichen) wertlos.

Allgemeine harmonische Reihen
Wir betrachten nun die Reihen von der Form
â
X
k=1

1
f (k)

mit f (k) > k fĂźr alle k. FĂźr f (k) = k ist das genau die harmonische Reihe.
â˛ Beispiel 3.32 : Harmonische Reihe
Hn =

n
X
1
k

ist divergent,

k=1

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

110

da (wie wir bereits gezeigt haben) Hn > ln n gilt und daher ist die Folge Hn nicht beschrĂ¤nkt.
Divergenz von Hn kann man auch direkt zeigen:
>2Âˇ 41 = 12

>4Âˇ 81 = 12

1 1
>8Âˇ 16
=2

z }| { z
}|
{ z
}|
{
â
X
1 1 1 1 1 1 1 1
1
1
1
=1+ + + + + + + + +
+...+
+...
k
2 3 4 5 6 7 8 9 10
16
k=1

Da unendlich oft mehr als 1/2 hinzuaddiert wird, ist lim Hn = â.
nââ

â˛ Beispiel 3.33 : Interessanterweise ist die âĂ¤hnlicheâ Reihe
n
X
1
Sn =
k2
k=1

bereits konvergent! Da die Folge <Sn > monoton wachsend ist, reicht es zu zegen, dass sie auch
beschrĂ¤nkt ist.
Behauptung: FĂźr alle n > 1 gilt Sn 6 2 â n1 .
Beweis: Induction nach n. FĂźr n = 1 ist die Aussage offensichtlich richtig. Induktionsschritt:
n 7â n + 1
1
1
1
1
Sn+1 = Sn +
62â +
62â
.
2
2
(n + 1)
n (n + 1)
n+1
Mit sogenanntem âVerdichtungskriteriumâ kann man zeigen, dass die Reihe
â
X
k=1

1
f (k)

bereits konvergiert, falls f (k) um mehr als einem logarithmischen Faktor schneller als k wĂ¤chst.
Satz 3.34. (Verdichtungssatz) Sei <ak > eine monoton fallende Nullfolge mit ak > 0 fĂźr alle k.
â
X
Dann konvergiert die Reihe
ak gegen einen Grenzwert S genau dann, wenn die âverdichteteâ
Reihe

â
X
k=0

k=1

2k Âˇ a2k gegen einen Grenzwert S â˛ konvergiert. AuĂerdem gilt 12 S â˛ 6 S 6 S â˛ .

Die Beweisidee ist einfach: Teile die Ausgangsreihe in Abschnitte der LĂ¤nge 2k fĂźr k = 0, 1, 2, . . . auf
und betrachte die Summe von der Folgenglieder in jedem Abschnitt als den entsprechenden Folgenglied der verdichteten Reihe.
Beweis. Wir setzen Sn :=

Pn

k=1

ak und Snâ˛ :=

Pn

k=1

2k a2k . Damit ist fĂźr n < 2k+1

Sn 6 a1 + (a2 + a3 ) + (a4 + . . . + a7 ) + . . . + (a2k + . . . + a2k +1â1 )
6 a1 + 2a2 + 4a4 + . . . + 2k a2k = Snâ˛

ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

111

und fĂźr n > 2k+1
Sn > a1 + a2 + (a3 + a4 ) + (a5 + . . . + a8 ) + . . . + (a2k +1 + . . . + a2k +1 )
1
> a1 + a2 + 2a4 + 4a8 + . . . + 2k a2k +1 > Snâ˛
2
Ist nun die verdichtete Reihe konvergent, d.h. lim Snâ˛ = S â˛ existiert, so ist auch die Ausgangsreihe
nââ

konvergent (Monotonie-Kriterium) mit dem Grenzwert 12 S â˛ 6 lim Sn 6 S â˛. Ist dagegen die verdichnââ

tete Reihe divergent, so folgt aus der fĂźr n > 2k+1 gĂźltigen Beziehung Sn > 12 Snâ˛ auch die Divergenz
der Ausgangsreihe.


Satz 3.35. Die Reihe

â
X
k=1

1
konvergiert, falls
f (k)
f (k) > k logr2 k

fĂźr ein festes r > 1 und alle k gilt.
P
âr mit r > 1. Anwendung des Verdichtungssatzes
Beweis. Zuerst betrachten wir die Reihe â
k=1 k
fĂźhrt auf
mit
x := 21âr .
2k a2k = 2k (2âkr ) = x k
P
k
Da r > 1, ist die verdichtete Reihe eine geometrische Reihe â
k=1 x mit |x| < 1, welche konvergent
ist mit dem Grenzwert
â
X
k=1

xk =

1
1
2r
1
=
=
= 1 + r â1
.
1âr
r
1âx 1â2
2 â2
2
â1

Wenn wir nun die verdichtete Reihe von

Pâ

1
k=1 f (k )

mit f (k) > k logr2 k betrachten, dann erhalten wir

2k
2k
1
6
= r.
f (2k )
2k (log2 (2k )) r
k
P
âr konvergiert, muss nach dem MajorantenkriDa (wie wir gerade gezeigt haben) die Reihe â
k=1 k
Pâ 2 k
terium fĂźr Reihen auch die Reihe k=1 f (2k ) und damit (nach dem Verdichtungssatz) auch die Reihe
Pâ
1

k=1 f (k ) konvergieren.
Verallgemeinerte harmonische Reihe: FĂźr jedes festes r > 1 gilt:
â
X
1
=a
kr
k=1

mit

1 < a 61+

1
2r â1

â1

.

(3.12)

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

112

3.3.1 Konvergenzkriterien fĂźr Reihen
Aus der Konvergenz der geometrischen Reihe ergeben sich die folgenden zwei wichtige spezifische
Konvergenzkriterien fĂźr Reihen.
Satz 3.36. Die Reihe
erfĂźllt sind:

Pâ

k=0

ak mit ak > 0 ist konvergent, wenn eine der folgenden zwei Bedingungen

1. Wurzelkriterium: Es gibt eine reelle Zahl 0 < Î¸ < 1, so dass fĂźr fast alle k gilt:

â
k

ak < Î¸.

2. Quotientenkriterium: Es gibt eine reelle Zahl 0 < Î¸ < 1, so dass fĂźr fast alle k sowohl ak , 0
wie auch aakk+1 6 Î¸ gilt.
P
k
Beweis. Um die erste Aussage zu beweisen, wende das Majorantenkriterium fĂźr Reihen auf â
k=0 Î¸
an.
P
k
Um die zweite Aussage zu beweisen, vergleiche mit der geometrischen Reihe â
k=0 Î¸ , 0
P6 Î¸ <k 1.
a k +1
k
Sei a k 6 Î¸ fĂźr alle k > n (n â N). Dann gilt: an+k 6 Î¸ an fĂźr alle k â N. Also ist â
k=0 Î¸ a n
Pâ
Pâ
Majorante von k=n ak und die Reihe k=0 ak konvergiert nach dem Majorantenkriterium.




Man beachte, dass die Bedingung im Quontientenkriterium nicht lautet
fĂźr fast alle k gilt:

ak+1
< 1,
ak

(â)

sondern âfĂźr fast alle k gilt aakk+1 6 Î¸â mit einem von n unabhĂ¤ngigen Î¸ < 1. (Dasselbe gilt auch fĂźr
das Wurzelkriterium.) Die Quotienten aakk+1 dĂźrfen also nicht beliebig nahe an 1 herankommen. Das
P
1
die Bedingung (â) nicht ausreicht, zeigt das Beispiel der divergenten harmonischen Reihe â
k=1 k .
Mit ak := 1/k gilt zwar
ak+1
k
<1
fĂźr fast alle k > 1,
=
ak
k +1
n
wegen lim n+1
= 1 gibt es jedoch kein Î¸ < 1 mit

ak+1
6Î¸
ak

fĂźr alle k > 1,

Eine wichtige Klasse von Funktionen sind Polynome
P(x) = a0 + a1 x + a2 x 2 + Âˇ Âˇ Âˇ an x n .
Sogenannte âPotenzreihenâ sind Verallgemeinerungen der Polynome, wenn man unendlich viele Terme zulĂ¤sst.
Definition: FĂźr eine gegebene Folge <an > heiĂt die Reihe
P(x) =

â
X

an x n

n=0

Potenzreihe mit Koeffizienten an .
ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

113

Man betrachtet auch allgemeinere Potenzreihen um ein Punkt x 0 â R
P(x, x 0 ) =

â
X
n=0

an (x â x 0 ) n .

Der Punkt x 0 wird auch Entwicklungspunkt der Potenzreihe genannt. Wir werden aber nur die Potenzreihen P(x) mit dem Entwicklungspunkt x 0 = 0 betrachten (die Eigenschaften der allgemeinen
Potenzreihen sind analog).
P
n
Einige Potenzreihen haben wir
gesehen: Die geometrische Reihe â
n=0 x und die verallgePbereits
â
n
meinerte geometrische Reihe n=0 n Âˇ x . Das sind die âeinfachstenâ Potenzreihen mit Koeffizienten
an = 1 bzw. an = n. Es gibt aber auch viele andere wichtige Potenzreihen. So kann man zum Beispiel
viele analytischen Funktionen f (x) (wie Exponent- oder Logarithmusfunktion) als entsprechende Potenzreihen darstellen. 9
â˛ Beispiel 3.37 : Die Potezreihen fĂźr e x und ln x sind:
e

x

â
X
xn
x2 x3
=
=1+x+
+
+...
n!
2
6
n=0

ln(1 + x) =

â
X

(â1) n

n=0

Die entsprechende Koeffizienten sind an =

1
n!

xn
x2 x3
=1â x+
â
Âą...
n
2
3

und an =

(â1) n
n .

Es ist klar, dass nicht alle Potenzreihen konvergieren. Wie kann man erkennen, ob eine gegebene
Potenzreihe konvergiert oder nicht? Und wenn konvergiert, dann fĂźr welche Werte von x tut sie das?
Alle diese Fragen kann man mit dem Begriff vom âKonvergenzradiusâ leicht beantworten.
Der Konvergenzradius R der Potenzreihe ist gegeben durch R = r1 , wobei r ist als
r = lim sup
nââ

p
n

|an |

(3.13)

definiert. Sind die Koeffizienten an einer Potenzreihe fĂźr fast alle n von Null verschieden, dann lĂ¤sst
sich r auch berechnen durch
an+1
r = lim sup
.
(3.14)
an
nââ
1
Satz 3.38.
PâSei R =n r , wobei r ist entweder durch (3.13) oder durch (3.14) gegeben. Die Potenzreihe
P(x) = n=0 an x ist

(a) absolut konvergent, falls |x| < R
(b) divergent, falls |x| > R

Beweis. Teil (a) folgt unmittelbar aus den Wurzel- und Quotientenkriterien.
9Wie man die Potenzreihen fĂźr Funktionen bekommt wird im Abschnitt 3.7 skiziert.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

114
Teil (b): Ist |x| > R =
lim sup

1
r

p
n

mit r durch (3.13) gegeben, so gilt

|an x n | = |x| Âˇ lim sup

p
n

|an | = |x| Âˇ

1
>1+Î´
r

mit

Î´>0

und damit ist die Folge |an x n | > (1 + Î´) n unbeschrĂ¤nkt. Das gleiche gilt auch wenn r als (3.14)
definiert ist.

â˛ Beispiel 3.39 : Wir betrachten die Potenzreihe e x =

â
X

an x n mit an =

ân n=0
Aus der Stirlingâs Formel fĂźr n! â an 6 ne
q 
â
n
n
â lim |an | âź lim n ne = lim ne = 0
nââ

nââ

1
.
n!

nââ

â Radius R = â â Ăźberall konvergiert.

Bemerkung 3.40. Der Satz sagt nichts Ăźber die Punkte x mit |x| = R aus. Diese Punkte mĂźssen fĂźr
jede Reihe neu untersucht werden.
Bemerkung 3.41. Eine wichtige eigenschaft der Potenzreihen ist, dass man sie gliederweise differentieren darf. Der Konvergenzradius Ă¤ndert sich dabei nicht.
â˛ Beispiel 3.42 : (Dezimaldarstellung) Jede Zahl in Interval [0, 1] lĂ¤sst sich als eine (potenziell unendliche) Folge 0, x 1 x 2 x 3 . . . mit x k â {0, 1, 2, . . . , 9} darstellen. Man kann auch eine umgekehrte
Frage stellen:
P
âk mit x â {0, 1, 2, . . . , 9} auch konvergent? Dazu wenden wir das
Ist jede Reihe â
k
k=1 x k 10
ak

bk

z }| { z }| {
P
Majorantenkriterium an. FĂźr alle k gilt: x k Âˇ 10âk 6 9 Âˇ 10âk , und â
k=1 bk ist konvergent:10
â
X
k=1

9 Âˇ 10âk =

â
X
k=0

0, 9 Âˇ 10âk = 0, 9 Âˇ

1
= 1.
1
1 â 10

â˛ Beispiel 3.43 : (Die Euler-Reihe) In der Beschreibung von Wachstumsverhalten (in Physik, Biologie und Wirtschaft) taucht die Folge


1 n
cn = 1 +
n
auf. Zum Beispiel cn ist der VergrĂśĂerungsfaktor eines Kapitals in einem Jahr bei Zinssatz 100%
und n-maliger Aufzinsung:
n=1
n = 12
n = 365
n = 525600
10

Pâ

âk
k=0 10

jĂ¤hrliche Aufzinsung
monatliche Aufzinsung
tĂ¤gliche Aufzinsung
Aufzinsung je Minute

1
c1 = 1 + 11 = 1 + 1 = 2

1 12
c12 = 1 + 12
= 2, 61303529

1 365
c365 = 1 + 365
= 2, 714567455
cn = 2, 718279 . . .

ist eine geometrische Reihe.
ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

115

Die Euler-Zahl e ist als der Grenzwert
e := lim en von en = 1 +
nââ

1
1
1
+ +...+
1! 2!
n!

definiert.
Existenz von e folgt aus dem Quotientenkriterium, da e =
gilt und <an /anâ1 > = <1/n> eine Nullfolge ist.

Pâ

n=0

an mit an = 1/n! (und 0! = 1)

Die Folgen en und cn sehen als vollkom verschieden aus. Nichtdestotrotz, gilt folgendes:


1 n
lim 1 +
=e
nââ
n
Das kann man relativ leight zeigen. Zuerst zeigt man, dass die Folge <cn > motonon wachsend
und durch e nach oben beschrĂ¤nkt ist. Damit gilt lim cn 6 e. Es reicht dann zu zeigen, dass
nââ
lim cn > e N fĂźr jedes festes N > 1 gilt. Die Gleichheit lim cn = e folgt dann aus dem
nââ

nââ

Vergleichskriterium. Wir lassen die Details als Ăbungsaufgabe (Aufgabe 15).
Die Darstellung von e als Grenzwert der Folge en ist gut, da die Folge en sehr rasch gegen e =
2, 7182818... konvergiert, wĂ¤hrend die Folge cn nur relativ langsam gegen denselben Grenzwert
e konvergiert. Deshalb ist en besser fĂźr die numerische Berechnung von e geeignet.
P
k
â˛ Beispiel 3.44 : Sei x â R, 0 6 x < 1. Wir haben bereits gezeigt , dass die Reihe â
k=0 k x gegen
â
X
x/(1 â x) 2 konvergiert. Wir zeigen nun, dass auch die allgemeinere Reihe
k c x k fĂźr jedes
k=1

c â R konvergiert.
â
k
Wurzelkriterium: Da lim k c = 1 (siehe (3.8)), erhalten wir
kââ

lim

kââ

â˛ Beispiel 3.45 : Die Reihe

â
X
ck
k=1

k!

p
k

k c x k | = lim

kââ

p
k

x k | = x < 1.

ist konvergent fĂźr jedes c > 0. Sei c > 0 (fĂźr c = 0 ist nichts zu

beweisen). Dann gilt lim ak+1 /ak = lim c/(k + 1) = 0 und wir kĂśnnen das Quotientenkriterium
kââ

kââ

anwenden.
Aus dem Cauchy-Kriterium fĂźr Folgen unmittelbar, dass eine Reihe <Sn > =
konvergiert, wenn es zu jedem ÇŤ > 0 ein n0 mit
|Sn â Sm | =

n
X

ak < ÇŤ

fĂźr alle n > m > n0

Pâ

n=0

an genau dann

(3.15)

k=m+1

gilt.
Es gibt auch einige spezielle Konvergenzkriterien fĂźr Reihen.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

116
Satz 3.46.
1. Ist <an > keine Nullfolge, so ist die Reihe

Pâ

k=0

ak divergent.

2. Dirichlet-Kriterium: Ist <ak > P
eine monoton fallende Nullfolge und <bk >Peine Folge mit ben
â
schrĂ¤nkten Partialsummen (d.h.
k=0 bk 6 B fĂźr alle n), dann konvergiert
k=0 ak bk .
3. Leibniz-Kriterium: EsPsei <ak > eine monoton fallende Nullfolge. Dann konvergiert die altern
k
nierende Reihe Sn =
k=0 (â1) ak gegen einen Wert, der zwischen S2n+1 und S2n fĂźr alle n
liegt.
Beweis. Die erste Aussage folgt aus (3.15): Ist <an > keine Nullfolge, dann gibt es ein ÇŤ > 0, so dass
|an | > ÇŤ fĂźr unendlich viele n. Dann gilt auch |Sn â Snâ1 | = |an | > ÇŤ fĂźr unendlich viele n und die
Folge <Sn > muss nach (3.15) divergieren.
Um die zweite Aussage zu beweisen, setze Bk = b0 + b1 + Âˇ Âˇ Âˇ + bk . Dann gilt
n
X

ak bk

k=m+1

=

an (Bn â Bm ) +
|{z}
>0

6 |(Bn â Bm )| an +
6 2B

an +

nâ1
X

k=m+1

nâ1
X

k=m+1
nâ1
X

k=m+1

(a â a ) (B â Bm )
| k {z k+1} k
>0

|Bk â Bm |(ak â ak+1 )

(ak â ak+1 )

!

= 2B(an + am+1 â an )
= 2Bam+1 .

ÇŤ
Sei nun ÇŤ > 0 beliebig klein (aber fest). Man wĂ¤hle n0 so, dass am+1 < 2B
fĂźr alle m > n0 gilt (an ist
monoton fallend). Dann ist
n
X
ÇŤ
ak bk < 2B Âˇ
=ÇŤ
2B
k=m+1
Pâ
fĂźr alle n > m > n0 und die Reihe k=0 ak bk konvergent nach Cauchy-Kriterium.
P
Um die dritte Aussage zu beweisen, setze bk := (â1) k und Bk := kj=0 b j . Es ist Bk = 0 oder Bk = 1,
P
k
d.h. Bk ist beschrĂ¤nkt und damit konvergiert â
k=0 (â1) ak nach Dirichlet-Kriterium gegen einen
Grenzwert S. Es gilt auch: S2n+2 â S2n = (â1) 2n+2 a2n+2 + (â1) 2n+1 a2n+1 = a2n+2 â a2n+1 6 0 (da ak
monoton fallend ist) und damit S0 > S2 > . . . > S2n > S2n+2 > . . . woraus S 6 S2n fĂźr alle n folgt.
Entsprechend ist wegen S2n+3 â S2n+1 = âa2n+3 + a2n+2 > 0 S1 6 S3 6 . . . 6 S2n+1 6 S2n+3 6 . . .
woraus S2n+1 6 S fĂźr alle n folgt. AuĂerdem gilt wegen S2n+1 â S2n = âa2n+1 6 0 S2n+1 6 S2n fĂźr
alle n = 0, 1, 2, . . .. Also gilt S2n+1 6 S 6 S2n fĂźr alle n = 0, 1, 2, . . ., wie gewĂźnscht.


Bemerkung 3.47. Die Umkehrung der ersten Aussage im Satz 3.46 gilt nicht!P
Zum Beispiel ist die
n
1
harmonische Folge ak = 1/k eine Nullfolge, aber die harmonische Reihe Hn = k=1
k divergiert:
2m
X
1
1
1
1
1
1
=
+
+...+
> mÂˇ
= .
k
m+1 m+2
2m
2m 2

k=m+1

Da es fĂźr ÇŤ < 1/2 kein n0 mehr gibt =â Divergenz nach Cauchy-Kriterium.
ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

117

3.3.2 Anwendung: Warum Familiennamen aussterben?
Es war schon seit langem aufgefallen, dass Familiennamen ehrwĂźrdiger Adelsgeschlechter Ăźber Jahrhunderte hinweg mit der Zeit aussterben. Dies inspirierte den englischen Naturforscher Franzis Galton
1873 das folgende Problem zu stellen:
Es seien p0 , p1 , p2 , . . . die Wahrscheinlichkeiten dafĂźr, dass ein Mann 0, 1, 2, . . . SĂśhne hat. Jeder Sohn habe die gleichen Wahrscheinlichkeiten fĂźr eigene SĂśhne usw. Wie groĂ ist die Wahrscheinlichkeit, dass die mĂ¤nnliche Linie nach n Generationen ausgestorben ist? D.h. mit welcher
Wahrscheinlichkeit stirbt der Name des Mannes aus?
Das ist eine typische Fragestellung aus der sogenanten Theorie der Verzweigungsprozesse, die heutzutage zahlreiche Anwendungen in der Physik, Chemie, Biologie, Sozialogie und Technik hat. Statt
dem Mann hĂ¤tten wir zum Beispiel ein Neutron betrachten kĂśnnen, das einen Urankern spaltet, wobei
wieder Neutronen (SĂśhne) freigesetzt werden usw.
Der Mann stelle die nullte Generation dar, dessen SĂśhne die erste usw. FĂźr n > 0 wollen wir mit an
die Wahrscheinlichkeit bezeichnen, dass die mĂ¤nnliche Linie nach n Generationen ausgestorben ist.
Offenbar gilt a0 = p0 .
Angenommen wir wĂźrden an kennen. Dann wissen wir auch, dass jede Verzweigungslinie der ersten
Generation mit Wahrscheinlichkeit an nach n weiteren Generationen ausgestorben ist. Damit ist die
Wahrscheinlichkeit, dass der Mann i SĂśhne hat und alle Nachkommen dieser i SĂśhne nach n weiteren
Generationen ausgestorben sind, gleich
pi Âˇ ani .
Insgesamt kĂśnnen wir demnach an+1 durch
an+1 = p0 + p1 an + p2 a2n + p3 a3n + . . .
berechnen, weil der Mann entweder gar keinen Sohn hat oder einen Sohn oder zwei SĂśhne oder . . .
hat.
Unmittelbar aus der Definition von an folgt, dass an 6 an+1 , und somit ist die Folge <an > monoton
wachsend. BerĂźcksicht man noch, dass die an als Wahrscheinlichkeiten durch 1 nach oben beschrĂ¤nkt
sind, so kann man nach Monotonie-Kriterium unmittelbar auf die Existenz von
a = lim an 6 1
nââ

schlieĂen. Die Zahl a beschreibt gerade die Wahrscheinlichkeit, ob die mĂ¤nnliche Line ausstirbt. Diese
Zahl hĂ¤ngt natĂźrlich von der Wahrscheinlichkeitsverteilung p0 , p1 , p2 , . . . ab. Als nĂ¤chstes wollen wir
diese AbhĂ¤ngigkeit genauer untersuchen.
Wenn wir die Potenzreihe
f (x) = p0 + p1 x + p2 x 2 + p3 x 3 + . . .
einfĂźhren, so haben wir die Rekursionsvorschrift
an+1 = f (an )

(3.16)

gefunden.
Zuerst beobachten wir, dass die Reihe f (x) fĂźr alle x â [0, 1] konvergiert. FĂźr x = 0 ist das klar,
da dann f (0) = p0 gilt. FĂźr x = 1 gilt f (1) = p0 + p1 + p2 + p3 + . . . = 1, da p0 , p1 , p2 , . . .
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

118

eine Wahrscheinlichkeitsverteilung ist. FĂźr 0 < x < 1 kĂśnnen wir Dirichlet-Kriterium anwenden:
(x k )P
ist eine monoton fallende Nullfolge und (pk ) ist eine Folge mit beschrĂ¤nkten Partialsummen
n
(da k=0
pk 6 1 fĂźr alle n > 0 gilt). Deshalb muss nach Dirichlet-Kriterium auch die Reihe
Pâ
f (x) = i=0 pi x i konvergieren. Ausserdem, kann man zeigen, dass die Funktion f auf dem Intervall
[0, 1] konvex11 und stetig ist.

Die Zahl a = lim an beschreibt gerade die Wahrscheinlichkeit, ob die mĂ¤nnliche Line ausstirbt. Geht
nââ
man nun in (3.16) zum Limes Ăźber, so ergibt sich unter Beachtung der Stetigkeit von f die Beziehung
a = f (a),

das heiĂt,
Pa ist ein Fixpunkt von f . Dieser Fixpunkt muss auf der Geraden y = x liegen. Da f (0) = p0 ,
f (1) = â
i=0 pi = 1 und die Funktion f (x) monoton wachsend im Intervall [0, 1] ist, muss der erste
Fixpunkt die kleinste Zahl zwischen p0 und 1 sein. Da f (1) = 1, ist a = 1 ein Fixpunkt von f . Gibt
es weitere Fixpunkte?
Um diese Frage zu beantworten, betrachten wir die Ableitung12
â
X
f â˛ (x) = p1 + 2p2 x + 3p3 x 2 + . . . =
ipi x iâ1 .
i=1

Das ist der Anstieg der Tangente an den Graphen von f im Punkt x. Da f auf [0, 1] konvex ist, liegen
alle Werte f (x) mit x â [0, 1] oberhalb dieser Tangente.

Wenn also f â˛ (1) 6 1 gilt, so liegen auch alle Werte f (x) mit x â [0, 1] oberhalb der Geraden y = x,
die den Anstieg 1 besitzt (siehe Abb. 3.2(A)). Im Falle f â˛ (1) 6 1 kann es also keinen weiteren
Fixpunkt als 1 geben.
Anders im Fall f â˛ (1) > 1. Hier muss der Graph der konvexen Funktion f die Gerade y = x fĂźr ein
x < 1 schneiden (da f â˛ (0) = p1 6 1), das heiĂt, es muss ein weiterer Fixpunkt existieren (siehe
Abb. 3.2(B)).
1

1

0.8

0.8

0.6
p0
0.4

0.6

0.2

0.4
p
0
0.2

0

0
0

0.2 0.4 0.6 0.8

0

(A)

0.2 0.4 0.6 0.8

(B)
Abbildung 3.2:

Zusammengefasst: Im Falle

â
X

ipi 6 1

i=0

f â˛â˛ (x)

Pâ

â 1)x iâ2

11Da die zweitePAbleitung
= i=2 i(i
nicht negativ ist.
12Die Summe â
i=0 ipi ist die erwartete Anzahl der SĂśhne, die ein Mann haben kann, unter der Wahrscheinlichkeitsverteilung p0 , p1 , p2 , . . .. Wir werden bald dieses Konzept genauer kennenlernen.
ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

119

stirbt demnach die mĂ¤nnliche Linie mit Wahrscheinlichkeit 1 aus, wĂ¤hrend es im Fall
â
X

ipi > 1

i=0

mit positiver Wahrscheinlichkeit immerfort NamenstrĂ¤ger gibt. Das bedeutet also, dass der Familienname langfristig nicht sicher ausstirbt,wenn im Durchschnitt mehr als ein Sohn da ist.
In der Biologie hat dieser Prozess Anwendung auf das Aussterben bzw. Ăberleben vorteilhafter Gene.
Hierbei setzt man meist noch
pi := eâÎť

Îťi
i!

(sogenannte Poisson-Verteilung, siehe Abschnitt 4.10)

so dass
âÎť

f (x) = e

In diesem Fall gilt Îť = p1 + 2p2 + 3p3 + . . . =

â
X
Îťi xi
= eÎť (xâ1) .
i!
i=0
f â˛ (1).

3.3.3 Umordnungssatz
Sei <ak > eine Folge und <Sn > die entsprechende
P Reihe der Partialsummen Sn =
Reihe <Sn > den Grenzwert S, so schreibt man â
k=0 ak = S.



Pn

k=0

ak . Hat die

Wir haben bereits erwĂ¤hnt, dass das nur eine Schreibweise ist â unendliche
Summen als solche
P
a
existieren nicht! Insbesondere kĂśnnen wir i.A. nicht die Terme in â
k=0 k umordnen (obwohl
fĂźr endlichen Summen das immer erlaubt ist, da + eine kommutative Operation ist, d.h. x + y = y + x
gilt).
â˛ Beispiel 3.48 : Nach dem Leibniz-Kriterium kovergiert die alternierende harmonische Reihe
Pâ (â1) k
gegen einen Grenzwert s. Wir vergleichen die Reihen, die s und 32 s darstellen:
k=1 k
1
2

â

1
3

+

1
4

â

1
5

+

1
6

â

1
7

+

1
8

...

1
1
s + s = â1 +
2
2

â

1
3

+

1
4

â

1
5

+

1
6

â

1
7

+

1
8

...

+

1
4

â

1
6

+

1
8

...

+

1
2

+

1
4

...

s

= â1 +

â
= â1

1
2
â

1
3

â

1
5

â

1
7

Damit ist:
s = â1 +
und

1 1 1 1 1 1 1 1
1
â + â + â + â +
Âą...
2 3 4 5 6 7 8 9 10

1
1 1 1 1 1 1
1
1
1
1
s + s = â1 â + â â + â â
+ â
â
Âą...
2
3 2 5 7 4 9 11 6 13 15
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

120

Was auffĂ¤llt: In der Reihe fĂźr s + 12 s tauchen dieselben Summanden auf, nur in einer anderen Reihenfolge. Wenn man in konvergenten Reihen die Reihenfolge der Summation Ă¤ndert, bekommt man
eventuell einen anderen Grenzwert! Der GrundP
fĂźr dieses PhĂ¤nomen liegt, so stellt sich heraus, in der
1
Tatsache, dass die Summe der AbsolutbetrĂ¤ge â
k=1 k divergiert (harmonische Reihe).
Pâ
Pâ
Die Reihe k=0 ak heiĂt absolut konvergent, wenn die
Reihe
k=0 |ak | konvergiert. Sie heiĂt bePâ
dingt konvergent, wenn sie zwar konvergiert, die Reihe k=0 |ak | aber divergiert. Eine solche ist zum
P
(â1) k
Beispiel die Reihe â
k=1 k .
Bedingt konvergente Reihen sollte man tunlichst meiden. Absolut konvergente Reihen aber sind harmlos. Mit ihnen kann man alles das machen, was man sich so naiverweise vorstellt! Es gilt nĂ¤hmlich
folgendes:

P
Satz 3.49. (Umordnungssatz) Sei â
k=0 ak absolut konvergent (gegen a) und sei
P Ď : N â N eine
beliebige bijektive Abbildung (Umordnung). Dann konvergiert auch die Reihe â
k=0 aĎ (k ) absolut
gegen a.
Pn
Pn
â˛
Beachte, dass die Folgen Sn = k=0
aP
k und Sn =
k=0 aĎ (k ) total verschieden sein kĂśnnen! Deshalb
ist der Satz nicht trivial: konvergiert â
|a
|,
so
haben die beiden Folgen <Sn > und <Snâ˛ > den
k=0 k
selben Grenzwert!
P
Beweis. Angenommen, konvergiert die Reihe â
k=0 ak absolut gegen einen Grenzwert A. Sei ÇŤ > 0
beliebig klein. Da die Reihe absolut konvergent ist, gibt es nach dem Cauchy-Kriterium fĂźr Reihen
ein n0 â N mit
â
X
ÇŤ
|ak | <
2
k=n 0 +1

Daraus folgt
n0
X
k=0

ak â A =

â
X

ak 6

k=n 0 +1

â
X

k=n 0 +1

|ak | <

ÇŤ
2

(3.17)

Da Ď bijektiv ist, gibt es ein N > n0 mit
Ď({0, 1, . . . , N }) â {0, 1, . . . , n0 }

(3.18)

Dann gilt fĂźr n > N:
n
X
k=0

aĎ (k ) â A

=

n
X
k=0

6

n
X
k=0

(3.17)

<

n
X
k=0

(3.18)

6

â
X

aĎ (k ) â
aĎ (k ) â
aĎ (k ) â

k=n 0 +1

|ak | +

n0
X

ak +

k=0

n0
X

ak â A

k=0
n0
X

ak +

k=0

n0
X

n0
X

k=0

ak +

k=0

ak â A

ÇŤ
2

ÇŤ
< ÇŤ,
2
ÂŠ 2003 by S. Jukna

3.3. UNENDLICHE REIHEN

121

die umgeordnete Reihe konvergiert also gegen denselben Grenzwert wie die Ausgangsreihe. Das die
umgeordnete
Reihe auch absolut konvergiert, folgt aus der Anwending des gerade Bewiesenen auf die
P
|a

Reihe â
k=0 k |.

S
Satz 3.50. (GroĂer Umordnungssatz) Sei I = j I j (endlich oder unendlich) mit I j âŠ Ik = â fĂźr
alle j , k. Dann gilt
X
XX
aj
ai =
i âI

i âI j

j

sofern eine der beiden Seiten existiert, mit a j durch |a j | ersetzt.
Specialfall: N2+ = {1, 2, . . .} Ă {1, 2, . . .} (Doppelreihensatz oder doppeltes ZĂ¤hlen)
X

an, m =

(n, m) âN2+

â
â X
X

an, m Zeilensummen

n=1 m=1

=

=

â
â X
X

m=1 n=0
â
X
X
N =1

=

an, m Spaltensummen

â
X

N =1

an, m Diagonalsummen

(n, m)
n+m=N

X

an, m Quadratsummen

(n, m)
max{n, m}=N

falls eine der 5 Summen existiert, mit an, m durch |an, m | ersetzt.

Mit diesem Fakt kann man einige âMonster-Reihenâ erledigen.
â˛ Beispiel 3.51 : Die Reihe
A=

â
â X
X

n=1 m=1

n2

1
+ m2

divergiert. Um das zu zeigen, betrachten wir die Quadratsummen
X
1
SN =
.
2
n + m2
(n, m)
max{n, m}=N

Da n2 + m2 6 2N 2 , haben wir
SN

>

Anzahl der Summanden
2N
1
=
= .
2
2
2N
2N
N
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

122

Damit
Pist <SN > eine Majorante der harmonischen Folge <1/N >, woraus die Divergenz der Reihe
A= â
N =1 SN folgt.
â˛ Beispiel 3.52 : Interessant ist, dass die âĂ¤hnlicheâ Reihe

â X
â
X

n=1 m=1

n3

1
bereits konvergiert! Um
+ m3

das zu zeigen, betrachten wir wieder die Quadratsummen
X

SN =

(n, m)
max{n, m}=N

n3

1
.
+ m3

Dann ist
SN

Anzahl der Summanden 2N
2
= 3 = 2,
3
N
N
N

6

da n3 + m3 > N 3 gilt. Damit gilt auch (nach Lemma ??)
â X
â
X

n=1 m=1

â
â
X
X
1
1
=
S
<
2
Âˇ
< 4.
N
3
3
n +m
N2
N =1

N =1

â X
â
X
1
6 2.
nk

â˛ Beispiel 3.53 :

n=2 k=2

â X
â  k
X
1
n=2 k=2

n

=

=

=

â X
â  k+2
X
1
n=2 k=0
â
X

Sn

n=2
â 
X
n=2

=

â
X
n=2

n

mit Sn :=

â
X

a k+2 und a :=

k=0

1
n 
Âˇ
n2 n â 1

da Sn = a2 Âˇ

â
X
k=0

1
n

a k = a2 Âˇ

1
(geom. Reihe)
1âa

1
6 2 (verallgemeinerte harmonische Reihe, Lemma ??)
n(n â 1)

â˛ Beispiel 3.54 :
â X
â  
X
n
m=0 n=0

m

ânâm

2

=

=

=

â X
â  
X
n
n=0 m=0
â 
X

1+

n=0
â 
X
n=0

3
4

m


2âm Âˇ 2ân

1 n ân
Âˇ2
2

n

=

(binomischer Lehrsatz)

1
= 4.
1 â 3/4
ÂŠ 2003 by S. Jukna

3.4. GRENZWERTE BEI FUNKTIONEN

123

3.4 Grenzwerte bei Funktionen
Folgen sind Funktionen von N nach R. Den Begriff des Grenzwertes kann man auch auf allgemeine
Funktionen f : R â R erweitern.

Sei I â R ein offenes Interval und f : I â R eine Funktion. Sei a â I (evtl. ist f in a nicht
definiert). Eine Îą-Umgebung einer Zahl y ist die Menge UÎą aller Zahlen, die sich von y um hĂśchstens
ÂąÎą unterscheiden.
Definition: Eine Zahl A heiĂt Grenzwert von f (x) in a, geschrieben
lim f (x) = A

xâa

falls
âÇŤ > 0 âÎ´ > 0 âx , a :

x â UÎ´ (a) â f (x) â UÇŤ ( f (a)).

Kurz: Es gibt eine Î´-Umgebung UÎ´ (a) von a, so dass in dieser Umgebung die Funktion f (x) sich um
hĂśchstens ÂąÇŤ vom A abweicht. Ausserdem kann ÇŤ > 0 beliebig klein gemacht werden.
Bemerkung 3.55. Ist f : N â R eine Folge, so kann man die Mengen {n0 , n0 + 1, n0 + 2, . . .} mit
n0 â N als âÎ´-Umgebungenâ der Unendlichkeit â betrachten. Damit ist der Begriff des Limes fĂźr
Folgen ein Spezialfall dieser mehr allgemeiner Definition.
Wir erinnern an die folgenden Regeln zur Grenzwertberechnung, die direkt aus der Definition des
Limes folgen. Aus lim f (x) = c und lim g(x) = d folgt
xâa

xâa

1. lim ( f (x) Âą g(x)) = c Âą d
xâa

2. lim ( f (x) Âˇ g(x)) = c Âˇ d
xâa

1.

lim f (x)
xâa g (x)

=

c
d

Specialfall: lim (b Âˇ f (x)) = b Âˇ d fĂźr b â R
xâa

(falls d , 0)

â˛ Beispiel 3.56 : Umformung von Termen

lim

xâ1

xn â 1
xâ1

(x â 1) Âˇ (x nâ1 + x nâ2 + . . . + x + 1)
xâ1
xâ1
= lim x nâ1 + lim x nâ2 + . . . + lim x + lim 1
=

lim

xâ1

xâ1

xâ1

xâ1

= n
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

124
â˛ Beispiel 3.57 :

lim

xâ0

â

x+1â1
x

=
=
=
=
=

lim

xâ0

lim

xâ0

lim

xâ0

lim

xâ0

â
â
( x + 1 â 1) Âˇ ( x + 1 + 1)
â
x Âˇ ( x + 1 + 1)
x+1â1
â
x Âˇ ( x + 1 + 1)
x
â
x Âˇ ( x + 1 + 1)
1
â
x+1+1

1
.
2

Auch wenn der Grenzwert lim f (x) = A, muss A = f (a) nicht unbedingt gelten! DafĂźr muss die
xâa
Funktion f (x) stetig in a sein. Zur Erinnerung: Eine reelle Funktion f heiĂt stetig in a â R, falls fĂźr
jede Folge <an > mit lim an = a gilt: lim f (an ) = f (a).
nââ

nââ

Satz 3.58. f (x) ist stetig in a ââ lim f (x) = f (a) gilt
xâa

Beweis. (â): Sei lim f (x) = f (a) und sei ÇŤ > 0 beliebig klein. Dann gibt es ein Î´ > 0, so dass
xâa
|x â a| < Î´ â | f (x) â f (a)| < ÇŤ. Sei nun <an > eine beliebige Folge mit lim an = a. Dann gibt es
nââ
ein n0 , so dass |an â a| < Î´ fĂźr alle n > n0 . Also muss auch | f (an ) â f (a)| < ÇŤ fĂźr alle n > n0 gelten,
d.h. lim f (an ) = f (a).
nââ

(â): Kontraposition: Nehmen wir an, dass lim f (x) = f (a) nicht gilt. Dann gibt es ein ÇŤ > 0, so
xâa
dass âÎ´ > 0 es ein âx Î´ mit |x Î´ â a| < Î´ und | f (x Î´ ) â f (a)| > ÇŤ gibt. Betrachte die Folge <an > mit
a1 := x 1 , a2 := x 1/2 , . . . , an := x 1/n . . .. Diese Folge konvergiert mit lim an = a (da |an â a| < 1/n
nââ
gilt), aber | f (an ) â f (a)| > ÇŤ fĂźr alle n â N und damit auch lim f (an ) , f (a). Somit ist die
nââ
Funktion f (x) nicht stetig in a.


3.5 Differentiation
Das Grundproblem der Differentialrechnung ist die Berechnung der Steigung einer Funktion f : R â
R in einem beliebigen Punkt x.
Definition: Die Ableitung einer Funktion f im Punkt x ist
f â˛ (x) := lim

hâ0

f (x + h) â f (x)
.
h

Hier sind die Ableitungen einiger hĂ¤ufig benutzten Funktionen.
ÂŠ 2003 by S. Jukna

3.5. DIFFERENTIATION

125

Satz 3.59. Seien c â R und n â N beliebige Konstanten.
f (x) = c â f â˛ (x) = 0

f (x) = x â f â˛ (x) = 1

f (x) = x n â f â˛ (x) = nx nâ1

f (x) = ln g(x) â f â˛ (x) =

1 â˛
g (x)
g(x)

Spezialfall: (ln x) â˛ =

1
1
â f â˛ (x) = â 2
x
x
f (x) = c g (x) â f â˛ (x) = (ln c)eg (x) g(x) â˛

1
x

f (x) =

f (x) = ecg (x) â f â˛ (x) = cecg (x) g â˛ (x)

Speczialfall: (c x ) â˛ = (ln c)e x
Spezialfall: (e x ) â˛ = e x

Beweis. Wir beweisen nur einige ausgewĂ¤hle FĂ¤lle, um den allgemeinen Zugang zu demonstrieren.
1. Sei f (x) = x n . Dann ist f â˛ (x) = n Âˇ x nâ1 :

(x + h) n = x n + n Âˇ x nâ1 Âˇ h + Âˇ Âˇ Âˇ + h n ,

1
(n Âˇ x nâ1 Âˇ h + Âˇ Âˇ Âˇ + h n ) = n Âˇ x nâ1 .
h

f â˛ (x) = lim

hâ0

2. Sei f (x) =

1
1
. Dann ist f â˛ (x) = â 2 :
x
x




1
1
1
1 xâxâh
1
â˛
f (x) = lim
â
= lim
= â 2.
hâ0 h
hâ0
x+h x
h x(x + h)
x

3. Sei f (x) = e x . Dann ist f â˛ (x) = e x :
f â˛ (x) = lim

hâ0
h


1 x+h
eh â 1
e
â e x = e x Âˇ lim
hâ0
h
h

1

Es reicht also zu zeigen, dass limhâ0 e hâ1 = 1. Wir haben e = limyâ0 (1 + y) y (Eulerische Zahl) und
somit
ln(1 + y)
lim
= ln e = 1
(da ln a1/b = (ln a)/b)
(3.19)
yâ0
y
Setzt man y := eh â 1, dann gilt: h = ln(1 + y). Somit folgt:
eh â 1
y
= lim
= lim
yâ0 ln(1 + y)
yâ0
hâ0
h
lim

4. Sei f (x) = ln x, 0 < x < â. Dann ist f â˛ (x) =

1
ln(1+y)
y

=

1
= 1.
ln e

1
x

f (x + h) â f (x) ln(x + h) â ln x 1 ln(1 + hx )
=
= Âˇ
h
h
h
x
x
Mit (3.19) folgt:
f â˛ (x) =

ln(1 + hx ) 1
1
lim
=
h
x hâ0
x
x

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

126

3.6 MittelwertsĂ¤tze der Differentialrechnung
Wozu sind Ableitungen gut? Ableitungen erlauben uns die Entwicklung der Funktion zu analisieren.
Zum Beispiel sie helfen bei:
-

Approximation von f (x) durch Polymome (Taylorentwicklung).
Bestimung lokaler Extremallstellen von f (x).
Bestimmung der Grenzwerte fĂźr Quotienten f (x)/g(x) (Regeln von de lâHoĚpital).

Als Basis fĂźr alle diese Anwendungen sind sogenannte âMittelwertsĂ¤tze der Differentialrechnungâ.
Geometrisch besagt der folgender Satz von Role (Michel Role, 18. Jahrhundert), dass eine differeinzierbare Funktion, die mit dem selben Wert startet und endet, irgendwo dazwischen eine waagrechte
Tangente haben muss:

Tangente

a

c

b

Satz 3.60. (Satz von Role) Sei f : [a, b] â R stetig und im offenen Interval (a, b) differeinzierbar.
Sei ferner f (a) = f (b). Dann gibt es ein c â (a, b) mit f â˛ (c) = 0.
Beweis. Ist f konstant, so ist f â˛ (x) = 0 fĂźr alle x. Sei nun f nicht konstant. Dann hat f in [a, b]
ein Minimum an einer Stelle x min und ein Maximum an einer Stelle x ma x (hier benutzen wir die
stetigkeit von f ). Da f nicht konstant ist, ist f (x min ) < f (x ma x ). Wegen f (a) = f (b) liegt einer
der beiden Punkte x min oder x ma x echt zwischen a und b. Angenommen, das ist x ma x (der Fall von
x min ist analog). Setze c := x ma x und betrachte die Steigung
S(x) :=

f (x) â f (c)
xâc

Dann ist f â˛ (c) der Grenzwert von S(x) wenn x von links (x < c) oder von rechts (x > c) gegen c
strebt. Im ersten Fall ist f â˛ (c) > 0 wĂ¤hrend im zweiten Fall ist f â˛ (c) 6 0. Zussamengenommen ergibt
dies f â˛ (c) = 0.

Eine differenzierbare Funktion wird lokal durch ihre Tangente gut approximiert. Lokal bedeutet hier
Fehler
in einer Umgebung von x 0 , und gut, dass der Fehler in VerhĂ¤ltnis zu (x â x 0 ) klein ist, d.h.
â0
x â x0
fĂźr x â x 0 . Wir haben also
f (x) â f (x 0 ) + (x â x 0 ) f â˛ (x 0 )
Genauer gilt
ÂŠ 2003 by S. Jukna

3.6. MITTELWERTSĂTZE DER DIFFERENTIALRECHNUNG

127

Satz 3.61. (1. Mittelwertsatz von Lagrange 1736) Ist f : [a, b] â R stetig und auf (a, b) differenzierbar, so gibt es ein c â (a, b) mit (mitlerer Ableitung)
f â˛ (c) =

f (b) â f (a)
bâa

Hinter dem Mittelwertsatz steht die folgende Ăberlegung: Es gibt ein c zwischen a und b, wo die
f (a)
ist, d.h. wo Tangente und Sekante
Tangentensteigung f â˛ (c) gleich der Sekantensteigung f (b)â
bâa
parallel sind:
Steigung der Tangente
=
Steigung der Sekante

a c

b

f (a)
Beweis. Wir wenden den Satz von Role auf g(x) := f (x) â f (b)â
(x â a) an. Wir kĂśnnen dies tun,
bâa
f (a)
â˛
.

da g(a) = f (a) = g(b). Es gibt danach ein c â (a, b) mit13 0 = g (c) = f â˛ (c) â f (b)â
bâa

Mit dem 1. Mittelwertsatz kann man die folgenden, in vielen Situationen sehr nĂźtzlichen AbschĂ¤tzungen beweisen.
Lemma 3.62. FĂźr jedes x â (â1, 1) gilt
e x/(1+x) 6 1 + x 6 e x .
Beweis. Wir betrachten die Funktion f (z) = ln z. Wir wĂ¤hlen zwei Punkte a = 1 und b = 1 + x. Nach
dem 1. Mitterwertsatz (Satz 3.61) gibt es dann c â (a, b) mit
f â˛ (c) =

f (b) â f (a) ln(1 + x)
=
.
bâa
x

Wegen f â˛â˛ (z) = â z12 6 0 ist die Ableitung f â˛ (z) monoton fallend, woraus f â˛ (a) > f â˛ (c) > f â˛ (b)
folgt. Da f â˛ (a) = f â˛ (1) = 1 und f â˛ (b) = f â˛ (1 + x) = 1/(1 + x), ergibt dies die Ungleichungen
1>

ln(1 + x)
1
>
x
1+x

oder Ă¤quivalent

x
6 ln(1 + x) 6 x.
1+x
Die Behauptung folgt durch Eponentieren.



13Da c â˛ = 0 und x â˛ = 1.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

128

Eine reellwertige Funktion f (x) heiĂt konvex, falls fĂźr alle Îť â (0, 1) gilt:
f (Îť x + (1 â Îť)y) 6 Îť f (x) + (1 â Îť) f (y),
Die Funktion f heiĂt konkav, wenn â f konvex ist.
KovexitĂ¤t von Funktionen muss man sehr oft bestimmen, wenn man zum Beispiel die Jensenâs Ungleichung 14 (siehe Satz 1.23) fĂźr nicht-triviale Funktionen f (x) anwenden will. GlĂźcklicherweise
gibt es dafĂźr ein einfaches Kriterium: Es reicht, dass die zweite Ableitung von f nicht negativ ist.
Lemma 3.63. (KonvexitĂ¤t) Gilt f â˛â˛ (x) > 0 fĂźr alle x, so ist f (x) konvex.
Beweis. Wegen f â˛â˛ (x) > 0 ist die Ableitung f â˛ monoton wachsend. Seien nun x, y und Îť â (0, 1).
Wir setzen z := Îť x + (1 â Îť)y. Nach dem 1. Mitterwertsatz (Satz 3.61) gibt es dann c â (x, z) und
d â (z, y) mit
c<d
z}|{ â˛
f (z) â f (x)
f (y) â f (z)
â˛
= f (c) 6 f (d) =
zâx
yâz
Wegen
z â x = Îť x + (1 â Îť)y â x = (1 â Îť)(y â x)
y â z = y â Îť x + (1 â Îť)y = Îť(y â x)

ergibt sich somit
f (z) â f (x)
f (y) â f (z)
6
1âÎť
Îť

bzw.

f (z) = Îť f (z) + (1 â Îť) f (z) 6 Îť f (x) + (1 â Îť) f (y).
Die Funktion ist also konvex.



Eine allgemeinere (als der 1. Mittelwertsatz) Aussage liefert der folgender Satz.15
Satz 3.64. (2. Mittelwertsatz von Cauchy 1823) Sei f : [a, b] â R stetig und auf (a, b) differenzierbar. Es habe g â˛ keine Nullstelle. Dann ist g(a) , g(b) und es gibt eine Zwischenstelle c â (a, b)
mit
f â˛ (c)
f (b) â f (a)
= â˛
g(b) â g(a)
g (c)
Beweis. Da g â˛ (x) , 0 fĂźr alle x, ist g streng monoton (wachsend oder fallend) und deshalb gilt
g(a) , g(b). Wir wenden nun den Satz von Role 16 auf


f (b) â f (a)
h(x) := f (x) â f (a) +
g(x) â g(a)
g(b) â g(a)
14Ist f konvex, so gilt:

f

X
r
i=1

Îťi xi



6

r
X

Îť i f (x i )

i=1

P
fĂźr alle 0 6 Îť i 6 1 mit ri=1 Îť i = 1.
15Satz von Lagrange entspricht dem Fall g(x) = x.
16Wir kĂśnnen das tun, da h(a) = h(b) = 0.
ÂŠ 2003 by S. Jukna

3.7. APPROXIMATION DURCH POLYNOME: TAYLORENTWICKLUNG

129

an und erhalten eine Zwischenstelle c mit
0 = h â˛ (c) = f â˛ (c) â

f (b) â f (a) â˛
Âˇ g (c).
g(b) â g(a)


3.7 Approximation durch Polynome: Taylorentwicklung
Durch die erste Ableitung kĂśnnten wir eine Funktion f schreiben als
f (x + h) = f (x) + f â˛ (x) Âˇ h +R(x, h)
|
{z
}
Geradengleichung
mit limhâ0 R(x, h) = 0. In vielen FĂ¤llen reicht diese âlokaleâ AnnĂ¤hrung durch eine Gerade nicht aus,
wir benĂśtigen AnnĂ¤rungen durch Parabeln, kubische Parabeln, . . ., kurz: durch Polynome.
Sei p(x) = a0 + a1 x + a2 x 2 + a3 x 3 + Âˇ Âˇ Âˇ + an x n ein Polynom n-ten Grades. Dann gilt:
pâ˛ (x) = a1 + 2a2 x + 3 Âˇ a3 x 2 + Âˇ Âˇ Âˇ + n Âˇ an x nâ1

pâ˛â˛ (x) = 2a2 + 2 Âˇ 3 Âˇ a3 x + Âˇ Âˇ Âˇ + n Âˇ (n â 1)an x nâ2
..
.
p(n) (x) = 1 Âˇ 2 Âˇ Âˇ Âˇ n Âˇ an
Damit kann man die Koeffizienten wie folgt bestimmen: 17
a0 = p(0),

a1 =

pâ˛ (0)
,
1!

a2 =

pâ˛â˛ (0)
,
2!

...,

an =

p(n) (0)
n!

und somit auch
p(x) = p(0) +

pâ˛ (0)
pâ˛â˛ (0) 2
p(n) (0) n
x+
x + ÂˇÂˇÂˇ +
x .
1!
2!
n!

(3.20)

Interessanterweise kann man auch Nicht-Polynome f (x) in der Ă¤hnlichen Form darstellen mit einem
Restglied

Rn (x, c) :=

f (n+1) (c) n+1
x .
(n + 1)!

17Mit f (k) (a) bezeichnet man die k-te Ableitung von f (x) im Punkt a ist, d.h. f (0) (x) := f (x) und f (k+1) (x) :=
( f (k) (x)) â˛ .
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

130

Taylorformel:a
Sei f : R â R beliebig oft differenzierbar. Dann gibt es fĂźr jede x â R
ein c zwischen 0 und x mit
n
X
f (k ) (0) k
f (x) =
Âˇ x + Rn (x, c).
k!

(3.21)

k=0

Gilt lim Rn (x, c) = 0 fĂźr alle c zwischen 0 und x, so gilt auch
nââ

â
X
f (k ) (0) k
f (x) =
x .
k!

(3.22)

k=0

a Brook

Taylor, 1685-1731

Beweis. Wir werden nur den ersten Teil (3.21) beweisen. Sei
n
X
f (k ) (0) k
f (2) (0) 2 f (3) (0) 3
f (n) (0) n
T (x) =
x = f (0) + f â˛ (0)x +
x +
x + ÂˇÂˇÂˇ +
x
k!
2!
3!
n!
k=0

das Taylorpolynom. Weiterhin sei h(x) := f (x) â T (x) und g(x) := x n+1 . Beachte, dass fĂźr alle
k â {1, 2, . . . , n} folgendes gilt: 18
T (k ) (0) =

und

k!
0
. . + 0} + f (k ) (0) + 0
+ . . . + 0 = f (k ) (0)
| + .{z
k! | {z }
da x = 0
da const â˛ = 0

g (k ) (x) = (n + 1)n(n â 1) Âˇ Âˇ Âˇ (n + 1 â k)x n+1âk .

Deshalb gilt h (k ) (0) = g (k ) (0) = 0 fĂźr alle k = 1, 2, . . . , n.

Wir wenden nun den 2. Mittelwertsatz auf die Funktionen f (k ) und g (k ) an und erhalten
h(x)
g(x)

h(x) â h(0) h â˛ (c1 )
=
g(x) â g(0) g â˛ (c1 )
h â˛ (x 1 ) â h â˛ (0) h â˛â˛ (c2 )
=
=
g â˛ (x 1 ) â g â˛ (0) g â˛â˛ (c2 )
= ...
h (n) (x n ) â h (n) (0) h (n+1) (cn+1 )
=
=
g (n) (x n ) â g (n) (0)
(n + 1)!
=

mit 0 6 cn+1 6 cn 6 . . . 6 c1 6 x. Multiplikation mit g(x) = x n+1 liefert die Behauptung (mit
c = cn+1 ).




Beachte, dass die zweite Formel (3.22) nur dann gilt, wenn das Restglied Rn (x, c) gegen 0
fĂźr alle 0 6 c 6 x konvergiert (wenn n â â). Das Problem ist, dass i.A. die Potenzreihe
Pâ f (k ) (0) k
k=0
k! x kann entweder Ăźberhaupt divergent sein oder kann nicht notwendig gegen f (x) kon2
vergieren. Nimmt man zum Beispiel die Funktion f : R â R mit f (x) = eâ1/x fĂźr x , 0 und
f (0) = 0, so kann man zeigen, dass f (k ) (0) = 0 fĂźr alle k = 0, 1, . . . gilt. Damit ist die Taylor-Reihe
P
f (k ) (0) k
T (x) = â
k=0
k! x von f identisch 0, wĂ¤hrend f (x) , 0 fĂźr alle x , 0 gilt.
18Da die k-te Ableitung von x n gleich n(n â 1) Âˇ Âˇ Âˇ (n â k + 1)x nâk ist.

ÂŠ 2003 by S. Jukna

3.8. EXTREMALSTELLEN

131

â˛ Beispiel 3.65 : Sei f (x) = e x . Dann ist f (x) beliebig oft differentierbar und es gilt: f (n) (x) = f (x)
fĂźr jedes n â N. Damit ist das Taylorpolynon n-ten Grades gleich
Tn (x) =

n
X
xk
k!
k=0

Das Restglied ist
Rn (x, c) =

ec x n+1
(n + 1)!

fĂźr ein c zwischen 0 und x. Es gilt:
|Rn (x, c)| =

ec x n+1
e | x | |x| n+1
.
6
(n + 1)!
(n + 1)!

Da lim nââ x n /n! = 0, haben wir
e | x | |x| n+1
= 0.
nââ (n + 1)!
lim

Also Rn (x) â 0 fĂźr n â â. Nach der Taylorformel gilt:

â

X xi
x2
xn
e =1+x+
+ ÂˇÂˇÂˇ +
+ ÂˇÂˇÂˇ =
.
2!
n!
i!
x

(3.23)

i=0

FĂźr x = 1 gibt dies die Formel zur Berechnung von e:
e= 1+1+

1
1
1
+ + ÂˇÂˇÂˇ +
+ Âˇ Âˇ Âˇ = 2, 7182818...
2! 3!
n!

Ăhnlich bekommt man

ln(1 + x) = x â

â

X
x2 x3 x4
xi
+
â
â... =
(â1) i+1
2
3
4
i

(3.24)

i=1

Dazu reicht es zu beobachten, dass fĂźr f (x) = ln(1+ x) nach der Quotientenregel fĂźr Ableitungen
gilt (nachrechnen!): f (k ) (0) = (â1) k+1 (k â 1)!.

3.8 Extremalstellen
Sei f : R â R eine funktion und a â R. Dann heiĂt a eine lokale Maximalstelle von f , falls es ein
ÇŤ > 0 gibt, so dass f (a) > f (x) fĂźr alle x â UÇŤ (a) gilt. Die lokale Minimalstelle ist analog definiert.
Lemma 3.66.
1. Extremalstellen-Test: Gilt f â˛ (a) = 0 und hat f â˛ (x) an der Stelle a ein Vorzeichenwechsel,
dann liegt in a ein Extremum vor.
Dabei ist a ein lokales Maximum, wenn f â˛ (x) von + nach â wechselt, sonst ist a ein lokales
Minimum.
2. Extremalstellen-Test Gilt f â˛ (a) = 0 und f â˛â˛ (a) > 0 (bzw. f â˛ (a) = 0 und f â˛â˛ (a) < 0), so ist a
lokale Minimalstelle (bzw. Maximalstelle) von f .
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

132

Beweis. Teil (1) folgt unmittelbar aus der Definition von f â˛ (x).
Teil (2): Sei f â˛â˛ (a) > 0. Zu ÇŤ = f â˛â˛ (a)/2 gibt es (nach der Definition der Ableitung) ein Î´ > 0 mit
â f â˛â˛ (a)/2 < f â˛â˛ (x) â f â˛â˛ (a) < f â˛â˛ (a)/2
also f â˛â˛ (x) > f â˛â˛ (a)/2 > 0 fĂźr alle x mit |x â a| < Î´. FĂźr solche x gibt es (laut Taylorformel) ein c
zwischen x und a (also insbesondere |c â a| < Î´) mit
f (x) = f (a) + f â˛ (a) (x â a) + f â˛â˛ (c)(x â c) 2 /2 = f (a) + f â˛â˛ (c)(x â c) 2 /2 > f (a).
| {z }
|
{z
}
=0

>0

f (a) ist also ein lokales Minimum. Ist f â˛â˛ (a) < 0, so betrachte g = â f .



3.9 Die Bachmann-Landau-Notation: klein o und groĂ O
In diesem Abschnitt werden wir die asymtotische Notation einfĂźhren, die Sie wĂ¤hrend Ihres weiteren
Studiums stĂ¤ndig begleiten wird.
Seien A und B zwei Algorithmen mit Laufzeiten TA (n) und TB (n). Um diese Algorithmen (bezĂźglich
ihrer Laufzeit) zu vergleichen, fragt man
-

Ist A im âwesentlichenâ genau so schnell wie B?

-

Ist A âvielâ schneller als B?

Um solche (und Ă¤hnliche) Fragen mathematisch zu prĂ¤zisieren, hat sich die folgende asymptotische
Notation als sehr hilfreich erwiesen. Die Notation O(x) war erstmal von Bachmann (1894) in seinem
Buch (Ăźber Zahlentheorie) und Landau (wie er selbst in 1909 schrieb) hat diese Notation erst aus
diesem Buch kennengelernt. Die Notation o(x) hat Landau selbst eingefĂźhrt.
Um unnĂśtigen âFeincheitenâ zu vermeiden, werden wir in diesem Abschnitt hauptsĂ¤chlich nur die
Funktionen f : N â R betrachten.
Definition:
1.

f = O(g) â Es gibt eine Konstante c > 0 und eine Zahl n0 â N, so dass f (n) 6 c Âˇ g(n) fĂźr alle
n > n0 gilt.

2.

f = o(g) â lim

f (n)
nââ g (n)

= 0 (man schreibt auch f âŞ g).

Davon abgeleitete Notationen:
3. f = âŚ(g) â g = O( f ).
g (n)
nââ f (n)

4. f = Ď(g) â lim

= 0 (man schreibt auch f âŤ g)

5. f = Î(g) â f = O(g) und g = O( f ) (man schreibt auch f â g)
g (n)
nââ f (n)

6. f âź g â f = (1 + o(1))g, d.h. wenn lim

=1

Was besagen die einzelnen Notationen? f = O(g) drĂźckt aus, dass f asymptotisch nicht stĂ¤rker als g
wĂ¤chst (obwohl f (n) durchaus stets grĂśĂer als g(n) sein kann). f = âŚ(g) besagt, dass f zumindest
ÂŠ 2003 by S. Jukna

3.9. DIE BACHMANN-LANDAU-NOTATION: KLEIN O UND GROSS O

133

so stark wie g wĂ¤chst. f = Î(g) impliziert, dass f und g gleichstark wachsen. f = o(g) drĂźckt aus,
dass f echt schwĂ¤cher als g wĂ¤chst.
Bemerkung 3.67. Um den Unterschied zwischen klein-o und groĂ-O besser zu verstehen, lĂśht es
sich ihre Definitionen formal zu vergleichen.

f = O(g)
f = o(g)

ââ

ââ

â c > 0 ân0 ân > n0 : f (n) 6 c Âˇ g(n)

â c > 0 ân0 ân > n0 : f (n) 6 c Âˇ g(n)

Der einziger Unterschied ist also im ersten Quantor! FĂźr f = O(g) reicht es, dass es mindestens eine
(auch wenn sehr groĂe!) Konstante c > 0 gibt, so dass ab bestimten Schwenllwert n0 die Funktion
f (n) durch cg(n) nach oben beschrĂ¤nkt ist. Im Gegensatz dazu sagt f = o(g), dass es fĂźr jede (auch
wenn sehr kleine!) Konstante c > 0 wird f (n) ab einem bestimten Schwenllwert n0 nicht mehr den
Wert cg(n) Ăźberschreiten.
Die asymptotischen Notationen erwecken zunĂ¤chst den Anschein, als wĂźrden einfach Informationen
weggeworfen. Dieser Eindruck ist auch sicher nicht falsch. Man mache sich jedoch klar, dass wir,
wenn wir eine Aussage wie etwa
2
1
4n3 + n â 2 = Î(n3 )
3
n
machen, nur untergeordnete Summanden und konstante Faktoren weglassen. Wir werfen also sehr
gezielt die Teile weg, die fĂźr hinreichend hohe Werte von n nicht ins Gewicht fallen (nachrangige
Terme), und wir verzichten auf die fĂźhrende Konstante (hier 4), da die konkreten Konstanten bei
realen Anwendungen ohnehin keine absolute konstanten sind. Man kann also sagen: âWir reduzieren
einen Ausdruck auf das asymptotisch Wesentliche.â
Alternativ formuliert, die asymptotischen Notationen erlauben es uns, Funktionen in verschiedene
Wachstumsklassen zusammenzufassen. Obiger Ausdruck hat die Bedeutung: âDer Ausdruck links gehĂśrt in die Klasse der Funktionen, die nicht schneller als n3 wachsen.â
Der Einwand, dass bei so einer Zielsetzung das Elementzeichen, also
2
1
4n3 + n â 2 â Î(n3 ),
3
n
wesentlich intuitiver wĂ¤re, ist gerechtfertigt und nur ungenĂźgend mit dem Verweis, das Gleichheitszeichen sei Konvention, zu entkrĂ¤ften.



Die Benutzung vom Gleichheitssymbol in der Bezeichnung â f = O(g)â ist zwar fast Ăźberal
in der Literatur verbreitet, man muss aber immer erinnern, dass das mit der Gleichheit zweier
Funktionen nichts zu tun hat! WĂ¤re nĂ¤mlich f = O(g) als eine Gleichheit verstanden, so kĂśnnte man
auch O(g) = f schreiben. Aber dann kommt man schnell zum Unsinn: es ist 2n = O(n) also ist
auch O(n) = 2n und, da n = O(n), sollte auch auch n = O(n) = 2n âgeltenâ. Wir werden deshalb
nie âO(g) = f â anstatt â f = O(g)â schreiben. Man muss sich immer erinnern, dass unter âO(g)â
eine Klasse von Funktionen versteckt ist. Deswegen schreibt man manchmal f â O(g) anstatt von
f = O(g)
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

134



Aus f = o(g) folgt zwar g = âŚ( f ), aber
f = o(g)

=â

g = O( f )

gilt nicht! In der Tat, wĂ¤re es g = O( f ), so sollte g(n) 6 c f (n) fĂźr eine positive (da g , 0) Konstante
1
c > 0 und alle n > n0 gelten. Aber dann hĂ¤tten wir lim gf (n)
6 lim cff(n)
(n) = c , 0, ein Widerspuch
nââ (n)
nââ
mit f = o(g).




Vorsicht mit Exponenten! Zum Beispiel ist 4n , O(2n ): 2n /4n = 1/2n und somit lim 2n /4n =
nââ
0. D.h. 2n = o(4n ) woraus 4n , O(2n ) folgt (siehe vorige Bemerkung).
Vorsich mit Logarithmen! FĂźr das Wachstum der Logarithmus-Funktion gilt:
f = O(g)

=â

f = o(g)

=â

loga f = O(loga g)

aber
loga f = o(loga g)
â
gilt im Allgemeinen nicht! Gegenbeispiel: f (n) = n und g(n) = n. Dann ist loga f (n) =
Î(loga g(n)).

1
2

loga n =

Die Symbolen O, o und âŚ, Ď haben unterschiedliche Bedeutung: Die ersten zwei geben eine obere
wĂ¤hrend die letzten zwei eine untere Schranke an.
obere Schranke

untere Schranke

f = O(g)

f = âŚ(g)

f = o(g)

f = Ď(g)

Gilt zum Beispiel f (n) = O(n2 ), dann bedeutet dies nur, dass f (n) nicht schneller als n2 wĂ¤chst â es
kann gut sein, dass (in Wirklichkeit) f (n) nur linear oder sogar noch langsammer wĂ¤chst. Zeigt man
aber, dass auch f (n) = âŚ(n2 ) gilt (eine untere Schranke), so kann man bereits sagen, dass (bis zu
multiplikativen Faktoren) die Funktion f (n) quadratisch wĂ¤chst.

f (n)
nââ g (n)

Behauptung 3.68. Der Grenzwert lim
1.

Wenn L = 0, dann ist f = o(g).

2.

Wenn L < â, dann ist f = O(g).

3.

Wenn 0 < L < â, dann ist f = Î(g).

Beweis. Ăbungsaufgabe.

= L mĂśge existieren.



Diese Behauptung erlaubt es also, eine asymptotische Relation durch Berechnung des entsprechenden
Grenzwertes zu verifizieren. Diese Methode versagt nur dann, wenn der Grenzwert nicht existiert.
ÂŠ 2003 by S. Jukna

3.9. DIE BACHMANN-LANDAU-NOTATION: KLEIN O UND GROSS O

135

â˛ Beispiel 3.69 : Sei f (n) = 16 n3 + 12 n2 + 13 n â 1 = Î(n3 ). Nach der Grenzwertregeln gilt:
lim

nââ

f (n)
n3

=
=
=

1 3
6n
nââ n3

lim

1 2
2n
nââ n3

+ lim

1
3n
nââ n3

+ lim

1
nââ n3

â lim

1
1
1
1
+ lim
+ lim
+ lim 3
2
nââ
nââ
nââ
6
2n
3n
n
1
.
6

Damit ist 0 < lim f (n)/n3 = 1/6 < â und somit auch f (n) = Î(n3 ).
nââ

Genauso kann man zu Beispiel zeigen, dass die Funktion
Ď 2 3nâ7 +

(2, 7n113 + n9 â 86) 4
â
â 1, 083n
n

nichts anderes als Î(3n ) ist.
â˛ Beispiel 3.70 : Wir definieren f (n) =



1 n gerade
0 sonst

Es sei g(n) = 1 fĂźr alle n â N. Dann existiert der Grenzwert von
unendlich oft auftauchen. Es ist aber
f = O(g)

f (n)
g (n)

nicht, da die Werte 0 und 1

(mit c = 1 und n0 = 0). Man verifiziere, dass mit Ausnahme der Beziehung g = âŚ( f ) alle
weiteren asymptotischen Relationen zwischen f und g nicht gelten. (Warum ist zum Beispiel die
Beziehung g = Î( f ) falsch?)
Um zu bestimmen, zu welcher Î-Klasse eine endliche Summe f (n) = g1 (n) + Âˇ Âˇ Âˇ + gk (n) (wenn k ist
eine Konstante und damit hĂ¤ngt von n nicht ab) gehĂśrt, reicht es die Î-Klassen der Funktionen gi (n)
zu bestimmen und die grĂśĂte davon zu nehmen.



Wenn es zwei (oder mehrere) gleichgrĂśĂe Terme sind, aber einige davon negativ sind, dann
muss man aufpassen, ob der grĂśĂter Term nach der Umformung da Ăźberhaupt bleibt. So ist
zum Beispiel
3n + 5n â 6n = 2n = Î(n) aber



3(n + 2) + 5(n + 3) â 8n = Î(1),
Pn
n2
n+1
i=1 i â 2 = 2 = Î(n).

GefĂ¤hrlich ist auch dann, wenn die Funktion unendlich oft negative Werte annimt:

2 + sin n = Î(1) aber
sin n , Î(1) (unendlich oft negativ)
1 + sin n , Î(1) (unendlich oft 0 vorkommt)
â2n , Î(n) (ist negativ)
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

136



Vorsicht mit Exponenten! Die Aussage
f = Î(g)

=â

F ( f ) = Î(F (g))

gilt nur, wenn F (x) durch ein Polynom nach oben beschrĂ¤nkt ist. So gilt zum Beispiel
f = Î(g) â f 2 = Î(g2 );
f = Î(g) â log f = Î(log g);
f = Î(g) â f 100 = Î(g100 )

aber f = Î(g) â 2 f = Î(2g ) gilt nicht!
Um f = o(g) zu zeigen, muss man den Grenzwert lim f (n)/g(n) der Quotientfunktion f (n)/g(n)
nââ
betrachten. Das ist aber oft nicht so einfach. Streben zum Beispiel die Funktionen f (n) und g(n) beide
gegen 0 oder beide gegen â, so bekommen wir unbestimmte AusdrĂźcke 0/0 oder â/â. Was dann?
Bedeutet nun das, dass f (n)/g(n) keinen Grenzwert hat? Nicht unbedingt! Um solche unbestimmte
AusdrĂźcke zu behandeln, gibt es einige Regeln.
Diese Regeln stammen aus dem Buch Analyse des infiniment petits (1696) von de lâHospital.19
Regeln von de lâHoĚpital: Sei a < b 6 â und ââ 6 L 6 +â. Ferner
seien f , g : [a, b) â R differenzierbare Funktionen mit g â˛ (x) , 0 fĂźr
alle x â [a, b), und es gelte lim f (x) = lim g(x) = 0 oder lim g(x) =
xâb

xâb

f â˛ (x)
f (x)
= L, so ist lim
= L.
Âąâ. Gilt dann lim â˛
xâb g (x)
xâb g(x)
Dasselbe gilt auch fĂźr GrenzĂźbergang x â a anstatt x â b.

xâb

Beweis. Wir betrachten nur den Fall lim xâb f (x) = lim xâb g(x) = 0 (der Fall lim g(x) = Âąâ ist
xâb

analog). Wir kĂśnnen o.B.d.A. annehmen, dass g(x) , 0 in [a, b) (sonst kleineres Interval wĂ¤hlen).20
Wir definieren zwei Hilfsfunktionen:


f (x) falls x , b
g(x) falls x , b
F (x) =
und
G(x) =
0 falls x = b
0 falls x = b
und erhalten mit dem zweiten Mittelwertsatz (Satz 3.64)
f (x)
F (x) â F (b)
F â˛ (c)
=
= â˛
g(x) G(x) â G(b) G (c)

mit x < c < b

Durch GrenzĂźbergang x â b (und damit auch c â b) folgt die Behauptung.



Bemerkung 3.71. Entsprechen f (n) und g(n) den Laufzeiten irgendwelchen Algorithmen, so sind
f , g Abbildungen von N (und nicht von R) nach R, da n die EingabelĂ¤nge ist, die Ăźblicher Weise
ganzzahlig ist. Deshalb wissen wir nicht, was die Ableitungen f â˛ (n) und g â˛ (n) sein sollten. Trotzdem
gibt es einen einfahen Trick, wie man dieses Problem umgehen kann: Man erweitert den Definitionsbereich von N auf R, indem man anstatt einer natĂźrlicher Zahl n in den Formeln fĂźr f (n) und g(n)
19Guilame Francois Antoine Marquis de lâHoĚpital (1661-1704) war der Schuler von Johann Bernoulli (1667-1748). Sein
Buch war das erste Buch in Analysis Ăźberhaupt. Der Satz selbst war eigentlich von Bernoulli bewiesen worden, trĂ¤gt aber
die Name des Buchauthors. Der Name wird entweder als âlâHoĚpitalâ (alt) oder als âlâHospitalâ (neu) geschrieben. Beide
spricht man als âLopitalâ aus.
20Da g â˛ (x) keine Nullstelle in (a, b) hat, kann g(x) nach dem Satz von Role hĂśchstens einmal den Wert 0 annehmen.
ÂŠ 2003 by S. Jukna

3.9. DIE BACHMANN-LANDAU-NOTATION: KLEIN O UND GROSS O
eine reelle Zahl betrachtet, und zeigt, dass

f (x)
g (x)

den Grenzwert 0 fĂźr x â â hat. Nach der Definition

des Grenzwertes reellen Funktionen, muss dann auch die Folge
â
â:

â˛ Beispiel 3.72 : Der Grenzfall
lâHospitalschen Regeln:

lim

xââ

137

f (n)
g (n)

den Grenzwert 0 haben.

FĂźr n â N erhĂ¤lt man durch n-malige Anwendung der

xn
nx nâ1
n!
=
lim
= . . . = lim x = 0.
x
x
xââ e
xââ e
e

â˛ Beispiel 3.73 : Der Grenzfall 00 : Auf dem Interval I = (0, 1) gilt:
lim

xâ1

ln x
x â1
= lim
=1
x â 1 xâ1 1

â˛ Beispiel 3.74 : Der Grenzfall 1â : lim x 1/(xâ1) =?
xâ1

Logarithmieren und Anwenden der lâHospitalschen Regeln ergibt:
ln x 1/(xâ1) =

ln x
xâ1

und somit lim x 1/(xâ1) = e1 = e.
xâ1

â
â:

â˛ Beispiel 3.75 : Der Grenzfall
lâHospitalschen Regeln:

lim

xââ

FĂźr n â N erhĂ¤lt man durch n-malige Anwendung der

xn
nx nâ1
n!
= lim
= . . . = lim x = 0.
x
xââ e x
xââ e
e

â˛ Beispiel 3.76 : Der Grenzfall 00 : Seien a, b > 0. Dann gilt
a x â bx
a
= ln
xâ0
x
b
lim

Beweis: Setze f (x) := a x â bx , g(x) := x â f â˛ (x) = a x ln a â bx ln b und g â˛ (x) = 1 â
a x â bx
f â˛ (x)
a
= lim â˛
= lim a x ln a â lim bx ln b = ln a â ln b = ln
xâ0
xâ0 g (x)
xâ0
xâ0
x
b
lim

Bemerkung 3.77. Neben der oben diskutierten GrenzĂźbergĂ¤ngen in Quotienten tretten auch irregulĂ¤re ProduktausdrĂźcke der folgender Art auf:
lim f (x) = 0, lim g(x) = â

xâa

xâa

=â

lim f (x) Âˇ g(x) =?

xâa

Die kann man hĂ¤ufig in der Form
lim f (x) Âˇ g(x) = lim

xâa

xâa

f (x)
g(x) â1

mit den obigen lâHospitalschen Regeln behandelt werden.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

138

Manchmal sind auch irregulĂ¤re ExponentialausdrĂźcke der Form
lim f (x) g (x) =?

xâa

zu untersuchen, was zu GrenzfĂ¤llen der Art 00 , â0 und 0â fĂźhren kann. In diesem Fall wird zunĂ¤chst
logarithmiert,
lim g(x) ln( f (x)) =?
xâa

was zu obinem Fall fĂźhrt. Der Grenzwert des gegebenen Ausdrucks ist dann wegen der Stetigkeit der
Exponentialfunktion gegeben durch


lim g(x) ln( f (x)) = exp lim g(x) ln( f (x)) .
xâa

xâa

Mittels der Tranformation

f (x) â g(x) =
wird der Grenzfall â â â in den Fall

0
0

1
g (x)

â

1
f (x)

1
f (x) Âˇg (x)

ĂźberfĂźhrt.

â˛ Beispiel 3.78 : lim x x =?. Logarithmieren und die lâHospital Regel ergibt:
xâ0

ln x
xâ0 x â1
1
= â lim x 2
xâ0 x
= 0

lim x Âˇ ln x =

xâ0

lim

(â

â
â

â kann lâHospital anwenden)

(lâHospital)

und somit
lim x x = e0 = 1.



xâ0

Manchmal funktionieren die Regeln von lâHospital nicht, da die AudrĂźcke hin-und-hier oszillieren. Zum Beispiel
lim

xââ

x
2
(x + 1) 1/2

1
(x 2 + 1) 1/2
=
lim
xââ x(x 2 + 1) â1/2
xââ
x
2
â1/2
x(x + 1)
x
= lim
= lim 2
.
xââ
xââ
1
(x + 1) 1/2
=

lim

Wir betrachten als nĂ¤chstes eine Wachstums-Hierarchie wichtiger Laufzeitfunktionen.
Lemma 3.79. (Wachstum von Standardfunktionen) Seien a, b â R+ , dann gilt:
1. Wenn a < b, dann gilt x a = o(x b ).
2. Es gilt (auch wenn b sehr groĂ und a sehr klein): (ln x) b = o(x a ). D.h. logarithmisches Wachstum
ist unwesentlich gegenĂźber dem Wachstum von Polynomen.
3. Es gilt (auch wenn b sehr groĂ und a sehr klein): x b = o(2a Âˇx ). D.h. polynomiales Wachstum ist
unwesentlich gegenĂźber dem Wachstum von Potenzen.
ÂŠ 2003 by S. Jukna

3.9. DIE BACHMANN-LANDAU-NOTATION: KLEIN O UND GROSS O

139

Beweis.
lim x a /x b = lim

xââ

lim

xââ

(ln x) b
xa

xb
xââ 2a Âˇx
lim

xââ

1
x bâa

(da b â a > 0).

=0

1
ln x
x
=
lim
a
xââ x a/b
xââ a x b â1
b
b
1
=
Âˇ lim a/b = 0
a xââ x

=

lim

=

lim

=

x

xââ 2a Âˇx/b

lim

xââ

x

= lim

xââ ec Âˇx

1
=0
cec Âˇx

(Regel von de lâHoĚpital)

mit c = (a ln 2)/b > 0

(Regel von de lâHoĚpital)


Als Folgerung haben wir die folgenden Wachstumsstufen.21 FĂźr a > 1, k > 1 und b > 1 gilt:
loga n âŞ n âŞ n loga n âŞ nk âŞ bn âŞ n!
â˛ Beispiel 3.80 : Gegeben sind 4 Algorithmen A1 , A2 , A3 , A4 mit entsprechenden Laufzeiten
T1 (n) = 1000n,
T2 (n) = 200n log2 n,
T3 (n) = 10n2 ,
T4 (n) = 2n .
ďŁą
ďŁ˛ A4
Schnellster Algorihmus : =
A
ďŁł 3
A1

fĂźr 1 6 n 6 9
fĂźr 10 6 n 6 100
fĂźr n > 101.

â˛ Beispiel 3.81 : Bei einer Millisekunde (10â3 ) pro Operation und einer Stunde zur VerfĂźgung ist die
grĂśĂte mĂśgliche Problemstellung (d.h. die grĂśĂte mĂśgliche Input-LĂ¤nge):

max n : T (n) 6 3, 6 Âˇ 103

A1
A2
A3 A4
3600 1600 600 21

Verzehnfachung der Maschinengeschwindigkeit:

max n : T (n) 6 3, 6 Âˇ

103

A1
A2
A3
A4
36000 13500 1900 25

21Zur Erinnerung: f âŞ g ist einfach eine andere Schreibweise fĂźr f = o(g).
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

140

â˛ Beispiel 3.82 : In einem Wettbewerb sollen 1.000.000 Zahlen sortiert werden. Teilnehmer sind ein
schneller Rechner und ein PC, auf deren aber verschieden schnelle Sortier-Algorithmen laufen.
Op. pro sek. Anzahl d. Operationen
100 Mio.
1 Mio

2n2
50n log n

schneller Rechner

2 Âˇ (106 ) 2 Oper.
108 Oper./sec

= 20.000 sec

â 5.56 h

PC

50 Âˇ 106 Âˇ log 106 Oper.
106 Oper./sec

â 1.000 sec

â 16, 67 min

schneller Rechner
PC

Rechenzeit:

Fazit: Es lohnt sich, sowohl eine grĂśĂenordnungsbezogene wie auch eine asymptotische Analyse
durchzufĂźhren.

3.10 Rekurrenzenâ
Im Spiel âTĂźrme von Hanoiâ (siehe Abschitt 1.4.1) haben wir drei StĂ¤be 1, 2 und 3. UrsprĂźnglich
besitzt Stab 1 n Ringe, wobei die Ringe in absteigender GrĂśĂe auf dem Stab aufgereiht sind (mit dem
grĂśĂten Ring als unterstem Ring). Die StĂ¤be 2 und 3 sind leer.
Ein Zug besteht darin, einen zuoberst-liegenden Ring von einem Stab zu einem anderen zu bewegen.
Der Zug ist aber nur dann erlaubt, wenn der Ring auf einen grĂśĂeren Ring gelegt wird oder wenn
der Stab leer ist. Das Spiel ist erfolgreich beendet, wenn alle Ringe von Stab 1 nach Stab 2 bewegt
wurden.

1

2

3

Wir haben dieses Spiel bereits in Abschnitt 1.4.1 betrachtet und haben einen folgenden rekursiven
Algorithmus entworfen:
Algorithmus Hanoi (n; 1, 2, 3)
While n > 0 do
Rufe Hanoi (n â 1; 1, 3, 2) auf [die obersten n â 1 Scheiben von Stapel 1 zum Stapel 2]
Verlege die zuoberst liegende Scheibe auf 1 nach 3
ÂŠ 2003 by S. Jukna

3.10. REKURRENZENâ

141

Rufe Hanoi (n â 1; 2, 1, 3) auf [verlege n â 1 Scheiben vom Hilfsstapel 2 nach 3]
Sei an die minimale Anzahl der ZĂźge, die ausreichen sind, alle Ringe von Stab 1 nach Stab 2 zu
bewegen. Aus dem oben angegebenen Algorithmus folgt: an 6 2 Âˇ anâ1 + 1. Man kann auch zeigen
(probiere das!), dass an > 2 Âˇ anâ1 + 1 gilt. Also haben wir die folgende Rekursionsgleichung:
a0 = 0
an = 2 Âˇ anâ1 + 1 fĂźr n > 0
Wenn wir nun die Rekursionsgleichung entwickeln wollen, bekommen wir in jedem Schritt einen
âstĂśrendenâ Term +1. Stattdessen wenden wir einen (of sehr hilfreichen) Trick an: konstruiere eine neue einfachere Rekursionsgleichung. In unserem Fall reicht es eine Eins auf beiden Seiten zu
addieren:
an + 1 = 1
an + 1 = 2 Âˇ anâ1 + 2 fĂźr n > 0
Nun setzen wir bn := an + 1 und erhalten
b0 = 1
bn

= 2 Âˇ bnâ1 fĂźr n > 0

und diese letzte Rekursionsgleichung ist leicht zu lĂśsen:
bn

= 2bnâ1 = 22 bnâ2 = 23 bnâ3 = . . . = 2i bnâi = . . . = 2n b0 = 2n .

Also ist an = bn â 1 = 2n â 1.
(Das Pizzaproblem, Jacob Steiner 1826) Wieviele Pizzascheiben kann
man bekommen, wenn man die Pizza mit n geraden Schnitten aufteilt?
Oder mehr mathematisch: Was ist die maximale Anzahl x n der FlĂ¤chen
in der Ebene, die man mit n Linien bekommen kann?
Bestimmt kann man denken, dass x n â 2n gelten sollte: jede neue Gerade verdoppelt die Anzahl der
Pizzascheiben! Diese erste Reaktion ist aber total daneben! Die Funtion x n wĂ¤chst viel langsammer,
nĂ¤hmlich x n = Î(n2 ).
1. Die n-te Gerade ergibt k neuen FlĂ¤chen genau dann, wenn sie k alten FlĂ¤chen schneidet, und sie
kann so viele alten FlĂ¤chen schneiden genau dann, wenn sie die alten Geraden in genau k â 1
Punkte trifft.
2. Zwei Geraden kĂśnnen sich in hĂśchstens einem Punkt treffen.
3. Aus (2) folgt, dass die neue Gerade kann die n â 1 alten Geraden in hĂśchstens n â 1 Punkt treffen.
4. Aus (1) und (3) folgt, dass k 6 n.
Damit erhalten wir die Rekursionsungleichung
x n 6 x nâ1 + n

fĂźr n > 0
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

142

Man kann zeigen, dass hier auch Gleichheit gilt: es reicht die neue Gerade so zu wĂ¤hlen, dass sie
parallel zu keiner der alten ist und keinen der alten Treffpunkte berĂźht. Damit haben wir die Rekursionsgleichung:
x0 = 1
xn
Damit ist x n =
(siehe (3.1):

Pn

k=1

= x nâ1 + n

fĂźr n > 0

k die (bereits und bekannte) arithmetische Reihe, und wir wissen die Antwort
xn =

n(n + 1)
= Î(n2 ).
2

Rekurrenzen vom Grad 2
Die Beiden oben betrachteten Folgen waren âlinearâ und hatten die Form x n = Ax nâ1 + B. In Anwendungen kommen aber kompliziertere Folgen vor, wo jeder Folgeglied x n durch eine lineare Funktion
von d > 2 letzten Folgeglieder x nâ1 , x nâ2 , . . . , x nâd bestimmt ist. Solche Rekurrenzen nennt man
Rekurrenzen vom Grad d. FĂźr solchen Folgen gibt es auch einige Tricks, um die geschlossene Form
fĂźr x n zu finden. In diesem Abschnitt betrachten wir den Falls d = 2. (Rekurrenzen vom hĂścheren
Grad werden wir im nĂ¤chsten Abschnitt betrachten.)
Die Folge <x n > = x 0 , x 1 , x 2 , . . . sei durch die ersten zwei Zahlen x 0 = a0 , x 1 = a1 und eine Rekurrenz
x n = Ax nâ1 + B x nâ2

(3.25)

oder Ă¤quivalent durch die Gleichung
x n â Ax nâ1 â B x nâ2 = 0
gegeben. Wir wollen eine Funktion f (n) mit x n = f (n) fĂźr alle n bestimmen. Dazu gibt es ein
allgemeines Verfahren. Dazu betrachtet man die Nullstellen des charakteristischen Polynoms z 2 â
Az â B, d.h. man betrachtet die LĂśsungen des quadratischen Gleichungs
z 2 â Az â B = 0.
Satz 3.83. Seien r und s die LĂśsungen von z 2 â Az â B = 0. Dann hat die LĂśsung der RekurrsionsGleichung (3.25) die Form
 n
ar + bs n
falls r , s
xn =
n
n
ar + bnr
falls r = s
wobei die Zahlen a und b (eindeutig) durch die Randbedingungen a + b = a0 , ra + sb = a1 bzw.
a = a0 , ra + sb = a1 bestimmt sind.
Beweis. Zuerst beobachten wir, dass x n = ar n fĂźr jede reele Zahl a , 0 und jedes r mit r 2 â ArâB = 0
eine LĂśsung der Rekursionsgleichung (3.25) ist:
ar n = Aar nâ1 + Bar nâ2

ââ

r 2 = Ar + B

ââ

r 2 â Ar â B = 0.
ÂŠ 2003 by S. Jukna

3.10. REKURRENZENâ

143

Es reicht also zu zeigen (Ăbungsafgabe!), dass die Summe un = s n + t n je zwei LĂśsungen s n und t n
von (3.25) auch eine LĂśsung von (3.25) ist. Um die Zahlen a und b zu bestimmen, reicht es die ersten
zwei Folgenglieder x 0 = a0 und x 1 = a1 zu betrachten:
a0 = ar 0 + bs0 = a + b
a1 = ar 1 + bs1 = ar + bs.

â˛ Beispiel 3.84 : Die berĂźhmte Folge der Fibonacci-Zahlen (x n ) = x 1 , x 2 , x 3 , . . . ist durch x 1 = x 2 =
1 und die Rekurrsion x n = x nâ1 + x nâ2 gegeben. Die Nullstellen des charakteristischen Polynoms
z 2 â r â 1 sind
â
â
1+ 5
1â 5
r=
und s =
.
2
2
â
â
Die Randbedingungen a + b = 1 und ar + bs = 1 ergeben: a = 1/ 5 und b = â1/ 5.
Damit gilt:
1
xn = â
5

â !n
1+ 5
1
â â
2
5

â !n
1â 5
.
2

â˛ Beispiel 3.85 : Sei die Folge (x n ) durch die Randbedingungen x 0 = 2, x 1 = 7 und die Rekurrsionsgleichung x n = x nâ1 + 2x nâ2 gegeben. Das charakteristische Polynom in diesem Fall ist
z 2 â z â 2 = (z â 2)(z + 1). Seine Nullstellen sind also r = 2 und s = â1. Damit hat x n die Form
x n = ar n + bs n = a2n + b(â1) n , wobei a und b sind durch das Gleichungssystem
a + b = x0 = 2
2a â b = x 1 = 7
bestimmt, woraus a = 3 und b = â1 folgt. Damit ist x n = 3 Âˇ 2n â (â1) n die gesuchte LĂśsung.
In der Stochastik tretten endliche Folgen x 0 , x 1 , . . . , x N , die durch die Rekursionsgleichungen von der
Form x n = px n+1 + (1 â p)x nâ1 mit x 0 = 0, x N = 1 und 0 < p 6 1/2 auf.
â˛ Beispiel 3.86 : (Gamblerâs Ruin) Ein Spieler namens Theo Retiker nimmt in einem Casino an
einem Spiel mit Gewinnwahrscheinlichkeit 0 < p 6 1/2 teil. 22 Zum Beispiel wirft man eine
(nicht unbedingt faire) MĂźnze, dessen Seiten mit rot und blau gefĂ¤rbt sind, und wir gewinnen,
falls rot kommt.
Wir nehmen an, dass Theo in jedem Schritt (oder Spielrunde) nur 1 e einsetzen kann. Geht die
Runde zu Theos Gunsten aus, erhĂ¤lt er den Einsatz zurĂźck und zusĂ¤tzlich denselben Betrag aus
der Bank (Gewinn = 1 e). Endet die Runde ungĂźnstig, verfĂ¤llt der Einsatz (Gewinn = â1 e).
Theo kommt ins Casino mit n Euro (Anfangskaptal) und sein Ziel ist m Euro zu gewinnen (dann
will er aufhĂśren); in diesem Fall sagen wir, dass Theo gewinnt. Theo spielt bis er m Euro gewinnt
oder bis er alle seine mitgenommenen n Euro verliert.
Sei N = n + m fest, und sei x n die Wahrscheinlichkeit, dass Theo gewinnt, wenn sein Anfangskapital n ist. Also x 0 = 0 und x N = 1. Nehmen wir nun an, dass Theo mit Anfangskapital n
22NatĂźrlich, wird kein Casino eine Gewinnwahrscheinlichkeit p > 1/2 zulassen.
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

144

(0 < n < N ) beginnt. Nach der ersten Runde wird Theo mit Wahrscheinlichkeit p gewinnen
und n + 1 Euro haben; danach wird er Gewinner mit Wahrscheinlichkeit x n+1 . Andererseits kann
Theo die erste Wette mit Wahrscheinlichkeit q = 1 â p verlieren; dann wird er nur n â 1 Euro
haben und kann nur mit Wahrscheinlichkeit x nâ1 das ganze Spiel gewinnen. Insgesamt ist Theo
der Gewinner mit Wahrscheinlichkeit x n = px n+1 + qx nâ1 .
Wir kĂśnnen die Gleichung x n = px n+1 + qx nâ1 als
px n+1 â x n + qx nâ1 = 0

(3.26)

umschreiben. Wir wiederum raten die LĂśsung in der Form x n = z n mit z > 0. Dann erhalten wir
aus (3.26) die Gleichung pz n+1 â z n + z nâ1 q = 0. Dividieren wir dann beide Seiten durch z nâ1 und
erhalten eine quadratische Gleichung:
pz 2 â z + (1 â p) = 0
LĂśsen wir diese Gleichung, so bekommen wir:
z1,2 =
=
=

1Âą

â

1 â 4p(1 â p)
2p
1 Âą (1 â 2p)
2p
1âp
oder 1
p

Die LĂśsungen sind verschieden genau dann, wenn p , 1/2. Damit ergeben sich zwei FĂ¤lle p < 1/2
und p = 1/2.
Fall 1: p < 1/2. Im diesem Fall haben wir zwei verschiedene Losungen r = (1 â p)/p und s = 1. Wir
kĂśnnen deshalb entweder x n = r n oder x n = s n = 1 nehmen, und die Gleichung (3.26) wird erfĂźllt.
Da die linke Seite von (3.26) fĂźr x n = r n und fĂźr x n = 1 gleich Null ist, wird auch
x n := a Âˇ r n + b Âˇ 1
fĂźr beliebige a und b die Gleichung (3.26) erfĂźllen. Es bleibt also die Parameter a und b zu bestimmen.
Hier benutzen wir die Randbedingungen:
0 = a0 = a + b
1 = xN

= a Âˇ r N + b.

Wir lĂśssen wir dieses Gleichungssytem und erhalten:
b = âa,
und deshalb
xn

=

a Âˇ rn + b =

a=

rN

1
â1

1
1
rn â 1
n
Âˇ
r
â
=
rN â 1
rN â 1 rN â 1

rn
rn
=
= r âm
n+m
rN
r
m
p
=
6 eâm/(1âp)
1âp
< eâm

6

ÂŠ 2003 by S. Jukna

3.10. REKURRENZENâ

145

Fall 2: p = 1/2. In diesem Fall hat die Gleichung pz 2 â z + q = 0 nur eine LĂśsung r = 1, und die
LĂśsung x n = r n = 1 fĂźr (3.26) sagt uns nichts. Aber in diesem Fall (wenn p = 1/2) ist x n = an + b
eine offensichtliche LĂśsung fĂźr (3.26). Aus x 0 = 0 und x N = 1 folgt, dass b = 0 und a = 1/N. Also
auch in diesem Fall haben wir die gleiche LĂśsung
n
n
xn =
=
.
N
n+m
Damit haben wir die folgende interessante Aussage Ăźber die Gewinchanchen bewiesen.
Satz 3.87. Die Gewinnwahrscheinlichkeit in jeder Spielrunde sei 0 < p 6 1/2. Die Gewinnwahrscheilichkeiten mit Anfangskapital n Euro und dem Wunsch, m Euro zu gewinnen, sind
1. genau n/(n + m), falls p = 1/2.
2. kleiner als eâm , falls p < 1/2.
â˛ Beispiel 3.88 : (Gamblerâs Ruin - Vortsetzung) Schauen wir nun an, was dieser Satz fĂźr Theo
bedeutet.
Fall 1: Faire MĂźnze (p = 1/2). In diesem Fall hĂ¤ngt fĂźr jedes (feste) m die Wahrscheinlichkeit
m Euro zu gewinnen vom Anfangskapital n ab: Je grĂśĂer es ist, desto grĂśĂer ist die Wahrscheinlichkeit. Wenn zum Beispiel Theo mit n = 500 e startet und m = 100 e gewinnen will, dann ist
seine Gewinnwahrscheinlichkeit
n
500
5
=
=
n + m 500 + 100 6
Nicht schlecht â mit einer fairen MĂźnze kann man wohl spielen! Wenn Theo mit n = 1.000.000 e
startet und m = 100 e gewinnen will, dann ist seine Gewinnwahrscheinlichkeit
n
> 0, 9999 . . .
n+m
In diesem Fall gewinnt Theo (ein MillionĂ¤r) seine 100 e fast sicher.
Fall 2: Unfaire MĂźnze (p < 1/2) (amerikanisches Casino). Nehmen wir nun an, dass Theo in ein
Casino in U.S.A. geht und immer auf ârotâ wettet. Das Rouletterad in Amerika hat 18 schwarze Nummern, 18 rote Nummern und 2 grĂźne Nummern. Mann kann aber nur auf eine rote oder
schwarze Nummer setzen, aber nicht auf grĂźnen. Also ist in diesem Fall die Gewinnwahrscheinlichkeit in jeder Runde gleich p = 18/38 â 0, 47. Die 0, 03 Gewinnchance hat das Casino fĂźr sich
selbst bestimmt aber die Gewinnchance fĂźr Theo sieht trotzdem âfastâ fair aus. NatĂźrlich ist dann
die Gewinnwahrscheinlichkeit ein bisschen kleiner als 5/6, aber Theo hofft, dass das nicht so
dramatisch ist: Wie kann dieser kleine â0, 03 Unterschied in Gewinnchance irgendwas essenziell
verĂ¤ndern? Leider, leider ... Nach dem Satz 3.87 ist in diesem Fall die Gewinnwahrscheinlichkeit sogar kleiner als 1/37.000, und zwar â egal mit welchem Anfangskapital n Theo sein Spiel
beginnt!
Beachte, dass die obere Schranke eâm in diesem Fall (wenn p < 1/2) nicht mehr vom Anfangskapital n abhĂ¤ngt! Und die Konsequenzen sind erstaunlich: Angenommen, Theo startet mit
n = 500 e und will m = 100 e gewinnen. Dann ist


 100

18/38 100
9
1
Pr Theo gewinnt <
=
<
.
20/38
10
37.648
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

146

Das ist ein dramatischer Unterschied zu dem fairen Spiel, wo (wie wir bereits wissen) die Gewinnchance sogar 5/6 war. Wir haben auch gesehen, dass im fairen Spiel mit Anfangskapital n = 1.000.000 e Theo die m = 100 e fast sicher gewinnen kann (mit Wahrscheinlichkeit
0, 9999). Aber im amerikanischen Casino wird Theo auch mit so groĂem Anfangskapital und so
kleinen Ambitionen (nur 100 e zu gewinnen) fast sicher sein Million verlieren!
Fazit: Wenn Ăźberhaupt, dann nicht in Amerika spielen!

Rekurrenzen vom hĂścheren Grad
Im allgemeinen haben (lineare) Rekurrenzen die Form fĂźr n > d
x n = A1 x nâ1 + A2 x nâ2 + . . . + Ad x nâd + f (n)

(3.27)

x n â A1 x nâ1 â A2 x nâ2 â . . . â Ad x nâd = â f (n).

(3.28)

oder Ă¤quivalent

Die Rekurrenz ist homogen, falls f (n) = 0 fĂźr alle n gilt, und fĂźr solchen Rekurrenzen kann man denselben Trick wie im Fall d = 2 anwenden. Man betrachtet wiederum das charakteristische Polynom
p(z) = z d â A1 z dâ1 â A2 z dâ2 â . . . â Anâd .
Eine Nullstelle r von p(z) hat Vielfachheit k, falls p(z) ist durch (z â r) k teilbar. Der folgender Satzt
ist eine direkte Verallgemeinerung des Satzes 3.83.
Satz 3.89. Sei r eine Nullstelle des charakteristischen Polynoms p(z) von (3.27) von Vielfachheit k.
Dann erfĂźllen alle k Folgen
r n , nr n , n2 r n , . . . , nkâ1 r n
die Rekursionsgleichung (3.27). AuĂerdem, lĂ¤Ăt sich jede LĂśsung von (3.27) als eine Linearkombination von solchen LĂśsungen Ăźber alle Nullstellen von p(z) darstellen.
â˛ Beispiel 3.90 : Betrachte die Rekursionsgleichung
x n = 3x nâ1 â 4x nâ3
mit Randbeningungen x 0 = 0, x 1 = 1 und x 2 = 13. Wenn man beginnt die ersten Folgeglieder zu
berechnen, bekommt man 39, 113, 287, 705, 1663, . . .. Es scheinnt keine vernĂźnftige Formel fĂźr
x n zu existieren. Trotzdem, wir kĂśnnen eine solche Formel ziemlich leicht finden. Das charakteristische Polynom fĂźr diese Rekurrenz hat die Form
p(z) = z 3 â 3z 2 + 4 = (z + 1)(z â 2) 2 .
Das Polynom hat also drei Nullstellen: â1 mit Vielfachheit 1 und 2 mit Vielfachheit 2. Nach
Satz 3.89 ist jede LĂśsung unserer Rekursionsgleichung eine Linearkombination von (â1) n , 2n
und n2n . D.h.
x n = A(â1) n + B2n + Cn2n
ÂŠ 2003 by S. Jukna

3.10. REKURRENZENâ

147

fĂźr irgendwelche Konstanten A, B und C. Um diese Konstanten zu bestimmen, benutzen wir die
Randbedingungen:
x0 = 0

=â

A+B =0

x1 = 1

=â

x 2 = 13

=â

âA + 2B + 2C = 1

A + 4B + 8C = 13.

LĂśst man dieses Gleichungssystem, so bekommt man A = 1, B = â1 und C = 2, und wir sind
fertig:
x n = (â1) n â 2n + 2n2n = (2n â 1)2n + (â1) n .
Die Situation mit inhomogenen Rekurrenzen, d.h. Rekurrenzen (3.27) mit f (n) , 0, ist komplizierter.
In solchen FĂ¤llen kann man versuchen, die Rekurrenz auf eine homogene Rekurrenz zu reduzieren.
â˛ Beispiel 3.91 : Betrachte die Rekurrenz
an+2 = an+1 + an + 2n .

(3.29)

Diese Rekurrenz ist inhomogen wegen dem Term f (n) = 2n . Man kann aber sie âhomogenizierenâ, indem man eine eine neue âverschobeneâ Rekurrenz betrachtet und ihre Vielfache aus der
Original-Rekurrenz abzieht. In unserem Beispiel kĂśnnen wir die Rekurrenz nach links verschieben
an+2 = an+1 + an + 2n
an+1 = an + anâ1 + 2nâ1 .
Nun kann man die zweifache der zweiten Gleichung von der ersten abziehen, um den Term 2n zu
eliminieren:
an+2 â 2an+1 + an+1 + an â 2an â 2anâ1
oder Ă¤quivalent
an+2 = 3an+1 â an â 2anâ1 .
Das ist bereits eine homogene Rekurrenz und wir kĂśnne sie mit Hilfe von Satz 3.89 lĂśsen.

Die Allgemeine Methode
Wir betrachten die folgende zwei Operatoren, die eine Folge <an > in eine ander Folge <bn > ĂźberfĂźhren:
E<an > := <an+1 >
und
c<an > := <can >.
D.h. der Oparator E Ăźberfuhrt die Folge a0 , a1 , a2 , . . . in die Folge a1 , a2 , a3 , . . . indem er einfach das
erste Element eliminiert. Der Operator Ăźberfuhrt die Folge a0 , a1 , a2 , . . . in die Folge ca0 , ca1 , ca2 , . . ..
Addition und Multiplikation von Operatoren A und B sind definiert durch:
(A + B)<an > := A<an > + B<an >
(A Âˇ B)<an > := A(B<an >).
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

148
Zum Beispiel

(2 + E)<an > = <2an + an+1 >
E2 <an > = <an+2 >.
Beobachtung: Ist <an > durch eine homogene Rekursionsgleichung definiert und ist p(z) das charakteristische Polynom dieser Gleichung, so gilt
p(E)<an > = <0> = 0, 0, 0, . . .
In diesem Fall sagt man auch, dass p(E) ein Annihilator fĂźr die Folge <an > ist. Zum Beispiel ist Eâ2
ein Annihilator fĂźr die Folge <2n >, da
(E â 2)<2n > = <2n+1 â 2 Âˇ 2n > = <0>.
Annihilatoren fĂźr einige wichtige Folgen sind:
Folge

Annihilator
Eâ1

<c>
<Polynom in n vom Grad k>
<c n >
<c n mal Polynom in n vom Grad k>

(E â 1) kâ1
(E â c)

(E â c) kâ1

So ist zum Beispiel (E â 2) 2 der Annihilator fĂźr die Folge <n2n >.
Eine nĂźtzliche Eigenschaft der Annihilatoren ist folgende:
Ist A ein Annihilator fĂźr <an > und B der Annihilator fĂźr <bn >, so ist A Âˇ B der Annihilator fĂźr
<an + bn >.
Zum Beispiel ist (E â 3) 2 (E â 1) ein Annihilator fĂźr <n2n + 1>.
Die allegemeine Vorgehensweise zur LĂśsung von Rekursionsgleichungen in der Form (3.28) ist folgende:
1. Wende den Annihilator fĂźr die rechte Seite von (3.28) auf beide Seiten an.
2. LĂśse die resultierende homogene Rekursionsgleichung.
â˛ Beispiel 3.92 : Wir betrachten die Rekursionsgleichung
an â 5anâ1 + 6anâ2 = 4
mit a0 = 5 und a1 = 7. In unsered neuen Notation hat diese Rekursionsgleichung die Form
(E2 â 5E + 6)<an > = <4>.
Wir wenden den Oprator E â 1 an, um 4 zu annihilieren:
(E â 1)(E2 â 5E + 6)<an > = <0>.
ÂŠ 2003 by S. Jukna

3.10. REKURRENZENâ

149

Das charakteristische Polynom hat also die Form p(z) = (zâ1)(z 2 â5z+6) = (zâ1)(zâ2)(zâ3),
und seine Nullstellen sind 1, 2 und 3. Das ergibt die LĂśsung in der Form
an = A + B2n + C3n .
Wir wissen, dass a0 = 5, a1 = 7 und a2 = 5a1 + 6a0 = 65. Das gibt uns die Gleichungssystem
A + B + C = a0 = 5
A + 2B + 3C = a1 = 7
A + 4B + 9C = a2 = 9
woraus A = 2, B = 4 und C = â1 folgt. Die LĂśsung der Rekursionsgleichung also ist
a n = 2 + 4 Âˇ 2n â 3n .
â˛ Beispiel 3.93 : Wir betrachten die Rekursionsgleichung
an â 2anâ1 = 2n â 1
mit a0 = 0. In unsered neuen Notation hat diese Rekursionsgleichung die Form
(E â 2)<an > = <2n+1 â 1>.
Wir wenden den Oprator (E â 2)(E â 1) an, um 2n+1 â 1 zu annihilieren:
(E â 1)(E â 2) 2 <an > = <0>.
Das charakteristische Polynom hat also die Form p(z) = (z â 1)(z â 2) 2 , und seine Nullstellen
sind 1 und 2 (mit Vielfachheit 2). Das ergibt die LĂśsung in der Form
an = (A + Bn)2n + C.
Wir wissen, dass a0 = 0, a1 = 2 Âˇ 0 + 21 â 1 = 1 und a2 = 2a1 + 22 â 1 = 5. Das gibt uns die
Gleichungssystem
A + 0 + C = a0 = 0
2A + 2B + C = a1 = 1
4A + 8B + C = a2 = 5
woraus A = â1, B = 1 und C = 1 folgt. Die LĂśsung der Rekursionsgleichung also ist
an = (n â 1)2n + 1.

3.10.1 Das Master Theorem
Viele Algorithmen arbeiten rekursiv, d.h. sie bestehen aus Sliefen, in denen der Algorthmus auf einigen kleineren Eingaben aufgerufen wird. Ihr Laufzeit-Funktion T (n) (= maximale Laufzeit der Eingaben der LĂ¤nge n) ist dann auch rekursiv definier. Damit bekommt man oft eine Rekursionsgleichung
der Form
T (n) = a Âˇ T (n/b) + f (n)
(3.30)
ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

150

Eine solche Rekursionsgleichung beschreibt zum Beispiel die Laufzeit eines Algorithmus, der die
Eingabe der LĂ¤nge n in a Teilprobleme der LĂ¤nge n/b zerlegt; diese durch a rekursive Aufrufe lĂśst,
und aus den erhaltenen TeillĂśsungen die GesamtlĂśsung zusammensetzt. Hierbei ist f (n) der Aufwand,
der fĂźr das Zerlegen in Teilprobleme und fĂźr das Zusammensetzen der GesamtlĂśsung benĂśtigt wird.
Wie findet man eine geschloĂene Form fĂźr solchen Rekursionsgleichungen? Es gibt ein Satzt, der
solchen Rekursionsgleichungen sehr einfach auslĂśsen lĂ¤sst. Da der Satz alle solchen Gleichungen mit
einem SchuĂ âmeistertâ, nennt man ihm das âMaster Theoremâ.
Satz 3.94. (Master Theorem) Gegeben sei eine Rekursionsgleichung der Form T (0) = 0 und
T (n) = a Âˇ T (n/b) + nk
wobei a > 1, b > 1. Dann kann T (n) asymptotisch wie folgt agbeschĂ¤tzt werden:
ďŁą
k
falls a < bk
ďŁ˛ Î(n )
T (n) =
Î(nk log n)
falls a = bk
ďŁł
log b a
Î(n
)
falls a > bk

Beweis. Sei f (n) := Î(nk ). Wir entwickeln die Rekurenz und bekommen

T (n) = f (n) + a f (n/b) + a2 f (n/b2 ) + Âˇ Âˇ Âˇ + a i f (n/bi ) + Âˇ Âˇ Âˇ + a L f (n/bL )
wobei n/bL = 1, d.h.
L = logb n.
Da f (n) = Î(nk ), erhalten wir
T (n) =

L
X

a i f (n/bi ) =

i=0

L
X
i=0

a i Âˇ (n/bi ) k = nk Âˇ

L
X

xi

(3.31)

i=0

| {z }
S (x)

mit x := a/bk . Die Summe S(x) =

PL

i=0

x i ist eine geometrische Reihe und wir wissen bereits, dass

S(x) =

x L+1 â 1 1 â x L+1
=
xâ1
1âx

fĂźr x , 1 gilt. Da x = a/bk eine Konstante ist, gilt
ďŁą
ďŁ˛ Î(1)
S(x) =
L
ďŁł
Î(x L )

falls x < 1
falls x = 1
falls x > 1

(3.32)

Da L = logb n und

 a log b n
alog b n
nlog b a
=
=
bk
bk Âˇlog b n
nk
liefern uns (3.31) und (3.32) folgendes:
xL =

-

Ist a < bk , so ist x < 1 und damit auch T (n) = nk Âˇ S(x) = Î(nk ).
ÂŠ 2003 by S. Jukna

3.11. AUFGABEN
-

151

Ist a = bk , so ist x = 1 und damit auch T (n) = nk Âˇ S(x) = nk Âˇ L = nk Âˇ logb n.

Ist a > bk , so ist x > 1 und damit auch T (n) = nk Âˇ S(x) = Î(nk Âˇ x L ) = Î(nlog b a ).


Bemerkung 3.95. Intuitiv ist Satz 3.94 ziemlich einfach. Betrachte den Rekursionsbaum fĂźr T (n) =
aT (n/b) + f (n):
f(n)

=

a
f(n/b)

f(n/b^2)

a

= a f(n/b)

f(n/b)

a

f(n)

a
f(n/b^2)

f(n/b^2)

f(n/b^2)

a

a

a

= a^2 f(n/b^2)

Der Baum hat die Tiefe L = logb n. Die i-te Ebene summiert sich zu a i Âˇ f (n/bi ). Der Wert T (n)
selbst ist die Summe Ăźber alle Ebenen. Ist f (n) groĂ, so kann man den Rest ignorieren: T (n) =
Î( f (n)). Ist f (n) klein, so trĂ¤gt jeder innere Knoten nur sehr wenig bei und die ganze Summe T (n)
ist im wesentlichen auf den BlĂ¤tter konzentriert; da jedes Blatt die selbe Konstante beitrĂ¤gt und wir
insgesammt nlog b a BlĂ¤tter haben, ist in diesem Fall T (n) = Î(nlog b a ).
â˛ Beispiel 3.96 : Die drei Rekursionsgleichungen
T (n) = 8 Âˇ T (n/3) + n2
T (n) = 9 Âˇ T (n/3) + n2

T (n) = 10 Âˇ T (n/3) + n2
haben der Reihe nach die LĂśsungen:
T (n) = Î(n2 )
T (n) = Î(n2 log n)
T (n) = Î(nlog3 10 ) = Î(n2,09 )

3.11 Aufgaben
3. 1. Richtig oder falsch: Die Folge <a n > konvergiert gegen a, wenn
(a) ... sie a immer nĂ¤her kommt.
(b) ... sie a beliebig nahe kommt.
(c) .. sie a beliebig nahe kommt, es aber nie erreicht.
3. 2. Benutze die Formel lim (1 + nx ) n = e x zur Untersuchung der Folge a n = (1 â
nââ
Bestimme gegebenfalls den Grenzwert lim a n .

1 n
n+1 )

auf Konvergenz.

nââ

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

152
n

3. 3. Es sei cn (x) = xn nn! , wobei x > 0. FĂźr welche x > 0 ist lim
nââ
Pâ
n=1 cn (x) auf Konvergenz in den FĂ¤llen x = 2 und x = 4.

c n+1 (x)
c n (x)

< 1? Untersuche die Reihe

3. 4. Ein Turm wird aus WĂźrfeln gebaut. Der erste WĂźrfel hat eine KantenlĂ¤nge von l = 1m, der zweite
l = 0, 5m. Jeder weitere hat die halbe KantenlĂ¤nge der darunter liegenden WĂźrfels. Welche HĂśhe nimmt der
Turm an, wenn unendlich viele WĂźrfel aufeinandergesetzt werden?
3. 5. Sei <a n > eine arithmetische Folge mit a n , 0 fĂźr alle n. Zeige, dass fĂźr alle n > 2 gilt:
1
1
1
nâ1
+
+ÂˇÂˇÂˇ+
=
.
a1 Âˇ a2 a2 Âˇ a3
a nâ1 Âˇ a n
a1 Âˇ a n
3. 6. Ein Ăsthet will seinen Weinkeller verschĂśnen. Dazu will er die a vorhandene Weinflaschen wie folgt
auslegen:

Dabei will er, dass: (i) mindestens zwei Reihen entstehen und (ii) die oberste Reihe vollstĂ¤ndig gefĂźhlt ist.
Gebe zuerst eine mathematische Formulierung des Problems an.
Angenommen, der Ăsthet hat a = pm Weinflaschen, wobei p 6 2m + 1 und p ungerade ist.
Zeige, dass dann das Problem lĂśsbar ist. Hinweis:
. . . + (m â 2) + (m â 1) + m + (m + 1) + (m + 2) + . . .
3. 7. Ein Frosch springt Ăźber die StraĂe. Beim ersten Sprung springt er 1 m. Dabei ermĂźdet er, so dass er bei
jedem folgenden Sprung nur noch 2/3 des vorigen Sprungs erreicht.
Welche WeglĂ¤nge wird der Frosch nach n SprĂźngen zurĂźcklegen?
Die StraĂe ist 3 m breit und nach 6 Sprungen wird an dieser Stelle ein Auto vorbei kommen. Wird dann der
Frosch Ăźberleben, d.h. nach diesen 6 SprĂźngen die StraĂe Ăźberquert haben?
3. 8. Wir betrachten ein Tenisturnier mit N Spielern. Die Spieler spielen paarweise und nach jedem Spiel fliegt
der Verlorene raus aus dem Turnier. Wieviele Spiele insgesammt mĂźssen gespielt werden, bis nur ein Gewinner
bleibt? Hinweis: Ist N eine Zweierpotenz, so kann man die Anzahl S der Spiele leicht bestimmen; Die Anzahl
der Spiele halbiert sich nach jeder Runde. Was aber wenn N keine Zweierpotenz ist? Probiere eine Bijektion
zwischen Spielen und bestimmten Spielern zu finden.
3. 9. Sie haben K Euro geerbt und legen diesen Betrag auf einem Konto an. Der Zinssatz betrĂ¤gt p%. Sie wollen
n Jahre lang einen festen Betrag von x Euro jeweils am Ende jedes Jahres aus dem Konto herausnehmen, so
dass nach n Jahren das Konto leer wird.
Wie groĂ ist der Betrag x?
Hinweis: Sei Fi (x) der Kontostand am Ende des i-ten Jahres. Probiere zuerst Fi (x) fĂźr die ersten iâs zu bestimmen, danach eine allgemeine Vermutung fĂźr allgemeines i zu finden und diese Vermutung mittels Induktion zu
beweisen. Die geometrische Reihe wird bestimmt in Spiel kommen.
3. 10. Wir erzeugen rekursiv bestimmte geometrische Figuren F0 , F1 , F2 , . . . wie folgt. Wir beginnen mit einem
gleichseitigen Dreieck F0 mit SeitenlĂ¤nge L. Dann teilen wir jede Kante in drei Teile auf, und erweitern jedes
mittlere Teil zu einem gleichseitigen Dreieck
ÂŠ 2003 by S. Jukna

3.11. AUFGABEN

153

L

L

L

F0

F1

und lassen dann im Inneren verlaufenden Kanten weg (siehe die Skitze). So erhalten wir die Figur F1 . Dann
teilen wir wieder jede Kante von F1 in drei Teile auf, erweitern jedes mittlere Teil zu einem gleichseitigen
Dreieck und lassen dann im Inneren verlaufenden Kanten weg. So erhalten wir die Figur F2 , usw. Die n-te
Figur besteht also aus 3 Âˇ 4n Kanten.

Sei a n der Umfang (d.h. die GesammtlĂ¤nge der Kanten in) der n-ten Figur und sei bn die FlĂ¤che der n-ten Figur,
n = 0, 1, 2, . . ..
Untersuche die Folgen <a n > und (bn ) auf Konvergenz.
Hinweis: Der Umfang der n-ten Figur ist gleich die Anzahl der Kanten mal die LĂ¤nge einer Kante. Zum Beispiel
a0 = 3 Âˇ L, a1 = (3 Âˇ 4) Âˇ (L/3), a2 = (3 Âˇ 4 Âˇ 4) Âˇ (L/32â), usw. Hat ein gleichseitiges Dreieck die SeitenlĂ¤nge â,
so hat es nach dem Satz von Pythagoras23 die FlĂ¤che 23 Âˇ â 2 .

3. 11. Die Folge <ak > sei monoton wachsend und mĂśge den Grenzwert a haben. Zeige, dass dann auch die
Folge <bn > mit
a0 + a1 + . . . + a n
bn =
n+1
gegen a konvergiert.
3. 12. Sei b â Z und x â R eine reelle Zahl mit |x| < 1. Zeige, dass dann lim nb x n = 0 gilt.
nââ

3. 13. Seien a, b â R, a > 0 und |b| > 1. Zeige, dass
â
(a) lim n a = 1
nââ
â
n
(b) lim na = 1
nââ

3. 14. (Multiplikation statt Division) Sei a > 0 eine reelle Zahl. Wir wollen 1/a berechnen, ohne dabei irgendwelchen Zahlen zu dividieren. Wir suchen also eine LĂśsung x fĂźr die Gleichung ax = 1. Diese Gleichung
lĂ¤Ăt sich Ă¤quivalent als x = 2x â ax 2 umschreiben; das gesuchte x ist dann die von Null verschiedene LĂśsung dieser Gleichung. Setzten wir f (x) := 2x â ax 2 = x(2 â ax), so erhalten wir die Rekursionsgleichung:
x n+1 = x n (2 â ax n ).
Zeige, dass die Folge (x n ) mit 0 < x 0 < 2/a und x n+1 = x n (2 â ax n ) gegen 1/a strebt.

Hinweis: Monotonie-Kriterium.
n
n
= e gilt. Hinweis: Zuerst zeige, dass die Folge cn = 1 + n1
motonon
3. 15. Zeige, dass lim 1 + n1
nââ
wachsend und durch e nach oben beschrĂ¤nkt ist. Dann zeige, dass lim cn > e N fĂźr jedes festes N > 1 gilt.
nââ
Um (1 + 1/n) n als eine Summe darzustellen, benutze den binomischen Lehrsatz.
P
Pâ sin(k x)
xk
.
3. 16. Zeige die absolute Konvergenz folgenden Reihen: â
k=1 k! und
k=1
k2

P
x k
3. 17. Seien x, q â R, x > 0 und 0 6 q < 1. Zeige, dass die binomische 24 Reihe â
k=0 k q ist absolut
23Im rechtwinkligen Dreieck ist die Summe
der Kathetenquadrate gleich dem Hypotenusenquadrat.
n
24Wir
haben
die
Binomialkoeffizienten
im
Abschnitt 1.6.2 nur fĂźr natĂźrliche Zahlen n und k definiert. Man kann aber
k
x
auch
fĂźr
x
â
R
und
k
â
N
als
Produkt
definieren:
k
  Y
k
x
x âi+1
:=
k
i
i=1

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

154
konvergent.
3. 18. Bestimme den Grenzwert lim x 1/x .
xââ

3. 19. Bestimme den Grenzwert lim

xâ0

e x â1âx
.
x2

x
x
xâ0 e

3. 20. Wie sehen die Grenzwerte lim

1
x
xâ0 e

und lim

aus?

3. 21. Bestimme den Grenzwert
lim

xâ1

Hinweis: de lâHospital

ln x
xâ1

3. 22. Seien a, b â R nicht negativ. Bestimme die folgenden Grenzwerte:

a b x
(a) lim 1 +
xââ
x


1
1
(b) lim
â
xâ1 ln x
xâ1
Hinweis: ExponentialausdrĂźcke der Form f (x) g (x) werden zunĂ¤chst logarithmiert. AusdrĂźcke der Form f (x) â
g(x), die zum GrenzfĂ¤ll der Art â â â fĂźhren kĂśnnen, kann man mit der Tranformation
f (x) â g(x) =
zu dem Fall

0
0

1
g (x)

â

1
f (x)

1
f (x)Âˇg (x)

ĂźberfĂźhren.

3. 23. Seien f , g : N â R zwei Funktionen mit f = O(g). Seien
F (n) :=

n
X

f (i) und G(n) :=

i=1

n
X

g(i).

i=1

Zeige oder wiederlege: F = O(G).
3. 24. Seien f 1 (n), . . . , f N (n) und g1 (n), . . . , gN (n) Funktionen mit
 f k (n) = O(gk (n)) fĂźr alle k = 1, 2, . . . , N.
PN
PN
Zeige oder wiederlege: Dann gilt k=1
f k (n) = O
g
(n)
.
k=1 k
3. 25. Sei g : R â R mit lim g(x) = â. Zeige oder wiederlege:
xââ



f (x) = o(g(x)) â e f (x) = o eg (x)
3. 26. FĂźr eine Funktion f (n) definieren wir drei Funktionen
g1 (n) := f (2n),

g2 (n) := f (n/2),

g3 (n) := f (n + 2).

In welcher asymptotischen Beziehung stehen die in der Tabelle angegebenen Funktionen f (n) mit den Funktionen g1 , g2 , g3 ?
Man sollte die stĂ¤rkstmĂśgliche Beziehung aus den fĂźnf mĂśglichen
f = o(g), f = O(g), f = Î(g), f = âŚ(g), f = Ď(g)
â

angeben. ZumBeispiel n = O(n) ist zwar richtig aber
richtig aber n2 = Î(n2 ) ist stĂ¤rker.

â

n = o(n) ist stĂ¤rker. Oder

n
2



= O(n2 ) ist zwar

ÂŠ 2003 by S. Jukna

3.11. AUFGABEN

155

f (n)

log2 n

â

n

n

n log2 n

n10

g1 (n)

nlog2 n

2n/100

Î

g2 (n)

g3 (n)
BegrĂźndung des Beispieleintrags: Da f (n) = n10 folgt
g1 (n) = (2n) 10 = 1024 Âˇ n10 = Î(n10 ) = Î( f (n)).
3. 27. Gebe die best mĂśgliche asymptotische Beziehungen zwischen Funktionen an:
â
2
(a) f (x) = e(ln ln x) und g(x) = x
â
(b) f (x) = x log4 x und g(x) = x(log2 x) 3
(c) f (x) = (log4 x) 1/2 und g(x) = (log2 x) 1/3
3. 28. Zeige, dass fĂźr beliebige zwei Zahlen a, b > 1 und fĂźr beliebige Funktion f : N â N die Beziehung
loga f (n) = Î(logb f (n)) gilt. Fazit: FĂźr den Wachstum von Logarithmen ist die Basis unwesentlich!
3. 29. Zeige, dass es zwei nicht fallende Funktionen f , g : R â R gibt (d.h. x 6 y â f (x) 6 f (y) und
g(x) 6 g(y)) gibt, so dass
f = O(g) aber weder f = o(g) noch f = Î(g) gilt.
Hinweis: FĂźr f , o(g) reicht es, dass f (x) = g(x) fĂźr unendlich viele x gilt. FĂźr f , Î(g) reicht es, dass die
Differenz |g(x) â f (x)| nicht von oben beschrĂ¤nkt ist.
3. 30. Lokalisiere so genau wie mĂśglich den Fehler im folgenden âInduktionsbeweisâ.
Behauptung:
n
X

(2i + 1) = O(n)

i=1

Beweis durch Induktion nach n. Induktionsbasis n = 1: In diesem Fall ist
1
X

(2i + 1) = 3 = O(1).

i=1

Induktionsschritt n 7â n + 1: Wir beginnen mit der Induktionsannahme
n
X

(2i + 1) = O(n).

i=1

Wir addieren auf beiden Seiten 2(n + 1) + 1 und erhalten
n
X

(2i + 1) + 2(n + 1) + 1 = O(n) + 2(n + 1) + 1.

i=1

ÂŠ 2003 by S. Jukna

KAPITEL 3. EINSCHUB AUS DER ANALYSIS

156
Nach einer Vereinfachung folgt
n+1
X

(2i + 1) = O(n) + 2n + 3.

i=1

Aber (2n + 3) ist sicher in O(n) und somit
n
X

(2i + 1) = O(n) + O(n) = O(n).

i=1

ÂŠ 2003 by S. Jukna

Kapitel 4

Diskrete Stochastik
Contents
4.1
4.2
4.3
4.4

4.5

4.6
4.7
4.8
4.9
4.10
4.11

4.12
4.13
4.14
4.15
4.16
4.17
4.18

Intuition und Grundbegriffe . . . . . . . . . . . . . . . . . .
Drei Modellierungsschritte . . . . . . . . . . . . . . . . . . .
4.2.1 Das Geburtstagsproblem . . . . . . . . . . . . . . . . .
Stochastische UnabhĂ¤ngigkeit . . . . . . . . . . . . . . . . .
Bedingte Wahrscheinlichkeit . . . . . . . . . . . . . . . . . .
4.4.1 Multiplikationssatz fĂźr Wahrscheinlichkeiten . . . . . .
4.4.2 Satz von der totalen Wahrscheinlichkeit . . . . . . . . .
4.4.3 Satz von Bayes . . . . . . . . . . . . . . . . . . . . . .
Stochastische Entscheidungsprozesse . . . . . . . . . . . . .
4.5.1 Das âMonty Hall Problemâ . . . . . . . . . . . . . . . .
4.5.2 Stichproben . . . . . . . . . . . . . . . . . . . . . . . .
4.5.3 Das âSekretĂ¤rinnen-Problemâ an der BĂśrse . . . . . . .
Zufallsvariablen . . . . . . . . . . . . . . . . . . . . . . . . .
Erwartungswert und Varianz . . . . . . . . . . . . . . . . . .
Analytische Berechnung von E [X] und Var [X] . . . . . . . .
Eigenschaften von E [X] und Var [X] . . . . . . . . . . . . . .
Verteilungen diskreter Zufallsvariablen . . . . . . . . . . . .
Abweichung vom Erwartungswert . . . . . . . . . . . . . . .
4.11.1 Markov-Ungleichung . . . . . . . . . . . . . . . . . . .
4.11.2 Tschebyschev-Ungleichung . . . . . . . . . . . . . . .
4.11.3 Chernoff-Ungleichungen . . . . . . . . . . . . . . . . .
Das Urnenmodel â Hashingâ . . . . . . . . . . . . . . . . . .
Bedingter Erwartungswertâ . . . . . . . . . . . . . . . . . .
Summen von zufĂ¤lliger LĂ¤nge â Waldâs Theorem . . . . . .
Irrfahrten und MarkovâKetten . . . . . . . . . . . . . . . .
Statistisches SchĂ¤tzen: Die Maximum-Likelihood-Methode â
Die probabilistische Methodeâ . . . . . . . . . . . . . . . . .
Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

157

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

158
161
162
163
165
168
169
171
174
177
178
179
184
186
190
191
199
205
206
208
212
218
224
228
232
240
243
247

KAPITEL 4. DISKRETE STOCHASTIK

158

4.1 Intuition und Grundbegriffe
Die Stochastik bedient sich gerne Beispielen aus der Welt des GlĂźcksspiels, sie ist deswegen aber noch
lange keine âWĂźrfelbudenmathematikâ. Ihr geht es darum, die Vorstellung einer Zufallsentscheidung
so allgemein zu fassen, dass sie auch in ganz anderen Bereichen â von der Genetik bis zur BĂśrse â
zum Tragen kommen kann.
Der Begriff Zufallsexperiment steht fĂźr jeden realen Vorgang, der vom Zufall beeinflusst wird. Typischerweise liefert ein Zufallsexperiment ein Ergebnis, das âzufĂ¤lligâ (zumindest teilweise) ist.
Beispiele fĂźr Zufallsexperimente:
- GlĂźcksspiele (z.B. MĂźnzwurf, WĂźrfeln, Lotto)
- 0-1-Experimente (Bernoulli-Experimente), wobei z.B. â1â fĂźr Erfolg und â0â fĂźr Misserfolg steht
(z.B. Therapie, Platzierung, SchieĂen). Das betrachtete Zufallsexperiment kann die einmalige
DurchfĂźhrung eines 0-1-Experiments sein oder auch eine mehrmalige unabhĂ¤ngige DurchfĂźhrung
eines 0-1-Experiments.
- ZufĂ¤llige Anzahlen (z.B. Anzahl von Kunden oder Jobs, Anzahl verkaufter Zeitungen, Anzahl von
VerkehrsunfĂ¤llen).
- Lebensdauer (z.B. von technischen Bauteilen, von Lebewesen).
Die mathematische Modellierung eines Zufallsexperiments erfolgt durch
- die Festlegung einer Menge âŚ, die alle mĂśglichen Ergebnisse des Zufallsexperiments enthĂ¤lt (das
ist i.d.R. eine leichte Aufgabe); die Elemente von âŚ heiĂen Elementarereignise
- die Festlegung einer passenden Wahrscheinlichkeitsverteilung auf âŚ (das ist i.d.R. die schwierige
Aufgabe); das bedeutet vereinfacht gesagt: FĂźr jedes mĂśgliche Elementarereignis Ď â âŚ ist die
Wahrscheinlichkeit Pr {Ď} seines Eintretens festzulegen.
Definition: Ein diskreter Wahrscheinlichkeitsraum besteht aus einer endlichen oder abzĂ¤hlbaren
Menge âŚ von Elementarereignissen und einer Funktion (einer Wahrscheinlichkeitsverteilung) Pr :
âŚ â [0, 1] mit der Eigenschaft, dass
X
Pr {Ď} = 1
Ď ââŚ

gilt. Eine Teilmenge A â âŚ heiĂt Ereignis. Seine Wahrscheinlichkeit ist durch
X
Pr { A} =
Pr {Ď}
ĎâA

definiert.
Die Menge1 âŚ ist die Menge aller mĂśglichen Ergebnissen eines Zufallsexperiments und Pr {Ď} ist die
Wahrscheinlichkeit, dass der Zufall das Ergebnis Ď liefern wird.
Die Funktion Pr selbst heiĂt WahrscheinlichkeitsmaĂ oder Wahrscheinlichkeitsverteilung. Zum Beispiel, Gleichverteilung (auch als Laplace-Verteilung bekannt) ist ein WahrscheinlichkeitsmaĂ Pr :
âŚ â [0, 1] mit Pr {Ď} = |âŚ1 | fĂźr alle Ď â âŚ. Damit ist
Pr { A} =

| A| Anzahl der gĂźnstigen Elementarereignisse
=
|âŚ|
Anzahl aller Elementarereignisse

1Auf englich heiĂt âŚ sample space.
ÂŠ 2003 by S. Jukna

4.1. INTUITION UND GRUNDBEGRIFFE

159

Diese Verteilung entspricht unserer gĂ¤ngigen Vorstellung: Ein Ereignis wird umso wahrscheinlicher,
je mehr Elementarereignisse an ihm beteiligt sind. Bei einer Gleichverteilung wird kein Element von
âŚ bevorzugt, man spricht daher auch von einer rein zufĂ¤lligen Wahl eines Elements aus âŚ.
â˛ Beispiel 4.1 : Zufallsexperiment: Einmaliges Werfen eines SpielwĂźrfels. Mit welcher Wahrscheinlichkeit kommt eine gerade Zahl?
1. Wahrscheinlichkeitsraum âŚ ist die Menge aller mĂśglichen Ergebnissen des Experiments, d.h.
Augenzahlen 1, 2, . . . , 6, je mit Wahrscheinlichkeit 1/6:
ďŁą
ďŁź
ďŁ´
ďŁ´
ďŁ˛
ďŁ˝
,. . . ,
ďŁ´
1/6 ďŁ´
ďŁž
ďŁł 1/6
2. Ereignisse sind Teilmengen von {1, 2, 3, 4, 5, 6}. Z.B. Ereignis âWĂźrfeln einer geraden Zahlâ
ist die Teilmenge E = {2, 4, 6}, und seine Wahrscheinlichkeit ist Pr {E} = 3 Âˇ (1/6) = 1/2.

In dieser Vorlesung werden wir nur WahrscheinlichkeitsrĂ¤ume âŚ betrachten, die entweder endlich
oder abzĂ¤hlbar sind â deshalb das Wort âdiskreteâ vor der âStochastikâ. Da die Informatik sich hauptsĂ¤chlich mit diskreten Strukturen beschĂ¤ftigt, reicht uns diese (einfachere) Teil der Stochastik vĂśllig
aus.
Ist âŚ ĂźberzĂ¤hlbar, so kann man nicht ohne weiteres die Wahrscheinlichkeiten Pr { A} fĂźr die Teilmengen A â âŚ (die Ereignisse) einfach als die Summe von Pr {Ď} Ăźber die Elementarereignisse Ď â A
definieren. Dazu braucht man den Begriff der sogenannten Ď-Algebra, den wir hier nicht betrachten
werden. Wir beschrĂ¤nken uns auf einem Beispiel.
â˛ Beispiel 4.2 : Romeo und Juliet haben eine Verabredung am bestimmten Zeitpunkt (sei es Zeitpunk
0) und jeder kann mit einer VerzĂśgerung von 0 bis auf 1 Stunde kommen. Die VerzĂśgerungszeiten
sind unabhĂ¤ngig und gleichwarscheinlich. Derjenige, der als erste kommt, wird nur 15 Minuten
warten, und dann wird weg gehen. Was ist die Wahrscheinlichkeit dafĂźr, dass Romeo und Juliet
sich treffen?
Wir kĂśnnen unseren Wahrscheinlichkeitsraum als das Quadrat âŚ = [0, 1]Ă[0, 1] darstellen, dessen
Elemente (x, y) (Elementarereignisse) alle mĂśgliche Ankunftszeiten von Romeo (x) und Julia
(y) sind. Es gibt ĂźberzĂ¤hlbar viele solche Elementarereignisse und wir kĂśnnen nicht zu jedem
seine Wahrscheinlichkeit zuweisen. Warum? Dann sollten wir fĂźr fast alle (x, y) (fĂźr alle auĂer
abzĂ¤hlbar vielen Paaren) Pr {x, y} = 0 setzen. In einer solchen Situation geht man anders rum.
Zuerst schaut man, welches Ereignis A â âŚ fĂźr uns interessant ist. In unserem Fall ist das die
Menge
A = {(x, y) : |x â y| 6 1/4, 0 6 x, y 6 1}
d.h. der schattierte Bereich im Abbildung 4.1. Man definiert dann die Wahrscheinlichkeit von A
als
FlĂ¤che von A
Pr { A} =
GesammtflĂ¤che
In unserem Beispiel ist die FlĂ¤che von A genau 1 minus die FlĂ¤che (3/4) Âˇ (3/4) = 9/16 von zwei
unschattierten Dreiecken. Da die GesamtflĂ¤che gleich 1 ist, gilt somit Pr { A} = 1 â 9/16 = 7/16.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

160
y
1

A
1/4
0

1

1/4

x

Abbildung 4.1: Das Ereignis, dass Romeo und Juliet sich treffen.

FĂźr (diskrete) WahrscheinlichkeitsmaĂe gelten die folgenden Rechenregeln. FĂźr ein Ereignis A â âŚ
ist A = âŚ \ A das komplementĂ¤re Ereignis zu A.
Satz 4.3. Sei (âŚ, Pr) ein endlicher Wahrscheinlichkeitsraum und A, B Ereignisse. Es gilt:
(a) Pr {âŚ} = 1, Pr {â} = 0 und Pr { A} > 0 fĂźr alle A â âŚ.
(b) Pr { A âŞ B} = Pr { A} + Pr {B} â Pr { A âŠ B}.

(c) A âŠ B = â =â Pr { A âŞ B} = Pr { A} + Pr {B} (disjunkte Ereignisse).

(d) Pr A = 1 â Pr { A} (komplementĂ¤re Ereignisse).

(e) Pr { A âŠ B} > Pr { A} â Pr B .

(f) Pr { A \ B} = Pr { A} â Pr { A âŠ B}.

(g) Ist A â B, so gilt Pr { A} 6 Pr {B} (Monotonie).
Beweis. (a) gilt nach der Definition von Pr. Zu (b):
X
Pr { A âŞ B} =
Pr {Ď}
Ď â AâŞB

=

X

ĎâA

Pr {Ď} +

X

Ď âB

Pr {Ď} â

X

Ď â AâŠB

= Pr { A} + Pr {B} â Pr { A âŠ B}

Pr {Ď}

(4.1)

da fĂźr Ď â A âŠ B, Pr {Ď} in (4.1) zweimal gezĂ¤hlt wird. (c) folgt aus (b). (d) folgt aus (c) und (a). (e)
folgt aus (b), da Pr { A âŞ B} 6 1 ist. (f) folgt aus (c).

â˛ Beispiel 4.4 : Wir wollen einen Schaltkreis mit n Verbindungen konstruieren. Aus frĂźheren Erfahrungen wissen wir, dass jede Verbindung mit Wahrscheinlichkeit p falsch sein kann. D.h. fĂźr
1 6 i 6 n ist

Pr i-te Verbindung ist falsch = p.
Was kann man Ăźber die Wahrscheinlichkeit, dass das Schaltkreis keine falschen Verbindungen
haben wird, sagen?

Sei Ai das Ereignis, dass die i-te Verbindung korrekt ist; also ist Pr Ai = p. Dann ist
(n
)
\

Pr alle Verbindungen sind richtig = Pr
Ai
i=1

ÂŠ 2003 by S. Jukna

4.2. DREI MODELLIERUNGSSCHRITTE

161

Obwohl es schwer ist, diese Wahrscheinlichkeit exakt auszurechnen, man kann vernĂźnftige AbschĂ¤tzungen finden. Einerseits, ist laut der Monotonie-Eigenschaft (g)
)
(
)
(n
n
\
\
Pr
Ai = Pr A1 âŠ ( Ai ) 6 Pr { A1 } = 1 â p.
i=1

i=2

Andererseits, ist laut der Eigenschaften (d) und (b)
(n
)
(n
)
(n
)
n
\
\
[
X

Pr
Ai = 1 â Pr
Ai = 1 â Pr
Ai > 1 â
Pr Ai = 1 â np.
i=1

i=1

i=1

i=1

Ist zum Beispiel n = 10 und p = 0, 01, so gilt

0, 9 = 1 â 10 Âˇ 0, 01 6 Pr alle Verbindungen sind richtig 6 1 â 0, 01 = 0, 99.

4.2 Drei Modellierungsschritte
Keiner weiĂ so genau, was der Zufall eigentlich ist, aber eine intutive Vorstellung darĂźber hat fast
jeder! Und genau da steckt die Gefahr â genauso wie mit der Unendlichkeit, versagt oft unsere Intuition wenn man mit dem Zufall als einem âhalb-definiertenâ Objekt jongliert. Deshalb werden wir uns
in diesem Vorlesung nur auf die (oben gegebene) mathematische Definition der Wahrscheinlichkeit
verlassen und unsere Intuition nur zur Interpetation der Resultate benutzen.
Will man ein Zufallsexperiment analysierten, so sind in vielen FĂ¤llen die folgende âDrei-SchrittMethodeâ sehr hilfreich.
1. Finde den Wahrscheinlichkeitsraum: Bestimme alle mĂśglichen Ergebnisse des Experiments und
ihre Wahrscheinlichkeiten, d.h. bestimme die Menge âŚ und die Wahrscheinlichkeiten Pr {Ď} der
Elementarereignise Ď â âŚ.
2. Bestimme die Ereignisse E: bestimme welche von der Ergebnissen E â âŚ âinteressantâ sind.
3. Bestimme die Wahrscheinlichkeit des Ereignis E: kombiniere
P die Wahrscheinlichkeiten der Elementarereignisse in E um Pr {E} zu bestimmen, Pr {E} = Ď âE Pr {Ď}.
â˛ Beispiel 4.5 : Zufallsexperiment: WĂźrfle zwei SpielwĂźrfel.

1. Wahrscheinlichkeitsraum âŚ: 62 = 36 mĂśglichen AusgĂ¤nge des Experiments, je mit Wahr1
scheinlichkeit 36
.
2. Ereignisse: MĂśgliche Ereignisse: E1 =âdie Summe der Augenzahlen ist > 10â oder E2 =âdie
zweite Zahl ist grĂśĂer als die ersteâ. D.h. E1 = {(5, 6), (6, 5), (6, 6)} und E2 = {(i, j) : 1 6
i < j 6 6}.
3. Wahrscheinlichleiten: Pr {E1 } =

|E1 |
36

=

1
12

und Pr {E2 } =

|E2 |
36

=

15
36

=

5
12 .

â˛ Beispiel 4.6 : In einem Dorf lebt die HĂ¤lfte aller Menschen alleine, die andere HĂ¤lfte mit genau
einem Partner.
Wenn ich zufĂ¤llig jemanden auf dem Marktplatz anspreche, mit welcher Wahrscheinlichkeit
lebt derjenige allein? Antwort: 1/2. Warum? In diesem Fall besteht der Wâraum âŚ aus allen
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

162

0/1-Strings (a1 , . . . , an ) mit ai = 1 genau dann, wenn der i-ter Mensch alleine lebt. Dann ist
Pr {ai = 1} = Pr {ai = 0} = 1/2.
Wenn ich nun zufĂ¤llig an eine WohnungstĂźr klopfe und frage, mit welcher Wahrscheinlichkeit lebt
dort jemand allein? Antwort: 2/3. Warum? In diesem Fall besteht der Wahrscheinlichkeitsraum
âŚ aus allen 0/1-Strings (b1 , . . . , bm ) mit bi = 1 genau dann, wenn das i-te Haus ein Familienhaus
ist. Da genau die HĂ¤lfte
 der Menschen alleine leben, in genau 1/3 der HĂ¤use Familien leben. Also
ist in diesem Fall Pr im Haus lebt jemend allein = 2/3.



Immer den richtigen Wahrscheinlichkeitsraum wĂ¤hlen!

4.2.1 Das Geburtstagsproblem
Um einen schnellen Zugriff auf Daten zu haben, kann man sie in Listen aufteilen. Beim Abspeichern
von Daten in Computern kommt diese Idee in der Technik des Hashings zur Anwendung. Nur bei
kurzen Listen sind auch die Suchzeiten kurz, daher stellt sich die Frage, mit welcher Wahrscheinlichkeit es zu âKollisionenâ kommt, zu Listen, die mehr als einen Eintrag enthalten. Wir betrachten diese
Wahrscheinlichkeit fĂźr n Listen und m Daten unter der Annahme, dass alle mĂśglichen Belegungen
der Listen mit den Daten gleich wahrscheinlich sind. Wir werden sehen, dass mit Kollisionen schon
â
dann zu rechnen ist, wenn m von der GrĂśĂenordnung n ist.
Diese Fragestellung ist in der Stochastik unter dem Namen Geburtstagproblem bekannt. Gefragt ist
nach der Wahrscheinlichkeit, dass in einer Klasse mit m SchĂźlern alle verschiedene Gerburtstage
haben.
1. Finde den Wahrscheinlichkeitsraum: Wir lassen uns von der Vorstellung leiten, dass das Tupel
Ď = (x 1 , . . . , x m ) der m Geburtstage ein rein zufĂ¤lliges Element aus

âŚ :=



(x 1 , . . . , x m ) : x i â {1, . . . , n}



ist, mit n = 365.
2. Bestimme das Ereignis: Uns interessiert das Ereignis E =âalle Geburtstage x 1 , . . . , x m sind verschiedenâ:
E=





(x 1 , . . . , x m ) â âŚ : x i , x j fĂźr alle i , j .

3. Bestimme die Wahrscheinlichkeit des Ereignis: Es gilt |E| = n(n â 1) Âˇ Âˇ Âˇ (n â m + 1). Nehmen
wir also an, dass es sich um eine rein zufĂ¤llige Wahl der Geburtstage aus âŚ handelt, so ist die gesuchte
ÂŠ 2003 by S. Jukna

4.3. STOCHASTISCHE UNABHĂNGIGKEIT

163

Wahrscheinlichkeit2
Pr {E}

=
(â)

6
(ââ)

=


mâ1 
i
|E| n(n â 1) Âˇ Âˇ Âˇ (n â m + 1) Y
=
=
1â
|âŚ|
nm
n
i=1
!
mâ1
mâ1
Y i
X i
ân
e = exp â
n
i=1
i=1


m(m â 1)
exp â
.
2n

(4.2)

â
FĂźr m = 1 + 2n ist diese Wahrscheinlichkeit durch eâ1 nach oben beschrĂ¤nkt und fĂ¤llt dann fĂźr
wachsendes m rapide gegen Null. Diese AbschĂ¤tzung drĂźckt das Geburtstag-PhĂ¤nomen aus:
â
In einer Gruppe von m = 1 + 2 Âˇ 365 6 28 Leuten haben zwei denselben Geburtstag mit
Wahrscheinlichkeit > 1 â eâ1 .
Oder in der Perspektive von Hashing mit Verkettung: AnfĂ¤nglich erhalten wir nur Einerlisten. Wenn
â
aber m in den Bereich von âŚ( n) kommt, dann entwickeln sich erste Zweierlisten.

4.3 Stochastische UnabhĂ¤ngigkeit
Definition: Zwei Ereignisse A und B sind (stochastisch) unabhĂ¤ngig, falls
Pr { A âŠ B} = Pr { A} Âˇ Pr {B}
gilt.
Das ist die Definition der UnabhĂ¤ngigkeit. Aussagen wie âzwei Ereignisse sind unabhĂ¤ngig, falls diese
Ereignisse einander nicht beeinflĂźssenâ sind keine Definitionen!



Erst richtig falsch ist zu sagen, dass je zwei disjunkte Ereignisse unabhĂ¤ngig sind. UnabhĂ¤ngigkeit von Ereignissen hat nichts mit ihrer Disjunktheit zu tun! Zum Beispiel, sind Pr { A} > 0,
Pr {B} > 0 und A âŠ B = â, dann sind A und B abhĂ¤ngig, da dann Pr { A âŠ B} = Pr {â} = 0 und
Pr { A} Âˇ Pr {B} > 0 gilt.
Um UnabhĂ¤ngigkeit von Ereignissen zu zeigen, ist der folgender einfacher Fakt oft nĂźtzlich.
Behauptung 4.7. Sind A und B zwei unabhĂ¤ngige Ereignise, so sind auch die Ereignise A und B wie
auch A und B unabhĂ¤ngig.
Beweis.

Pr A âŠ B

= Pr { A} â Pr { A âŠ B}

= Pr { A} â Pr { A} Âˇ Pr {B}


= Pr { A} (1 â Pr {B}) = Pr { A} Âˇ Pr B .

2Hier haben wir in (â) die Ungleichung 1 + x 6 e x (gĂźltig fĂźr alle x â R) und in (ââ) die Gleichung
(arithmetische Reihe) ausgenutzt.

P mâ1
i=1

i = m(mâ1)
2

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

164


â˛ Beispiel 4.8 : Wir werfen zweimal eine faire MĂźnze und betrachten die Ereignisse:
A =

âerster Wurf ergibt Wappenâ

B =

âbeide AusgĂ¤nge sind gleichâ

A =

âbeide AusgĂ¤nge sind Wappenâ

Obwohl die Ereignisse A und B sich gegenwĂ¤rtig zu âbeeinflĂźĂenâ scheinen, sind sie in Wirklichkeit unabhĂ¤ngig:
Pr { A âŠ B} = Pr {W W } =

1
4

Pr { A} Âˇ Pr {B} = Pr {W W, W K } Âˇ Pr {W W, K K } =

1 1 1
Âˇ = .
2 2 4

Die Ereignise A und C sind aber bereits abhĂ¤ngig!
Pr { A âŠ C} = Pr {W W } =

1
4

Pr { A} Âˇ Pr {B} = Pr {W W, W K } Âˇ Pr {W W } =

1 1 1
Âˇ = .
2 4 8

â˛ Beispiel 4.9 : Wir werfen dreimal eine faire MĂźnze. Der Wahrscheinlichkeitsraum besteht aus 8
Elementarereignissen
âŚ = {K K K, K KW, KW K, KW W, W K K, W KW,WW K,WW W }
und jedes davon kann mit Wahrscheinlichkeit 1/8 eintreten. Wir betrachten die Ereignisse:
A =

âes gibt mehr Wâs als Kâsâ

B =

âdie ersten zwei AusgĂ¤nge sind gleichâ

Dann ist
A = {KW W, W KW, W W K, WWW } â Pr { A} = 1/2

B = {K K K, K KW, W W K, W W W } â Pr {B} = 1/2

A âŠ B = {W W W, W W K } â Pr { A âŠ B} = 1/4

Also sind A und B unabhĂ¤ngig. Aber beide Ereignisse A und B beinhalten die AusgĂ¤nge der
ersten zwei WĂźrfe, und es ist schwer zu argumentieren, warum denn ein der Ereignisse keinen
EinflĂźss auf den anderen hat (oder haben soll). Wenn wir zum Beispiel das Ereignis
C =

âWappen im dritten Schrittâ

betrachten, dann haben wir
C = {W W W, W KW, KW W, K KW } â Pr {C} = 1/2

A âŠ C = {W W W, W KW, KW W } â Pr { A âŠ C} = 3/8

und somit sind A und C schon abhĂ¤ngig! Obwohl, wie es leicht zu sehen ist, B und C immer noch
unabhĂ¤ngig sind.
ÂŠ 2003 by S. Jukna

4.4. BEDINGTE WAHRSCHEINLICHKEIT

165

Wir haben gesehen, dass es kann drei Ereignisse A, B, C geben, so dass A und B wie auch B und
C unabhĂ¤ngig sind, aber die Ereignisse A und C sind nicht unabhĂ¤ngig. Waren nun alle drei Paare
unabhĂ¤ngig, kĂśnnte dann man daraus schlieĂen, dass Pr { A âŠ B âŠ C} = Pr { A} Âˇ Pr {B} Âˇ Pr {C} gelten
soll? Leider, so einfach ist die Sache nicht ...
â˛ Beispiel 4.10 : Wir werfen dreimal eine MĂźnze und betrachten die Ereireignisse
A = âdie ersten zwei AusgĂ¤nge sind gleichâ
B = âder erste und der dritte AusgĂ¤ng sind gleichâ
C = âdie letzten zwei AusgĂ¤nge sind gleichâ
Dann gilt Pr { A} = Pr {B} = Pr {C} = 1/2, und alle Ereignisse A âŠ B, B âŠ C, A âŠ C und A âŠ B âŠ C
sind gleich dem Erreignis {W W W, K K K }, das mit mit Wahrscheinlichkeit 1/4 eintritt. Damit sind
alle drei Paare unabhĂ¤ngig, aber
Pr { A âŠ B âŠ C} = 1/4

Pr { A} Âˇ Pr {B} Âˇ Pr {C} = 1/8
Also sind die Ereignisse A, B, C nicht âtotalâ unabhĂ¤ngig.

Die allgemeine Definition ist folgende. Die Ereignisse A1 , . . . , An sind (heiĂen) total unabhĂ¤ngig,
falls fĂźr alle 1 6 i1 < i2 < . . . < i k 6 n mit k > 1 die Ereignisse
Ai 1 âŠ Ai 2 âŠ . . . âŠ Ai k â1

und

Ai k

unabhĂ¤ngig sind.
Behauptung 4.11. Sind die Ereignisse A1 , . . . , An total unabhĂ¤ngig, so gilt:
Pr { A1 âŠ A2 âŠ . . . âŠ An } = Pr { A1 } Âˇ Pr { A2 } Âˇ Âˇ Âˇ Pr { An }
Beweis. Induktion Ăźber n.



4.4 Bedingte Wahrscheinlichkeit
Alice und Bob gehen zum Abendsessen. Um zu entscheiden, wer jetzt bezahlen soll, werfen sie dreimal eine faire MĂźnze. Falls es mehr mals Wappen (W) als Kopf (K) rauskommt, bezahlt Alice, sonst
bezahlt Bob. Es ist klar, dass die Chancen gleich sind. Der Wahrscheinlichkeitsraum sieht folgendermaĂen aus
âŚ = {K K K, K KW, KW K, KW W, W K K, W KW, WW K,WWW }
und die Eriegnisse âbezahlt Aliceâ und âbezahlt Bobâ sind entsprechend
A = {KW W, W KW, W W K, WWW }

B = {K K K, K KW, KW K, W K K }

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

166

Sie werfen die MĂźnze einmal und das Resultat ist âWappenâ; bezeichne dieses Ereignis durch E. Wie
sollte man jetzt (nachdem das Ereignis E bereits eingetreten ist) die Chancen berechnen? Es gilt
E = {W W W, W W K, W KW,W K K }
Da wir bereits wissen, dass E eingetreten ist, hat sich unser Wahrscheinlichkeitsraum von âŚ auf
E verkleinert, da die Ereignisse, die nicht in E liegen, nicht mehr mĂśglich sind. In diesem neuen
Experiment sehen die AusgĂ¤ngen âbezahlt Aliceâ und âbezahlt Bobâ folgendermaĂen aus:
A âŠ E = {W KW, W W K, W W W }

B âŠ E = {W K K }

Die neuen Wahrscheinlichkeiten, wer nun bezahlen soll, sind jetzt 3/4 fĂźr Alice und nur 1/4 fĂźr Bob.
Die allgemeine Situation ist folgende: Ist ein Ereignis E bereits eingetreten, wie sieht dann die Wahrscheinlichhkeit, dass ein anderes Ereignis A eintreten wird? Im Allgemeinen kĂśnnen wir nicht mehr
einfach die Wahrscheinlichkeiten der Elementarereignisse Ď â A aufsummieren, denn (nachdem E
eingetreten ist) werden sich auch die Wahrscheinlichkeiten der Elementarereignisse Ă¤ndern. Die allgemeine Definition ist folgende:
Definition: FĂźr zwei Ereignisse A und B mit Pr {B} , 0 die bedingte Wahrscheinlichkeit Pr { A | B }
fĂźr das Ereignis A unter der Bedingung B ist definiert durch
Pr { A | B } =

Pr { A âŠ B}
.
Pr {B}

(4.3)

Die Wahrscheinlichkeit Pr { A | B } bezeichnet man als a-posteriori-Wahrscheinlichkeit von A (bezĂźglich B).
B
A

Anteil von

in

B

Abbildung 4.2: Bedingte Wahrscheinlichkeit bei der Gleichverteilung.

FĂźr den Beispiel oben (mit Alice und Bob) gilt
Pr { A | E } =
Pr {B | E } =

Pr { A âŠ E} 3/8 3
=
= ,
Pr {E}
1/2 4
Pr {B âŠ E} 1/8 1
=
=
Pr {E}
1/2 4

Bemerkung 4.12. Mit Hilfe der bedingten Wahrscheinlichkeit kann man eine Ă¤quivalente Definition
der stochastischer UnabhĂ¤ngigkeit zweier Ereignisse A und B angeben:
A und B sind unabhĂ¤ngig ââ Pr { A âŠ B} = Pr { A} Pr {B} ââ Pr { A | B } = Pr { A}
ÂŠ 2003 by S. Jukna

4.4. BEDINGTE WAHRSCHEINLICHKEIT

167

Bemerkung 4.13. Die bedingte Wahrscheinlichkeit Pr { A | B } kann als Wahrscheinlichkeit fĂźr das
Eintreten des Ereignisses A interpretiert werden, wenn das Ereignis B bereits eingetreten ist. 3 Ist Pr
eine Gleichverteilung, dann ist die angegebene Definition von Pr { A | B } intuitiv klar: Ist das Ereignis
B eingetreten, dann sind jene Elementarereignisse aus B fĂźr das Ereignis A gĂźnstig, die zu A gehĂśren,
und dies sind die Elementarereignisse aus A âŠ B; damit gilt
Pr { A | B } =

| A âŠ B| | A âŠ B| |âŚ| Pr { A âŠ B}
=
Âˇ
=
.
|B|
|âŚ|
|B|
Pr {B}

Insbesondare sind die Ereignisse A und B genau dann unabhĂ¤ngig, wenn der Anteil | A|/|âŚ| des Ereignisses A im ganzen Wahrscheinlichkeitsraum âŚ gleich dem Anteil | A âŠ B|/|B| des Teilereignisses
A âŠ B von A in dem Ereignis B ist (siehe Abb. 4.2).
â˛ Beispiel 4.14 : Die Polizei stoppt Autos an der Miquell-Allee rein zufĂ¤llig. Aus der Erfahrung folgendes ist bekannt: Mit 3/100 Wahrscheinlichkeit wird das (angehaltene) Auto gelb sein und mit
1/50 Wahrscheinlichkeit 4 wird das Auto gelb und der Fahrer blond sein.
Nun kommen wir eines Tages die Allee entlang und sehen, dass die Polizei gerade ein gelbes
Auto angehalten hat. Wie groĂ ist die Wahrscheinlichkeit, dass der Fahrer blond ist?
Wir haben also zwei Ereignisse G = âdas Auto ist gelbâ und B = âder Fahrer ist blondâ, und
wissen, dass Pr {G} = 3/100 und Pr {B âŠ G} = 1/50. Also gilt
Pr {B | G } =

Pr {B âŠ G} 0, 02
1
=
= 0, 667 > .
Pr {G}
0, 03
2

â˛ Beispiel 4.15 : In einem groĂen Haus wohnen mehrere Familien, je mit zwei Kindern. Wir wissen
auch, dass es immer die TĂźr von einem Jungen geĂśffnet wird, falls mindestens ein Junge in
Familie ist (Jungs sind schneller).
Wir klingeln an einer WohnungstĂźr und ein Junge, der Peter, hat uns gerade die TĂźr geĂśffnet. Nun
biete ich Ihnen eine Wette an. Wenn das andere Kind ebenfalls ein Junge ist, kriegen Sie 5,â e,
wenn es ein MĂ¤dchen ist, kriege ich 5,â e.
Ist dies eine faire Wette? NatĂźrlich nicht, denn meine Gewinnchancen stehen 2 : 1 dabei!
Nehmen wir der Einfachheit halber an, ein Neugeborenes sei mit Wahrscheinlichkeit 1/2 ein
MĂ¤dchen (M) bzw. ein Junge (J), unabhĂ¤ngig vom Geschlecht frĂźher oder spĂ¤ter geborener Geschwister. Dann gibt es unter BerĂźcksichtigung der Reihenfolge der Geburten bei 2 Kindern die
vier gleichwahrscheinliche FĂ¤lle: âŚ = {M M, M J, J M, J J}. Durch die Beobachtung von Peter (J)
scheidet der Fall M M aus. Bei den verbleibenden FĂ¤llen A = {M J, J M, J J} ist ein M doppelt so
wahrscheinlich wie ein zweites J.
Warum wĂ¤re die Wette fair, wenn uns der Peter gesagt hĂ¤tte, dass er das Ă¤ltere Kind ist? Da dann
hĂ¤tten wir anstatt A = {M J, J M, J J} das Ereignis {M J, J J}.
â˛ Beispiel 4.16 : Beim gleichzeitigen Werfen zweier SpielwĂźrfel ist die Menge der Elementarereignisse âŚ = {(i, j) : 1 6 i, j 6 6}, also |âŚ| = 36. Ist A das Ereignis, dass die gewĂźrfelte Augensumme mindestens 10 betrĂ¤gt, so sind die fĂźr A gĂźnstigen Elementarereignisse gegeben durch:
3Aber das ist nur eine Interpretation, die nicht 100% richtig ist: Es gibt keinen Grund, warum das Ereignis B vor dem
Ereignis A eintreten sollte! Die Situation ist Ă¤hnlich wie mit der logischer Implikation B â A. Diese Implikation sagt nicht,
dass die Aussage B bereits als wahr bewiesen wĂźrde; sie sagt nur, dass A richtig wĂ¤re, falls B gelten wĂźrde.
4Gelbe Autos sind also selten, aber die blondhaarige wĂ¤hlen ein gelbes Auto noch seltener.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

168

6
(4, 6), (5, 5), (5, 6), (6, 4), (6, 5), (6, 6). Damit erhĂ¤lt man: Pr { A} = 36
= 16 . Sei nun B das Ereignis, dass die gewĂźrfelte Augensumme gerade ist. Von der 36 mĂśglichen Elementarereignissen
1
sind die 18 Ereignisse fĂźr B gĂźnstig und man erhĂ¤lt: Pr {B} = 18
36 = 2 . FĂźr das Ereignis A âŠ B
4
sind nur die Ereignisse (4, 6), (5, 5), (6, 4), (6, 6) gĂźnstig, so dass Pr { A âŠ B} = 36
= 19 gilt.

WeiĂ man nun, dass Ereignis B bereits eingetreten ist, so findet man fĂźr die a-posteriori2
1
Wahrscheinlichkeit von A: Pr { A | B } = (1/9)
(1/2) = 9 < 3 .
WeiĂ man, dass Ereignis A bereits eingetreten ist, so findet man fĂźr die a-posteriori-Wahrschein2
lichkeit von B: Pr {B | A } = (1/9)
(1/6) = 3 .

4.4.1 Multiplikationssatz fĂźr Wahrscheinlichkeiten
Ist Pr {B} , 0 und kennt man die Wahrscheinlichkeit Pr { A | B }, dann kann man auch Pr { A âŠ B}
berechnen:
Pr { A âŠ B} = Pr {B} Âˇ Pr { A | B } .
(4.4)
Man nennt diese Reformulierung von (4.3) Multiplikationssatz fĂźr Wahrscheinlichkeiten.
Korollar 4.17. Seien A und B zwei Ereignisse mit Pr {B} , 0. Dann sind A und B unabhĂ¤ngig genau
dann, wenn Pr { A | B } = Pr { A} gilt.
Der Multiplikationssatz fĂźr Wahrscheinlichkeiten gilt auch fĂźr mehrere Ereignisse:
Satz 4.18. (Multiplikationssatz fĂźr Wahrscheinlichkeiten) Ist Pr { A1 âŠ A2 âŠ . . . âŠ An } > 0, so gilt:
(n
)
\
Ai = Pr { A1 } Âˇ Pr { A2 | A1 } Âˇ Pr { A3 | A1 âŠ A2 } Âˇ . . . Âˇ Pr { An | A1 âŠ . . . âŠ Anâ1 }
Pr
i=1

Beweis. Pr

Tn

i=1

Ai ist gleich

Pr { A1 } Âˇ

Pr { A2 âŠ A1 } Pr { A3 âŠ A2 âŠ A1 }
Pr { An âŠ . . . âŠ A1 }
Âˇ
Âˇ...Âˇ
Pr { A1 }
Pr { A2 âŠ A1 }
Pr { Anâ1 âŠ . . . âŠ A1 }


â˛ Beispiel 4.19 : Es ist aufgrund einer Umfrage bei der Studierenden der Informatik bekannt, dass die
Wahrscheinlichkeit, die Klausur zur Grundvorlesung âTheoretische Informatik 2â beim ersten
Versuch zu bestehen, gleich p1 , 1 ist. Die Wahrscheinlichkeit dafĂźr, dass ein Studierender die
Nachklausur besteht, wenn er beim erstenmal durchgefallen ist, betrĂ¤gt p2 . Mit welcher Wahrscheinlichkeit sind mehr als zwei Versuche notwendig, die Klausur zu bestehen?
Seien Ai die Ereignisse, dass der i-te Versuch erfolglos war, i = 1, 2. Unser Ziel
 ist Pr { A1 âŠ A2 }
zu berechnen. Wir wissen, dass Pr { A1 } = 1 â p1 , 0 und Pr { A2 | A1 } = 1 â Pr A2 | A1 = 1 â p2
gilt. Deshalb ist die gesuchte Wahrscheinlichkeit Pr { A1 âŠ A2 } = Pr { A1 } Âˇ Pr { A2 | A1 } = (1 â
p1 )(1 â p2 ).
ÂŠ 2003 by S. Jukna

4.4. BEDINGTE WAHRSCHEINLICHKEIT

169

â˛ Beispiel 4.20 : Gegeben sind 32 Spielkarten, darunter 4 Buben. Zwei Spieler, jeder erhĂ¤lt 3 Karten.
Ereignis A = âJeder der beiden Spieler erhĂ¤lt genau 1 Bubenâ. Frage: Pr { A} =? LĂśsung: A =
A1 âŠ A2 , wobei Ai = âSpieler i erhĂ¤lt genau 1 Bubenâ. Dann ist
Pr { A1 âŠ A2 } = Pr { A1 } Âˇ Pr { A2 | A1 } =



4
1

28
2
32
3



Âˇ



3
1

26
2
29
3



â 0, 081.

Erklerung: Wieviele MĂśglichkeiten gibt es,
 3 Spielkarten aus 32 auszuwĂ¤hlen, so dass unter ihnen
genau 1 Buben sein wird? Wir haben 41 MĂśglichkeiten, 1 Buben aus 4 auszuwĂ¤hlen, und 28
2
MĂśglichkeiten, noch 2 Spielkarten aus den restlichen 28 Spielkarten
(ohne
Buben)
auszuwĂ¤hlen.
 
Das ergibt nach dem Produktregel (siehe Satz 1.34(3)) genau 41 28
2 MĂśglichkeiten. Somit ist
4 28
( )( )
Pr { A1 } = 1 32 2 . Nachdem die drei Spielkarten entfernt sind, bleibt es nur noch 29 Spielkarten
(3)
(3)(26)
mit nur 3 Buben darunter. Deshalb ist auch Pr { A2 | A1 } = 1 29 2 .
(3)
Es gibt ein Paar Regeln, die den Umgang mit bedingter Wahrscheinlichkeit erleichtern. Zuerst beachten wir, dass Pr B (A) := Pr { A | B } eine Wahrscheinlichkeitsverteilung Pr B auf âŚ definiert, denn:
X
X
Pr B (Ď) =
Pr {Ď | Ď â B }
Ď ââŚ

Ď ââŚ

=

X Pr {Ď â B}
1 X
1
=
Pr {Ď â B} =
Âˇ Pr {B} = 1.
Pr {B}
Pr {B}
Pr {B}

Ď ââŚ

Ď ââŚ

Es gelten also alle im Satz 4.3 angegebene Eigenschaften auch fĂźr Pr B (A). Man kann diese Regeln
auch direkt aus den Regeln fĂźr Pr { A} ableiten. Zum Beispiel:


Pr A âŠ B
Pr {B \ A} Pr {B \ (A âŠ B)}
Pr B (A) = Pr A | B =
=
=
Pr {B}
Pr {B}
Pr {B}
Pr {B} â Pr { A âŠ B}
=
= 1 â Pr { A | B }
Pr {B}
= 1 â Pr B (A),
andere Eigenschaften analog.

4.4.2 Satz von der totalen Wahrscheinlichkeit
Manchmal wissen wir die Wahrscheinlichkeit von einem Ereignis A im voraus nicht, wissen aber wie
die Wahrscheinlichkeit von A aussehen wĂźrde, falls irgendwelche andere Ereignisse bereits eingetreten wĂźrde.
â˛ Beispiel 4.21 : (EisverkĂ¤ufer) Ein EisverkĂ¤ufer muss sich entscheiden, ob er mehr Eis fĂźr den Feiertag bestellen sollte. Aus der Erfahrung weiss er, dass seine Chance alles zu verkaufen sehr stark
vom den Wetter abhĂ¤ngt: Es gibt 90% Chance, falls der Tag sonnig ist, 60% Chance, falls es wollkig ist, und nur 20% Chance, falls es regnet. Nach der Wettervorhiersage wird es am den Tag mit
30% Wahrscheinlichkeit sonnig, mit 45% Wahrscheinlichkeit wolkig und mit 25% Wahrscheinlichkeit regnen. Mit welcher Wahrscheinlichkeit wird der EisverkĂ¤ufer alles verkaufen?
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

170

Diese Frage lĂ¤Ăt sich mit dem folgenden Satz zu beantworten. Eine Zerlegung des Wahrscheinlichkeitsraums âŚ ist eine Menge der Ereignisse B1 , . . . , Bn , so dass B1 âŞ B2 âŞ. . . âŞ Bn = âŚ und Bi âŠ B j = â
fĂźr alle i , j gilt.
Satz 4.22. (Satz von der totalen Wahrscheinlichkeit) Ist B1 , . . . , Bn eine Zerlegung des Wahrscheinlichkeitsraums mit Pr {Bi } , 0 fĂźr alle i, so gilt:
Pr { A} =

n
X
i=1

Pr { A âŠ Bi } =

B1

n
X
i=1

Pr {Bi } Âˇ Pr { A | Bi } .

A
B2

B3

Diesen Satz bezeichnet man auch als Adamâs Satz, da er so oft von verschiedenen Mathematiker
wiedererfunden wurde.
Beweis.
Pr { A} = Pr {âŚ âŠ A} = Pr
=

n
X
i=1

(

n
[

i=1

)

Bi âŠ A

=

n
X
i=1

Pr {Bi âŠ A}

Pr {Bi } Âˇ Pr { A | Bi } .


â˛ Beispiel 4.23 : (EisverkĂ¤ufer â Fortsetzung) Wir betrachten die Ereignisse B1 = âes ist sonnigâ, B2
= âes ist bevĂślktâ und B3 = âes regnetâ. Diese Ereignisse zerlegen den Wahrscheinlichkeitsraum,
und die entsprechende Wahrscheinlichkeiten sind
Pr {B1 } = 0, 3 Pr {B2 } = 0, 45 Pr {B3 } = 0, 25
Sei A das Ereignis, dass der VerkĂ¤ufer alles verkauft. Wir haben die folgenden partiellen Informationen:
Pr { A | B1 } = 0, 9 Pr { A | B2 } = 0, 6 Pr { A | B3 } = 0, 2
Nach dem Satz von der totalen Wahrscheinlichkeit gilt:
Pr { A} = (0, 3 Âˇ 0, 9) + (0, 45 Âˇ 0, 6) + (0, 25 Âˇ 0, 2) = 0, 59
Da A, A fĂźr jedes Ereignis A eine Zerlegung des Wahrscheinlichkeitsraums bilden, liefert Satz 4.22
den folgenden:
ÂŠ 2003 by S. Jukna

4.4. BEDINGTE WAHRSCHEINLICHKEIT

171

Korollar 4.24. Seien A und B Ereignisse mit 0 < Pr {B} < 1. Dann gilt:


Pr { A} = Pr {B} Âˇ Pr { A | B } + Pr B Âˇ Pr A B
â˛ Beispiel 4.25 : (Aus dem Hut gezaubert) In einem Hut befinden sich drei Karten. Eine ist auf beiden Seiten mit einem Pik versehen â â , eine mit einem Pik und einem Karo â âŚ,und eine mit Karo
auf beiden Seiten âŚâŚ. Wir ziehen rein zufĂ¤llig eine Karte aus dem Hut. Sei p die Wahrscheinlichkeit dafĂźr, dass die gezogene Karte auf beiden Seiten zwei verschiedene Symbole trĂ¤gt.
Da wir nur eine solche Karte (aus drei) in dem Hut haben, ist diese Wahrscheinlichkeit offensichtlich 1/3. Wir wollen aber âzeigenâ, dass p = 1/2 gilt.
Wir denken uns eine Karte so herausgezogen, dass nur eine Seite sichtbar ist.
Fall 1: Wir sehen ein Pik. Dann kann die gezogene Karte nicht âŚâŚ sein. Beide anderen Karten â âŚ
und â â  hatten die gleiche Chance gezogen zu werden. Deshalb ist in diesem Fall p = 1/2.
Fall 2: Wir sehen ein Karo. Dieser Fall ist analog.
Wir kĂśnnen diesen âBeweisâ auch formalizieren. Sei P das Ereignis âIch sehe einen Pikâ und
V sei das Ereignis
âdie gezogene Karte trĂ¤gt auf beiden Seiten zwei verschiedene Symboleâ. Da

Pr {P} = Pr P = 1/2 und Pr {V | P } = Pr V P = 1/2, liefert uns der Satz von der totalen
Wahrscheinlichkeit


1 1 1
Pr {V } = Pr {P} Âˇ Pr {V | P } + Pr P Âˇ Pr V P = + = .
4 4 2
Wo ist der Fehler? Der Fehler liegt im falsch gewĂ¤hlten Wahrscheinlichkeitsraum! Da wir nur eine
Seite der gezogenen Karte sehen, mĂźssen wir auch unterscheiden, welche von zwei Seiten das ist.
Deshalb sind die Elementarereignisse in diesem Experiment nicht die 3 mĂśgliche Kartentypen
{â , â }, {â , âŚ}, {âŚ, âŚ}
sondern 6 mĂśgliche Paare (Kartentyp, Seite die ich sehe):
(â â , 1), (â â , 2), (â âŚ, 1), (â âŚ, 2), (âŚâŚ, 1), (âŚâŚ, 2)
Deshalb ist Pr {V | P } nicht
Pr {V | P } =
sondern
Pr {V | P } =

1
|{â , âŚ}|
=
|{â â , â âŚ}| 2

|{(â âŚ, 1)}|
1
=
|{(â â , 1), (â â , 2), (â âŚ, 1)}| 3

4.4.3 Satz von Bayes
Es gibt einen groĂen Unterschied zwischen Pr { A | B } und Pr {B | A }.
Die Wissenschaftler wollen einen Test fĂźr eine Erbkrankheit entwickeln. NatĂźrlich, gibt es keinen perfekten Test: Es werden einige gesunde als kranke eingestufft und umgekehrt. Sei zum Beispiel A das
Ereigniss âdie Testperson ist krankâ und B das Ereignis âder Test ist positivâ. FĂźr die Wissenschaftlern
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

172

wichtig ist, mit welcher
der Testergebnis falsch wird, d.h. fĂźr sie sind die Wahr Wahrscheinlichkeit

scheinlichkeiten Pr B A und Pr B | A von der Bedeutung. FĂźr die Testperson sind dagegen die
Wahrscheinlichkeiten Pr { A | B } und Pr A B von groĂer Bedeutung. Ich bin als positiv getested,
mit welcher Wahrscheinlichkeit bin ich wirklich krank? Ich bin als negativ getested, wie sicher kann
ich sein, dass ich tatsĂ¤chlich gesund bin?
Die bedingten Wahrscheinlichkeiten Pr { A | B } und Pr {B | A } sind durch folgenden Satz verbunden:
Satz 4.26. (Satz von Bayes) Seien A und B Ereignisse mit Pr { A} , 0 und Pr {B} , 0. Dann gilt:
Pr { A | B } =

Pr { A}
Âˇ Pr {B | A }
Pr {B}

Beweis.
Pr {B} Âˇ Pr { A | B } = Pr { A âŠ B} = Pr { A} Âˇ Pr {B | A }

Sind 0 < Pr { A} < 1 und Pr {B} , 0, so liefert uns Korollar 4.24 die folgende Gleichung:
Pr { A | B } =

Pr { A} Âˇ Pr {B | A }


Pr { A} Âˇ Pr {B | A } + Pr A Âˇ Pr B A

Oft ist der Satzt von Bayes in dieser Form formuliert.

â˛ Beispiel 4.27 : Es gibt drei Beutel, die erste enthĂ¤lt zwei goldene MĂźnzen, die zweite enthĂ¤lt eine
goldene und eine silberne MĂźnze, und die dritte enthĂ¤lt zwei silberne MĂźnzen. Ich wĂ¤hle rein zufĂ¤llig eine aus der drei Beutel und anschlieĂend ziehe aus dieser Beutel (auch rein zufĂ¤llig) eine
der zwei MĂźnzen. Angenommen, ich habe eine goldene MĂźnze gezogen. Wie groĂ ist die Wahrscheinlichkeit, dass ich die Beutel mit zwei goldenen MĂźnzen gewĂ¤hlt habe? Sei A das Ereignis
âich habe eine goldene MĂźnze gezogenâ und B1 das Ereignis âich habe die 1. Beutel (mit zwei
goldenen MĂźnzen) gewĂ¤hltâ. Gesucht ist also Pr {B1 | A }. Der entsprechende Entscheidungsbaum
sieht folgendermaĂen aus:
G
{G,G}
{G,S}

Ereignis A
G
G
S

{S.S}

S
S

Aus dieser Diagramm sehen wir, dass Pr { A} = 1/2, Pr {B1 } = Pr {GG} = 1/3 und Pr { A | B1 } = 1.
Die Formel von Bayes liefert uns das Ergebnis:
Pr {B1 | A } =

Pr {B1 }
(1/3) Âˇ 1 2
Pr { A | B1 } =
=
Pr { A}
(1/2)
3
ÂŠ 2003 by S. Jukna

4.4. BEDINGTE WAHRSCHEINLICHKEIT

173

â˛ Beispiel 4.28 : (AIDS-Test) 0, 1% aller untersuchten Personen sind HIV-positiv. Der Test ist nicht
perfekt: 0, 2% der HIV-positiven werden als negative eingestuft; 0, 3% der HIV-negativen werden
als positive eingestuft.

Frage: Pr HIV-positiv positives Testergebnis =?

LĂśsung: A = âHIV-positives Testergebnisâ, B1 =âHIV-positivâ, B2 =âHIV-negativâ. Nach der Formel von der totalen Wahrscheinlichkeit:
Pr { A} = Pr {B1 } Pr { A | B1 } + Pr {B2 } Pr { A | B2 } = 0, 001 Âˇ 0, 998 + 0, 999 Âˇ 0, 003 = 0, 003996.
Nach der Formel von Bayes:
Pr {B1 | A } =

Pr {B1 }
0, 001 Âˇ 0, 998
Pr { A | B1 } =
â 0, 25.
Pr { A}
0, 003996

Obwohl der Testfehler so klein ist, wird mit Wahrscheinlichkeit 3/4 eine als HIV-kranke eingestufte Person tatsĂ¤chlich gesund sein! Intuition hier ist klar: obwohl der Fehler wirklich sehr klein
sind, ist die (abgeschĂ¤tzte) Anteil der HIV-positiven Personen noch kleiner.
â˛ Beispiel 4.29 : Wir haben eine faire MĂźnze, deren Wurf mit gleicher Wahrscheinlichkeit Kopf oder
Zahl ergibt, und eine unfaire MĂźnze, deren Wurf immer Kopf ergibt. Wir wĂ¤hlen eine der beiden
MĂźnzen zufĂ¤llig aus und werfen sie zweimal. Angenommen, beide WĂźrfe ergeben Kopf. Wie
groĂ ist dann die Wahrscheinlichkeit, dass die unfaire MĂźnze ausgewĂ¤hlt wurde? Dazu betrachten
wir zwei Ereignisse:
A =

es wurde unfaire MĂźnze gewĂ¤hlt

B =

beide WĂźrfe der MĂźnze ergeben Kopf

Es ist also Pr { A | B } zu bestimmen. Wir haben:

Pr { A} = Pr A = 1/2

Pr {B | A } = 1

1
Pr B A
=
4

Aus der Formel von der totalen Wahrscheinlichkeit folgt:


Pr {B} = Pr { A} Pr {B | A } + Pr A Pr B A
1
1 1 5
=
Âˇ1+ Âˇ = .
2
2 4 8
Nun kĂśnnen wir die Formel von Bayes anwenden und erhalten schlieĂlich
Pr { A | B } =

Pr { A}
Pr {B | A } =
Pr {B}

1
2

Âˇ1

5
8

=

4
5

â˛ Beispiel 4.30 : (AusschĂźsse in Lieferungen) Ein Hersteller von Computern bezieht ein bestimmtes
Bauteil von drei Zulieferern: 1, 2 und 3. Die folgenden Anteilswerte an der Gesamtlieferung
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

174

sowie die jeweiligen Ausschussanteile innerhalb der drei Lieferungen sind auf Grund lĂ¤ngerer
Erfahrung bekannt:
Anteil an Gesamtlieferung

1
2
3
60% 25% 15%

Ausschussanteil in Lieferung 8%

12% 4%

Wir fassen die relativen HĂ¤ufigkeiten als Wahrscheinlichkeiten auf: Betrachte die Menge âŚ aller
gelieferten Bauteile mit Gleichverteilung auf âŚ. Dann haben wir fĂźr die Teilmengen von âŚ
A =
Bi =

die Ausschuss-Bauteile
die von Zulieferer i stammenden Bauteile, i = 1, 2, 3

die folgenden Wahrscheinlichkeiten und bedingten Wahrscheinlichkeiten:
Pr {B1 } = 0, 6
Pr { A | B1 } = 0, 08

Pr {B2 } = 0, 25
Pr { A | B2 } = 0, 12

Pr {B3 } = 0, 15
Pr { A | B3 } = 0, 04

Frage 1: Wie groĂ ist der Ausschussanteil in der Gesamtlieferung? Mit der Formel von der totalen
Wahrscheinlichkeit:
Pr { A} =

3
X
i=1

Pr {Bi } Pr { A | Bi } = 0, 08 Âˇ 0, 6 + 0, 12 Âˇ 0, 25 + 0, 04 Âˇ 0, 15 = 0, 084.

Frage 2: Welche Anteile am Gesamtausschuss haben die einzelnen Zulieferer? Mit der Formel
von Bayes:
Pr {B1 }
0, 08 Âˇ 0, 6
Pr { A | B1 } =
â 0, 57;
Pr {B1 | A } =
Pr { A}
0, 084
ein Ausschussanteil stammt also mit 57%-iger Wahrscheinlichkeit von Zulieferer Nr. 1. Analog
erhalten wir
Pr {B2 | A } =
Pr {B3 | A } =

0, 12 Âˇ 0, 25
â 0, 36
0, 084
0, 04 Âˇ 0, 15
â 0, 07.
0, 084

Die Anteile der Zulieferer 2 und 3 am Gesamtausschuss betragen also 36% bzw. 7%.

4.5 Stochastische Entscheidungsprozesse
Bei Modellbildung von Experimenten, die in mehreren Schritten ablaufen, ist es oft hilfreich, den
ersten Modellierungsschritt (Bestimmung des Wahrscheinlichkeitsraums) als ein Entscheidungsbaum
darzustellen:
-

Verteilung p1 , . . . , pn des Schrittes 1 sei bekannt;

-

Verteilung des Schrittes k sei bekannt unter der Bedingung, dass Ergebnisse der Schritte 1, . . . , k â
1 vorliegen.
ÂŠ 2003 by S. Jukna

4.5. STOCHASTISCHE ENTSCHEIDUNGSPROZESSE

175

Dies liefert Wahrscheinlichkeiten fĂźr spezifische Ergebnisfolgen. Im Allgemeinen gilt fĂźr WahrscheinlichkeitsrĂ¤ume, die als ein EntscheidungsbĂ¤ume dargestellt sind, folgendes:
- Jede Kante bekommt ihren Gewicht (eine Zahl in [0, 1]), so dass die Summe der Gewichte der
aus einem Knoten ausgehenden Kanten gleich 1 ist. Der Gewicht von e = (u, v) kann man als
die bedingte Wahrscheinlichkeit interpretieren, entlang dieser Kante weiter zu spazieren, falls man
bereits an Knoten u angekommen ist.
- Elementarereignisse Ď sind die Wege von der Wurzel zu einem Blatt.
- Das Gewicht Pr {Ď} eines Weges Ď ist das Produkt der Gewichte seiner Kanten. Das folgt aus dem
Multiplikationssatz fĂźr Wahrscheinlichkeiten (Satz 4.18).
- Ereignisse E sind Teilmengen der Wege.
- Pr {E} ist die Summe der Gewichte der zu E gehĂśhrenden Wege.
â˛ Beispiel 4.31 : Box enthalte 4 weiĂe und 6 schwarze Kugeln. Ziehe zweimal ohne ZurĂźcklegen.
1. Wahrscheinlichkeitsraum âŚ = {ww, ws,sw,ss}. Hier âwwâ entspricht âerste Kugel weiĂ und
zweite Kugel weiĂâ, âwsâ entspricht âerste Kugel weiĂ und zweite Kugel schwarzâ usw. Das
kann man als Entscheidungsbaum darstellen (das Ereignis âKugel ist weiĂâ ist mit Kante
â â â â markiert, und das Ereignis âKugel ist schwarzâ ist mit ââââ markiert):
3/9

4/10
6/9

6/10

4/9

5/9

Der Baum besteht aus zwei Ebenen. Die erste Ebene entspricht der Ziehung der ersten
Kugel. Die zweite Ebene entspricht der Ziehung der zweiten Kugel unter der Bedingung,
dass die erste Kugel bereits gezogen ist! So wird z.B. im ersten Schritt die weiĂe Kugel
mit Wahrscheinlichkeit 4/10 gezogen. Aber die Wahrscheinlichkeit, dass die zweite Kugel auch weiĂ wird, nachdem die erste gezogene Kugel bereits weiĂ war, ist gleich 3/9:
Nach dem ersten Schritt bleiben noch 9 Kugeln und nur 3 davon sind weiĂ. Deshalb ist
Pr {ww} = (4/10) Âˇ (3/9) = 2/15.
2. Ereignisse. FĂźr i = 1, 2 betrachten wir die Ereignisse Wi =âi-te Kugel weiĂâ und Si =âi-te Kugel schwarzâ. Nach dem Multiplikationssatz fĂźr Wahrscheinlichkeiten gilt dann zum Beispiel
folgendes:

Pr {W1 âŠ W2 } = Pr {W1 } Âˇ Pr {W2 | W1 } =

4 3
2
Âˇ =
10 9 15
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

176
und

Pr {W2 } = Pr {W2 âŠ W1 } + Pr {W2 âŠ S1 }

= Pr {W1 } Âˇ Pr {W2 | W1 } + Pr {S1 } Âˇ Pr {W2 | S1 }
6 4 36
4
4 3
Âˇ +
Âˇ =
=
=
10 9 10 9 90 10
= Pr {W1 } .

â˛ Beispiel 4.32 : Wir spielen ein Spiel gegen einen Gegner. Er hat n Zahlen 1, . . . , n. Der Gegner
wĂ¤hlt sich zwei Zahlen y < z aus und schreibt sie fĂźr uns nicht sichtbar auf je einem Zettel. Wir
wĂ¤hlen zufĂ¤llig einen Zettel und lesen die darauf stehende Zahl r â {y, z}, sei s = {y, z} \ {r} die
verbleibende Zahl (diese Zahl sehen wir nicht). Wir mĂźssen nun entscheiden, welche der beiden
Zahlen r (die uns bekannte Zahl) und s (die uns unbekannte Zahl) grĂśĂer ist.
Annahme: Um unsere Entscheidung zu treffen, kĂśnnen wir einen beliebigen Zufallsgenerator
benutzen.
KĂśnnen wir unsere Gewinnchancen grĂśĂer als 50% machen?
Die Antwort ist: Ja, wir kĂśnnen
Pr {Gewinn} >

1
1
+
2 2n

erreichen. FĂźr n = 10 ist das sogar 55%.
Gewinnstrategie:
1. Rate zufĂ¤llig eine Zahl x aus


1
1
1
1 â ,2 â ,. . . ,n â
2
2
2
mit Wahrschinlichkeit 1/n. (Da wir â 12 nehmen, gehĂśrt x nicht zu 1, . . . , n.)
2. WĂ¤hle einen der beiden Zettel mit Wahrscheinlichkeit je 1/2. Diese Wahl ist unabhĂ¤ngig von
der Wahl von x. Sei r die Zahl auf diesem Zettel.
3. Hoffe, dass y < x < z und gebe die Antwort

r>s
Antwort =
r<s

falls r > x
falls r < x

Damit ist (siehe Abb. 4.3):
Pr {Gewinn}

=
=
=
>

y
zây zây nâz
+
+
+
2n
2n
2n
2n
n+zây
2n
1 zây
+
2
2n
1
1
+
2 2n

wobei wir in der letzten Ungleichung die Bedingung y < z ausgenutzt haben.
ÂŠ 2003 by S. Jukna

4.5. STOCHASTISCHE ENTSCHEIDUNGSPROZESSE

_

y/2n

+

y/2n

+

(zây)/2n

+

(zây)/2n

r=y

+

(nâz)/2n

r=z

_

(nâz)/2n

r=y

x<y

177

r=z

r=y
y<x<z

r=z

x>z

Abbildung 4.3: + = wir gewinnen, â = wir verlieren

4.5.1 Das âMonty Hall Problemâ
In einer Game Show (z.B. âGehe auf Ganzeâ) ist hinter einer von drei TĂźren ein Hauptpreis (rein
zufĂ¤llig) verborgen.
Ein Zuschauer rĂ¤t eine der drei TĂźren und der Showmaster Monty Hall wird daraufhin eine weitere
TĂźr Ăśffnen, hinter der sich aber kein Hauptpreis verbirgt. Der Zuschauer erhĂ¤lt jetzt die MĂśglichkeit,
seine Wahl zu Ă¤ndern. Sollte er dies tun?
Wir mĂźssen zuerst das Problem genauer beschreiben. Wir nehmen an, dass die folgende drei Bedingungen erfĂźllt sind:

1. Der Hauptpreis ist mit gleicher Wahrscheinlichkeit 1/3 hinter jeder der drei TĂźren verborgen. Der
Schaumaster weiĂ, wo der Preis ist, der Zuschauer weiĂ es natĂźrlich nicht.
2. Egal wo der Haupreis ist, wĂ¤hlt der Zuschauer eine der drei TĂźren mit gleicher Wahrscheinlichkeit
1/3.
3. Egal wo der Haupreis ist, Ăśffnet der Schaumaster jede der mĂśglichen TĂźren (d.h. eine TĂźr hinter
der kein Preis ist) mit gleicher Wahrscheinlichkeit. Also ist diese Wahrscheinlichkeit 1/2, falls der
Zuschauer die TĂźr mit Hauptpreis gewĂ¤hlt hat, und ist 1 sonst.

Wir betrachten zwei Ereignisse: R = âZuschauer wĂ¤hlt die richtige TĂźr (die mit dem Preis)â und W =
âZuschauer gewinnt, wenn er die TĂźr stets wechseltâ.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

178
Tuer
richtig/falsch

Gewinnt wenn
wechselt / nicht wechselt
0

1/3
1

1
2/3

0

Dann gilt:


1
2
2
Pr {W } = Pr {R} Âˇ Pr {W | R } + Pr R Âˇ Pr W R = Âˇ 0 + Âˇ 1 =
3
3
3

1
und Pr W = 1 â Pr {W } = 3 . Der Zuschauer sollte seine Wahl stets Ă¤ndern!

Zu dem selben Ergebnis kann man auch einfach kommen, wenn man die âDrei-Schritt-Methodeâ
anwendet. In unserem Fall besteht âŚ aus 9 Elementarereignissen Ď = (i, j) mit i, j â {1, 2, 3}. Hier i
ist die von dem Zuschauer gewĂ¤hlte TĂźr und j ist die TĂźr mit dem Preis. Die Wahrscheinlichkeiten
sind Pr {Ď} = 1/9 fĂźr alle Ď â âŚ. FĂźr uns von Interesse ist das Ereignis W = {(i, j) : i , j}
(Zuschauer gewinnt, wenn er die TĂźr stets wechselt) und Pr {W } = 6/9 = 2/3.

4.5.2 Stichproben
Wir haben einen (potentiell unendlichen) Datenstrom x 1 , x 2 , . . ., wobei alle Elemente x i verschieden
sind. Die Elementen des Datenstroms kommen zu uns ein nacheinaderem, und verchwinden dann fĂźr
immer. In jedem Zeitpunkt passen in unsem Speicher nur ein der Elemente.
Ein Representant des Datenstroms ist ein zufĂ¤llig gewĂ¤hltes Element x mit der Eigenschaft, dass
Pr {x = x i } = 1/n fĂźr jedes n > 1 und fĂźr jedes Element x i mit i â {1, . . . , n} gilt. D.h. x muss in jedem
Interval x 1 , . . . , x n gleichverteilt sein. Dieses Problem kann man mit dem folgenden Algorithmus
(bekannt als Reservoir Sampling) lĂśssen.
- FĂźr n = 1 setze x = x 1
- FĂźr n > 2 werfe eine MĂźnze mit Erfolgswahrscheinlichkeit 1/n. Bei einem Erfolg setze x = x n .
Bei einem Misserfolg wird nichts unternommen.
Satz 4.33. FĂźr jedes n > 1 und fĂźr jedes 1 6 i 6 n gilt: Pr {x = x i } = 1/n.
Beweis.

Pr {x = x i } = Pr x i ist in i-ten Schritt gewĂ¤hlt Ă

ĂPr keiner von x i+1 , . . . , x n ist spĂ¤ter gewĂ¤hlt

n 
1 Y
1
=
Âˇ
1â
i
j
j=i+1

=

1
i
i+1 i+2
mâ2 mâ1 1
Âˇ
Âˇ
Âˇ
ÂˇÂˇÂˇ
Âˇ
= .
i i+1 i+2 i+3
mâ1
m
n
ÂŠ 2003 by S. Jukna

4.5. STOCHASTISCHE ENTSCHEIDUNGSPROZESSE

179


Nun wollen wir nicht nur einen Representanten sondern eine Menge der Representanten (eine Stichâ1
fĂźr jedes
probe) S â {x 1 , x 2 , . . .} mit |S| = s Elementen auswĂ¤hlen. Es muss Pr {S = T } = ns
n > 1 und fĂźr jede s-elementige Teilmenge T â {x 1 , . . . , x n } gelten. Hier nehmen wir an, dass unser
Speicher bis zu s Elementen aufnehmen kann. Um das Problem zu lĂśssen, reicht es den ReservoirSampling Algorithmus fĂźr s = 1 nur leicht zu modifizieren.
- Wenn n 6 s, dann fĂźge das Element x n in S ein.
- FĂźr n > s werfe eine MĂźnze mit Erfolgswahrscheinlichkeit s/n. Bei einem Erfolg bestimme
zufĂ¤llig einen Element aus S und entferne das Element; das aktuelle Element x n wird eingefĂźgt.
Bei einem Misserfolg wird nichts unternommen.
Satz 4.34. FĂźr jede n > s und fĂźr jede s-elementige Teilmenge T â {x 1 , . . . , x n } gilt:
 â1
n
.
Pr {S = T } =
s
Beweis. Sei Sn die zum Zeitpunkt n vom Algorithmus gewĂ¤hlte s-elementige Stichprobe. Wir wissen,
dass Pr {x n â Sn } = s/n fĂźr jedes n > s gilt.

Wir fĂźhren Induktion nach n. Induktionsbasis n = s gilt, da dann T = {x 1 , . . . , x s } die einzige sâ1
elementige Teilmenge ist und deshalb Pr {Ss = T } = 1 = ss
gilt.
Induktionsschritt n â 1 â n: Sei T â {x 1 , . . . , x n } eine beliebige s-elementige Teilmenge. Wir haben
nur zwei MĂśglichkeiten: entweder T enthĂ¤lt das Element x n oder nicht.
x n < T â Pr {Sn = T } = Pr {x n < Sn } Âˇ Pr {Snâ1



 â1

s  n â 1 â1
n
= T} = 1 â
=
n
s
s



 â1
s n â 1 â1
n
x n â T â Pr {Sn = T } = Pr {x n â Sn } Âˇ Pr {Snâ1 = T \ {x n }} =
=
.
n sâ1
s
 nâ1 n
 nâ1
n
n
Hier haben wir die Gleichhungen ns
= nâs
benutzt.
sâ1 = s und s
s



4.5.3 Das âSekretĂ¤rinnen-Problemâ an der BĂśrse
Wie wĂ¤hlt man unter 10 SekretĂ¤rinnen die beste aus, wenn wĂ¤hrend des BewerbungsgesprĂ¤ches die
Zusage erteilt werden soll? Mit diesem âSektretĂ¤rinnen-Problemâ wird in der Literatur die folgende
Aufgabenstellung veranschaulicht: Unter n aufeinanderfolgenden âGelegenheitenâ, fĂźr die noch keine
Rangfolge bekannt ist, soll die beste ausgewĂ¤hlt werden, indem sie geprĂźft und sofort zugegriffen
wird, andernfalls ist sie fĂźr immer verpasst.
In manchen LehrbĂźchern findet man dafĂźr die Bezeichnung: âVermĂ¤hlungs-Problemâ. Wie wĂ¤hlt eine
Frau am effizientesten unter allen ihren Bekannten einen Mann fĂźr das Leben? Dabei entscheidet sie
bei jedem Mann, ob er ihr Traummann ist oder nicht; wenn sie ihn abgelehnt hat, kann sie spĂ¤ter nicht
mehr auf ihn zurĂźckgreifen.
ÂŠ 2003 by S. Jukna

180

KAPITEL 4. DISKRETE STOCHASTIK

Die allgemeine Strategie lautet: Teste eine gewisse Anzahl von MĂśglichkeiten und triff deine Wahl
aufgrund der Testergebnisse. In unserem Beispiel wird also ein Teil der mĂ¤nnlichen Bekannten einer
Testprozedur unterzogen und dann trifft die Frau ihre Entscheidung.
Zwei Dinge sind klar:

-

Sie sollte nicht den ersten Mann nehmen, denn wer weiĂ, was noch kommt. Mit anderen Worten:
Sehr wahrscheinlich ist der âErstbesteâ nicht der beste.

-

Andererseits sollte sie auch nicht zu lange warten, denn dann hat sie mit groĂer Wahrscheinlichkeit
den besten abgelehnt und muss sich also mit einem Mann minderer QualitĂ¤t begnĂźgen.

Daraus ergibt sich folgende Strategie: Sie testet eine gewisse Anzahl von MĂ¤nnern mit Hilfe eines
Verfahrens, Ăźber das sie uns nichts zu verraten braucht - und nimmt keinen von diesen! Danach fĂźhrt
sie ihr Testverfahren weiter und nimmt dann den ersten, der besser als alle bisherigen ist.
Die Frage ist nur, wie viele MĂ¤nner sich ohne Aussicht auf Erfolg dem Testverfahren unterwerfen
mĂźssen. Man kann beweisen (und wir werden das bald tun), dass die Frau 37% ihrer in Frage kommenden Bekannten testen soll. Genauer gesagt soll sie einen Bruchteil von 1/e testen, wobei e = 2, 718...
die Eulersche Zahl ist.
Interessanterweise ist der Prozentsatz unabhĂ¤ngig von der Zahl der TestmĂ¤nner: Egal, ob sie 10 oder
1000 Heiratskandidaten ernsthaft in ErwĂ¤gung zieht, stets ist die beste Strategie, zunĂ¤chst 37% auszuprobieren und diese zu verwerfen.
Dabei ergeben sich - mindestens aus mĂ¤nnlicher Sicht - verschiedene Fragen: Was ist, wenn ich, der
Idealmann, unter den ersten 37% und damit von vornherein ausgeschlossen bin? Und was ist, wenn
ich, der beste, erst am Ende getestet werde, und also gar nicht zum Zuge komme? Ist das nicht ein
total unfaires Verfahren?
Nein! Dieses Verfahren ist ganz gut! Denn mit einer Wahrscheinlichkeit von immerhin 1/e = 37%
findet Frau mit dieser Strategie tatsĂ¤chlich den Mann ihrer TrĂ¤ume! Beachte, dass mit einer triviallen
Startegie (wĂ¤hle aus n bekannten MĂ¤nner den Traummann rein zufĂ¤llig einen mit Wahrscheinlichkeit
1/n) wĂźrde bereits bei n = 4 Heiratskandidaten die Frau mit viel kleiner Wahrscheinlichkeit glĂźcklich
sein.
Aber nicht nur Heiratswillige, sondern auch AktionĂ¤re interessieren sich fĂźr die LĂśsung dieses Problems. Die LĂśsungsstrategie zum SekretĂ¤rinnenproblem wird beim Aktienhandel angewandt, wenn
der Kurs einer Aktie stĂ¤ndig schwankt und nicht vorhersagbar ist. Wenn man innerhalb von einem
Monat entsprechende Aktien verkaufen mĂśchte, wie kann man mit dieser Strategie den gĂźnstigsten
Verkaufstag finden?
Um die LĂśsung dieses Problems zu demonstrieren,5 nehmen wir an, dass die Kurse am keinen zwei
Tagen gleich sind. Dann gilt der folgende Satz:

5Ist der Wahrscheinlichkeitsraum endlich, so ist die ganze Stochastik nichts anderes als ein Teil der Kombinatorik. In
dieser (endlichen) Form war eigentlich die Stochastik geboren. Das Ziel dieses Abschnitts ist zu zeigen, wie man mit Hilfe
von (relativ einfachen) kombinatorischen Ăberlegungen einige nicht triviale SchlĂźssfolgerungen ziehen kann.
ÂŠ 2003 by S. Jukna

4.5. STOCHASTISCHE ENTSCHEIDUNGSPROZESSE

181

Satz 4.35. Wenn die Anzahl der Handelstage n groĂ ist, dann sollten die Aktienkurse der ersten
j = n/e (knapp 37%) Tage lediglich notiert sein und dann die nĂ¤chste bessere Gelegenheit ausgewĂ¤hlt
werden. Die Wahrscheinlichkeit P( j), dann den gĂźnstigsten Verkaufstag zu wĂ¤hlen, betrĂ¤gt


j
1
1
P( j) = Âˇ
+...+
n
j
nâ1
FĂźr j = n/e ist P( j) â 1/e = 0, 367.
Zum Beispiel fĂźr n = 20 (Handelstage) ist die optimale Stoppzahl j = 7. Die Wahrscheinlichkeit,
dann den gĂźnstigsten Verkaufstag zu wĂ¤hlen, betrĂ¤gt P(7) = 7/20(1/7 + ... + 1/19) = 0, 384... FĂźr
n = 100 ist die Stoppzahl j = 37 und die Erfolgswahrscheinlichkeit P( j) rund 37/100 betrĂ¤gt. FĂźr
n = 1000 ergibt sich die optimale Stoppzahl j = 368 und P( j) = 368/1000.
Beweis. Eine LĂśsungsstrategie zum BĂśrse-Problem (wie auch zum Heirats- oder SekretĂ¤rinen Problem) ist sehr kurz:
j-te Stoppstrategie: Bei n > 1 vorgegebenen Tagen verhalte auf folgende Weise: An den ersten j
Tagen ( j â {1, . . . , n â 1}) wird lediglich der Kurs beobachtet (und notiert). Sobald der Kurs
an einem der nachfolgenden Tage k > j hĂśher ist als das Maximum der j beobachteten Kurse,
werden die Aktien verkauft.
Es ist klar, dass die Wahrscheinlichkeit, den bestmĂśglichen Verkaufstag zu wĂ¤hlen, bei gegebenem
n von der Stoppzahl j abhĂ¤ngt. Deswegen bezeichnen wird diese Wahrscheinlichkeit mit P( j) und
berechnen sie wie folgt:
FĂźr k â { j + 1, . . . , n} betrachten wir das Ereignis
Ak = âder k-te Tag Tk ist der beste und Tk wird ausgewĂ¤hlt.â
Die Wahrscheinlichkeit, dass der k-te Tag Tk der beste ist, betrĂ¤gt 1/n, da wir n Tage haben und jeder
davon kĂśnnte der beste sein.
Behauptung 4.36.
Pr { Ak } =

j
1
Âˇ
n k â1

(â)

Wir werden diese Behauptung erst spĂ¤ter ĂźberprĂźfen (der Beweis ist rein kombinatorisch). An dieser
Stelle nehmen wir an, dass die Behauptung 4.36 gilt, und fĂźhren den Beweis weiter.
Die j-te Stoppstrategie ist genau dann erfolgreich, wenn dass Ereignis A j+1 âŞ A j+2 âŞ Âˇ Âˇ Âˇ âŞ An eintritt.
Weil fĂźr alle r, s â { j + 1, . . . , n} mit r , s folgt 6 Ar âŠ As = â, erhalten wir:



P( j) = Pr j-te Stoppstrategie ist erfolgreich = Pr A j+1 + Pr A j+2 + . . . + Pr { An }


j
1
1
=
Âˇ
+...+
n
j
nâ1
6Da nur ein Tag ausgewĂ¤hlt sein kann.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

182

Um das
j zu finden, mĂźssen wir die Funktion P( j) maximieren. Da die harmonische Reihe
Poptimale
n 1
Hn = i=1
asymptotisch
gleich ln n ist, erhalten wir:
i
P( j) =
Die Funktion f (x) = x ln

1
x

 j n
j
Hnâ1 â H jâ1 âź ln .
n
n j

sieht folgendermaĂen aus:

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0.2

0.4

0.6

0.8

1

x

und erhĂ¤lt ihr Maximum fĂźr x = 1/e: Die erste Ableitung f â˛ (x) = ln(1/x) â 1 ist in diesem Punkt
gleich Null, und die zweite Ableitung f â˛â˛ (x) = â(1/x) ist fĂźr x = 1/e negativ.

Es bleibt uns, die Behauptung 4.36 zu beweisen. Und hier kommt die Kombinatorik ins Spiel. Wir
mĂźssen die Wahrscheinlichkeit Pr { Ak } des Ereignisses Ak bestimmen. Dazu benutzen wir unsere
âDrei-Schritte-Methodeâ.
1. Finde den Wahrscheinlichkeitsraum: Dazu beobachten wir, dass fĂźr uns die tatsĂ¤chliche KursWerte in einem Aktienkursverlauf irrelevant sind â wichtig ist nur die reliative Gute dieser Werte:
Uns intereesiert nur, ob am einem Tag der Kurs besser oder schlechter als am einem anderen ist.
Damit kĂśnnen wir7 jeden Kursverlauf mit Hilfe eines Urnenmodels betrachten. Wir stellen n Urnen
U1 , . . . ,Un in einer Reihe auf. Dann nehmen wir n durchnummerierte BĂ¤lle 1, 2, . . . , n, die den n
Handelstagen entsprechen, und verteilen die BĂ¤lle (= Tage) in Urnen, so dass jede Urne genau einen
Ball enthĂ¤lt. (Eine Verteilung der BĂ¤ller ist also nichts anderes als eine Permutation der Handlstage
1, . . . , n.) Eine solche verteilung der BĂ¤ller entspricht der Anordnung der Handelstage nach ihrem
Kurswert. Der Tag in Urne 1 war der schlechteste, der Tag in U2 war schon besser, der Tag in U2 war
noch besser, usw.
Damit besteht unser Wahrscheinlichkeitsraum âŚ aus allen |âŚ| = n! mĂśglichen Verteilungen der n
Tage in n Urnen.
2. Bestimme das Ereignis: Welche Elementarereignisse âŚ gehĂśren zu dem Ereignis Ak ? Das Ereignis
Ak besteht aus zwei Ereignissen:
7Dieser Schritt â mathematische Modellierung des Problems â ist sehr wichtig in allen Anwendungen!
ÂŠ 2003 by S. Jukna

4.5. STOCHASTISCHE ENTSCHEIDUNGSPROZESSE

1

2

3

4

5

6

7

8

183

9 10 11 12

Abbildung 4.4: Aktienkurverlauf in zwei Wochen. Die entsprechende Verteilung der 12 BĂ¤lle (= Tage)
fĂźr diesen Verlauf ist: 7, 2, 10, 4, 12, 3, 6, 11, 5, 1, 9, 8. Tag 8 ist der beste Tag.

1. Zuerst muss der k-te Tag Tk der beste sein. Dies bedeutet, dass der Ball k muss in der letzten Urne
Un sitzen.
2. Dann muss noch der Tag Tk auch ausgewĂ¤hlt sein. Dies bedeutet folgendes: Wenn wir nur die
Urnen anschauen, die die ersten k â 1 BĂ¤lle 1, . . . , k â 1 enthalten, dann muss die letzte von diesen
Urnen einen der ersten j BĂ¤lle s â {1, . . . , j} enthalten.8
3. Bestimme die Wahrscheinlichkeit des Ereignis: Man kann die zum Ereignis Ak gehĂśrende Verteilungen der BĂ¤lle wie folgt erzeugen.
1. Werfe zuerst den Ball k in die letzte Urne Un â nur eine MĂśglichkeit.
2. FĂźr jedes s = 1, 2, . . . , j tue folgendes:
(a) Werfe die BĂ¤lle k + 1, k + 2, . . . , n in die Urnen U1 , . . . ,Unâ1 (die Urne Un ist ja bereits besetzt)
m!
â P(n â 1, n â k) MĂśglichkeiten, wobei P(m, r) = m(m â 1) . . . (m â r + 1) = (mâr
)! die
Anzahl der r-Permutationen von {1, . . . , m} ist
(b) Werfe den Ball s in die letzte von k â 1 noch freien Urnen (nur eine MĂśglichkeit), und verteile
anschlieĂend die verbleibende k â 2 BĂ¤lle in k â 2 noch verbleibende freie Urnen â (k â 2)!
MĂśglichkeiten.
Nach der Produkregel gilt somit:
| Ak | =
=

j Âˇ P(n â 1, n â k) Âˇ (k â 2)!
(n â 1)!
jÂˇ
Âˇ (k â 2)!
((n â 1) â (n â k))!
|
{z
}
= (kâ1)!

=
=

(n â 1)!
jÂˇ
Âˇ (k â 2)!
(k â 1)!
j (n â 1)!
.
k â1

8Der beste der ersten k â 1 Tage muss ein der ersten j Tage gewesen sein: Nach dem j-ten Tag wĂ¤hlt die j-te Stoppstrategie den ersten(!) Tag, der besser als alle Tage 1, . . . , j ist.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

184

Da wir insgesamt |âŚ| = n! Verteilungen der BĂ¤lle haben, ist die Wahrscheinlichkeit
Pr { Ak } =

| Ak |
j Âˇ (n â 1)! 1
j
=
= Âˇ
.
|âŚ|
(k â 1) Âˇ n! n k â 1

Somit ist die Behauptung (â) und damit auch der Satz 4.35 bewiesen.



4.6 Zufallsvariablen
âThe Holy Roman Empire was neither holy nor Roman, nor an Empireâ
âVoltaire
Genauso sind âZufallsvariablenâ: Sie sind weder zufĂ¤llig noch Variablen:
Eine Zufallsvariable ist eine auf dem Wahrscheinlichkeitsraum definierte Funktion.
Die Funktion selbst kann beliebige Werte annehmen, aber normalerweise werden die Zufallsvariablen
als Funktionen X : âŚ â S mit S â R betrachtet.
â˛ Beispiel 4.37 : Wir wĂźrfeln zweimal einen SpielwĂźrfel und sind in der Augensumme interessiert.
Der Wahrscheinlichkeitsraum ist âŚ = {(i, j) : 1 6 i, j 6 6}, und die entsprechende Zufallsvariable ist mit X (i, j) = i + j gegeben; der Bildbereich von X ist in diesem Fall S = {2, 3, . . . , 12}.
Sei X : âŚ â S eine Zufallsvariable. Die wichtigste Frage, die wir betrachten werden ist: FĂźr ein
gegebenes Element a â S, was ist die Wahrscheinlichkeit, dass X den Wert a annimmt? In anderen
Worten was ist die Wahrscheinlichkeit fĂźr das Ereignis
A = {Ď â âŚ : X (Ď) = a}
Weil solche Ereignisse sehr oft auftreten, benutzt man die AbkĂźrzung
Pr {X = a}
fĂźr
Pr {{Ď â âŚ : X (Ď) = a}} .
Die Verteilung einer Zufallsvariablen X : âŚ â S ist eine durch f (a) := Pr {X = a} definierte Abbildung f : S â [0, 1].
â˛ Beispiel 4.38 : Wir werfen dreimal eine MĂźnze und sei X die Anzahl der AusgĂ¤nge âWappenâ. Die
mĂśgliche Werte von X sind S = {0, 1, 2, 3} und die Verteilung sieht folgendermaĂen aus:
a

0

1

2

3

Pr {X = a}

1
8

3
8

3
8

1
8

ÂŠ 2003 by S. Jukna

4.6. ZUFALLSVARIABLEN

185



Beachte, dass verschiedene Zufallsvariablen dieselbe Verteilung haben kĂśnnen. Wenn wir z.B.
den obigen Beispiel betrachten und die Anzahl der AugĂ¤nge âKopfâ mit Y bezeichnen, dann
sind die Zufallsvariablen X und Y verschieden (da Y = 3 â X gilt) aber die Verteilungen von Y und X
sind gleich.
Zur Sprechweise: Wir sagen, dass X1 , . . . , X n unabhĂ¤ngige Kopieen einer Zufallsvariable X sind, falls
die Zufallsvariablen X i die gleiche Verteilung wie X haben.
Die einfachsten (und deshalb die wichtigsten) Zufallsvariablen sind Bernoulli-Variablen. Jede solche
Zufallsvariable X kann nur zwei mĂśgliche Werte 0 und 1 annehmen; p = Pr {X = 1} heiĂt dann die Erfolgswahrscheinlichkeit. Beispiel: Einmaliges werfen einer MĂźnze, wobei der Ausgang âWappenâ mit
Wahrscheinlichkeit p kommt. Das entsprechende Zufallsexperiment nennt man Bernoulli-Experiment.
Definition: Die Indikatorvariable fĂźr ein Ereignis A â âŚ ist die Zufallsvariable X A : âŚ â {0, 1} mit

1
falls Ď â A
X A (Ď) =
0
falls Ď < A
Beachte, dass eine Indikatorvariable X A eine Bernoulli-Variable mit der Erfolgswahrscheinlichkeit
Pr {X A = 1} = Pr { A} ist. Somit kann man die Ereignisse als einen Speziallfal der Zufallsvariablen â
nĂ¤hmlich als 0-1-wertige Zufallsvariablen â betrachten.
Den Begriff der UnabhĂ¤ngigkeit kann man auch auf Zufallsvariablen Ăźbertragen.
Zwei Zufallsvariablen X : âŚ â S und Y : âŚ â T sind unabhĂ¤ngig, falls fĂźr jede s â S und t â T mit
Pr {Y = t} , 0 gilt
Pr {X = s | Y = t } = Pr {X = s}
oder Ă¤quivalent
Pr {X = s â§ Y = t} = Pr {X = s} Âˇ Pr {Y = t} .
â˛ Beispiel 4.39 : Wir wĂźrfeln zwei (faire) SpielwĂźrfel. Wir kĂśnnen die entsprechende Augenzahlen
als zwei Zufallsvariablen X1 und X2 betrachten. Zum Beispiel ist das Elemetarereignis Ď = (3, 5),
dann ist X1 (Ď) = 3 und X2 (Ď) = 5.
Betrachten wir nun Y = X1 + X2 . Dann ist Y auch eine Zufallsvariable, denn Y weist jedem
Elementarereignis Ď eine reelle Zahl Y (Ď) = X1 (Ď) + X2 (Ď) zu. Sei auch

1 falls Y = 7
I :=
0 falls Y , 7
Dann sind Y und X1 abhĂ¤ngig, da zum Beispiel
1
= Pr {Y = 2} .
36
Intuitiv, sollten deshalb auch I und X1 abhĂ¤ngig sein: Der Wert von I hĂ¤ngt doch davon ab,
welchen Wert die Zufallsvariable Y annimmt, und Y hĂ¤ngt doch von X1 ab.
Pr {Y = 2 | X1 = 3 } = 0 ,

Ăberraschenderweise sind I und X1 unabhĂ¤ngig!
Zu zeigen, dass zwei Zufallsvariablen abhĂ¤ngig sind, ist oft viel leichter (es reicht ein Gegenbeispiel, wie oben). UnabhĂ¤ngigkeit braucht mehr Arbeit: Man muss zeigen, dass fĂźr alle x, y â R
mit Pr {X1 = x} , 0 gilt:
Pr {I = y | X1 = x } = Pr {I = y}
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

186

Zu betrachten sind also 12 FĂ¤lle: y â {0, 1} und x â {1, 2, 3, 4, 5, 6}. Zwei Beobachtungen machen
uns das Leben einfacher:
(a) Pr {I = 1} = 6/36 = 1/6, da I (Ď) = 1 genau dann, wenn eines der 6 unabhĂ¤ngigen Ereignisse
Ď â {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)} eintrit.
(b) Pr {I = 1 | X1 = x } = 1/6 fĂźr alle x â {1, 2, 3, 4, 5, 6}, da nachdem der erste WĂźrfel x Augen
gezeigt hat, dass Ereignis âI = 1â nur dann mĂśglich ist, wenn der zweite WĂźrfel 7 â x Augen
zeigen wird.
Damit haben wir
Pr {I = 1 | X1 = x } =

1
= Pr {I = 1}
6

fĂźr alle x = 1, 2, 3, 4, 5, 6

5
= Pr {I = 0}
6

fĂźr alle x = 1, 2, 3, 4, 5, 6

Behauptung 4.7 sagt, dass dann auch
Pr {I = 0 | X1 = x } =
Also sind I und X1 unabhĂ¤ngig!
Die Zufallsvariablen X1 , . . . , X k : âŚ â S mit S â R heiĂen total unabhĂ¤ngig, falls fĂźr alle a1 , . . . , ak â
S die Gleichung
Pr {X1 = a1 und X2 = a2 und . . . und X k = ak } =

k
Y
i=1

Pr {X i = ai }

gilt.
â˛ Beispiel 4.40 : Seien die Zufallsvariablen X1 , . . . , X k : âŚ â S mit S â R total unabhĂ¤ngig. Wie
groĂ kann dann k sein? Um diese Frage zu beantworten, nehmen wir an, dass die Zufallsvariablen
nicht trivial sind, d.h. keine der Variablen X i konstant ist. Dann kann jede der Variablen X i mindestens zwei verschiedene Werte ai , bi â S annehmen (d.h. Pr {X i = ai } > 0 und Pr {X i = bi } > 0
gilt). Wir haben mindestens 2k mĂśglichkeiten, einen Vektor c = (c1 , . . . , ck ) mit ci â {ai , bi } auszuwĂ¤hlen, und fĂźr jede solche Auswahl
hat das Ereignis Ac := {Ď â âŚ : X1 = c1 , . . . , x k = ck }
Q
die Wahrscheinlichkeit Pr { Ac } = ki=1 Pr {X i = ci } > 0. Somit sind die Eriegnisse Ac nicht leer.
Wir haben also mindestens 2k disjunkten, nicht leeren Teilmengen Ac in âŚ gefunden. Somit muss
auch die Ungleichung 2k 6 n := |âŚ| gelten.



Fazit: In einem Wahrscheinlichkeitsraum âŚ der GrĂśĂe n = |âŚ| kann es hĂśchstens log2 n total
unabhĂ¤ngigen nicht trivialen Zufallsvariablen geben.

4.7 Erwartungswert und Varianz
Hat man eine Zufallsvariable X : âŚ â S, so will man die Wahrscheinlichkeiten Pr {X â R} fĂźr
verschiedene Teilmengen R â S bestimmen (oder mindestens gut abschĂ¤tzen). Dazu haben sich zwei
nummersiche Charakteristiken der Zufallsvaiablen â Erwartungswert und Varianz â als sehr hilfreich
erwiesen.
ÂŠ 2003 by S. Jukna

4.7. ERWARTUNGSWERT UND VARIANZ

187

Sei X eine Zufallsvariable, die die Werte a1 , . . . , an annimmt.
Definition: Der Erwartungswert E [X ] von X ist definiert durch:
E [X ] :=

n
X
i=1

ai Âˇ Pr {X = ai }

D.h. wir multiplizieren die Werte, die X annehmen kann, mit der entsprechenden Wahrscheinlichkeiten, und summieren die Terme auf. Der Ervartungswert ist also ein âverallgemeinerter Durchschnittswertâ: Falls X jeden Wert ai mit gleicher Wahrscheinlichkeit 1/n annimmt, so ist
E [X ] =

a1 + . . . + a n
.
n

Man kann den Erwartungswert auch rein mechanisch interpretieren. Wenn wir n Objekte mit Gewichten pi = Pr {X = ai } auf der x-Ache in der Positionen ai (i = 1, . . . , n) ablegen, dann wird der
MaĂ-Zentrum genau an der Stelle E [X ] sein.

1

2

3

4

6

7

8

â˛ Beispiel 4.41 : Wir wĂźrfeln zwei (nummerierte) SpielwĂźrfeln und Y sei die Summe der Augenzahlen. Dann ist
E [Y ] =

12
X
k=2

=


k Âˇ Pr Summe der beiden Augenzahlen ist k

12 X
X

k=2 i+ j=k


k Âˇ Pr erster WĂźrfel hat i Augen und der zweite hat j

1
2
3
3
2
1
+ 3 Âˇ 2 + 4 Âˇ 2 + ... + 10 Âˇ 2 + 11 Âˇ 2 + 12 Âˇ 2
62
6
6
6
6
6
252
=7
36

= 2Âˇ
=

Falls die Zufallsvariable unendlich viele Werte a1 , a2 , . . . annehmen kann, dann ist der Erwartungswert
als
n
â
X
X
E [X ] := lim
ai Pr {X = ai } =
ai Pr {X = ai }
nââ

i=1

i=1

definiert. NatĂźrlich mĂźssen wir dann uns darum kĂźmmern, ob der Grenzwert Ăźberhaupt existiert, d.h.
ob die Reihe Ăźberhaupt konvergiert! Sind alle Zahlen ai nicht negativ, so ist die Reihe monoton wachsend und (nach Monotonie-Kriterium, Satz 3.46(1)) kovergiert die Reihe genau dann, wenn sie beschrĂ¤nkt ist.9
9Eine Reihe

Pâ

k=0 ak

ist beschrĂ¤nkt, falls es eine Zahl L mit

Pn

k=0 ak

6 L fĂźr alle n gilt.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

188


â˛ Beispiel 4.42 : Sei X eine Zufallsvariable mit der Verteilung Pr X = 2i
1, 2, . . .. Das ist eine legale Verteilung, da

= 1/2i fĂźr alle i =

â
â
X
1
1 X 1
=
â1=
â1=1
i
i
2
2
1 â (1/2)
i=1

i=0

gilt. Aber der Erwartungswert ist nicht definiert:
E [X ] =

â
â
X
1 i X
2
=
1 = â.
2i
i=1

i=1

In diesem Vorlesung werden wir meist nur die FĂ¤lle treffen, wenn
sche (und damit auch konvergente) Reihe ist.

Pâ

i=1

ai Pr {X = ai } eine geometri-

Definition: Die Varianz Var [X ] einer Zufallsvariable X ist definiert durch:


Var [X ] = E (X â E [X ]) 2


Was bedeutet eigentlich der Ausdruck E (X â E [X ]) 2 ? Der Ausdruck X â E [X ] ist genau die
Abweichung der Zufallsvariable X vom seinen Erwartungswert. Dann ist die Zufallsvariable Y =
(X â E [X ]) 2 nah an 0, wenn X nah an E [X ] ist, und ist eine groĂe Zahl, wenn X weit links oder
rechts von E [X ] liegt. Die Varianz ist einfach der Durchschnitt dieser Zufallsvariable. Damit ist die
Intuition, die dahinter steckt, geklĂ¤rt:
-

Wenn X immer nahe an E [X ] liegt, dann ist Var [X ] klein.

-

Wenn X oft weit von E [X ] entfernt liegt, dann ist Var [X ] groĂ.



Bemerkung 4.43. Die Definition der Varianz E (X â E [X ]) 2 als ein Quadrat von der Abweichung
vom Erwartungswert sieht irgendwie kĂźnstlich aus. Warum kann man nicht einfach E [X â E [X ]]
nehmen? Antwort: E [X â E [X ]] = E [X ] â E [E [X ]] = E [X ] â E [X ] = 0. Also hĂ¤tte dann jede
Zufallsvariable die Varianz 0. Nicht sehr nĂźtzlich!
NatĂźrlich kĂśnnte man die Varianz als E [|X â E [X ] |] definieren. Es spricht nichts dagegen. Trotzdem
hat die Ăźbliche Definition von Var [X ] einige mathematische Eigenschaften, die E [|X â E [X ] |] nicht
hat.
â˛ Beispiel 4.44 : Um die Relevanz der Varianz zu demonstrieren, betrachten wir die zwei folgenden
Casino-Spiele.
Spiel A: Wir gewinnen 2 e mit Wahrscheinlichkeit 2/3 und verlieren 1 e mit Wahrscheinlichkeit 1/3.
Spiel B: Wir gewinnen 1002 e mit Wahrscheinlichkeit 2/3 und verlieren 2001 e mit Wahrscheinlichkeit 1/3.
Welches Spiel ist fĂźr uns gĂźnstiger? In beiden FĂ¤llen ist unsere Gewinnwahrscheinlichkeit gleich
2/3. Seien A und B die Zufallsvariablen, die dem Gewinn in beiden Spielen entsprechen. Zum
ÂŠ 2003 by S. Jukna

4.7. ERWARTUNGSWERT UND VARIANZ

189

Beispiel ist A = 2 mit Wahrscheinlichkeit 2/3, und â1 mit Wahrscheinlichkeit 1/3. Dann sind die
Erwartungswerte beide gleich:
2
1
+ (â1) Âˇ = 1
3
3
2
1
E [B] = 1002 Âˇ + (â2001) Âˇ = 1.
3
3
E [A] = 2 Âˇ

Aber das sagt uns nicht die ganze Wahrheit: Intuitiv sieht Spiel B viel gefĂ¤hrlicher aus! Und
diesen Unterschied kĂśnnen wir mit Hilfe der Varianz belegen:

1 mit Wahrscheinlichkeit 2/3
A â E [A] =
â2 mit Wahrscheinlichkeit 1/3

1 mit Wahrscheinlichkeit 2/3
(A â E [A]) 2 =
4 mit Wahrscheinlichkeit 1/3


2
1
E (A â E [A]) 2 = 1 Âˇ + 4 Âˇ
3
3
Var [A] = 2
Andererseits, haben wir fĂźr das Spiel B:

B â E [B] =





1001
â2002

mit Wahrscheinlichkeit 2/3
mit Wahrscheinlichkeit 1/3

1.002.001 mit Wahrscheinlichkeit 2/3
4.008.004 mit Wahrscheinlichkeit 1/3


2
1
E (B â E [B]) 2 = 1.002.001 Âˇ + 4.008.004 Âˇ
3
3
Var [B] = 2.004.002
(B â E [B])

2

=

Damit ist die Varianz im Spiel A nur 2, wĂ¤hrend sie im Spiel B mehr als zwei Millionen ist! D.h.
im Spiel A sollte der Gewinn Ăźblicherweise nah an erwarteten Gewinn von 1 e sein, wĂ¤hrend er
im Spiel B sehr weit von E [B] = 1 entfernt liegen kann.
GroĂe Varianz verbindet man oft mit hohem Risiko. So zum Besipiel erwarten wir im Spiel A
in 10 Runden einen Gewinn von 10 e zu erzielen, aber auch einen Verlust von 10 e mĂźssen in
Kauf nehmen. Im Spiel B kĂśnnen wir auch erwarten, 10 e zu gewinnen, kĂśnnen aber mehr als
20.000 e verlieren!


Die Varianz E (X â E [X ]) 2 ist als ein Quadrat von der Abweichung vom Erwartungswert definiert.
Infolge dieser Quadrierung kann die Varianz sehr weit von der tatsĂ¤chlichen Abweichung entfernt
sein. Zum Beispiel im Spiel B oben is die Abweichung von E [B] gleich 1001 oder ist gleich â2002.
Aber die Varianz ist ganze kolossale 2.004.002(!) Ausserdem ist die Varianz nicht in Euro sondern in
âQuadrat-Euroâ gemessen. Um diese Effekte zu vermeiden, benutzt man oft anstatt der Varianz die
sogenannte Standardabweichung, die als
q 
p

Ď X :=
Var [X ] =
E (X â E [X ]) 2
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

190

definiert ist. Intuitiv misst Ď X die âerwartete (durchschnittliche) Abweichungâ von dem Erwartungswert E [X ]. Zum Beipiel ist die Standardabweichung fĂźr das Spiel B oben gleich
p
â
Ď B = Var [X ] = 2.004.002 â 1416.

Erwartungswert

0

1
Standardabweichung

Abbildung 4.5: Die Standardardabweichung von der W-Verteilung sagt wie breit ihre âHauptanteilâ
ist.
In der Tat weicht die Zufallsvariable B von E [B] entweder um 1001 oder um â2002 ab; also beschreibt
Ď B die âdurchschnittlicheâ Abweichung ziemlich gut (siehe Abbildung 4.5).

4.8 Analytische Berechnung von E [X] und Var [X]
Es gibt auch andere allgemeine Methode, die oft hilft, den Erwartungswert E [X ] und die Varianz
Var [X ] von kompliziert definierten Zufallsvariablen X zu berechnen.
Sei X eine diskrete Zufallsvariable mit der Verteilung pk := Pr {X = k} fĂźr alle k in dem Wertebereich
von X . Die erzeugende Funktion von X ist definiert durch
X
pk x k
FX (x) :=
wobei die Summe Ăźber alle k in dem Wertebereich von X ist. Dann gilt folgendes:
Satz 4.45.
(a) E [X ] = FXâ˛ (1)
(b) Var [X ] = FXâ˛â˛ (1) + E [X ] â E [X ]2
P
P
Beweis. Zu (a): FXâ˛ (x) =
k pk x kâ1 â FXâ˛ (1) =
k pk = E [X ].
P
â˛â˛
kâ2
Zu (b): FX (x) =
k (k â 1)pk x
â
X
X
X
 
FXâ˛â˛ (1) =
k (k â 1)pk =
k 2 pk â
k pk = E X 2 â E [X ] .
ÂŠ 2003 by S. Jukna

4.9. EIGENSCHAFTEN VON E [X ] UND VAR [X ]

191

Wenn wir also E [X ] dazuaddieren und E [X ]2 subtrahieren, kommt gerade die Varianz Var [X ] raus.


4.9 Eigenschaften von E [X] und Var [X]



Die allerwichtigste Eigenschaft des Erwartungswertes Ăźberhaupt ist seine âLinearitĂ¤tâ. Diese
Eigenschaft des Erwartungwertes ist sehr robust (und deshalb sehr wichtig): sie gilt fĂźr beliebige (nicht nur unabhĂ¤ngige) Zufallsvariablen!
Satz 4.46. (LinearitĂ¤t des Erwartungswertes) FĂźr beliebigen Zufallsvariablen X,Y gilt:
E [X + Y ] = E [X ] + E [Y ] .
Beweis. Sei a1 , . . . , an bzw. b1 , . . . , bm der Wertebereich von X bzw. Y . Da die Ereignisse Y = b j fĂźr
verschiedene b j disjunkt sind, gilt nach dem Satz von der totalen Wahscheinlichkeit
Pr {X = ai } =

m
X
j=1


Pr X = ai ,Y = b j


und eine analoge Formel fĂźr Pr Y = b j . Deshalb gilt:
E [X + Y ] =

=

n X
m
X
i=1 j=1
n X
m
X
i=1 j=1

=

n
X
i=1


(ai + b j )Pr X = ai ,Y = b j

m X
n
X


ai Pr X = ai ,Y = b j +
b j Pr X = ai ,Y = b j

ai Pr {X = ai } +

= E [X ] + E [Y ] .

j=1 i=1

m
X
j=1


b j Pr Y = b j


Als nĂ¤chstes schauen wir wie verhalten sich der Erwartungswert und die Varianz, wenn wir die Zufallsvariable durch eine Konstante multiplizieren oder zu einer Zufallsvariable eine Konstante addieren. Dazu bezeichnen wir mit C eine konstante Zufallsvariable, die nur einzigen Wert c â R annimt.10
Lemma 4.47. Sei X eine beliebige Zufallsvariable und C eine konstante Zufallsvariable, die den Wert
c annimmt. dann gilt:
(a) E [C] = c, Var [C] = 0
(b) E [cX ] = cE [X ], Var [cX ] = c2 Var [X ]
(c) E [X + c] = E [X ] + c, Var [X + c] = Var [X ]
10FĂźr diejenigen, die sich unbequem mit dem Begriff âkonstante Variableâ fĂźhlen, sei es erinnert, dass eine Zufallsvariable
X eigentlich keine âVariableâ sondern eine Funktion X : âŚ â R ist.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

192

Beweis. (a) Die Zufallsvariable C nimmt nur
Wert c mit Wahrscheinlichkeit Pr {C = c} = 1.
 einen

Also gilt E [C] = c Âˇ 1 = c und Var [C] = E C 2 â E [C]2 = c2 â c2 = 0.
Zu (b): Sind a1 , . . . , an die Werte von X , so sind ca1 , . . . , can die Werte von cX und
Pr {cX = cai } = Pr {X = ai }
gilt. Somit erhalten wir
E [cX ] =

n
X
i=1

cai Pr {X = ai } = c

n
X
i=1

ai Pr {X = ai } = cE [X ]

und



Var [cX ] = E (cX â E [cX ]) 2


= E c2 X 2 â 2cE [cX ] Âˇ X + E [cX ]2
 
= c2 E X 2 â 2c2 E [X ]2 + c2 E [X ]2
= c2 Var [X ] .

Zu (c): Die erste Gleichung E [X + c] = E [X ] + c folgt aus der linearitĂ¤t des Erwartungswertes. Die
zweite lassen wir als Ăbungsaufgabe.

Ăhnlich kann man die folgende nĂźtzliche Formel fĂźr die Berechnung der Varianz beweisen.
 
Satz 4.48. Var [X ] = E X 2 â E [X ]2
Beweis.


Var [X ] = E (X â E [X ]) 2


= E X 2 â 2E [X ] Âˇ X + E [X ]2
 
= E X 2 â 2E [X ]2 + E [X ]2
 
= E X 2 â E [X ]2 .



â˛ Beispiel 4.49 : Sei A ein Ereignis und
X A (Ď) =



1
0

falls Ď â A
falls Ď < A

sei seine Indikatorvariable. Dann gilt:

E [X A ] = Pr { A} und Var [X A ] = Pr { A} â Pr { A} 2 = Pr { A} Pr A


da E [X A ] = 1 Âˇ Pr { A} + 0 Âˇ Pr A und Pr X A2 = 1 = Pr {X A = 1} gilt.

(4.5)

ÂŠ 2003 by S. Jukna

4.9. EIGENSCHAFTEN VON E [X ] UND VAR [X ]

193

â˛ Beispiel 4.50 : (ZufĂ¤llige Teilmengen) Wir wĂ¤hlen rein zufĂ¤llig eine m-elementige Teilmenge A
aus einer n-elementigen Menge âŚ. D.h.

1
Pr Teilmenge A wird ausgewĂ¤hlt = n  .
m

Gegeben sei nun eine Teilmenge S â âŚ. Was kann man Ăźber die erwartete GrĂśĂe E [| A âŠ S|] des
Schnitts A âŠ S sagen?

n â1
Jede m-elementige Teilmenge A wird mit Wahrscheinlichkeit m
ausgewĂ¤hlt. Sei Ix, A die Indikatorvariable fĂźr das Ereignis âx â Aâ. Dann wird jedes Element x â âŚ mit Wahrscheinlichkeit


Pr Ix, A = 1 = Pr {x â A} =



  
nâ1 . n
m
=
n
mâ1
m

in der ausgewĂ¤hlten Teilmenge A liegen.11 Nach der LinearitĂ¤t des Erwartungswertes gilt somit:

E [| A âŠ S|] = E



"
X
x âS

#

Ix, A =

X 
X m m|S|
 X 
E Ix, A =
Pr Ix, A = 1 =
=
.
n
n
x âS

x âS

x âS

Sind die Zufallsvariablen X und Y nicht unabhĂ¤ngig, so gilt im Allgemeinen die Gleichung
Var [X + Y ] = Var [X ] + Var [Y ] nicht! Nimm zum Beispiel Y = âX . Dann gilt Var [X + Y ] =
Var [0] = 0 aber Var [X ] + Var [Y ] = Var [X ] + Var [âX ] = 2Var [X ]. Ein anderes Beispiel: Wir verfen
eine MĂźnze und seien X und Y Indikatorvariablen fĂźr die Ereignisse âes kommt Kopfâ und âes kommt
Wappenâ. Dann ist Var [X ] = Var [Y ] = 1/4 aber Var [X + Y ] = Var [1] = 0.



Im Allgemeinen ist auch die Produktregel
E [X Âˇ Y ] = E [X ] Âˇ E [Y ] falsch! Sei
 
 X
 auf {0, 1}
gleichwertig verteilt. Dann gilt: E X 2 = E [X ] = 1/2 =â E [X ]2 = 1/4 , E X 2 .

Die Produktregel gilt nur wenn die Zufallsvariablen unabhĂ¤ngig sind.

Satz 4.51. Sind X und Y unabhĂ¤ngige(!) Zufallsvariablen und a, b beliebige reelle Zahlen, so gilt
Var [X + Y ] = Var [X ] + Var [Y ]
und
E [X Âˇ Y ] = E [X ] Âˇ E [Y ]




 
2 = E X 2 + 2XY + Y 2 = E X 2 +
Beweis. LinearitĂ¤t
des
Erwartungswertes
gibt
us:
E
(X
+
Y
)
 
2E [XY ] + E Y 2 . Wir mĂźssen den Term E [XY ] berechnen. Dazu benutzen wir die UnabhĂ¤ngigkeit

nâ1 
n
n nâ1 gilt.
11Warum? Da wir genau mâ1
m-elementige Teilmengen A mit x â A haben und es m
= m
mâ1

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

194
von X und Y :
E [XY ] =

=

=

n X
m
X
i=1 j=1
n X
m
X

i=1 j=1
n
X
i=1


ai b j Pr X = ai ,Y = b j

ai b j Pr {X = ai } Âˇ Pr Y = b j

ai Pr {X = ai }

= E [X ] Âˇ E [Y ] .

(UnabhĂ¤ngigkeit von X und Y )

m
 X


Âˇ
b j Pr Y = b j
j=1

Somit erhalten wir


Var [X + Y ] = E (X + Y ) 2 â E [X + Y ]2
 
 
= (E X 2 + 2E [XY ] + E Y 2 ) â (E [X ]2 + 2E [X ] Âˇ E [Y ] + E [Y ]2 )
 
 
= (E X 2 â E [X ]2 ) + 2(E [XY ] â E [X ] Âˇ E [Y ]) + (E Y 2 â E [Y ]2 )
= Var [X ] + Var [Y ]


Ist X : âŚ â S mit S â R eine Zufallsvariable und f : S â S eine Funktion, so ist f (X ) auch
eine Zufallsvariable, die jedem Elementarereignis Ď â âŚ die Zahl f (X (Ď)) zuweist. Wie sieht ihr
Erwatungswert E [Y ] aus? Nach der Definition ist

 X
E f (X ) =
y Âˇ Pr { f (X ) = y} .
y âS

Man kann aber leicht zeigen, dass auch folgendes gilt.
Lemma 4.52. Ist X : âŚ â S eine Zufallsvariable und f : S â S eine Funktion, so gilt

 X
E f (X ) =
f (x) Âˇ Pr {X = x}
x âS

Beweis. Sei y â S ein festes Punkt in dem Wertebereich von f . Dann gilt
Pr { f (X ) = y} = Pr {{Ď â âŚ : f (X (Ď)) = y}}

= Pr {Ď â âŚ : X (Ď) â f â1 (y)}
X
=
Pr {{Ď â âŚ : X (Ď) = x}}
x:f (x)=y

=

X

x:f (x)=y

Pr {X = x}
ÂŠ 2003 by S. Jukna

4.9. EIGENSCHAFTEN VON E [X ] UND VAR [X ]

195

Somit gilt
X


E f (X ) =
y Âˇ Pr { f (X ) = y}
y

X

=

y

yÂˇ

X

x:f (x)=y

X X

=

y

x:f (x)=y

X X

=

y

X

=

x

x:f (x)=y

Pr {X = x}

y Âˇ Pr {X = x}
f (x) Âˇ Pr {X = x}

f (x) Âˇ Pr {X = x}

(da y , y â˛ â f â1 (y) âŠ f â1 (y â˛ ) = â)






Ist f (x) eine nicht lineare Funktion, so gilt E f (X ) = f (E [X ]) im Allgemeinen nicht! Das
2
zu âBehauptenâ ist ein sehr hĂ¤ufiger Fehler! Ist zum
 Beispiel f (x) = x und X eine2Indikatorvariable mit Pr {X = 1} = p > 0, dann haben wir E f (X ) = E [X ] = p aber f (E [X ]) = p .



FĂźr allgemeine Funktionen kann man manchmal nur einige AbschĂ¤tzungen von E f (X ) mittels
f (E [X ]) finden. So folgt zum Beispiel aus Jensenâs Ungleichung fĂźr konvexe Funktionen 12 die folgende Ungleichung:

Lemma 4.53. (Jensenâs Ungleichung) Sei X : âŚ â R eine Zufallsvariable mit endlichem Wertebereich f (âŚ) und f : R â R sei eine konvexe Funktion. Dann gilt:


E f (X ) > f (E [X ])
Eine Booleâsche Funktionen in n Variablen ist eine Abbildung f : {0, 1} n â {0, 1}. Eine solche
Funktion ist monoton (bzw. anti-monoton), falls aus13 x 6 y stets f (x) 6 f (y) (bzw. f (x) > f (y))
folgt.
Lemma 4.54. (Harrisâs Ungleichung) Seien f , g : {0, 1} n â {0, 1} Booleâsche Funktionen und
seien x 1 , . . . , x n unabhĂ¤ngige Bernoulli-Variablen. Sein weiterhin X = f (x 1 , . . . , x n ) und Y =
g(x 1 , . . . , x n ). Sind die Funktionen f und g beide monoton oder beide anti-monoton, so gilt:
E [X Âˇ Y ] > E [X ] Âˇ E [X ] .
12Sind 0 6 Îť i 6 1 mit

Pr

i=1

Îť i = 1 und ist f eine konvexe Funktion, so gilt (siehe Satz 1.23):
r
X
i=1

Îť i f (x i ) > f

X
r
i=1


Îťi xi .

13x 6 y ââ x i 6 yi fĂźr alle i.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

196

Beweis. Induktion nach n. Wir werden nur Induktionsbasis n = 1 verifizieren. In diesem Fall haben
wir X = f (x) und Y = g(x), wobei x eine Bernoulli-Variable mit Pr {x = 1} = p und Pr {x = 0} =
q = 1 â p ist. Dann gilt:
E [X Âˇ Y ] â E [X ] Âˇ E [X ] = f (1)g(1)p + f (0)g(0)q â ( f (1)p + f (0)q)(g(1)p + g(0)q)
oder Ă¤quivalent (nach der Umformung)
E [X Âˇ Y ] â E [X ] Âˇ E [X ] = pq( f (1) â f (0))(g(1) â g(0)),
was nicht negativ sein muss, da f (1) > f (0) und g(1) > g(0) gilt.
Den Induktionsschritt n â n + 1 kann man zeigen, indem man die bedingte Erwarungswerte unter der
Bedingungen x n+1 = 1 und x n+1 = 0 betrachtet.

Besteht unser Wahrscheinlichkeitsraum aus allen 0-1 Vektoren der LĂ¤nge n (d.h. âŚ = {0, 1} n ), so sagt
man, dass ein Ereignis A â âŚ monoton ist, falls aus x â A und x 6 y stets y â A folgt. Und fĂźr solche
Ereignisse liefert Harrisâs Ungleichung die folgende interesante untere Schranke.
Korollar 4.55. (Das âWurzel Trickâ) Seien A1 , . . . , Am monotone Ereignisse in âŚ = {0, 1} n mit
Pr { A1 } = . . . = Pr { Am }. Dann gilt fĂźr jedes 1 6 i 6 m:
ďŁŤ

Pr { Ai } > 1 â ďŁ­1 â Pr

ďŁą
m
ďŁ˛[
ďŁł

j=1

ďŁźďŁś1/m
ďŁ˝
Aj ďŁ¸
.
ďŁž

(4.6)

Beweis.
m
Y

 m

1 â Pr âŞm
A
=
Pr
âŠ
A
>
Pr A j = (1 â Pr { A1 }) m ,
j
j
j=1
j=1
j=1

wobei die Ungleichung aus Harrisâs Ungleichung folgt.





Beachte, dass die MonotonitĂ¤t der Ereignisse in Korollar 4.55 sehr wichtig ist. Nehme zum Beispiel eine gleichmĂ¤Ăige Verteiling auf âŚ und sei A1 , . . . , Am eine ZerlĂ¤gung von âŚ in gleichgrĂśĂe Ereignisse. Dann ist die linke Seite von (4.6) gleich 1/m, wĂ¤hrend die rechte Seite gleich 1
ist.
Wenn die Zufallsvariable X nur natĂźrliche Zahlen als seine Werte annimmt, gibt es eine alternative
(und oft mehr geeignete) Art und Weise den Erwartungswert E [X ] zu bestimmen.
Satz 4.56. (Erwartunswert diskreter Zufallsvariablen) Ist X : âŚ â N und E [X ] < â, so gilt
E [X ] =

â
X
i=0

Pr {X > i} .

ÂŠ 2003 by S. Jukna

4.9. EIGENSCHAFTEN VON E [X ] UND VAR [X ]

197

Beweis. Da X nur Zahlen 0, 1, 2, . . . als seine Werte annimmt, gilt
Pr {X > i} = Pr {X = i + 1} + Pr {X = i + 2} + Pr {X = i + 3} + . . .
und deshalb
â
X
i=0

Pr {X > i} = Pr {X > 0} + Pr {X > 1} + Pr {X > 2} + . . .
= Pr {X = 1} + Pr {X = 2} + Pr {X = 3} + . . .
|
{z
}
Pr {X >0}
+ Pr {X = 2} + Pr {X = 3} + . . .
|
{z
}
Pr {X >1}
+ Pr {X = 3} + . . .
|
{z
}
Pr {X >2}
= Pr {X = 1} + 2 Âˇ Pr {X = 2} + 3 Âˇ Pr {X = 3} + . . .
â
X
i Âˇ Pr {X = i}
=
i=1

= E [X ] .


â˛ Beispiel 4.57 : Wir haben ein Kommunikations-Netz, in dem viele Pakete verschickt werden sollen.
Angenommen, der Versand eines Pakets kann sich nur mit Wahrscheinlichkeit 1/k um k oder
mehr Sekunden verzĂśgern. Das klingt gut; es ist nur 1% Chance, dass der Versand eines Pakets
um 100 oder mehr Sekunden verzĂśgert wird. Aber wenn wir die Situation geneuer betrachten, ist
das Netz gar nicht so gut. TatsĂ¤chlich ist die erwartete VerzĂśgerung eines Pakets unendlich! Sei
X die VerzĂśgerung eines Pakets. Dann gilt nach Satz 4.56:
E [X ] =

â
X
i=0

Pr {X > i} >

â
X
1
i+1
i=0

= â (unendliche harmonische Reihe divergiert, siehe Beispiel 3.32).
In manchen Situationen haben wir mit unendlichen Summen (Reihen) der Zufallsvariablen zu tun.
Hier kĂśnnen wir nicht mehr ohne weiteres die LinearitĂ¤t des Erwartungswertes benutzen, da diese
Eigenschaft nur fĂźr endlichen Summen gilt. Obwohl gilt die Gleichung
" n
#
n
X
X
E
Xi =
E [X i ]
i=0

i=0

fĂźr alle n, kĂśnnen wir daraus nicht (ohne weiteres) schliessen, dass auch
"
#
n
n
X
X
X i = lim
E [X i ]
E lim
nââ

i=0

nââ

i=0

gelten muss. Dazu brauchen wir zusĂ¤tzlich, dass der Grenzwert lim

nââ

Pn

i=0

E [|X i |] existiert.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

198

Satz 4.58. (Unendliche
LinearitĂ¤t des Erwartungswertes) Seien X0 , X1 , . . . Zufallsvariablen, so
Pâ
dass die Reihe i=0 E [|X i |] konvergiert. Dann gilt
" â
#
â
X
X
E
Xi =
E [X i ]
i=0

i=0

Pâ
Beweis. Sei Y
i=0 X i . Wir lassen es als Ăbungsaufgabe, zu verifizieren, dass (wegen der KonP:=
vergenz von â
E
[|X
i |]) alle Summen in den folgenden Ableitungen absolut konvergent sind, was
i=0
ihre Vertauschung erlaubt (siehe Satz 3.49).
â
X

E [X i ] =

â X
X

i=0 Ď ââŚ

i=0

=

X

Ď ââŚ

X i (Ď) Âˇ Pr {Ď} =

â
XX

X i (Ď) Âˇ Pr {Ď}

Ď ââŚ i=0
" â
X

Y (Ď) Âˇ Pr {Ď} = E [Y ] = E

i=0

#

Xi .


â˛ Beispiel 4.59 : (Casino) Wir nehmen in einem Casino an einem Spiel mit Gewinnwahrscheinlichkeit p = 1/2 teil. Zum Beispiel wirft man eine faire MĂźnze, deren Seiten mit rot und blau gefĂ¤rbt
sind, und wir gewinnen, falls rot kommt. Wir kĂśnnen einen beliebigen Betrag einsetzen. Geht
das Spiel zu unseren Gunsten aus, erhalten wir den Einsatz zurĂźck und zusĂ¤tzlich denselben Betrag aus der Bank. Endet das Spiel ungĂźnstig, verfĂ¤llt unser Einsatz. Wir betrachten die folgende
Strategie:
In jedem Schritt verdoppeln wir unseren Einsatz bis erstmals die Farbe rot kommt.
Wir wollen den erwarteten Gewinn dieser Strategie bestimmen. Sei K unser erster Einsatz. Sei
X i das im i-ten Schritt gewonnene Kapital. Dann ist
Y=

â
X

Xi

i=0

das gewonnene Gesamtkapital. Da in jedem Schritt die Gewinnchance p = 1/2 ist, werden wir im
i-ten Schritt (i = 0, 1, 2, . . .) mit gleicher Wahrscheinlichkeit entweder K Âˇ 2i Euro gewinnen oder
denselben Betrag verlieren, d.h. der Gewinn im i-ten Schritt ist entweder positiv (+K2i ) oder
negativ (âK2i ). Also ist der erwartete Gewinn E [X i ] = 0 fĂźr alle i = 0, 1, . . .. Daraus âfolgtâ:
" â
#
â
â
X
X
X
E [Y ] = E
Xi =
E [X i ] =
0 = 0 !?
i=1

i=1

i=1

Aber in jedem Schritt ist die Gewinnwahrscheinlichkeit > 0. Also soll es mit Sichercheit irgendwann mal die MĂźnze auf rot landen. D.h. wir sollten mit Wahrscheinlichkeit 1 mindestens K
Euro gewinnen. Was war dann in unserer Argumentation falsch? Um diese Frage zu beantworten,
mĂźssen wir das Problem genauer betrachten.
Die Wahrscheinlichkeitsraum âŚ besteht aus Strings B kâ1 R, k = 1, 2, . . ., wobei B fĂźr âblauâ
steht und R fĂźr ârotâ. Sei X i das gewonnene Kapital im i-ten Schritt (wir nehmen o.B.d.A. an,
ÂŠ 2003 by S. Jukna

4.10. VERTEILUNGEN DISKRETER ZUFALLSVARIABLEN

199

dass X i (Ď) = 0 fĂźr Elementarereignisse Ď = B k R mit k < i â 1). Auf jedem Elementarereignis
Ď = B kâ1 R nimmt X i einen der drei mĂśglichen Werte an:

X i (B kâ1 R) =

ďŁą
ďŁ˛

0
K Âˇ 2i
ďŁł
âK Âˇ 2i

falls k < i
falls k = i
falls k > i

Unsere Argumentation, dass E [X i ] = 0 fĂźr alle i, war richtig. Falsch war aber zu sagen, dass
E

" â
X
i=1

gilt, da die Reihe
deshalb

Pâ

i=1

#

Xi =

â
X

E [X i ]

i=1

E [|X i |] nicht konvergent ist: |X i | = K Âˇ 2i mit Wahrscheinlichkeit 2âi und
â
X
i=1

E [|X i |] =

â
X
i=1

= K Âˇ 2i Âˇ 2âi =

â
X
i=1

K = â.

kâ1
Die richtige
PâArgumentation ist folgende: Auf jedem Elementarereignis Ď = B R ist der Wert
von Y = i=1 X i gleich

Y (Ď) = K Âˇ 2k â K Âˇ

kâ1
X

2i = K

i=0

| {z }
2k â1

woraus trivialerweise folgt,14 dass der Erwartungswert von Y auch gleich K sein muss.

4.10 Verteilungen diskreter Zufallsvariablen
In diesem Abschnitt betrachten wir einige wichtige Verteilungen der Zufallsvariablen, die in vielen
Anwendungen immer wieder vorkommen, und berechnen ihr Erwartungswert und ihre Varianz.

Bernoulli-Verteilung
Das ist die einfachste Verteilung Ăźberhaupt: Jede solche Zufallsvariable X hat nur zwei mĂśgliche Werte 0 und 1; p = Pr {X = 1} heiĂt dann die Erfolgswahrscheinlichkeit und die Wahrscheinlichkeit eines
Misserfolges ist q = 1 â p. Beispiel: Einmaliges werfen einer MĂźnze, wobei der Ausgang âWappenâ mit Wahrscheinlichkeit p kommt. Das entsprechende Zufallsexperiment nennt man BernoulliExperiment. Den Erwartungswert wie auch die Varianz einer solchen Zufallsvariable sind leicht zu
berechnen:
E [X ] = 1 Âˇ Pr {X = 1} + 0 Âˇ Pr {X = 0} = p,
 
Var [X ] = E X 2 â E [X ]2 = p â p2 = p(1 â p).

14Da Y eine konstante Funktion mit Y (Ď) = K fĂźr alle Ď ist.

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

200

Binomialverteilung B(n, p)
Eine solche Zufallsvariable Sn gibt uns die Anzahl der Erfolge in n unabhĂ¤ngig voneinander ausgefĂźhrten Bernoulli-Experimenten X1 , . . . , X n mit Erfolgswahrscheinlichkeit Pr {X i = 1} = p fĂźr alle
i = 1, . . . , n. D.h.
Sn = X1 + X2 + . . . + X n .
Man kann das Experiment auch als ein Urnenmodell vorstellen: Man hat
eine Urne mit r roten und s schwarzen Kugeln (also N = r + s Kugeln
insgesammt) und zieht n Kugeln rein zufĂ¤llig eine nacheinander mit ZurĂźcklegen. Erfolg ist dann eine rote Kugel und Erfolgswahrscheinlichkeit ist dann p = r/N.
Bei einer unabhĂ¤ngigen Wiederholung des Bernoulli-Experiments multiplizieren sich die Wahrscheinlichkeiten, die Wahrscheinlichkeit
fĂźr genau k Erfolge (und n â k Misserfolge) ist also pk q nâk mit
n
q = 1 â p. Da es k MĂśglichkeiten gibt, k Erfolge in einer Versuchsserie
der LĂ¤nge n unterzubringen,

ist die Wahrscheinlichkeit, dass X den Wert k annimmt, gerade nk pk q nâk . Damit gilt:
 
n k nâk
Pr {Sn = k} =
p q
k

Nach dem binomischen Lehrsatz gilt
n
X
k=0

Pr {Sn = k} =

n  
X
n
k=0

k

pk q nâk = (p + q) n = 1,

wie dies auch sein sollte. Da E [X i ] = p, Var [X i ] = pq und die Zufallsvariablen X1 , . . . , X n unabhĂ¤ngig sind, kann man den Erwartungswert wie auch die Varianz leicht berechnen:
E [Sn ] = p + p + . . . + p = np
Var [Sn ] = pq + pq + . . . + pq = npq.
â˛ Beispiel 4.60 : Wir verteilen m Bonbons an n Kinder. Dazu werfen wir wiederholt ein Bonbon in
eine Gruppe aus n Kindern. Der Versuch eines Kindes, das geworfene Bonbon zu fangen, ist
ein Bernoulli-Versuch. Jedes der Kinder fĂ¤ngt mit gleicher Wahrscheinlichkeit ein Bonbon. Die
Erfolgswahrscheinlichkeit jedes Kindes ist dann p = n1 , und die Wahrscheinlichkeit eines Mis1
serfolges ist q = nâ1
n = 1 â n.
Wie groĂ ist nun die Wahrscheinlichkeit, dass ein bestimmtes Kind von m geworfenen Bonbons
genau k fĂ¤ngt? Sei X die Zufallsvariable, deren Wert die Anzahl der von diesem Kind gefangenen
Bonbons beschreibt. Dann ist Pr {X = k} durch die Binomial-Verteilung B(m, p) bestimmt. Die

m
1
erwartete Anzahl gefangener Bonbons ist also np = m
n , und die Varianz ist np(1âp) = m 1 â n .

Wir wissen bereits, dass fĂźr jedes Îą â (0, 1)
 
n
1
âź â
Âˇ 2n ÂˇH (Îą)
Îąn
2ĎÎą(1 â Îą)n

(4.7)

mit H (Îą) = â(Îą log2 Îą + (1 â Îą) log2 (1 â Îą)) gilt.
ÂŠ 2003 by S. Jukna

4.10. VERTEILUNGEN DISKRETER ZUFALLSVARIABLEN

201

Somit gilt fĂźr die Verteilung Pr {Sn = k} einer binomiel B(n, 1/2)-verteilten Zufallsvariable
 
n
1
Âˇ 2ân âź â
Pr {Sn = Îąn} =
Âˇ 2ân(1âÂˇH (Îą))
Îąn
2ĎÎą(1 â Îą)n
Dann ist fĂźr Îą = 1/2
Pr {Sn = n/2} âź

r

(4.8)

2
.
Ďn

Die asymptotische Gleichung (4.8) sagt uns, dass eine binomiel-verteilte
Zufallsvariable tatsĂ¤chlich nie ihrem Erwartungswert gleich sein wird.
Wenn wir zum Beispiel n = 100 mal eine faire MĂźnzen werfen wĂźrden,
dann ist die Wahrscheinlichkeit, dass genau 50 mal das Wappen kommt
ungefĂ¤hr 8%. Aber wenn wir die Wahrscheinlichkeit, dass genau 25 mal
das Wappen kommt, betrachten (d.h. wenn wir Îą = 1/4 nehmen), dann
ist diese Wahrscheinlichkeit nur Pr {Sn = n/4} âź 1, 913Âˇ10â7 , was sogar
kleiner als 1 zu 5 Millionen ist!
Im Allgemeinen liefert uns die asymptotische Gleichung (4.8) folgendes: Ist Îą , 1/2, so ist die
Differenz ÇŤ := 1 â H (Îą) positiv, und damit gilt

Pr {Sn = Îąn} = O 2âÇŤn .

Somit haben wir die folgende Merkregel:

FĂźr Îą , 1/2 ist Pr {Sn = Îąn} exponetiell klein im Vergleich mit n

Geometrische Verteilung G(p)
Wir wiederholen das Bernoulli-Experiment X1 , X2 , . . . mit Erfolgswahrscheinlichkeit p > 0 vielmals
und wollen die Anzahl der Versuche bis zu erstem Erfolg bestimmen.
Man kann auch dieses Experiment auch als ein Urnenmodell vorstellen:
Man hat eine Urne mit r roten und s schwarzen Kugeln (also N = r + s
Kugeln insgesammt) und zieht n Kugeln rein zufĂ¤llig eine nacheinander
mit ZurĂźcklegen. Erfolg ist dann eine rote Kugel und Erfolgswahrscheinlichkeit ist dann p = r/N.
Die entsprechende Zufallsvariable
X := min{i : X i = 1}
heiĂt dann geometrisch verteilt, da ihre Verteilung eine geometrische Folge ist:
Pr {X = i} = qiâ1 p

mit q = 1 â p

Wenn wir diese Wahrscheinlichkeiten aufsummieren, so erhalten wir
â
X
i=1

qiâ1 p = p Âˇ

â
X
t =0

qi =

p
p
=
= 1,
1 â q 1 â (1 â p)
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

202

wie auch es sein sollte. Um E [X ] zu berechnen, kĂśnen wir Satz 4.56 (Ăźber diskretwertige Zufallsvariablen) anwenden. Dazu beobachten wir:
â
X

Pr {X > i} =

k=i+1


1 â qi
= (1 â p) i .
qkâ1 p = 1 â p 1 + q + q2 + . . . + qiâ1 = 1 â p Âˇ
1âq

Nach Satz 4.56 gilt:
E [X ] =
=

â
X
i=0
â
X
i=0

=

Pr {X > i}
(1 â p) i =

1
1 â (1 â p)

1
.
p

Wir kĂśnnen E [X ] auch mittels der Methode der erzeugenden Funktionen leicht berechnen. Diese
Methode ist gut, da sie erlaubt auch ânebenbeiâ die Varianz Var [X ] zu berechnen.
Die erzeugende Funktion von X ist
F (x) =

â
X
i=1

q

iâ1

i

px = px Âˇ

â
X

qi x i =

i=0

px
.
1 â qx

(geometrische Reihe)

Die erste und die zweite Ableitung von F (x) sind
F â˛ (x) =
F â˛â˛ (x) =

(1 â qx)p + pxq
p
=
2
(1 â qx)
(1 â qx) 2
2pq
.
(1 â qx) 3

Setzen wir nun x = 1, so erhalten wir (nach Satz 4.45)
E [X ] = F â˛ (1) =

p
1
=
2
(1 â q)
p

und
Var [X ] = F â˛â˛ (1) + E [X ] â E [X ]2 =

1
2pq 1
1âp
+ â 2 =
.
p3
p p
p2

â˛ Beispiel 4.61 : Wir verteilen wiederum Bonbons an n Kinder. Dazu werfen wir wiederholt ein Bonbon in eine Gruppe aus n Kindern. Der Versuch eines Kindes, das geworfene Bonbon zu fangen,
ist ein Bernoulli-Versuch. Jedes der Kinder fĂ¤ngt mit gleicher Wahrscheinlichkeit p = n1 ein
Bonbon.
Wie viele Bonbons mĂźssen geworfen werden, bis ein bestimmtes Kind ein Bonbon gefangen hat?
Sei X die Zufallsvariable, deren Wert die Nummer des Versuchs an, bei dem das Kind erstmals ein
Bonbon fĂ¤ngt. Dann ist Pr {X = k} = (1 â p) kâ1 Âˇ p durch die geometrische Verteilung bestimmt.
Also ist die erwartete Anzahl der Versuche ein Bonbon zu fangen bis es erstmals klappt genau
die Anzahl E [X ] = p1 = n der Kinder, die sich auch auf ein Bonbon warten.
ÂŠ 2003 by S. Jukna

4.10. VERTEILUNGEN DISKRETER ZUFALLSVARIABLEN

203

Poisson-Verteilung P(Îť)
In vielen Anwendugen (Physik, Biologie, usw.) taucht eine Wahrscheinlichkeitsverteilung â die sogenannte âPoisson-Verteilungâ â sehr oft auf. Diese Verteilungen sind nichts anderes als die Grenzwerte
der binomiellen Verteilungen B(n, p), wennn n â â, p â 0 und np = Îť konstant bleibt.
Zum Besipiel wollen wir die Ankunft von Paketen auf einem Internetrouter modellieren. Wir wissen,
dass im Durchschnitt der Router Îť Pakete pro Sekunde abfertigt. Falls wir diesen Durchschnittswert
wissen, wie kĂśnnen wir die tatsĂ¤chliche Ankunft der Pakete in einer Sekunde modellieren? Eine MĂśglichkeit ist, die Sekunde in sehr kurze Intervalle der LĂ¤nge Î´ > 0 (mit Î´ < Îť) aufzuteilen; damit haben
wir eine groĂe Anzahl n = 1/Î´ der Intervalle. Dann nehmen wir an, dass ein Paket in einem Intervall mit Wahrscheinlichkeit p = ÎťÎ´ ankommt (dies ergibt die richtige durchschnittliche Anzahl
np = (1/Î´)(ÎťÎ´) = Îť der Pakete pro ganze Sekunde). In diesem Model ist die Anzahl X der Intervalle,
in denen ein Paket ankommen wird, als binomielle Zufallsvariable verteilt:15
 
 
1/Î´
n k
nâk
Pr {X = k} =
p (1 â p)
=
(ÎťÎ´) k (1 â ÎťÎ´) 1/Î´âk .
k
k
Nur lassen wir Î´ beliebig klein sein (Î´ â 0), halten aber k fest. Dann gilt:
 
 
1/Î´
1/Î´
k
1/Î´âk
Pr {X = k} =
(ÎťÎ´) (1 â ÎťÎ´)
=
(ÎťÎ´) k (1 â ÎťÎ´) (1âÎ´k )/Î´
k
k
(1/Î´) k
Îťk
Îť k âÎť
â
(ÎťÎ´) k (1 â ÎťÎ´) 1/Î´ =
(1 â ÎťÎ´) 1/Î´ â
e .
k!
k!
k!
Die resultierende Verteilung
Îť k âÎť
e
Pr {X = k} =
k!
ist als Poisson-Verteilung bekannt. Die Tatsache, dass das wirklich eine Wahrecheinlichkeitsverteilung
P
Îťk
ist, folgt aus der Taylorfomel e = â
k=0 k! fĂźr die Euler-Zahl e:
â
X
k=0

â
â
X
X
Îť k âÎť
Îťk
âÎť
Pr {X = k} =
e =e
= eâÎť eÎť = 1.
k!
k!
k=0

k=0

Was den Erwartungswert E [X ] betrifft, ist er gleich Îť (wie es sein sollte):
E [X ] =

â
X
k=0

k Âˇ Pr {X = k} = eâÎť

â
X
k=0

kÂˇ

â

X Îť kâ1
Îťk
= eâÎť Îť
= Îť.
k!
(k â 1)!
k=1

â˛ Beispiel 4.62 : Wir verteilen wiederum m Bonbons an n Kinder. Dazu werfen wir wiederholt einen
Bonbon in eine Gruppe aus n Kindern. Der Versuch eines Kindes, das geworfene Bonbon zu
fangen, ist ein Bernoulli-Versuch. Jedes der Kinder fĂ¤ngt mit gleicher Wahrscheinlichkeit p = 1/n
ein Bonbon.
Mit welcher Wahrscheinlichkeit pk wird ein bestimmtes Kind genau k Bonbon fangen? Da es
hier sich um eine binomial verteillte mit Parametern B(m, 1/n) Zufallsvariable handelt, ist

   k 
mâk
mâk
m
1
1
1 m(m â 1) Âˇ Âˇ Âˇ (m â k + 1)
1
pk =
1â
=
1
â
.
k
n
n
k!
nk
n
15Beachte, dass das nicht genau die Anzahl der AnkĂźnfte der Pakete entspricht, da mehr als ein Paket in einem Zeitintervall ankommen kann. Aber wenn wir die Intervalle wirklich sehr kurz wĂ¤hlen werden, wird in einem Interval nur ein Paket
kommen kĂśnnen.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

204
Wenn m und n viel grĂśĂer als k sind, dann haben wir
pk â

Îť k âÎť
e
k!

mit Îť =

m
.
n

Also ist die Anzahl der Bonbons, die ein bestimmtes Kind am Ende haben wird, Poison P(Îť) mit
Îť = m/n verteilt.

Hypergeometrische Verteilung
Die hypergeometrische Verteilung ist fĂźr die Stichprobentheorie von besonderer Bedeutung.16 Diese Verteilung ergibt die Anzahl der Erfolge bei n-fachem Wiederholen eines 0-1 Experiments ohne
ZurĂźcklegen. D.h. die Erfolgswahrscheinlichkeit in jedem Schritt kann von frĂźheren AusgĂ¤ngen des
Experiments abhĂ¤ngen.
Man kann das Experiment auch als ein Urnenmodell vorstellen: Man hat
eine Urne mit r roten und s schwarzen Kugeln (also N = r + s Kugeln
insgesammt) und zieht n Kugeln rein zufĂ¤llig eine nacheinander ohne
ZurĂźcklegen. Erfolg ist dann eine rote Kugel. Diesmal hĂ¤ngt aber die
Erfolgswahrscheinlichkeit von der Anzahl der bereits gezogenen roten
Kugeln ab!
Wir betrachten die Zufallsvariable
X = Anzahl der roten Kugeln in der Stichprobe.
Was ist die Wahrscheinlichkeit, dass X den Wert k annimmt? Dazu mĂźssen k rote und n â k schwar
s
ze Kugeln ausgewĂ¤hlt werden, was (ohne BerĂźcksichtigung der Reihenfolge) auf kr bzw. nâk
=


N âr
N
Weisen
mĂśglich
ist.
Insgesamt
gibt
es
Stichproben,
die
gesuchte
Wahrscheinlichkeit
ist
nâk
n
daher
 
 
r
N âr
N
Pr {X = k} =
.
(4.9)
k
nâk
n
Unsere Ableitung der hypergeometrischen Verteilung ergibt als Nebenresultat die kombinatorische
IdentitĂ¤t (fĂźr jedes 0 6 r 6 N)
  X

n  
N
r
N âr
=
(4.10)
n
k
nâk
k=0

denn die Wahrscheinlichkeiten in (4.9) summieren sich zu 1 auf. Wir nutzen sie zur Berechnung des
16Die hypergeometrische Verteilung kommt zum Beispiel in der QualitĂ¤tskontrolle zur Anwendung. Will man die GĂźte
einer Lieferung durch eine Stichprobe ĂźberprĂźfen, so mĂźssen sich die Beteiligten darauf einigen, wieviele fehlerhafte StĂźcke
X die Stichprobe enthalten darf. Der VerkĂ¤ufer wird darauf achten, dass eine Lieferung mit einem geringen Anteil von
Ausschuss mit hoher Wahrscheinlichkeit akzeptiert wird. Der KĂ¤ufer hat das Interesse, dass eine Lieferung von schlechter
QualitĂ¤t die Kontrolle mit nur geringer Wahrscheinlichkeit passiert. Diese unterschiedlichen Interessen werden sich nur
dann vereinbaren lassen, wenn die StichprobengrĂśĂe groĂ genug gewĂ¤hlt ist. Die Wahrscheinlichkeiten werden unter der
Annahme bestimmt, dass X hypergeometrisch verteilt ist.
ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT

205

Erwartungswertes einer hypergoemetrisch verteilten Zufallsvariable X :17
n
X
k=0

 

r
N âr
kÂˇ
k
nâk

also
E [X ] =

(â)

=





 
n 
X
r â1
N â r (ââ)
N â1
nr N
r
= r
=
,
k â1
nâk
nâ1
N n
k=1

n
X
k=0

k Âˇ Pr {X = k} =

nr
r
= np mit p = .
N
N

Auf den Erwartungswert hat es also keinen Einfluss, ob man eine Stichprobe mit oder ohne ZurĂźcklegen zieht, er ist in beiden FĂ¤llen gleich np. Die Varianz ist (wir verzichten aus dem Beweis)
Var [X ] = n

r N âr N ân
N ân
= npq Âˇ
.
N N N â1
N â1

ân
D.h. bis auf einem âKorrekturs-Faktorâ N
N â1 , der fĂźr N â â und festem n gegen 1 strebt, ist die
Varianz dieselbe wie die fĂźr binomial B(n, p)-verteilte Zufallsvariable mit p = r/N.

Sind r und s groĂ im Vergleich zu n, so nĂ¤hert sich die hypergeometrische Verteilung der Binomialverteilung an, denn (mit l = n â k)
r
k

r k sl
 
Âˇ
n k nâk
r
l
k!
l!
 â
=
p q , mit p = .
N
Nn
k
N
n
n!

 s

â˛ Beispiel 4.63 : Eine Urne enthĂ¤lt 4 Kugeln: 2 rote und 2 schwarze. Wir ziehen (ohne ZurĂźcklegen)
eine Stichprobe aus 2 Kugeln. Sei X die Anzahl der roten Kugeln in der Stichprobe. Hier haben
wir also mit einer hypergeometrischen Verteilung mit Parametern N = 4 und r = s = n = 2 zu
tun. Es gilt also fĂźr jedes k = 0, 1, 2
 
 
 

2
2
4
1 2
2
Pr {X = k} =
=
k
2âk
2
6 k
2âk
D.h. Pr {X = 0} = 1/6, Pr {X = 1} = 2/3 und Pr {X = 2} = 1/6. Das kann man auch aus der
entsprechenden Ziehungs-Diagramm (wie im Beispiel 4.31) entnehmen.

4.11 Abweichung vom Erwartungswert
Bis jetzt haben wir auf den Erwartungswert fokusiert, da er dem âDurchschnittswertâ entspricht. Was
aber das eigentlich bedeutet? Der Erwartungswert E [X ] ist nur eine (speziel definierte) Zahl und

râ1
17Hier benutzt (â) die IdentitĂ¤t kr = kr kâ1
und (ââ) die Cauchy-Vandermonde IdentitĂ¤t:

 X

z  
x+y
x
y
=
z
i
zâi
i=0

Beweis. In
bilden. Die
Clubs, und

einer Stadt wohnen x Frauen und y MĂ¤nner, und die Einwohner haben Lust so viele Clubs
 wie mĂśglich zu
einzige EinschĂ¤nkung ist, dass jeder Club genau z Teilnehmer haben muss. Dann ist x+y
die Anzahl aller
z
x y 
die
Anzahl
aller
Clubs
mit
i
Frauen
und
z
â
i
MĂ¤nner.

i zâi
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

206

als solche sagt uns (bis jetzt) Ăźberhaupt nichts. Mehr noch: Die Erwartungswert muss nicht mal im
Wertbereich der Zufallsvariable liegen! Zum Beispiel, ist X eine gleichmĂ¤Ăig verteilte Zufallsvariable,
die die Werte in {0, 1, 9, 10} annimmt, dann ist
E [X ] = 0 Âˇ

1
1
1
1
+ 1 Âˇ + 9 Âˇ + 10 Âˇ = 5,
4
4
4
4

die Zahl die zum keinem der Werte 0, 1, 9 oder 10 nah liegt!



Erwarte nicht immer das Erwartete!

Was uns wirklich interessiert ist die Frage, mit welcher Wahrscheinlichkeit wird die Zufallsvariable
nahe an ihrem Erwartungswert liegen?
GlĂźcklicherweise haben wir ein paar mĂ¤chtigen Intrumente, um diese Wahrscheinlichkeit zu bestimmen. Dazu gehĂśhren die Ungleichungen von Markov, Tschebyschev und Chernoff, die wir jetzt kennenlernen werden.

4.11.1 Markov-Ungleichung

Markov-Ungleichung
Sei X : âŚ â R+ eine nicht-negative Zufallsvariable. Dann gilt fĂźr alle
k > 0:
E [X ]
Pr {X > k} 6
.
k
Oder Ă¤quivalent, fĂźr alle Îť > 0 gilt
Pr {X > Îť Âˇ E [X ]} 6

1
Îť

Beweis.

E [X ] =

X
x

x Âˇ Pr {X = x} >

X
x>k

k Âˇ Pr {X = x} = k Âˇ Pr {X > k} .


ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT



207

Warum muss die Zufallsvariable X nicht negativ sein? Sei X â {â10, 10} mit Pr {X = â10} =
Pr {X = 10} = 1/2. Dann ist
E [X ] = â10 Âˇ

1
1
+ 10 Âˇ = 0.
2
2

Wir wollen nun die Wahrscheinlichkeit Pr {X > 5} ausrechnen. Wenn wir Markovâs-Ungleichung âanwendenâ kommt
E [X ] 0
= =0
Pr {X > 5} 6
5
5
raus. Aber das ist doch falsch! Es ist offensichtlich, dass X > 5 mit Wahrscheinlichkeit 1/2 gilt (da
X = 10 mit dieser Wahrscheinlichkeit gilt). Nichtdestotrotz kann man auch in diesem Fall MarkovâsUngleichung anwenden, aber fĂźr eine modifizierte Zufallsvariable. Setze nĂ¤hmlich Y := X +10. Das ist
bereits eine nicht-negative Zufallsvariable mit E [Y ] = E [X + 10] = E [X ] + 10 = 10, und MarkovâsUngleichung ergibt Pr {Y > 15} 6 10/15 = 2/3. Da aber Y > 15 ââ X > 5, haben wir die
AbschĂ¤tzung Pr {X > 5} 6 2/3 erhalten.
â˛ Beispiel 4.64 : (Klausuren) Ich nehme den Stapel Ihrer Klausuren, mische ihn, und verteile wieder
die Klausuren an Sie. Jeder bekommt genau eine Klausur und muss sie korrigieren. Sei X die
Anzahl von Studenten, die ihre eigene Klausur zurĂźck bekommen. Wie sieht E [X ] aus?
Wenn wir direkt die Warscheinlichkeiten Pr {X = i} ausrechnen wollten, wĂ¤re es nicht so einfach.
Wir kĂśnnen aber X als die Summe X = X1 + X2 + . . . + X n von Indikatorvariablen darstellen,
wobei X i = 1 wenn der i-te Student seine eigene Klausur bekommt, und X i = 0 sonst. Da jede
X i eine Indikatorvariable ist, gilt E [X i ] = Pr {X i = 1} (siehe (4.5)). Wie groĂ ist die Wahrscheinlichkeit Pr {X i = 1}? Jede Verteilung der Klausuren kann man als eine Permutation f : [n] â [n]
darstellen; der i-te Student seine eigene Klausur bekommt genau dann, wenn f (i) = i gilt. Damit
ist fĂźr jedes i
E [X i ] = Pr {X i = 1} =

Anzahl der Permutationen f mit f (i) = 1 (n â 1)! 1
=
=
Anzahl aller Permutationen f
n!
n

und die LinearitĂ¤t des Erwartungswertes gibt uns die Antwort:
E [X ] = E [X1 ] + E [X2 ] + . . . + E [X n ] = 1.
Nun wollen wir die Varianz Var [X ] berechnen. Obwohl X die Summe von Indikatorvariablen ist,
kĂśnnen wir nicht den Satz 4.51 benutzen, da die Indikatorvariablen X i nicht unabhĂ¤ngig sind:
Nach dem Multiplikationssatz fĂźr Wahrscheinlichkeiten gilt fĂźr i , j

Pr X i = 1, X j = 1


1
1
= Pr {X i = 1} Âˇ Pr X j = 1 | X i = 1 = Âˇ
n nâ1

1 1
,
Âˇ = Pr {X i = 1} Âˇ Pr X j = 1 .
n n

 
Wir mĂźssen also die Varianz Var [X ] = E X 2 â E [X ]2 direkt ausrechnen. Wir wissen bereits,
dass



1
E X i Âˇ X j = Pr X i = 1, X j = 1 =
n(n â 1)
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

208
gilt. Somit gilt auch


E X

2



n
n X
n
X
 2 X


=
E Xi +
E Xi X j
i=1

= nÂˇ
= 2

i=1

j=1
j,i

1
1
+ n(n â 1) Âˇ
n
n(n â 1)

und
 
Var [X ] = E X 2 â E [X ]2 = 2 â 1 = 1.

Die nĂ¤chste Frage: Wie groĂ ist die Wahrscheinlichkeit, dass mindestens k Studenten ihre eigene
Klausur zur Korrektur zurĂźckbekommen werden? Nach Markovâs-Ungleichung gilt:
Pr {X > k} 6



E [X ] 1
= .
k
k

Somit gibt es zum Beispiel hĂśchstens 20% Chance, dass 5 Sudenten ihre eigene Klausuren bekommen.
Beachte, dass in diesem Beispiel weder der Erwartungswert noch die Varianz von der Anzahl
n der Studenten abhĂ¤ngt!

4.11.2 Tschebyschev-Ungleichung
Die Markov-Ungleichung sagt nur, dass mit groĂer Wahrscheinlichkeit der eigentliche Wert von X
nicht viel grĂśĂer als der Erwartungswert E [X ] sein wird. Sie sagt aber nicht mit welcher Wahrscheinlichkeit X nah an E [X ] sein wird â es kann gut sein, dass der eigentliche Wert von X viel kleiner als
E [X ] wird. Es macht deshalb Sinn, die Wahrscheinlichkeit Pr {|X â E [X ] | > k} zu betrachten.
Da fĂźr jede Zufallsvariable Y der Betrag |Y | und damit auch ihre Exponenten |Y | r nicht negativ sind,
kĂśnnen wir Markov-Ungleichung anwenden. Damit gilt fĂźr alle ÇŤ > 0 und alle r > 1:

E [|Y | r ]
Pr {|Y | > k} = Pr |Y | r > k r 6
kr

Wenn wir die Zufallsvariable Y := X â E [X ] betrachten, ergibt dies (mit r = 2)


E (X â E [X ]) 2
Pr {|X â E [X ] | > k} 6
.
k2
D.h. die Wahrscheinlichkeit, dass die Zufallsvariable X vom ihrem
E [X ] um mehr als

 Erwartungswert
2
2
Âąk abweicht, kann nicht grĂśĂer als 1/k mal die Konstante(!) E (X â E [X ]) sein. Diese Konstante
haben wir bereits frĂźher kennengelernt und als die VarianzVar [X ] von X bezeichnet:


 
Var [X ] = E (X â E [X ]) 2 = E X 2 â E [X ]2
Damit haben wir die folgende Ungleichung bewiesen.

ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT

209

Tschebyschev-Ungleichung
 
Sei X : âŚ â R eine Zufallsvariable mit E X 2 < â. Dann gilt fĂźr alle
k > 0:
Var [X ]
Pr {|X â E [X ] | > k} 6
.
k2

â˛ Beispiel 4.65 : (OptimalitĂ¤t der Tschebyschev-Ungleichung) Dieses Beispiel soll zeigen, dass
Tschebyschev-Ungleichung auch optimal ist. Sei a â R, a > 1 und betrachte die Zufallsvariable
X , deren Verteilung folgender MaĂen definiert ist:
Pr {X = âa} =

1
1
1
, Pr {X = 0} = 1 â 2 und Pr {X = a} = 2
2a2
a
2a

Dann gilt
E [X ] =

a
âa
+0+ 2 =0
2
2a
2a

und



 
a2
a2
Var [X ] = E (X â E [X ]) 2 = E X 2 = 2 + 0 + 2 = 1.
2a
2a
Setsz man k = a, so erhĂ¤lt man unter Beachtung der vorgegebenen Verteilung
Pr {|X â E [X ] | > a} = Pr {|X | > a} = Pr {X , 0} =

1
.
a2

Andererseits ist auch der rechter Term Var [X ] /a2 gleich 1/a2 . D.h. in diesem Fall wird die durch
die Tschebyschevâsche Ungleichung gegebene obere Schranke auch tatsĂ¤chlich angenommen.
â˛ Beispiel 4.66 : (Klausuren - Fortsetzung) Ich nehme den Stapel Ihrer Klausuren, mische ihn, und
verteile wieder die Klausuren an Sie. Jeder bekommt genau eine Klausur und muss sie korrigieren. Sei X die Anzahl von Studenten, die ihre eigene Klausur zurĂźck bekommen. Dann gilt
E [X ] = Var [X ] = 1 (siehe Beispiel 4.64). In diesem Beispiel haben wir Markov-Ungleichung
benutzt, um
E [X ] 1
=
Pr {X > k} 6
k
k
zu zeigen. Tschebyschev-Ungleichung liefert:
Pr {X > k}

= Pr {X â E [X ] > k â E [X ]}

= Pr {X â E [X ] > k â 1}
Var [X ]
1
6
=
(k â 1) 2
(k â 1) 2

(ziehe E [X ] von beiden Seiten ab)
(setze E [X ] = 1 ein)

Und diese obere Schranke ist sogar quadratisch besser, als die Pr {X > k} 6 1/k, die wir vorhier
aus der Markov-Ungleichung abgeleitet haben.
â˛ Beispiel 4.67 : Ist X = X1 + . . . + X n die Summe von n unabhĂ¤ngigen Bernoulli Variablen je mit
Erfolgswahrscheinlichkeit p, so gilt: E [X ] = np und Var [X ] = np(1 â p) (siehe Abschnitt 4.10).
Die Tschebyschev-Ungleichung ergibt dann
Pr {|X â np| > k} 6

np(1 â p)
n
6 2,
2
k
4k
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

210

da p(1 â p) 6 1/4 fĂźr alle 0 6 p 6 1 gilt: Ist p = 1/2 + c fĂźr ein â1/2 6 c 6 1/2, so gilt
p(1 â p) = (1/2 + c)(1/2 â c) = 1/4 â c2 6 1/4.
Werfen wir zum Beispiel eine faire 0-1 MĂźnze n mal, dann kĂśnnen wir n/2 Einsen erwarten. Die
â
Wahrscheinlichkeit, dass die tatsĂ¤chliche Anzahl der Einsen um mehr als Îť n von n/2 abweichen wird, ist damit hĂśchstens 1/4Îť 2 .
â˛ Beispiel 4.68 : (Faire oder unfaire MĂźnze?) Wir haben zwei MĂźnzen. Wir wissen, dass nur eine
der MĂźnzen fair ist. Die andere ist prĂ¤pariert, so dass die Wahrscheinlichkeit fĂźr den Ausgang
âWappenâ gleich 3/4 ist. Rein Ă¤uĂerlich aber sehen die beiden MĂźnzen vollig gleich aus.
Wir wĂ¤hlen rein zufĂ¤llig eine der MĂźnzen und werfen sie mehrmals. Wieviel mal mĂźssen wir die
MĂźnze werfen, um mit der Wahrscheinlichkeit 0.95 zu bestimmen, welche der MĂźnzen gewĂ¤hlt
war?
Sei X die Anzahl der AusgĂ¤ng âWappenâ nach n Wurfen. Um den Typ der MĂźnze festzustellen,
wĂ¤hlen wir einen Schwellenwert t zwischen 1/2 und 3/4, und schauen, ob die Anteil X/n der
AusgĂ¤nge âWappenâ kleiner oder grĂśsser als dieser Schwellenwert t ist. Als natĂźrlichen Schwellenwert wĂ¤hlen wir
t = 5/8
(genau in der Mitte von 1/2 und 3/4). Wir mĂźssen die MĂźnze so lange werfen bis
Pr {X/n > t} 6 0.05

Pr {X/n 6 t} 6 0.05

falls die MĂźnze fair ist
falls die MĂźnze prĂ¤pariert ist

Dann geben wir die Antwort
Antwort =



fair
prĂ¤pariert

falls X/n 6 t
falls X/n > t.

War die MĂźnze tatsĂ¤chlich fair, so erhalten wir die richtige Antwort mit Wahrscheinlichkeit

Pr Antwort richtig = Pr {X/n 6 t} = 1 â Pr {X/n > t} > 1 â 0.05 = 0, 95.
War die MĂźnze prĂ¤pariert, so erhalten wir die richtige Antwort mit Wahrscheinlichkeit

Pr Antwort richtig = Pr {X/n > t} = 1 â Pr {X/n 6 t} > 1 â 0.05 = 0, 95.

Um die Wahrscheinlichkeit Pr {X/n > 5/8} (und die Wahrscheinlichkeit Pr {X/n < 5/8}) abzuschĂ¤tzen, wenden wir die Tschebyschev-Ungleichung an. Dazu mĂźssen wir das Ereignis
âX/n > 5/8â in der Form â|X â E [X ] | > kâ darstellen.18 Beachte, dass die Zufallsvariable X
binomielverteilt zum Parameter B(n, p) ist, wobei p = 1/2 falls die MĂźnze fair ist, und p = 3/4
falls die MĂźnze prĂ¤pariert ist. Wir wissen bereits (siehe Abschnitt 4.10), dass fĂźr solcher Zufallsvariablen E [X ] = np und Var [X ] = np(1 â p) gilt. Also, falls die MĂźnze fair ist, dann gilt:
19




n
X
5
X 1 5 1
n no
Pr
>
= Pr
â > â
= Pr X â >
n
8
n 2 8 2
2 8
n
n
no
no
= Pr X â E [X ] >
6 Pr |X â E [X ] | >
8
8
Var [X ]
n/4
16
6
= 2
=
(Tschebyschev-Ungleichung)
(n/8) 2
n /64
n
18Dies ist ein der wichtigsten Schritte, wenn man Tschebyschev-Ungleichung anwenden will!
19Denn dann E [X ] = n/2 und Var [X ] = n/4.
ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT

211

Ist die MĂźnze prĂ¤pariert, so gilt: 20

Pr



X
5
6
n
8








3 X
3 5
3n
n
= Pr
â > â
= Pr
âX >
4 n
4 8
4
8
n
o
n
n
no
6 Pr |X â E [X ] | >
= Pr E [X ] â X >
8
8
Var [X ] 3n/16 12
=
(Tschebyschev-Ungleichung)
6
= 2
(n/8) 2
n /64
n

Wir mĂźssen also die Anzahl der Wurfe n so wĂ¤hlen, dass die beiden Zahlen 16/n und 12/n nicht
grĂśĂer als 0, 05 sind. Es reicht also n = 320 zu nehmen.
Das Gesetz der groĂen Zahlen besagt, dass sich die relative HĂ¤ufigkeit eines Zufallsergebnisses immer weiter an die theoretische Wahrscheinlichkeit fĂźr dieses Ergebnis (Erwartungswert) annĂ¤hert, je
hĂ¤ufiger das Zufallsexperiment durchgefĂźhrt wird.
Wiederholt man ein Zufallsexperiment X mit Erfogswahrscheinlichkeit p, so stabilisiert sich die relative HĂ¤ufigkeit H = X/n der Erfolge mit wachsender Versuchzahl n bei p. Allgemeiner gilt, dass
das arithmetische Mittel von n identisch verteilten, unabhĂ¤ngigen Zufallsvariablen mit wachsendem n
gegen den Erwartungswert strebt. In diesem Fall spricht man von einem âGesetz der groĂen Zahlenâ.
Eine einfache Version dieses Gesetzes ist das folgende Resultat.
Satz 4.69. (Schwaches Gesetz der groĂen Zahlen) Sei X eine reellwertige Zufallsvariable mit endlichem Erwartungswert und endlicher Varianz. Seien X1 , . . . , X n unabhĂ¤ngige Kopien von X . Dann
gilt fĂźr alle ÇŤ > 0


X1 + Âˇ Âˇ Âˇ + X n
Var [X ]
Pr
â E [X ] > ÇŤ 6
.
n
n Âˇ ÇŤ2
Insbesondare strebt diese Wahrscheinlichkeit gegen 0 fĂźr n â â.
Beweis. Sei Y :=

1
n

(X1 + Âˇ Âˇ Âˇ + X n ). Dann gilt:
#
n
n
1 X
1
1X
E [Y ] = E
Xi = Âˇ
E [X i ] = Âˇ nE [X ] = E [X ]
n
n
n
"

i=1

i=1

und
"

#
n
1X
n Âˇ Var [X n ] Var [X ]
Var [Y ] = Var
Xi =
=
.
n
n2
n

(UnabhĂ¤ngigkeit von X i âs)

i=1

Aus der Tschebyschev-Ungleichung folgt fĂźr jedes ÇŤ > 0
Pr {|Y â E [X ] | > ÇŤ } 6

Var [Y ] Var [X ]
=
.
ÇŤ2
n Âˇ ÇŤ2


20Denn dann E [X ] = 3n/4 und Var [X ] = n(3/4)(1/4) = 3n/16.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

212

â˛ Beispiel 4.70 : (Wahlvorhersage) Gesucht ist der Umfang n von Stichproben fĂźr die Vorhersage
des Stimmenanteil p einer Partei mit PrĂ¤misse

Pr Fehler fĂźr Vorhersage von p maximal 0,01 > 0.95.

Seien X1 , . . . , X n unabhĂ¤ngige Zufallsvariablen mit den Werten 0 (gegen Partei), 1 (fĂźr Partei)
mit Wahrscheinlichkeiten 1 â p bzw. p. Dann ist Y = n1 (X1 + Âˇ Âˇ Âˇ + X n ) die Zufallsvariable, die
den Stimmenateil in n Stichproben ergibt. Es gilt E [Y ] = n1 Âˇ np = p und
Var [Y ] =

n
Var [X1 ] p(1 â p)
1 X
1
Var [X i ] =
=
6 .
2
n
n
n
n
i=1

Nun wollen wir ein n bestimmen, so dass
Pr {|Y â p| > ÇŤ } 6 Î´
mit ÇŤ = 0, 01 = 10â2 und Î´ = 0, 05 gilt. Aus Tschebyschev-Ungleichung folgt
Pr {|Y â p| > ÇŤ } 6

1
Var [Y ]
=
.
2
ÇŤ
4nÇŤ 2

Da diese Wahrscheinlichkeit nicht grĂśĂer als Î´ = 0, 05 sein darf, folgt die Bedingung
n>

1
1
100
=
= 10000 Âˇ
= 50000.
2
â2
2
4ÇŤ Î´ 4(10 ) (5/100)
4Âˇ5

Es reicht also n = 50.000 zu nehmen, um die PrĂ¤misse zu erfĂźllen.
Das starke Gesetz der groĂen Zahlen besagt, dass fĂźr eine unendliche Folge von Zufallsvariablen
X1 , X2 , X3 , . . ., die unabhĂ¤ngig und identisch verteilt sind sowie den selben Erwartungswert Âľ haben,
gilt:


X1 + Âˇ Âˇ Âˇ + X n
Pr lim
= Âľ =1
nââ
n
d.h. die reprĂ¤sentative Stichprobe konvergiert fast sicher gegen Âľ.

4.11.3 Chernoff-Ungleichungen
In Ăźblicher (kontinuerlicher) Stochastik ist das sogenannte âCentral Limit Theoremâ von groĂer Bedeutung. In der Informatik aber benutzt man stattdessen die Chernoff-Ungleichungen. Diese Ungleichungen sind SpezialfĂ¤lle der Markoff Ungleichung, angewandt auf Summen von unabhĂ¤ngigen 0-1
Zufallsvariablen.
Die sogenannte âMurphy-Regelâ (Murphyâs Rule) besagt: Wenn man erwartet, dass einige Sachen
schief gehen kĂśnnten, dann wird mit Sicherheit irgendetwas schief gehen. Der folgende Satz formalisiert die Regel.
Satz 4.71. Seien A1 , A2 , . . . , An unabhĂ¤ngige Ereignisse, und X sei die Anzahl der Ereignisse die
tatsĂ¤chlich vorkommen. Die Wahrscheinlichkeit, dass keines der Ereignisse vorkommen wird, ist 6
eâE[X] , d.h.
Pr {X = 0} 6 eâE[X]
ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT

213

Beweis. Sei X i die Indikatorvariable fĂźr das i-te Ereignis, i = 1, . . . , n. Dann ist X = X1 +X2 +. . .+X n .
Es gilt:


Pr {X = 0} = Pr A1 âŞ A2 âŞ . . . âŞ An

= Pr A1 âŠ A2 âŠ . . . âŠ An
n
Y

=
Pr Ai
=

6

i=1
n
Y

i=1
n
Y
â

=e

eâPr { A i }

Pn

i=1 E[X i ]

âE[X]

=e

(De Morgan-Regel)
(UnabhĂ¤nigkeit von Ai âs)

(1 â Pr { Ai })

i=1
Pn
â i=1
Pr { A i }

=e

(Definition von X )

(da 1 + x 6 e x fĂźr alle x â R gilt)
(Algebra von Exponenten)
(Erwartungswert der Indikatorvariablen)
(LinearitĂ¤t des Erwartungswertes)



â˛ Beispiel 4.72 : Wir konstruieren einen Mikroprozessor und wissen, dass jeder Transistor nur mit
Wahrscheinlichkeit 10â5 beschĂ¤digt sein kann. Das klingt gut. Aber heutzutage enthĂ¤lt ein Mikroprozessor ca. 106 (und sogar mehr) Transistoren. Deshalb ist die erwartete Anzahl der beschĂ¤digten Transistoren in unserem Mikrochip gleich 10. Laut Satz 4.71 wird der Mikroprozessor nur
mit Wahrscheinlichkeit eâ10 (kleiner als 1 zu 3 Millionen!) defekt-frei sein!

Der Satz oben sagt Folgendes: Wenn E [X ] die erwartete Anzahl der tatsĂ¤chlich vorkommenden Ereignisse aus A1 , A2 , . . . , An ist, dann wird mit Wahrscheinlichkeit Pr {X > 1} > 1 â eâE[X] mindestens
eines der Ereignisse vorkommen. Nun betrachten wir den allgemeinen Fall: Wie groĂ ist die Wahrscheinlichkeit Pr {X > k}? Die Antwort ist mit folgendem Satz gegeben:
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

214

Satz 4.73. (Chernoffâs Ungleichungen) Seien X1 , . . . , X n unabhĂ¤ngige Bernoulli-Variablen mit
Pr {X i = 1} = pi und sei X = X1 + . . . + X n . Sei Âľ = E [X ] = p1 + . . . + pn . Dann gilt:
1. fĂźr jedes Î´ > 0
Pr {X > (1 + Î´) Âľ} 6 F (Âľ, Î´) mit F (Âľ, Î´) :=
2. fĂźr jedes Î´ > 2e â 1



eÎ´
(1 + Î´) 1+Î´

Pr {X > (1 + Î´) Âľ} 6 2â(1+Î´) Âľ

Âľ

(4.11)

(4.12)

3. fĂźr jedes 0 < Î´ < 1
Pr {X > (1 + Î´) Âľ} 6 eâÂľÎ´

2 /3

(4.13)

4. fĂźr jedes R > Âľ
Pr {X > R} 6 e(1âln

R
Âľ ) ÂˇRâÂľ

(4.14)

5. fĂźr jedes 0 < Î´ < 1
Pr {X < (1 â Î´) Âľ} 6 eâÂľÎ´

2 /2

(4.15)

Beweis. Wir beweisen nur die ersten vier Ungleichungen (der Beweis der letzten ist analog). Markovâs
Ungleichung gibt uns fĂźr jedes a > 0



Pr {X > t} = Pr ea ÂˇX > ea Âˇt 6 eâa Âˇt Âˇ E ea ÂˇX ,
wobei (wegen der (UnabgĂ¤ngigkeit von X i âs)

" n
#
n
h Pn
i
Y
Y
 a ÂˇX 


a Âˇ i=1 X i
a ÂˇX i
E e
=E e
e
E ea ÂˇX i
=E
=
i=1

i=1

gilt. Wenn wir die Parameter t und a auf
t := (1 + Î´) Âľ
a := ln(1 + Î´)
setzen und die Ungleichung


E (1 + Î´) X i = pi (1 + Î´) + (1 â pi ) = 1 + Î´ Âˇ pi 6 eÎ´ Âˇp i

benutzen, dann bekommen wir

Pr {X > (1 + Î´) Âˇ Âľ} 6 (1 + Î´) â(1+Î´) ÂˇÂľ Âˇ

n
Y


E (1 + Î´) X i ,
i=1

wobei
n
n
Y
Y
Pn


E (1 + Î´) X i 6
eÎ´ Âˇp i = e i=1 Î´ Âˇp i = eÎ´ Âľ
i=1

i=1

ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT

215

ist, und die Ungleichung (4.11) folgt.
Die Ungleichung (4.12) folgt unmittelbar aus (4.11), da fĂźr Î´ > 2e â 1 gilt
(1 + Î´) 1+Î´ > (2e) 1+Î´ > 21+Î´ eÎ´ .
Um (4.13) zu zeigen, reicht es zu zeigen, dass fĂźr alle 0 < x < 1 die Ungleichung
F (Âľ, x) = e Âľ(xâ(1+x) ln(1+x)) 6 eâÂľ x

2 /3

oder Ă¤quivalent, dass fĂźr alle 0 < x < 1 die Ungleichung f (x) 6 0 mit
f (x) := x â (1 + x) ln(1 + x) + x 2 /3
gilt. DafĂźr berechnen wir die Ableitungen von f (x):
1+x
2
2
â ln(1 + x) + x = â ln(1 + x) + x
1+x
3
3
1
2
f â˛â˛ (x) = â
+ .
1+x 3
f â˛ (x) = 1 â

Wir sehen, dass f â˛â˛ (x) < 0 fĂźr 0 6 x < 1/2 und f â˛â˛ (x) > 0 fĂźr x > 1/2. Das bedeutet, dass f â˛ (x) im
Interval [0, 1] zuerst fĂ¤llt und dann wĂ¤chst. Da f â˛ (0) = 0 und f â˛ (1) < 0, muss f â˛ (x) 6 0 in ganzem
Interval [0, 1] gelten, woraus (wegen f (0) = 0) f (x) 6 0 fĂźr alle x in diesem Interval folgt.
Um (4.14) zu zeigen, reicht es Î´ := R/Âľ â 1 in (4.11) zu nehmen:21

Âľ
eÎ´
Pr {X > R} = Pr {X > (1 + Î´) Âľ} 6
(1 + Î´) 1+Î´
e RâÂľ
=
(R/Âľ) R
= e(1âln

R
Âľ ) ÂˇRâÂľ

.


Eine schĂśne Eigenschaft der Chernoff-Ungleichung ist die Tatsache, dass wir weder die Wahrscheinlichkeiten von einzelnen Ereignissen Ai noch ihre Anzahl n wissen brauchen: Es reicht nur zu wissen,
dass die Ereignisse unabhĂ¤ngig sind und wie die erwartete Anzahl Âľ = E [X ] der tatsĂ¤chlich vorkommenden Ereignisse aussieht.



Der grĂśĂte Nachteil dieser Ungleichungen ist aber die Tatsache, dass die Ereignisse unabhĂ¤ngig sein mĂźssen!

â˛ Beispiel 4.74 : (Experimentelle Bemessung des Erwartungswertes) Wir nehmen an, dass ein
Zufallsexperiment X vorliegt, dessen Erwartungswert Âľ = E [X ] wir experimentell messen
mĂśchten. (Wir wissen die eigentliche Verteilung von X nicht.) Unser âToleranzintervallâ ist
T = [ÂľâÎ´, Âľ+Î´]; je kleiner Î´ ist desto besser. Die Tschebyschev Ungleichung liefert das Ergebnis
Pr {X < T } = Pr {|X â E [X ] | > Î´} 6 ÇŤ :=

Var [X ]
.
Î´2

21Beachte, dass Î´ > 0 gilt, da R > Âľ gefĂśrdert wird.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

216

Wenn aber Î´ klein ist oder wenn unser Experiment instabil ist (d.h. die Varianz von X groĂ ist),
dann sind wir verloren. Wirklich? Nein, wir kĂśnnen âboostenâ, d.h. das Experiment n mal wiederholen. Sei X i das Ergebnis des i-ten Experiments. Wir betrachten den arithmetischen Mittel
Y =

X1 + X2 + . . . + X n
n

und beachten, dass E [Y ] = E [X ] und Var [Y ] = Var [X ] /n gilt (siehe Beweis von Satz 4.69).
Die Tschebyschev Ungleichung liefert jetzt die AbschĂ¤tzung
Pr {Y < T } = Pr {|Y â E[Y ]| > Î´} 6

Var [Y ] Var [X ] 1
=
= ÂˇÇŤ
Î´2
n Âˇ Î´2
n

und groĂe Abweichungen vom Erwartungswert sind n mal unwahrscheinlicher geworden.
Die Fehlerwahrscheinlichkeit fĂ¤hlt also proportional zu n. Wir kĂśnnen aber diese Wahrscheinlichkeit noch scheller gegen 0 treiben, wenn wir anstatt des arithmetischen Mittels den Median
nehmen.22 Sei also
M = der Median von X1 , . . . , X n .
Wir wissen bereits, dass X mit Wahrscheinlichkeit mindestens p := 1 â ÇŤ = 1 â Var [X ] /Î´2 in
T liegt. Wenn aber der Median M auĂerhalb des Toleranzintervalls liegt, dann liegen mindestens
n/2 EinzelschĂ¤tzungen auĂerhalb. Wenn also Zi die
fĂźr das Ereignis âX i < Tâ
PIndikatorvariable
n
ist, dann impliziert M < T, dass die Summe Z = i=1
Zi mindestens n/2 sein muss. Aber nur
E [Z] 6 (1 â p) Âˇ n = Îľ Âˇ n auĂerhalb liegende EinzelschĂ¤tzungen zu erwarten sind. Wenn wir also
Î˛ = (1 â 2 Âˇ Îľ)/(2Îľ) wĂ¤hlen, dann gilt (1 + Î˛) Âˇ Îľ Âˇ n = n/2. Wir wenden die Chernoff Ungleichung
an und erhalten
Pr {M < T } 6 Pr { Z > n/2} 6 Pr {Z > (1 + Î˛)E [Z]}
6 eâÎľ Âˇn ÂˇÎ˛

2 /3

6 eââŚ(Îľ Âˇn)

und die Fehlerwahrscheinlichkeit in diesem Fall fĂ¤llt sogar negativ exponentiell.
Im Satz 4.73 ist die unabhĂ¤ngigkeit der Zufallsvariablen sehr wichtig. Trotzdem kann man eine Ă¤hnliche Schranke auf fĂźr Summen von abhĂ¤ngigen Zufallsvariablen zeigen.
Satz 4.75.
PnSeien Y1 , . . . ,Yn (nicht unbedingt unabhĂ¤ngigege!) binomiell verteilte Zufallsvariablen,
und Y = i=1
Yi . Sei Âľ = E [Y ]. Dann gilt fĂźr jedes B > Âľ
Pr {Y > B} 6 n Âˇ e

(1 â ln

B
B
)Âˇ
nÂľ n

Beweis. Aus Y > B folgt Yi > B/n fĂźr mindestens ein i. Wir kĂśnnen also (4.14) benutzen und
erhalten
B

B

Pr {Y > B} 6 n Âˇ max Pr {Yi > B/n} 6 n Âˇ e(1âln n Âľ ) Âˇ n .
i


22Der Median einer Menge S von |S| = n Zahlen (n ungerade) ist die ân/2-grĂśĂte Zahlâ x â S. D.h. es muss |{y â
S : y 6 x}| = |{z â S : x 6 z}| gelten. Ist z.B. S = {1, 3, 8, 10, 1000}, so ist x = 8 der Median.
ÂŠ 2003 by S. Jukna

4.11. ABWEICHUNG VOM ERWARTUNGSWERT

217

Die oben erwĂ¤hnten Chernoff-Ungleichungen betrachten nur Bernoulli-Variablen, die nur die Werte 0
oder 1 annehmen kĂśnnen. Zum Schluss geben wir (ohne Beweis) noch eine allgemeinere Version der
Chernoff-Ungleichung an.
Satz 4.76. Seien X1 , . . . , X n unabhĂ¤ngige Zufallsvariablen mit 0 6 X i 6 1. Sei X =
gilt:

â
2
Pr X â E [X ] > c n 6 eâc /2 .

Pn

i=1

X i . Dann

â˛ Beispiel 4.77 : (Job Scheduling) Wir wollen n Jobs auf m gleich schellen Prozessoren aufteilen.
Die LĂ¤nge (=Abfertigungszeit) des i-ten Jobs ist irgendeine Zahl L i im Intervall [0, 1]. Wir wollen
Jobs so verteilen, dass keiner der Prozessoren viel lĂ¤nger als die Durchschnittsbelastung
n

1 X
Li
L=
m
i=1

aller Prozessoren belastet wird. Eine optimale Verteilung zu finden ist sehr schwer. Das Problem
ist noch schwieriger, wenn wir die JoblĂ¤ngen L i im Voraus nicht kennen.
Stattdessen wenden wir die folgende einfachste ârandomisierteâ Strategie an: FĂźr jeden Job i
wĂ¤hlen wir rein zufĂ¤llig einen Prozessor j und weisen i dem Prozessor j zu. Es wird sich herausstellen, dass diese âdumme Affenstrategieâ eigentlich nicht so schlecht ist, auch wenn wir weder
die Anzahl n der Jobs noch ihre Laufzeiten kennen!
Zuerst betrachten wir einen beliebigen (aber festen) Prozessor j â {1, . . . , m}. FĂźr diesen Prozessor sei X i die Zeit, die der Prozessor braucht, um den i-ten Job abzufertigen, d.h.

Li
falls der i-te Job dem Prozessor j zugewiesen war
Xi =
0 sonst.
Pn
Die gesamte Laufzeit des Prozessors j ist also X = i=1
X i . Da jeder der n Jobs dem Prozessor
j mit gleicher Wahrscheinlichkeit 1/m zugewiesen wird, ist die erwartete Laufzeit des Prozessors
genau
n
n
X
X
1
Âˇ L i = L.
E [X ] =
E [X i ] =
m
i=1

Nach Satz 4.76 haben wir

i=1


â
2
Pr X > L + c n 6 eâc /2

2

Damit wissen wir, dass jeder einzelne Prozessor nur mit Wahrscheinlichkeit 6 eâc /2 lĂ¤nger als
â
L + c n beschĂ¤ftigt sein wird. Da wir insgesamt nur m Prozessoren haben, ist die Wahrscheinâ
lichkeit, dass mindestens ein Prozessor lĂ¤nger als L + c n belĂ¤stigt sein wird, nicht grĂśĂer als
2
m Âˇ eâc /2 . Also gilt:

â
2
Pr Kein Prozessor wird lĂ¤nger als L + c n laufen > 1 â m Âˇ eâc /2
FĂźr Summen von unabhĂ¤ngigen Âą1-wertigen Zufallsvariablen hat Chernoff-Ungleichung die folgende
Form:
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

218

Satz 4.78. Seien X1 , . . . , X n unahĂ¤ngige Âą1-wertige Zufallsvariablen mit Pr {X i = â1}
Pr {X i = +1} = 1/2. Dann gilt fĂźr jedes Îť > 0:
Pr {|X1 + . . . + X n | > Îť} 6 2eâÎť

=

2 /2n

Welche der drei Ungleichungen (Markov, Tschebyschev oder Chernoff) ist besser? NatĂźrlich die
Chernoff-Ungleichung, da sie eine exponentiell kleine obere Schranke gibt. Man muss aber beachten, dass diese Ungleichung nur fĂźr Zufallsvariablen X gilt, die Summen unabhĂ¤ngiger BernoulliVariablen sind, wĂ¤hrend es fĂźr Tschebyschev reicht, dass X 2 einen endlichen Erwartungswert hat.
SchlieĂlich, reicht es fĂźr Markov, dass X nicht negativ ist. Zusammengefasst:
ďŁą
1
ďŁ´
Markov, wenn X > 0
ďŁ´
ďŁ´
ďŁ´
1+ÇŤ
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁ˛ Var [X ]
 2
Tschebyschev,
wenn
E
X <â
Pr {X > (1 + ÇŤ )E [X ]} 6
2 E [X ]2
ÇŤ
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁ´
âÇŤ 2 E[X]/3
ďŁ´
ďŁ´
Chernoff, wenn X = X1 + . . . + X n ,
ďŁ´ e
ďŁł
X i Bernoulli und unabhĂ¤ngig

4.12 Das Urnenmodel â Hashingâ

Das sogenannte âWortebuchsproblemâ in der Informatik ist folgende. Wir haben eine sehr grĂśĂe Menge (das Universum) von mĂśglichen Dateien (z.B. alle mĂśgliche Namen). Wir wollen eine beliebige
(aber relativ kleine) m-elementige Teilmengen S â U so abschpeichern zu kĂśnnen, dass die Frage âist
x â Sâ fĂźr beliebiges x â U schnell beantwortent lĂ¤Ăt. Das âHashingâ-Verfahren nimmt ein Array mit
n (n > m) Zellen, wĂ¤hhlt eine ZufĂ¤llige Hashfunktion h : U â {1, . . . , n} und speichert jedes x â S
in der Zelle h(x). Da |U | viel grĂśĂer als Anzahl n der Zellen ist, wird es bestimmt Zellen i geben, in
deren sehr viele (mindestens |U |/n) Elementen des Universums abgebildet werden. Dann wĂ¤re eine
feste Hashfunktion h fĂźr Teilmengen S â hâ1 (i) nutzloss. Um das zu vermeiden, wĂ¤hlt man deshalb
die Hashfunction h zufĂ¤llig aus.
Dieses Verfahrenâwie auch viele andere stochastische Prozesseâlassen sich gut in einem sogenannten âUrnenmodelâ darstellen.
Wir haben m Kugeln und n Urnen, und werfen jede Kugel zufĂ¤llig und unabhĂ¤ngig in diese Urnen.
Jede Kugel kann mit gleicher Wahrscheinlichkeit 1/n in jede der n Urnen landen. Also
m = Anzahl der Kugeln
n = Anzahl der Urnen

1
= Pr eine Kugel flieg in eine bestimmte Urne
n

Man kann dann verschiedene Fragen stellen. Zum Beispiel, wie viele Kugeln werden (im Durchschnitt) in einer bestimmten Urne landen, wie viele Urnen (im Durchschnitt) werden leer bleiben,
usw. Mit unseren jetztigen Kentnissen kĂśnnen wir solche Fragen ziemlich leicht beantworten.
Erwartete Anzahl der Kugeln in einer Urne Sei
X = Anzahl der Kugel in der ersten Urne.
ÂŠ 2003 by S. Jukna

4.12. DAS URNENMODEL â HASHINGâ

219

Dann ist X = X1 + . . . + X m wobei X i die Indikatorvariable fĂźr das Ereignis âi-te Kugel fliegt in die
erste Urneâ ist. Die LinearitĂ¤t des Erwartungswertes ergibt:
E [X ] =

m
X
i=1

E [X i ] =

m
X
1
i=1

n

=

m
.
n

Erwartete Anzahl der Urnen mit genau einer Kugel Sei nun
Y = Anzahl der Urnen mit genau einer Kugel
Dann ist Y = Y1 + . . . + Yn wobei Yj die Indikatorvariable fĂźr das Ereignis â j-te Urne enthĂ¤lt genau
eine Kugelâ. Das Ereignis Yj = 1 tritt genau dann ein, wenn eine der m Kugeln in die j-te Urne fliegt
und die verbleibenden m â 1 Kugeln diese Urne vermeiden. Deshalb gilt
 

E Yj = Pr Yj = 1
m
X

=
Pr nur Kugel i fliegt in Urne j
i=1

1
= mÂˇ
n

und somit auch



1 mâ1
1â
n


mâ1
n
X
 
1
E [Y ] =
E Yj = m 1 â
âź meâ(mâ1)/n .
n
j=1

Erwartete Anzahl der Wurfe bis eine leere Urne getroffen wird Angenommen, k Urnen sind
bereits besetzt (enhalten mindestens eine Kugel). Wie lange mĂźssen wir dann noch werfen bis die
Kugel in eine leere Urne fliegt? Sei Tk die enstprechende Zufallsvaiable,
Tk = Anzahl der Versuche bis eine Kugel in eine leere Urne fliegt
Dann ist
E [Tk ] =
=

â
X
i=0
â
X
i=0

=

(Satz 4.56)


Pr alle ersten i Kugeln fliegen in besetzten Urnen

â  i
X
k
i=0

=

Pr {Anzahl der Versuche > i}

n

1
n
=
1 â k/n n â k

(geometrische Reihe)

Man kann auch anders Ăźberlegen. Die Wahrscheinlichkeit eine leere Urne zu treffen ist p = (n â
k)/n. Das ist also ein Bernoulli-Experiment mit der Erfolgswahrscheinlichkeit p, und wir wollen die
erwartete Anzahl E [Tk ] der Versuche bis zum erstem Erfolg bestimmen. Wir werden bald sehen (im
Abschnitt 4.10), dass Tk eine geometrisch verteillte Zufallsvariable ist und solche Zufallsvariablen
den Erwartungswert 1/p = (n â k)/n haben.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

220

Das Coupon Collector Problem Es gibt eine Serie von n Sammelbildern; in jede Runde kauft
ein Sammler rein zufĂ¤llig ein Bild. Was ist die erwartete Anzahl der Runden, bis der Sammler alle
n Bilder hat? Dieses Problem ist als âCoupon Collector Problemâ bekannt. Diese Frage kann man
wiederum in einem Urnenmodel stellen. Kugeln sind nun die Runden und Urnen sind die Bilder. Die
Frage ist, wieviel Kugeln mĂźssen wir werfen, bis es keine Urne leer bleibt? Sei
X = Anzahl der Versuche bis keine Urne leer wird.
Um E [X ] zu berechnen, summieren wir fĂźr alle i = 1, . . . , n die erwartete Anzahl der Versuche, bis
der Ball erstmals in i-te Urne fliegt:
E [X ] =

nâ1
X

E [Tk ] =

k=0

= nÂˇ

k=0

n
X
1
j=1

nâ1
X

j

nâ1

X 1
n
=nÂˇ
nâk
nâk
k=0

= nHn (harmonische Reihe)

â n ln n.
Man kann auch anders Ăźberlegen. Sei

Z = die Anzahl der leeren Urnen
Dann ist Z = Z1 + . . . + Z n , wobei Z j die Indikatorvariable fĂźr das Ereignis â j-te Urne bleibt leerâ
ist. Nun haben wir


 

1 m
E Z j = Pr Z j = 1 = 1 â
n

da jede Kugel die j-te Urne mit Wahrscheinlichkeit 1 â 1/n vermeidet und Z j = 1 genau dann, wenn
alle m Kugeln dies tun. Damit ist


n
X
 
1 m
E [Z] =
E Zj = n 1 â
âź n Âˇ eâm/n
n
j=1

Wenn wir also m > n ln n Kugeln in n Urnen werfen, dann kann man keine leere Urne mehr erwarten:
In diesem Fall ist E [Z] âź n Âˇ eâm/n < n Âˇ eâ ln n = 1.
Anzahl der âĂźberfĂźlltenâ Urnen Wir haben bereits m BĂ¤ller in n Urnen geworfen. Wir sagen, dass
eine Urne ĂźberfĂźllt ist, falls sie mehr als k Kugeln enthĂ¤lt (k ist ein Parameter). FĂźr welche k wird es
im Duchschnitt keine ĂźberfĂźllte Urnen geben? Einfachheitshalber betrachten wir nur den Fall m = n
(genauso viele Kugeln wie Urnen).
Dazu betrachten wir die Zufallsvariable X = X1 + Âˇ Âˇ Âˇ + X n , wobei X i die Indikatorvariable fĂźr das
Ereignis âi-te Urne hat mehr als k BĂ¤llerâ ist. Dann ist X genau die Anzahl der ĂźberfĂźllten Urnen.
Wir vollen ein k bestimmen, fĂźr das E [X ] < 1 gilt. Dazu betrachten wir die Ereignisse
Ai, j = âi-te Urne enthĂ¤lt genau j Kugelnâ


Dann ist Pr Ai, j gleich die Anzahl nj der MĂśglichkeiten, die j BĂ¤ller auszuwĂ¤hlen, mal die Wahrj
nâ j
scheinlichkeit n1 , dass alle diese BĂ¤lle in Urne i fliegen. mal die Wahrscheinlichkeit 1 â n1
,
ÂŠ 2003 by S. Jukna

4.12. DAS URNENMODEL â HASHINGâ

221

dass keiner der verbleibenden n â j BĂ¤ller in diese Urne fliegt:
   j 


n
1
1 nâ j
Pr Ai, j
=
1â
n
n
j
   j
n
1
6
j
n
 j  j  j
ne
e
1
6
=
j
n
j
und damit auch
E [X i ] = Pr {X i = 1} =

n
X
j=k

n
X

Pr Ai, j 6
j=k


 j   
e k
e
e  e 2
6
+ ÂˇÂˇÂˇ .
1+ +
j
k
k
k

FĂźr k â â kĂśnnen wir die Summe in Klammern ignorieren (sie strebt gegen 1) und wir haben eine
(asymptotische) ungleichung
n
 e k
X
E [X ] =
E [X i ] 6 n Âˇ
.
k
i=1

Es reicht also k so auszuwĂ¤hlen, dass
k (1 â ln k) < â ln n gelten. Falls wir


e k

k

< 1/n gilt. Nach der Logaritmieren, muss die Ungleichung
k :=

ln n
ln ln n

wĂ¤hlen, dann gilt:
ln n
(1 â ln ln n + ln ln ln n) âź â ln n.
ln ln n
Also, wenn wir n Kugeln in n Urnen werfen, dann kĂśnnen wir erwarten, dass keine Urne mehr als ln n
Kugelt enthalten wird.
k (1 â ln k) =

Mehrfaches hashing â Bloom-Filter Um den Speicherplatz (= Anzahl n der Speicherzellen = Anzahl der benĂśtigten Urnen) zu sparen, benutzt man oft Hashing mit mehreren Hashfunctionen. Das
entsprechende Verfahren ist als âBloom-Filterâ bekannt und hat seit 1970 viele Anwandungen gefunden (das Program ispel ist nur ein Beispiel).
Sei U ein Universum und sei H die Menge aller Funktionen, die U auf die Menge {1, . . . , n} abbilden.
Ein Bloom-Filter reprĂ¤sentiert eine Menge S â U durch ein Booleâsches Array B der LĂ¤nge n und
benutzt dazu k rein zufĂ¤llig und unabhĂ¤ngig voneinander gewĂ¤hlten Hashfunktionen h1 , . . . , hk â H :
AnfĂ¤nglich ist S = â und alle Zellen von B sind auf Null gesetzt. Ein Element x â
U wird in die Menge S eingefĂźgt, indem das Array B nacheinander an den Stellen
h1 (x), . . . , hk (x) auf Eins gesetzt wird.
Um nachzuprĂźfen, ob x ein Element von S ist, wird B an den Stellen h1 (x), . . . , hk (x)
ĂźberprĂźft. Wenn B an allen Stellen den Wert 1 besitzt, dann wird die Vermutung âx â Sâ
ausgegeben und ansonsten wird die definitive Ausgabe âx < Sâ getroffen.
Offensichtlich erhalten wir konventionelles Hashing aus Bloom-Filtern fĂźr k = 1. Wir beachten, dass
eine negative Antwort auf eine âist x â S?â Anfrage stets richtig ist. Eine positive Antwort kann
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

222

allerdings falsch sein und Bloom-Filter produzieren damit âfalsche Positiveâ. D.h. x â U ist ein
falsches Positives, wenn x < S, aber der Filter mit âJaâ geantwortet hat. Wann passiert das? Wenn in
alle k Zellen h1 (x), . . . , hk (x) irgendwelche Elemente y , x aus S gehasht sind.
Wie groĂ ist die Wahrscheinlichkeit p+ einer falschen positiven Antwort, wenn eine Menge S der
GrĂśĂe |S| = s durch ein Booleâsches Array der GrĂśĂe n mit Hilfe von k zufĂ¤llig aus H gewĂ¤hlten
Hashfunktionen reprĂ¤sentiert wird? Die AbschĂ¤tzung p+ 6 P mit
P :=



1
1â 1â
n

k s !k

war von Bloom angegeben und ist seit 1970 mehmals in der Literatur wiederholt. Praktiker haben aber
bemerkt, dass mit dieser AbschĂ¤tzung irgendwas nicht stimmt: die tatsĂ¤chliche Wahrscheinlichkeit p+
einer falschen positiven Antwort oft grĂśĂer als Bloomâs AbschĂ¤tzung P war. Und das ist wirklich der
Fall, wie das folgende Beispiel zeigt.
â˛ Beispiel 4.79 : (Gegenbeispiel zur Bloomâs AbschĂ¤tzung) Wir betrachten den Fall wenn n = k =
2 und s = 1. Sei U = {x, y}, S = {x} und h1 , h2 : U â {1, 2}. Element y ist ein falsches Positives
genau dann, wenn beide h1 (y) und h2 (y) in der Menge {h1 (x), h2 (x)} liegen.
Sei Ai das Ereignis âhi (y) â {h1 (x), h2 (x)}â Dann gilt p+ = Pr { A1 âŠ A2 }. Der Wahrscheinlichkeitsraum besteht aus 16 Elementarereignissen:
h1 (x) h2 (x)
1
1
1
2
2
1
2
2

Ă

h1 (y) h2 (y)
1
1
1
2
2
1
2
2

â Wahrscheinlichkeit eine Eins (eine bereits besetzte Zelle) zu erwischen ist
Pr { Ai } =

3 + 3 + 3 + 3 12 3
=
=
16
16 4

Bloom sagt: Pr { A1 âŠ A2 } = Pr { A1 } Âˇ Pr { A2 } =

9
16 .

Das ist aber falsch, da tatsĂ¤chlich gilt
Pr { A1 âŠ A2 } =



3 + 2 + 2 + 3 10
=
.
16
16

Warum passiert das? Da Pr { A1 âŠ A2 } , Pr { A1 } Âˇ Pr { A2 }, d.h. die Ereignisse nicht unabhĂ¤ngig
sind!
Teufel steckt oft in Deteils! Stochastische AbhĂ¤ngigkeit von Ereignissen ist oft nicht offensichtlich und man soll damit vorsichtig umgehen.

Wie soll aber eine richtige AbschĂ¤tzung fĂźr p+ aussehen? Um diese Frage zu beantworten, kĂśnnen
wir wiederum das Urnen-Model benutzen.
ÂŠ 2003 by S. Jukna

4.12. DAS URNENMODEL â HASHINGâ

223

Wir werfen m = k s blauer BĂ¤ller23 in n Urnen mit

Pr Ball b fliegt in j-te Urne = 1/n

fĂźr alle BĂ¤ller b und alle Urnen j. Wir sagen, dass eine Urne âblauâ ist, falls sie mindestens einen
blauen Ball enthĂ¤lt.
Danach werfen wir k rote BĂ¤ller.24 Uns unteressiert das Ereignis
A = alle rote BĂ¤ller landen in blauen Urnen.

FĂźr jede Teilmenge der Urnen I â {1, . . . , n} betrachten wir das Ereignis
B I = I ist genau die Menge der blauen Urnen.
Nach der Formel von der totalen Wahrscheinlichkeit gilt
Pr { A} =

X
I

Pr { A|B I } Âˇ Pr {B I } =

m  
X
m
i=1

i

pi qi ,

wobei pi = Pr { A|E I } und qi = Pr {B I } fĂźr eine fixierte Teilmenge I der Urnen mit |I | = i ist. Die
Wahrscheinlichkeiten pi sind leicht zu bestimmen:
 k
i
.
pi = Pr { A|B I } =
n
Was aber mit der Wahrscheinlichkeiten qi = Pr {B I }? Aus der Definition von B I folgt
Pr {B I } =

Anzahl aller surjektiven Abbildungen f : [m] â [i]
.
Anzahl aller Abbildungen f : [m] â [n]

Sei âm (i) die Anzahl aller surjektiven Abbildungen f : [m] â [i]. Dann gilt also
qi =
Damit erhalten wir

âm (i)
.
nm

n    k
1 X n
i
p+ = Pr { A} = m
Âˇ âm (i).
n
n
i
i=1

Ein Ausdruck fĂźr die Zahlen âm (i) ist nicht allzu schwer zu bekommen (Ăbungsaufgabe!):
 
i
X
j i
âm (i) =
(â1)
jm.
j
j=1

In unserem speziellen (im Beispiel betrachteten) Fall haben wir m = k s = 2 und n = 2. Da â2 (1) = 1
und â2 (2) = 2, ergibt das
2    2
1 X 2
i
Pr { A} =
Âˇ â2 (i)
2
2
i
2
i=1
 2
1
1
1
1 1 5 10
=
Âˇ2Âˇ
Âˇ 1 + Âˇ 1 Âˇ (1) 2 Âˇ 2 = + = = .
4
2
4
8 2 8 16
23Jeder Wurf entspricht einem der Werte hi (x) fĂźr x â S und i â [k] = {1, . . . , k}.
24Jeder solcher Wurf entspricht einem der Werte hi (y) fĂźr i â [k].
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

224

4.13 Bedingter Erwartungswertâ
Definition: Sei X eine Zufallsvariable und A ein Ereignis. Der bedingte Erwartungswert E [X | A]
von X unter der Bedingung A ist definiert durch:
X
E [X | A] =
x Âˇ Pr {X = x | A } .
x

Wegen Pr {X = x | A } 6 Pr {X = x} /Pr { A} ist mit dem Erwartungswert E [X ] auch E [X | A] wohldefiniert und endlich (natĂźrlich nur wenn Pr { A} , 0 gilt).

â˛ Beispiel 4.80 : Wir wĂźrfeln einmal einen SpielwĂźrfel und X sei die gewĂźrfelte Augenzahl. Was ist
dann E [X |X > 4] ?

E [X |X > 4] =

6
X
i=1

i Âˇ Pr {X = i | X > 4 } =

6
X
i=4

iÂˇ

6

6

i=4

i=4

X (1/6) X 1
Pr {X = i}
=
=
iÂˇ
i Âˇ = 5.
Pr {X > 4}
(1/2)
3

Beachte, dass in diesem Fall der (unbedingte) Erwartungswert viel kleiner ist: E [X ] =
3, 5.

P6

1
i=4 iÂˇ 6

=

Der bedingte Erwartungswert E [X | A] ist einfach der Erwartungswert von X in einem anderen Wahrscheinlichkeitsraum, wo die Wahrscheinlichkeiten durch das Ereignis A bestimmt sind. Deshalb gelten
fĂźr E [X | A] dieselbe Regeln wie fĂźr E [X ]. Insbesondere gilt der LinearitĂ¤tssatz (Satz 4.46) auch fĂźr
E [X | A] .
Wo wir vom bedingten Erwartungswert profitieren kĂśnnen, ist die Tatsache, dass er oft ermĂśglicht,
komplizierte Berechnungen von dem Erwartungswert E [X ] auf einfachere FĂ¤lle zu reduzieren.

Satz 4.81. (Regel des totalen Erwartungswertes) Sei X : âŚ â S eine Zufallsvariable mit |S| < â.
Ist A1 , . . . , An eine disjunkte Zerlegung des Wahrscheinlichkeitsraums âŚ, so gilt
E [X ] =

n
X
i=1

Pr { Ai } Âˇ E [X | Ai ]

ÂŠ 2003 by S. Jukna

4.13. BEDINGTER ERWARTUNGSWERTâ

225

Beweis.
E [X ] =

X

x Âˇ Pr {X = x}

X

xÂˇ

x âS

=

x âS

=
=

=

i=1

n
XX
x âS i=1
n X
X
i=1 x âS
n
X
i=1

=

n
X

n
X
i=1

(Definition von E [X ])

Pr { Ai } Âˇ Pr {X = x | Ai }

(totale Warscheinlichkeit)

x Âˇ Pr { Ai } Âˇ Pr {X = x | Ai }
x Âˇ Pr { Ai } Âˇ Pr {X = x | Ai }

Pr { Ai }

X
x

(Umordnung der Summen)

x Âˇ Pr {X = x | Ai }

Pr { Ai } Âˇ E [X | Ai ]

(Definition von E [X | Ai ] )


Sind nun X,Y : âŚ â R zwei Zufallsvariablen,

 so kann man fĂźr jedes y in dem Wertebereich von
Y den bedingten Erwartungswert E X |Y = y von X unter der Bedingung, dass das Ereignis Y = y
eingetroffen ist, betrachten.
Nehmen wir nun an, dass Y nicht fixiert ist. Dann kannman fĂźr jedes
 Elementarereignis Ď â âŚ zuerst
den Wert y = Y (Ď) bestimmen und dann den Wert E X |Y = y berechnen. So bekommt man eine
neue Zufallsvariable, die man mit E [X |Y ] bezeichnet.



Beachte, dass (im Unterschied zu E [X ]) E [X |Y ] keine Zahl
 sonderneine Zufallsvariable ist!
D.h. E [X |Y ] ist eine
f (Y ), die die Werte E X |Y = y fĂźr Y = y annimt. Oder
 Zufallsvariable

anders gesagt, f (y) := E X |Y = y definiert eine Abbildung f : R â R, und E [X |Y ] ist dann die
Abbildung f (Y ) von âŚ nach R.
Um den Wert Z (Ď) dieser neuen Zufallsvariable Z = f (Y ) auf
 Ď â âŚzu bestimmen, bestimmt man
zuerst den Wert y = Y (Ď) und nimmt den Erwartungswert E X |Y = y als den Wert von Z (Ď).

â˛ Beispiel 4.82 : Wir wĂźrfeln einmal zwei SpielwĂźrfel und betrachten die Zufallsvariable X = X1 +
X2 , wobei X i die Augenzahl des i-ten WĂźrfels ist. Um E[X |X1 ] zu bestimmen, berechnen wir
zuerst die entsprechende Abbildung f (y) = E X |X1 = y :


f (y) = E X |X1 = y

=

=

12
X

x Âˇ Pr {X = x | X1 = y }

x=2
y+6
X

x=y+1

x Âˇ Pr {X2 = x â y}

6
X
1
= (6y +
i)
6
i=1

7
= y+ .
2

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

226
Damit ist E [X |X1 ] = X1 + 72 .

â˛ Beispiel 4.83 : Wir wĂźrfeln n mal einen WĂźrfel, und sei X i (1 6 i 6 6) die Anzahl der Wurfe, die
die Augenzahl i ergeben. Dann ist
E [X1 |X6 ]

n â X6
.
5

=

Warum? Wir mĂźssen die Werte von Z := E [X |Y ] auf Elementarereignissen Ď â âŚ bestimmen,
wobei die Elementarereignisse in diesem Fall alle Strings Ď â {1, . . . , 6} n sind. Zuerst bestimmen
wir den Wert y = X6 (Ď). Das ist genau die Anzahl der 6âen in Ď. Wissen wir nun, dass genau y
Wurfe ein â6â ergeben, so ist die Wahrscheinlichkeit, in jeder von n â y verbleibenden Wurfen
ein â1â zu bekommen, gleich 1/5. Damit ist E [X1 |X6 ] = (n â X6 )/5, wie behauptet.
Genauso bekommt man z.B.
E [X1 |X2 , X3 ]

n â X2 â X 3
.
4

=

Satz 4.84. Sind die Zufallsvariablen X und Y unabhĂ¤ngig, so gilt E [X |Y ] = E [X ].
Beweis. Wegen der UnabhĂ¤ngigkeit von X und Y gilt fĂźr jedes y


E X |Y = y

=

X
x

=

X
x

x Âˇ Pr {X = x | Y = y }
x Âˇ Pr {X = x}

(UnbhĂ¤ngigkeit)

= E [X ] .


Da E X |Y = y = E [X ] fĂźr jeden mĂśglichen Wert y von Y gilt, muss auch E [X |Y ] = E [X ] gelten.

â˛ Beispiel 4.85 : Sei X = X1 + X2 die Zufallsvariable aus dem Beispiel 4.82. Da E [X |X1 ] eine
Zufallsvariable ist, kĂśnnen wir den ihre Erwartungswert berechnen:



7
7
E [E [X |X1 ] ] = E X1 +
= E [X1 ] + =
2
2

!
6
1X
7 7 7
i + = + = E [X1 ] + E [X2 ] = E [X ] .
6
2 2 2
i=1

Eine interessante Eigenschaft von Zufallsvariablen Z := E [X |Y ] ist, dass die im vorigen Beispiel
entdekte Eigenschaft auch im Allgemeinen gilt!

Satz 4.86. (Regel vom doppelten Erwartungswert)
E [E [X |Y ] ] = E [X ]
ÂŠ 2003 by S. Jukna

4.13. BEDINGTER ERWARTUNGSWERTâ

227



Beweis. Sei f (y) := E X |Y = y . Dann gilt:


E [E [X |Y ] ] = E f (X )
X
=
f (y)Pr {Y = y}

(Lemma 4.52)

y

=

x Âˇ Pr {X = x | Y = y } Pr {Y = y}

X X

Pr {X = x,Y = y}
xÂˇ
Pr {Y = y}

y

=

x

y

=

x

XX
y

=

x

!



(Def. von f (y) = E X |Y = y )

Pr {Y = y}

x Âˇ Pr {X = x,Y = y}

X X
x
Pr {X = x,Y = y}
x

=

!

X X

X
x

y

xPr {X = x}

(Satz 4.22)

= E [X ] .

Definition: Eine Folge von Zufallsvariablen X0 , X1 , . . . heiĂt ein Martingal, falls fĂźr alle i > 0 gilt:
E [X i+1 |X0 , . . . , X i ] = X i .
â˛ Beispiel 4.87 : Wir haben eine Urne mit b blauen Kugeln und r roten Kugeln. In jedem Schritt wir
ziehen eine Kugel rein zufĂ¤llig und ersetzen diese mit zwei neuen Kugeln von derselben Farbe.
Sei X i die Anteil der roten Kugeln in der Urne nach i Schritten. So ist insbesondare
r
X0 =
.
r+b
Behauptung: X0 , X1 , . . . ist ein Martingal.
Beweis. Sei ni = r + b + i die Gesamtzahl der Kugeln nach i Schritten. Dann ist X i ni die Anzahl
der roten Kugeln nach i Schritten. Es ist klar, dass diese Anzahl nur von der Anzahl der roten
Kugeln nach i â 1 abhĂ¤ngen kann. Also gilt
E [X i+1 |X0 , X1 , . . . , X i ]

= E [X i+1 |X i ]
X i ni + 1
X i ni
= Xi Âˇ
+ (1 â X i ) Âˇ
ni+1
ni+1
Xi
X i ni
=
+
ni+1 ni+1
X i (ni + 1)
=
ni + 1
= Xi .

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

228

Lemma 4.88. Ist X0 , X1 , . . . ein Martingal, so gilt E [X i ] = E [X0 ] fĂźr alle i > 0.
Beweis. Induktion Ăźber i. Es gelte E [X i ] = E [X0 ]. Da wir ein Martingal haben, gilt
X i = E [X i+1 |X0 , . . . , X i ] .
Wir nehmen den Erwartungswert von beiden Seiten und benutzen die Regel von doppelten Erwartungswert (Satzt 4.86);
E [X i ] = E [E [X i+1 |X0 , . . . , X i ] ] = E [X i+1 ] .


4.14 Summen von zufĂ¤lliger LĂ¤nge â Waldâs Theorem
In diesem Abschnitt betrachten wir die folgende Fragestellung:
Wie kann man die erwarteten totalen Kosten eines Schritt-fĂźr-Schritt Zufallsexperiments berechnen, wenn sowohl die Kosten in jedem Schritt wie auch die Anzahl der Schritte von den vorherigen
Ereignissen abhĂ¤ngen kĂśnnen?
â˛ Beispiel 4.89 : Wir wĂźrfeln einen SpielwĂźrfel bis eine 6 rauskommt und summieren die bis dahin
erschienene Augenzahlen. Welchen Erwartungswert hat diese Summe?
Wir denken uns jedes WĂźrfeln als eine Zufallsvariable: fĂźr jedes i = 1, 2, . . . sei X i die im i-ten
WĂźrfeln erschienene Augenzahl. (Wir setzen X i = 0, falls die Augenzahl 6 vor dem Schritt i
ausgewĂźrfelt wurde.) Also nimmt jedes X i die Werte 0, 1, . . . , 6 an. Setze T := min{i : X i = 6}.
Das ist auch eine Zufallsvariable, die die Werte
PT in N+ annimmt. Wir interessieren1 uns also fĂźr die
Zufallsvariable Y := X1 + X2 + . . . + XT = i=1 X i . Wir wissen, dass E [X i ] = 6 (1 + 2 + 3 + 4 +
1
5 + 6) = 3, 5 fĂźr jedes i, und dass25 E [T] = (1/6)
= 6 ist.
Es gibt ein Resultat â Waldsâs Theorem â das uns sofort die Antwort liefert: E [Y ] = 6 Âˇ 3, 5 = 21.
D.h. wenn wir einen SpielwĂźrfel bis eine 6 rauskommt wĂźrfeln, dann wird die erwartete Summe
der insgesammt ausgewĂźrfelten Augenzahlen gleich 21 sein.
Im Allgemeinen haben wir ein System, das Schritt-fĂźr-Schritt funktioniert und in i-tem Schritt die
Kosten X i verursacht. Damit haben wir eine (potentiel unendliche) Folge X1 , X2 , . . . der verursachten
Kosten. Die Kosten sind zufĂ¤llig und kĂśnnen voneinander abhĂ¤ngen. Die Lebensdauer T des Systems
ist auch zufĂ¤llig (und kann auch von bis dahin verursachten Kosten abhĂ¤ngen). Wie bestimmt man in
einer solchen Situation die erwartete Gesamtkosten bis das System stirbt? Wegen so vielen mĂśglichen
AbhĂ¤ngigkeiten, sieht die Frage sehr schwer aus â wir wissen ja nicht, wann das System tatsĂ¤chlich
stirbt.
Zuerst betrachten wir den Fall, wenn (i) die Kosten X i gleichverteilt sind und (ii) die Lebensdauer T
des Systems von der verursachten Kosten unabhĂ¤ngig ist.
Bemerkung 4.90. Oft sagt man âseien X1 , X2 , . . . unabhĂ¤ngige Kopien einer Zufallsvariable X â. Das
bedeutet natĂźrlich nicht, dass alle X i âs eine und dieselbe Zufallsvariable X ist. Unter dessen versteht
25T ist geometrisch verteilt
ÂŠ 2003 by S. Jukna

4.14. SUMMEN VON ZUFĂLLIGER LĂNGE â WALDâS THEOREM

229

man, dass X i âs beliebige Zufallsvariablen sein kĂśnnenâdie einzige Bedingung ist, dass jede von dieser
Variablen X i dieselbe Verteilung wie X haben muss. Zum Beispiel, wenn wir die Gleichverteilung auf
âŚ mit Pr {Ď} = 1/|âŚ| fĂźr alle Ď â âŚ betrachten, dann ist Y : âŚ â S eine Kopie von X : âŚ â S
genau dann, wenn |Y â1 (a)| = |X â1 (a)| fĂźr alle a â S gilt. Es ist klar, dass dann Y â1 (a) = X â1 (a)
nicht unbedingt gelten muĂ!
Satz 4.91. Seien X1 , X2 , . . . unabhĂ¤ngige Kopien einer reellwertigen Zufallsvariable X , und sei T eine
davon unabhĂ¤ngige Zufallsvariable mit den Werten in N und einem endlichen Erwartungswert. Dann
gilt
E [X1 + X2 + . . . + XT ] = E [T] Âˇ E [X ]
Beweis. Sei Y = X1 + X2 + . . . + XT . Wegen der UnabhĂ¤ngigkeit von T und X i âs gilt 26
Pr {Y = y | T = t } = Pr {X1 + . . . + X t = y | T = t }
= Pr {X1 + . . . + X t = y} ,

also
E [Y |T = t] = E [X1 + . . . + X t ] = t Âˇ E [X ] .
Wir wenden die Regel des totalen Erwartungswertes an und erhalten
X
E [Y ] =
Pr {T = t} Âˇ E [Y |T = t]
t

=

X
t

Pr {T = t} Âˇ t Âˇ E [X ]

= E [X ] Âˇ

X
t

t Âˇ Pr {T = t}

= E [X ] Âˇ E [T] .

Im vorigen Satz spielt UnabhĂ¤ngigkeit der Zufallsvariablen eine groĂe Rolle. In dem nĂ¤chsten Satz
spielt dagegen die UnabhĂ¤ngigkeit keine Rolle mehr. Es reicht, dass (i) die Kosten nicht-negativ sind
und (ii) die erwarteten Kosten in jedem Schritt, unter der Bedingung, dass System bis dahin noch lebt,
alle gleich sind.
Satz 4.92. (Waldâs Theorem) Sei X1 , X2 , . . . eine Folge von nicht-negativen Zufallsvariablen und
T : âŚ â N+ eine Zufallsvariable, alle mit endlichen Erwartungswerten, so dass
E [X i |T > i] = Âľ
fĂźr ein festes Âľ â R und alle i > 1 gilt. Dann ist
E [X1 + X2 + . . . + XT ] = Âľ Âˇ E [T]
26Sind die Zufallsvariablen X,Y, Z unabhĂ¤ngig, dann sind auch die Zufallsvariablen X + Y und Z unabhĂ¤ngig. Ăbungsaufgabe!
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

230

Beweis. Sei Ik die Indikatorvariable fĂźr das Ereignis T > k, d.h. Ik = 1 falls der Prozess mindestens
k Schritte lĂ¤uft, und Ik = 0 falls der Prozess vor Zeitpunkt k endet. FĂźr jedes k = 1, 2, . . . gilt:
E [X k Ik ] = E [X k Ik |Ik = 1] Âˇ Pr {Ik = 1} + E [X k Ik |Ik = 0] Âˇ Pr {Ik = 0}

(Satz 4.81 )

= E [X k Âˇ 1 |Ik = 1] Âˇ Pr {Ik = 1} + E [X k Âˇ 0 |Ik = 0] Âˇ Pr {Ik = 0}
= E [X k |Ik = 1] Âˇ Pr {Ik = 1} + 0

= E [X k |T > k] Âˇ Pr {T > k}
= Âľ Âˇ Pr {T > k}

Somit erhalten wir:
â
X
k=1

E [X k Ik ] = Âľ Âˇ

â
X
k=0

Pr {T > k}

= Âľ Âˇ E [T]

(diskretwertige Zufallsvariablen, Satz 4.56)
P
Da Âľ Âˇ E [T] endlich ist und alle E [X k Ik ] nicht-negativ sind, ist die Reihe â
k=1 E [X k Ik ] absolut
konvergent und wir kĂśnnen den Satz 4.58 (unendliche linearitĂ¤t des Erwartungswertes) anwenden:
" â
#
â
X
X
X k Ik =
E [X k Ik ] = Âľ Âˇ E [T]
E [X1 + X2 + . . . + XT ] = E
k=1

k=1


Korollar 4.93. (Waldâs Theorem - einfachste Form) Sei T : âŚ â N+ und seien X1 , X2 , . . . unabhĂ¤ngige Kopien einer nicht-negativen Zufallsvariable X . Sind die Erwartungswerte von T und X
endlich, so gilt
E [X1 + X2 + . . . + XT ] = E [T] Âˇ E [X ]
Beweis. In diesem Fall gilt E [X i |T > i] = E [X1 ] = E [X ], da der erste Versuch jedenfalls statfinden
muss und jeder Versuch X i die gleiche Verteilung wie der erste Versuch X1 hat (falls man zu diesem
Versuch X i Ăźberhaupt kommt!).

â˛ Beispiel 4.94 : (Runs: Anzahl der Versuche) Wir versuchen ein System (z.B. einen Computer) mit
n Komponenten konstruieren. In jedem Schritt setzen wir eine neue Komponente ein. Aber die
Komponenten sind unsicher (âno nameâ Produkte) und jedes Einsetzen kann das ganze System
mit Wahrscheinlichkeit p kaputt machen. Falls dies geschieht, mĂźssen wir die Konstruktion neu
anfangen. Wir nehmen an, dass jede neue Komponente mit gleicher Wahrscheinlichkeit p das
(bereits vorhandene) System zerstĂśren kann. Was ist die erwartete Anzahl der Schritte bis das
System fertig ist?
Wir kĂśnnen die Elementarereignisse als Strings von Einsen und Nullen kodieren, wobei eine
1 heiĂt âdie Komponente war sicher gesetzt, dass System lebt weiterâ, und eine 0 heiĂt âdie
Komponente hat das System zerstĂśrtâ. D.h. wir haben eine Folge Y1 ,Y2 , . . . von unabhĂ¤ngigen
0-1-Zufallsvariablen mit
Pr {Yi = 0} = p

Pr {Yi = 1} = q (= 1 â p)

und die Frage lautet: Wie lange mĂźssen wir warten bis ein Block aus n Einsen kommt?
ÂŠ 2003 by S. Jukna

4.14. SUMMEN VON ZUFĂLLIGER LĂNGE â WALDâS THEOREM

231

Wir kĂśnnen die Folge Y1 ,Y2 , . . . in BlĂścke (Versuche) aufteilen, wobei ein nicht erfogreicher Versuch die Form27 1k 0 mit 0 6 k < n hat; ein erfolgreicher Versuch hat die Form 1n . Zum Beispiel
fĂźr n = 3 hat die Folge 110100111 vier Versuche, wobei die ersten drei nicht erfolgreich sind:
110 |{z}
10 |{z}
0 |{z}
111
|{z}

X1 =3 X2 =2 X3 =1 X4 =3

Wenn wir die LĂ¤nge eines Versuchs mit X bezeichnen, so bekommen wir eine Folge X1 , X2 , . . .
von unahĂ¤ngigen Kopien dieser Zufallsvariable. Uns interessiert also die folgende Zufallsvariable
S := X1 + X2 + . . . + XT
wobei
T := min{i : i-ter Versuch war erfolgreich}.
Waldâs Theorem (Korollar 4.93) sagt, dass dann
E [S] = E [T] Âˇ E [X ]
gilt. Die Zufallsvariable T beschreibt den ersten Versuch, der erfolgreich war. Da die Erfolgswahrscheinlichkeit fĂźr jede eingesetzte Komponente 1 â p ist und ein erfolgreicher Versuch aus n
erfolgreich angesetzten Komponenten besteht, ist die Erfolgswahrscheinlichkeit (1 â p) n . Damit
haben wir 28
1
1
E [T] =
= n
mit q := 1 â p
(1 â p) n
q
Andererseits gilt nach Satz 4.56: 29

E [X ] =

â
X
i=0

Pr {X > i} =

nâ1
X
i=0

Pr {X > i} .

Da30 Pr {X > i} = (1 â p) i = qi , folgt
E [X ] =

nâ1
X

qi =

i=0

1 â qn 1 â qn
=
.
1âq
p

Damit ist die erwartete Anzahl E [S] der Schritte (bis das System fertig ist):
E [S] = E [X ] Âˇ E [T] =


1 â qn 1
1 â qn
1 1
Âˇ n =
=
â
1
.
p
q
pq n
p qn

Zum Beispiel, wenn nur eine 1% Chance besteht, dass eine eingefĂźgte Komponente das System
zerstĂśrt (also p = 0, 01), dann ist die erwartete Anzahl der Schritte fĂźr ein System mit n = 10
Komponenten ungefĂ¤hr 10. FĂźr n = 100 sind das schon ungefĂ¤hr 173 Schritte. Aber fĂźr n = 1000
sind das bereits satte 2.316.257 Schritte! Fazit: Man sollte das System modular ausbauen: 10
Module, je mit 10 Untermodulen, je aus 10 Komponenten.
271k 0 bezeichent eine Folge aus k Einsen mit einer Null am Ende.
28Geometrisch verteilte Zufallsvariable
29Pr {X > n} = 0, da kein einzelner Versuch lĂ¤nger als n Schritte dauert.
30Siehe Abschnitt 4.10.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

232

4.15 Irrfahrten und MarkovâKetten
Ein stochastischer Prozess â als Markov-Kette (oder Irrfahrt oder random walk) bekannt â kommt in
der Physik wie auch in der Informatik ziemlich hĂ¤ufig vor. Die Situation ist folgende.
Wir haben ein Zufallsexperiment X1 , X2 , . . ., das in mehreren Schritten ablĂ¤uft. In jedem Schritt liegt
das Ergebnis des Experiments in einer (uns bekannten) endlichen Menge S (die âZustandsmengeâ).
Die Bedingung ist die sogenannte âGedĂ¤chtnislosigkeitâ des Experiments: In jedem Schritt i hĂ¤ngen die Ereignisse âX i = sâ nur von der Ereignissen âX iâ1 = râ ab. D.h. die Wahrscheinlichkeit,
in welchen Zustand das System (unser Experiment) im i-ten Schritt Ăźbergehen wird, hĂ¤ngt nur von
dem Zustand ab, in dem das System sich gerade (im Schritt i â 1) befindet (und hĂ¤ngt nicht von der
Vergangenheit ab). 31
Die Theorie der Markov-Ketten ist umfangreich. Wir beschrĂ¤nken uns auf ein paar AnsĂ¤tzen und
Beispielen. Einige weitere AnsĂ¤tze basieren sich auf dem MatrizenkalkĂźl und wir werden sie im
Abschnitt 5.13.6 behandeln.
â˛ Beispiel 4.95 : Zwei Spieler (Alice und Bob) spielen das folgende Spiel. ZunĂ¤chst wĂ¤hlt Alice einen
String a â {0, 1} k und Bob einen String b â {0, 1} k . Danach werfen sie eine faire 0-1 MĂźnze bis
einer dieser zwei Strings erscheinnt. Derjenige gewinnt, dessen String als erstes erscheinnt.
Da in jedem Wurf 0 und 1 mit gleicher Wahrscheinlichkeit 12 kommt, sagt uns die âIntuitionâ,
dass die Gewinnchancen auch gleich sein sollten. Die genaue Analyse zeigt aber, 32 dass das nicht
unbedingt der Fall sein soll!
Dazu betrachten wir beispielsweise das Spiel mit k = 3, a = 110 und b = 100. Dieses Spiel kann
man als eine Markov-Kette mit 6 ZustĂ¤nden betrachten:
1
3

0

5

gewinnt Alice

1
0

1

1

2

0
1

4

0

6

gewinnt Bob

Wie groĂ sind die Chanzen, dass Alice gewinnt? Dazu betrachten wir fĂźr alle ZustĂ¤nde i die
Wahrscheinlichkeit pi , ausgehend aus dem Zustand i in den Zustand 5 zu gelangen. Gesucht ist
also p1 . Durch Zerlegung der Wahrscheinlichkeiten nach dem ersten Sprung aus i heraus kĂśnnen
31Eigentlich haben wir Markov-Ketten â die EntscheidungsbĂ¤ume â bereits (schweigend) eingefĂźhrt und benutzt. Das
sind die einfachsten Markov-Ketten: Das system startet in dem Wurzel des Baums, wĂ¤hlt zufĂ¤llig einen Nachbarn, lĂ¤uft
zu diesem Nachbarn, dann wieder wĂ¤hlt zufĂ¤llig einen Nachbarn und lĂ¤uft zu ihm, usw. bis ein Blatt erreicht ist. D.h. das
System bewegt sich nur in einer Richtung, Zyklen sind nicht erlaubt. In allgemeinen Makrov-Ketten, die wir jetzt betrachten,
kann das System sich in beiden Richtungen bewegen.
32Das PhĂ¤nomen selbst ist noch Ăźberraschender: Falls Bob sein String b zuerst auswĂ¤hlen und Alice zeigen muss, dann
kann Alice immer einen String a finden, so dass sie mit grĂśĂerer Wahrscheinlichkeit gewinnen wird! Dieses PhĂ¤nomen ist
unter dem Namen âbest bets for simpletonsâ bekannt. Mehr dazu kann man z.B. in meinem Buch (Abschnitt 24.4) finden.
ÂŠ 2003 by S. Jukna

4.15. IRRFAHRTEN UND MARKOVâKETTEN

233

wir das folgende Gleichungssystem aufstellen:
p1 = p2
1
1
p2 =
p3 + p4
2
2
p3 = 1
1
.
p4 =
4
Durch AuflĂśsen folgt p1 = 5/8. Damit sind Aliceâs Gewinnchanzen um 8/5 > 4/3 groĂer als die
von Bob.

Irrfahrten mit absorbierenden ZustĂ¤nden
In einem Teich befinden sich N + 1 Steine 0, 1, . . . , N. Ein Frosch sitzt anfĂ¤nglich auf irgendeinem
Stein n , 0. Der Stein 0 ist fĂźr den Frosch gefĂ¤hrlich â da steht ein Storch, der sofort den Frosch
fĂ¤ngt, wenn er auf diesen Stein springt.

p
0

. . .

1
1âp

1âp

p

p
nâ1
1âp

p
n

1âp

p

p
. . .

n+1
1âp

111111111
000000000
N
000000000
111111111

1âp

Der Frosch beginnt von Stein n mit Wahrscheinlichkeit p nach rechts und mit mit Wahrscheinlichkeit
q = 1 â p nach links springen. Wenn er den Stein N erreicht hat, bleibt er da fĂźr immer (das ist sein
Zuhause, da ist er sicher). Uns interessiert die Wahrscheinlichkeit
w = Pr {Frosch Ăźberlebt} ,
dass der Frosch den (sicheren) Stein N erreicht.
Beachte, dass die Situation Ă¤quivalent zu dem Casino-Spiel ist, das wir im Abschnitt 3.10 betrachtet
haben (Gamblerâs Ruin). Ein Spieler namens Theo Retiker nimmt in einem Casino an einem Spiel
mit Gewinnwahrscheinlichkeit 0 < p 6 1/2 teil. Zum Beispiel wirft man eine (nicht unbedingt faire)
MĂźnze, dessen Seiten mit rot und blau gefĂ¤rbt sind, und wir gewinnen, falls rot kommt.
Wir nehmen an, dass Theo in jeder Spielrunde nur 1 e einsetzen kann. Geht die Runde zu Theos
Gunsten aus, erhĂ¤lt er den Einsatz zurĂźck und zusĂ¤tzlich denselben Betrag aus der Bank (Gewinn
= 1 e). Endet die Runde ungĂźnstig, verfĂ¤llt der Einsatz (Gewinn = â1 e). Theo Retiker kommt ins
Casino mit n Euro (Anfangskaptal) und sein Ziel ist am Ende N Euro in der Tasche haben, d.h. er will
N â n Euro gewinnen (dann will er aufhĂśren); in diesem Fall sagen wir, dass Theo gewinnt. Er spielt
bis er m = N â n Euro gewinnt oder bis er alle seine mitgenommenen n Euro verliert.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

234

n+m

Elementarereignis:
GVVGVGGVVV

n

0
Zeit

Beachte, dass Theo und Frosch denselben Prozess modelieren: Theo will N Euro am Ende haben und
der Frosch will den Stein N (sein Haus) erreichen; Theo ist bankrot, wenn er nichts mehr in der Tasche
hat, und der Frosch ist jedenfalls âbankrotâ, wenn er den Stein 0 (mit dem Storch) erreicht. Deshalb
gilt

w = Pr Theo gewinnt .
Wir haben bereits bewiesen (siehe Satz 3.87), dass fĂźr p = 1/2 die Gewinnchanzen fĂźr Theo (oder die
Ăberlebenschancen fĂźr den Frosch) immerhin
n
w=
N

betragen. So sind z.B. fifty-fifty Chance, dass Theo sein Anfangskapital verdoppeln kann.
Wir haben aber auch gezeigt, dass sich die Situation dramatisch verĂ¤ndert, wenn p < 1/2 ist, d.h. wenn
man in einem amerikanischem Casino spielt oder wenn der Froch mit grĂśĂere Wahrscheinlichkeit
nach links (nahe zum Storch) als nach rechts springt. Dann gilt nĂ¤hmlich
w < eâ(N ân) .
D.h. sind dann z.B. Theos Chanzen, 10 e zu gewinnen, kleiner als 2â10 auch wenn er eine Million
Euro mit sich mitnimmt; dann wird er alles mit Wahrscheinlichkeit 1 â 2â10 verlieren!
Nun wollen wir wissen, wieviele Spielrunden Theo erwarten kann, bis er gewinnt oder alles verliert.
Sei
T

=

G =

die Anzahl der Spielrunden, bis das Spiel zu Ende ist
das Geld, das Bob am Ende gewinnt oder verliert.

Da Theo mit Wahrscheinlichkeit w gewinnt und mit Wahrscheinlichkeit 1 â w verliert, ist
E [G] = (N â n) Âˇ w â n Âˇ (1 â w) = N w â n.
Wir wollen aber die erwartete Spieldauer E [T] bestimmen. Dazu beachten wir, dass G = X1 +. . .+ XT ,
wobei X i das in der i-ten Spielrunde gewonnene (+1) oder verlorene (â1) Kapital ist. Die Zufallsvariablen X1 , . . . , XT sind unabhĂ¤ngige Kopien einer Zufallsvariable X , die das gewonnene Kapital in
einer Spielrunde angibt. Da E [X ] = (+1) Âˇ p + (â1) Âˇ (1 â p) = 2p â 1, liegt es nahe, das Waldâs
Theorem anzuwenden, was uns die Gleichheit
E [G] = (2p â 1)E [T]

(4.16)
ÂŠ 2003 by S. Jukna

4.15. IRRFAHRTEN UND MARKOVâKETTEN

235

liefern wĂźrde; dann wĂ¤re die gesuchte erwartete Spieldauer E [T] = E [G] /(2p â 1).

Die Sache hat aber einen Haken: Die Zufallsvariable X ist nicht positiv â ihr Wertebereich ist {â1, +1}.
Deshalb kĂśnnen wir nicht das Waldâs Theorem direkt anwenden. In solchen FĂ¤llen wendet man einen
einfachen Trick: Verschiebe die Zufallsvariablen nach rechts, um nich-negative Zufallsvariablen zu
bekommen. In unserem Fall kĂśnnen wir anstatt X die Zufallsvariable Y := X + 1 betrachten. Dann ist
E [Y ] = E [X ] + 1 = 2p. Da Y nicht-negativ ist, kĂśnnen wir die einfachste Form des Waldâs Theorems
anwenden und erhalten
" T #
X
E
Yi = 2pE [T]
i=1

Andererseits, gilt
#
" T
#
" T
" T
#
" T #
T
X
X
X
X
X
Yi
Xi +
1 =E
Xi + T = E
X i + E [T] = E [G] + E [T]
= E
E
i=1

i=1

i=1

i=1

i=1

Somit haben wir die Gleichhung E [G] + E [T] = 2pE [T] bewiesen, woraus (4.16) unmittelbar folgt.
Damit haben wir bewiesen, dass fĂźr p < 1/2


 N Âˇ Pr Theo gewinnt
E Anzahl der Spielrunden =
2p â 1

gilt. Was passiert aber, wenn das Spiel fair ist, d.h. wenn p = 1/2 ist? Wie lange kann dann Theo
spielen? Dieser Fall ist komplizierter und wir nur erwĂ¤hnen (ohne Beweis33) was dabei rauskommt:


E Anzahl der Spielrunden = n(N â n).

Irrfahrten mit reflektierenden ZustĂ¤nden
Wir betrachten wiederum die Springerei vom Frosch in einem Teich mit N Steinen 0, 1, . . . , N, wobei
wiederum der Stein 0 gefĂ¤hrlich ist â da ist der Storch. Der Unteschied nun ist, dass neben dem letzten
Stein N ein Baum steht, so dass der Frosch am diesem Stein âreflektiertâ wird. D.h. erreicht er den
Stein N, so springt er im nĂ¤chsten Sprung mit Sicherheit zurĂźck auf den einzigen Nachbarn N â 1.
Einfachheitshalber, nehmen wir nun auch an, dass p = 1/2 gilt: Befindet er sich zu irgend einem
Zeitpunkt auf einem Stein, der einen linken und einen rechten Nachbarstein hat, so springt er jeweils
mit Wahrscheinlichkeit 1/2 auf einen der beiden.
1/2

0

. . .

1
1/2

1/2

1/2

1/2

nâ1
1/2

1/2

n
1/2

1/2

111111111
000000000
N
000000000
111111111

. . .

n+1
1/2

1/2

1/2

1

Der Frosch sitzt anfĂ¤nglich auf irgendeinem Stein n.
Frage: Wie lange wird der Frosch springen, bis er gefressen wird? D.h. nach wievielen SprĂźngen
wird der Frosch erwartungsgemĂ¤Ă zum ersten mal Stein 0 erreichen, wenn er am Stein n startet? Uns
interessiert also


x n = E Anhahl der Sprunge, wenn am Stein n gestartet .
33Der Beweis ist Ă¤hnlich wie der von der Gleichung (4.17): Es reicht in diesem Beweis die Ranbedingug mit En =
Nnâ1 + 1 und En = 0 ersetzen.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

236
Wir wollen zeigen, dass

x n = n Âˇ (2N â n)

(4.17)

fĂźr alle i = 1, . . . , n gilt. Damit ist34 zum Beispiel x 1 = 2N â 1 bzw. x N = N 2 .
Beweis. Es gelten die folgende 3 Aussagen:
x 0 = 0, x N = x N â1 + 1
sowie
xn = 1 +

1
1
Âˇ x nâ1 + Âˇ x n+1 fĂźr n = 1, . . . , N â 1.
2
2

Die ersten beiden Aussagen sind klar, die dritte gilt, weil der Frosch in Position 1 6 n 6 N â 1 einen
Sprung macht und dann mit Wahrscheinlichkeit 1/2 in Position n â 1 bzw. n + 1 ist.35 Wenn wir in der
dritten Gleichung auf beiden Seiten 12 Âˇ x n subtrahieren, dann durch 2 multiplizieren und umstellen,
erhalten wir die Aussage
x n â x nâ1 = x n+1 â x n + 2 fĂźr n = 1, . . . , N â 1.
Diese Gleichungen kĂśnnen wir anders hinschreiben. Wenn wir die Differenzen
Dn := x n â x nâ1
fĂźr n = 1, . . . , N betrachten, dann ist D N = x N â x N â1 = 1 und:
Dn = Dn+1 + 2.
Damit ist D N = 1, D N â1 = 3, D N â2 = 5, D N â3 = 7, ..., D N ân = 2n + 1,...,D1 = 2N â 1 oder in einer
ânormalenâ Reihenfolge
Dn = 1 + 2(N â n) fĂźr alle n = 1, 2, . . . , N
Wie bekommen wir aus den Dn -Werten die uns eigentlich interessierenden x n -Werte? Dazu boebachten wir:
D1 + D2 + . . . + Dn = (x 1 â x 0 ) + (x 2 â x 1 ) + . . . + (x n â x nâ1 ) = x n â x 0 = x n .
Also ist
xn

=

n
X
k=1

Dk =

n
X
k=1

(1 + 2(N â k)) = 2N Âˇ n + n â 2 Âˇ

= 2N Âˇ n + n â 2 Âˇ

n(n + 1)
= n Âˇ (2N â n).
2

n
X

k

k=1


34Mit diesem Modell kann man einen randomisierten Algorithmus entwerfen, der die sogenannte 2-SAT Problem in
quadratischer Zeit lĂśst.
35Strenggenommen mĂźssten wir zunĂ¤chst zeigen, dass die Erwartungswete endlich sind, damit wir solche Gleichungen
schreiben kĂśnnen, aber wir wollen uns diese kleine mathematische NachlĂ¤ssigkeit gĂśnnen.
ÂŠ 2003 by S. Jukna

4.15. IRRFAHRTEN UND MARKOVâKETTEN

237

Irrfahrten in Zd
âA drunk man will find his way home, but a drunk bird may get lost forewerâ
- Shizuo Kakutani

Zuerst betrachten wir den fall d = 1. In einem Teich befinden sich Steine . . . , â2, â1, 0, 1, 2, . . . in
einer Reihe. Ein Frosch sitzt anfĂ¤nglich auf Stein 0. Dann beginnt er mit gleicher Wahrscheinlichkeit
1/2 entweder nach rechts oder nach links springen.

1/2

1/2

â2

. . .
1/2

â1
1/2

1/2

1/2

0
1/2

1/2

+1
1/2

1/2

. . .

+2
1/2

1/2

Frage 1: Mit welcher Wahrscheinlichkeit wird der Frosch nach n Sprungen vom Anfangsstein 0 um
mindestens t Steine entfernt sein?
Wenn wir die SprĂźnge durch +1 (nach rechts) und â1 (nach links) bezeichnen, dann besteht unser
Wahrscheinlichkeitsraum aus allen Worter der LĂ¤nge n Ăźber dem Alphabet {â1, +1}, und jedes Wort
dieselbe Wahrscheinlichkeit 1/2n hat. Sei nun X der Stein, auf dem sich der Frosch nach n SprĂźngen
befindet. Dann ist
X = X1 + X2 + . . . + X n

wobei
Xi =



+1 falls der Frosch im i-ten Schritt nach rechts springt
â1 sonst

Pn
Da E [X i ] = (â1) Âˇ 12 + 1 Âˇ 12 = 0 fĂźr alle i, gilt E [X ] =
i=1 E [X i ] = 0. Also ist die erwartete
Entfernung nach beliebig viel SprĂźngen gleich Null. Aber das sagt uns nicht die ganze Wahrheit: Es
ist doch klar, dass zum Beispiel nach einem Sprung wird der Frosch um 1 von Null entfernt sein. Das
ist noch ein Beispiel dafĂźr, dass der Erwartungswert allein uns Ăźberhaupt nichts sagt. Wir mĂźssen die
Abweichungswahrscheinlichkeit von diesem Wert bestimmen!
Die Entfernung vom Stein 0 ist durch Zufallsvariable |X | gegeben. Wir wollen also die Wahrscheinlichkeit dafĂźr, dass |X | grĂśĂer als eine gegebene Zahl t ist, bestimmen. Tschebyschewâs Ungleichung
gibt uns
Pr {|X | > t} = Pr {|X â E [X ] | > t} 6

Var [X ]
t2
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

238
wobei

 
Var [X ] = E X 2 â E [X ]2
(Definition von Var [X ])
 2
= E X
(da E [X ] = 0)
n
h X
2 i
= E
Xi
i=1

= E

n
hX

X i2 +

X
i, j

i=1

Xi X j

i

n
X
  X 

=
E X i2 +
E Xi X j

=

i=1
n
X
i=1

= n.

(LinearitĂ¤t des Erwartungswertes)

i, j

1+

X
i, j

0



 
(X i2 = 1 und E X i X j = E [X i ] E X j = 0)

Also gilt fĂźr jedes Îą > 0

â
Pr |X | > Îą n 6

(Îą

n
â

n) 2

=

1
.
Îą2

Zum Beispiel, wenn der Frosch n = 106 (eine Million!) Sprunge macht, dann wird er nur mit Wahrscheinlichkeit 6 0, 01 um mehr als 10.000 Steine von usprĂźnglichem Stein 0 entfernt sein. 36
Frage 2: Mit welcher Wahrscheinlichkeit wird der Forsch zurĂźck zum usprĂźnglichen Stein 0 kommen?
WĂ¤hrend seiner Sprungerei wird der Forsch gelegentlich den UsprĂźngsstein 0 besuchen; wir bezeichnen das als âHausbesuchâ.
Behauptung 4.96. Sei p die Wahrscheinlichkeit, dass der Frosch sein Haus nicht mehr wieder besucht. Ist p > 0, so ist die erwartete Anzahl der Hausbesuche gleich 1/p.
Beweis. Jedes Mal wenn der Frosch sein Haus (Stein 0) verlĂ¤sst, wird er mit Wahrscheinlichkeit p nie
mehr zurĂźck kommen, und mit Wahrscheinlichkeit 1 â p wird er doch zurĂźck kommen. Falls er zurĂźck
kommt, dann ist er wieder in dieselben Situation. Wir kĂśnnen also den Prozess als eine geometrische
Verteilung mit der Erfolgswahrscheinlichkeit p betrachten, 37 und wie wir bereits wissen, ist dann die
erwartete Anzahl der Versuche bis zum ersten Erfolg gleich 1/p.

Sei Yn die Anzahl der Hausbesuche in der ersten 2n Schritten.
Behauptung 4.97.

â
E [Yn ] = Î( n)

36Wir nehmen also Îą = 10.
37âErfolgâ hier ist ânie mehr zurĂźckâ. Misserfolg ist also ein Hausbesuch.
ÂŠ 2003 by S. Jukna

4.15. IRRFAHRTEN UND MARKOVâKETTEN

239

Beweis. Sei At das Ereignis, dass der Frosch zum Zeitpunkt 2t (d.h. nach genau 2t SprĂźnge) sein
Haus besucht, und sei X t die Indikatorvariable fĂźr At . Dann gilt: Yn = X1 + X2 + . . . + X n . Nach der
linearitĂ¤t des Erwartungswertes gilt:
" n
#
n
n
X
X
X
E [Yn ] = E
Xt =
E [X t ] =
Pr { At } .
t =1

t =1

t =1

FĂźr jedes t = 1, . . . , n besteht unser Wahrscheinlichkeitsraum aus allen endlichen Worter a = (a1 , . . . , am )
Ăźber dem Alphabet {â1, +1}; eine +1 bzw. â1 in i-ter Position bedeutet, dass der Frosch im i-ten
Schritt nach rechts bzw. nach links springt. Das Ereignis At besteht aus allen Worter a = (a1 , . . . , am )
der LĂ¤nge m = 2t mit der Eigenschaft, dass die Summe a1 + a2 + . . . + am gleich 0 ist. Oder anders
gesagt, At enthĂ¤lt alle Worter der LĂ¤nge 2t mit genau t Buchstaben +1. Deshalb ist die Wahrscheinlichkeit Pr { At } nach der Stirling-Formel38 asymptotisch gleich:

2t
1
t
Pr { At } = 2t âź â .
2
Ďt
Somit haben wir
E [Yn ] âź

n
n
X
â
1
1 X 1
â = â
â = Î( n).
Ď t =1 t
Ďt
t =1

â
Die letzte AbschĂ¤tzung kann man mit Integral-Kriterium (siehe Satz 3.11) zeigen. Sei f (x) = 1/ x
â
und F (x) = 2 x. Da
â
1 1
F â˛ (x) = 2 Âˇ x 2 â1 = 1/ x = f (x)
2
gilt, ist F (x) eine Stammfunktion fĂźr f (x). Da f (x) monoton fallend (mit wachsendem x) ist, liefert
uns das Integral-Kriterium die AbschĂ¤tzung:
n
X
â
1
â 6 F (n) â F (0) = 2 n
2 n + 1 â 2 = F (n + 1) â F (1) 6
t
t =1

â


â
â
Da E [Yn ] = Î( n), strebt mit n â â die erwarete Anzahl Î( n) der Hausbesuche gegen Unendlich. Die Behauptung 4.96 sagt uns, dass in diesem Fall p = 0 gelten muss. D.h. in diesem Fall wird
der Frosch mit Wahrscheinlichkeit 1 sein Haus irgenwanmal besuchen.
Was passier aber, wenn wir die Steine nicht in eine Linie (Z) sondern als ein Gitter (Z2 ) anordnen?
Einfachheithalber nehmen wir an, dass dann der Frosch sich in jedem Schritt entlang der beiden Achsen (x- und y-Achse) bewegen kann. In anderen Worten kann man die Situation mit zwei FrĂśschen
vorstellen, die sich unabhĂ¤ngig von eineander entlang der x-Achse und y-Achse springen. Ein Hausbesuch kommt nur dann zustande, wenn beide FrĂśsche sich im Punkt (0, 0) treffen. In diesem Fall
ist
â
â
E [X t ] = Pr { At } = Î(1/ t) Âˇ Î(1/ t) = Î(1/t)
und somit39
E [Yn ] =

n
X

E [X t ] = Î(1/1 + 1/2 + . . . + 1/n) = Î(log n).

t =1

â
n 
38 n/2
âź 2n+1 / 2Ďn.
39Harmonische Reihe

ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

240

Da der Erwartungswert E [Yn ] = Î(log n) gegen â strebt, werden auch in diesem Fall die FrĂśsche mit
Wahrscheinlichkeit 1 sein Haus wieder besuchen.
Fazit: Ein betrunkener Frosch wird auf jedem Fall ein Weg nach Hause finden! Wie ist aber mit einem
betrunkenen Vogel?
Der Vogel bewegt sich in Z3 . In diesem Fall haben wir
â
E [X t ] = Pr { At } = Î((1/ t) 3 ) = Î(1/t 3/2 ).
P
â3/2 konvergiert. Also gibt es eine Zahl
Es ist aber bekannt (siehe Lemma ??), dass die Reihe â
t =1 t
L, so dass lim E [Yn ] < L. Laut der Behauptung 4.96 ist dann eine p > 1/L > 0 Chance, dass der
nââ
Fogel nie mehr nach Hause kommt!

4.16 Statistisches SchĂ¤tzen: Die Maximum-Likelihood-Methode â
Ein Fischteichbesitzer mĂśchte seinen Fischbestand N schĂ¤tzen. Er markiert dazu einige Fische. In
einem spĂ¤teren Fang findet er dann markierte wie unmarkierte Fische. Der Teichbesitzer Ăźberlegt:
Der Anteil der markierten Fische im Fang wird vermutlich die VerhĂ¤ltnisse im Teich widerspiegeln.
Ist also
r = Anzahl der markierten Fische
n = Anzahl der Fische im Fang
x = Anzahl der markierten Fische im Fang,

so ist zu erwarten, dass r/N und x/n einen Ă¤hnlichen Wert haben. Dies macht (rn)/x zu einem plausiblen SchĂ¤tzer fĂźr N.
Zu demselben Resultat fĂźhrt ein allgemeines statistisches Prinzip â bekannt als Maximum-LikehoodPrinzip â das besagt:
WĂ¤hle als SchĂ¤tzer von N diejenige ganze Zahl NĚ , fĂźr die das beobachtete Ereignis maximale
Wahrscheinlichkeit bekommt.
Um das Prinzip in unserem Fall anzuwenden, machen wir die Annahme, dass die Anzahl X der markierten Fische in dem Fang eine hypergeometrisch verteilte Zufallsvariable ist (siehe âErgĂ¤nzungenâ).
Gesucht ist dasjenige N, das
 
 
r
N âr
N
L x (N ) =
x
nâx
n
(die Statistiker sprechen von der Likehoodfunktion) maximiert. Da
 

a
aâ1
(a)b
b!
a
aâ1 aâ2
aâb+1
a
Âˇ
=
Âˇ
Âˇ
Âˇ...
=
=
b
b
b! (a â 1)b
aâ1 bâ2 bâ3
aâb
aâb
eine einfache Rechnung ergibt
L x (N â 1)
N 2 â Nr â N n + N x
= 2
.
L x (N )
N â Nr â N n + nr
ÂŠ 2003 by S. Jukna

4.16. STATISTISCHES SCHĂTZEN: DIE MAXIMUM-LIKELIHOOD-METHODE â

241

Daher gilt L x (N â1) 6 L x (N ) genau dann, wenn N x 6 nr. Die Likehoodfunktion L x (N ) wĂ¤chst also
fĂźr kleine Werte und fĂ¤llt fĂźr groĂe Werte von N. Der Wechsel findet bei nr/x statt. Als MaximumLikehood-SchĂ¤tzer von N erhalten wir
j nr k
NĚ =
.
x

Die allgemeine Situation ist folgende. Sei X : âŚ â S eine Zufallsvariable mit einer endlichen Menge
S (mit endlichem Wertebereich S), und die Verteilung von X beinhalte einen unbekannten (mĂśglicherweise mehrdimensionalen) Parameter a, deren Werte in einer uns bekannten Menge A liegen.40 D.h.
wir haben die Wahrscheinlichkeiten
Pra {X = x} fĂźr alle x â S und alle a â A.
Lesen wir diese Wahrscheinlichkeiten als Funktionen von a â A fĂźr ein festes x â S (das als beobachteter Wert der Zufallsvariablen X interpretiert wird), dann haben wir die Likelihood-Funktion (zur
Beobachtung x), d,h,
L x (a) = Pra {X = x} fĂźr alle a â A

In der Likelihood-Methode will man einen Wert aĚ â A finden, der die Funktion L x (a) maximiert,
L x ( aĚ) = max L x (a).
aâA

Intuitiv ist die Methode ganz vernĂźnftig: Wir probieren einen solchen Wert aĚ des (unbekannten) Parameters a zu finden, der mit grĂśĂter Whrscheinlichkeit den beobachteten Wert x der Zufallsvariablen
X erzeugt hat.
Man kann die allgemeine Situation anschaulich als eine Matrix M mit m = | A| Zeilen und n = |S|
Spalten vorstellen: In Zeile zu a â A und Spalte zu x â S steht die Wahrscheinlichkeit Pra {X = x}.
Ist der beobachtete Wert x der Zufallsvariable X vorhanden, so sucht man den grĂśĂten Wert in der
Spalte zu x.
â˛ Beispiel 4.98 : 41 Ein Spieler spielt mit einer fairen MĂźnze oder mit einer prĂ¤parierten MĂźnze. Bei
der prĂ¤parierten MĂźnze ist die Wahrscheinlichkeit 0.75 fĂźr den Ausgang âWappenâ. SchlieĂlich
sei bekannt, dass der Spieler zu Anfang die faire MĂźnze mit Wahrscheinlichkeit 0.5 wĂ¤hlt und
die benutzte MĂźnze nicht mehr wechselt.
Wie kann man von dem Spielverlauf auf die benutzten MĂźnzen zurĂźckschlieĂen? Wir nehmen an,
dass der Spieler seine MĂźnze nicht wechselt. Wir haben also einen (uns unbekanten) Parameter
a, der in der Menge A = {fair, prĂ¤pariert} liegt. Ein Spiel ist eine Reihenfolge (X1 , . . . , X n ) von
Bernoulli-Experimenten, wobei fĂźr alle i = 1, . . . , n entweder Pr {X i = 1} = 1/2 (falls der Spieler
mit einer fairen MĂźnze spielt) oder Pr {X i = 1} = 3/4 (falls der Spieler mit einer prĂ¤parierten
MĂźnze spielt) gilt. Dementsprechend ist die Zufallsvariable X = X1 + X2 + . . . + X n entweder
binomial B(n, 1/2)-verteilt oder binomial B(n, 3/4)-verteilt. Damit sind uns die Wahrscheinlichkeiten Pra {X = x} fĂźr alle x â S = {0, 1, . . . , n} und a â A = {fair, prĂ¤pariert} bekannt. Wenn
wir n MĂźnzwĂźrfe mit dem Resultat x â S (x-maligem Auftreten des Wappens) beobachten, dann
erhalten wir
   n
 
n
1
1 n
Pr {X = x | faire MĂźnze } =
= n
x
2
2 x
40Zum Beispiel kann A eine Menge der Wahrscheinlichkeitsverteilungen sein.
41Siehe auch Beispiel 4.68.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

242
und

Damit ist

    x  nâx
 
n
3
1
3x n
Pr {X = x | unfaire MĂźnze } =
= n
.
x
4
4
4 x
L x (a) =

und

 
1 n
, falls a = fair
2n x

 
3x n
L x (a) = n
, falls a = prĂ¤pariert
4 x

Da

1
3x
n
>
ââ 2n > 3 x ââ x <
,
2n
4n
log2 3
wĂ¤hlen wir âder Spieler hat wahrscheinlich die faire MĂźnze benutztâ falls x < n/(log2 3), da dann
L x (fair) > L x (prĂ¤pariert); und wĂ¤hlen âder Spieler hat wahrscheinlich die prĂ¤pariert MĂźnze
benutztâ, falls x > n/(log2 3).
â˛ Beispiel 4.99 : Ein Spieler spielt mit einer mĂśglichst prĂ¤parierten MĂźnze. Sei a > 0 die (uns unbekannte) Wahrscheinlichkeit fĂźr den Ausgang âWappenâ. Sei auĂerdem X die Anzahl der WĂźrfe,
bis ein Wappen kommt. Wir bitten den Spieler die MĂźnze mehrmals zu werfen, bis ein Wappen
kommt. Er macht das, und sei x die Anzahl Wurfe mit dem Ausgang âKopfâ. Die LikelihoodFunktion (zur Beobachtung x) ist
L x (a) = Pra {X = x} = a(1 â a) x fĂźr alle a â A = (0, 1]
Wir mĂźssen also den Maximum von L x (a) Ăźber alle a â (0, 1] bestimmen. Dazu leiten wir diese
Funktion nach a ab:
L x (a) â˛ = (1 â a) x â ax(1 â a) xâ1 = (1 â a) xâ1 [1 â a(x + 1)]
1
1
Dann ist L x (a) â˛ = 0 genau dann, wenn a = 1 order a = x+1
gilt. Wegen L x (1) = 0 < L x ( x+1
)
(falls42 x > 1) kann a = 1 als globale Maximalstelle ausgeschlossen werden. Aufgrund von

a<
hat L x (a) â˛ im Punkt aĚ :=
Maximalstelle von L x .

1
x+1

1
ââ 1 â a(x + 1) > 0
x+1

einen Vorzeichenwechsel von Minus nach Plus, und aĚ ist globale

Wenn die Zufallsvariablen X1 , . . . , X n als stochastisch unabhĂ¤ngig vorausgesetzt werden (wie im Fall
eines Zufallsexperiments, das aus n unabhĂ¤ngigen Einzelnexperimenten besteht), dann haben wir
Pra {X1 = x 1 , . . . , X n = x n } =

n
Y
i=1

Pra {X i = x i }

und die Likelihood-Funktion (zu gegebenen x = (x 1 , . . . , x n )) ist gleich
L x (a) =

n
Y
i=1

Pra {X i = x i } .

1 auch die globale Maximalstelle von L
42Wenn x = 0 (Wappen ist nach dem ersten Wurf erschienen), dann aĚ = 1 = x+1
x
ist.

ÂŠ 2003 by S. Jukna

4.17. DIE PROBABILISTISCHE METHODEâ

243

Um das Maximierungproblem zu vereinfachen, betrachtet man statt L(a) die Funktion (die LogLikelihood-Funktion)
â(a) = ln L(a)
(wenn im Voraus bekannt ist, dass der Fall L(a) = 0 nicht auftritt). Die Log-Likelihood-Funktion
hat oftmals eine einfachere Gestalt als die Likelihood-Funktion, z.B. werden Produkte zu Summen,
allerdings um den Preis, dass jetzt Logarithmen auftreten.

4.17 Die probabilistische Methodeâ
Bis jetzt haben wir die Stochastik als eine Theorie betrachtet, die uns manche âreellenâ Modelle mit
Zufall analysieren lĂ¤sst. Es gibt aber auch eine andere Seite der Stochastik: Man kann mit ihrer Hilfe
Aussagen auch in Situationen, wo Zufall keine Rolle spielt, treffen!
Die Hauptidee der sogenannten probabilistischen Methode ist folgende: Will man die Existenz eines
Objekts mit bestimmten Eigenschaften zeigen, so definiert man einen entsprechenden Wahrscheinlichkeitsraum und zeigt, dass ein zufĂ¤llig gewĂ¤hltes Element mit positiver Wahrscheinlichkeit die
gewĂźnschte Eigenschaft hat.
Ein Prototyp dieser (sehr mĂ¤chtigen) Methode ist das folgende âMittel-Argumentâ:
Sind x 1 , . . . , x n â R und die Ungleichung
x1 + Âˇ Âˇ Âˇ + x n
>a
n

(4.18)

xj > a

(4.19)

gilt, so muss es mindestens ein j mit
geben.
Die NĂźtzlichkeit dieses Argument liegt in der Tatsache, dass es oft viel leichter ist, die Ungleichung
(4.18) zu beweisen, als ein x j , fĂźr das die Ungleichung (4.19) gilt, zu finden.
Wir demonstrieren die probabilistische Methode auf ein paar Beispielen. 43
Sei G = (V, E) ein ungerichteter Graph und S â V eine Menge seiner Knoten. Dann ist S eine Clique,
falls je zwei Knoten u , v â S mit einer Kante aus E verbunden sind. Sind keine zwei Knoten in S
mit einer Kante aus E verbunden, so heiĂt S unabhĂ¤ngige Menge.
Gibt es Graphen mit nur sehr kleinen Cliquen wie auch mit nur sehr kleinen unabhĂ¤ngigen Mengen?
Solche Graphen nennt man Ramsey-Graphen.
Satz 4.100. (ErdoĚs 1947) Es gibt Graphen mit n Knoten, die weder Cliquen noch unabhĂ¤ngige Mengen der GrĂśĂe 2 log n besitzen.
Beweis. Um die Existenz solchen (sehr merkwĂźrdigen!) Graphen zu beweisen, betrachten wir Zufallsgraphen Ăźber der Knotenmenge V = {1, . . . , n}: Wir werfen fĂźr jede potentielle Kante {u, v} eine
faire MĂźnze und setzen die Kante ein, wenn das Ergebnis âWappenâ ist.
43Mehr (zum Teil Ăźberraschenden) Anwendungen kann man in dem Buch The Probabilistic Method von Noga Alon und
Joel Spencer finden.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

244

Abbildung 4.6: Eine Clique S1 der GrĂśĂe 4 und eine unabhĂ¤ngige Menge S2 der GrĂśĂe 4

Wir fixieren eine Knotenmenge S â V der GrĂśĂe k und sei AS das Ereignis âS ist eine Clique oder
eine unabhĂ¤ngige Mengeâ. Es ist
k
Pr { AS } = 2 Âˇ 2â ( 2 ) ,

denn entweder ist S eine Clique und alle k2 Kanten sind vorhanden oder S ist eine unabhĂ¤ngige

Menge und keine der k2 Kanten ist vorhanden. Wir sind vor Allem an der Wahrscheinlichkeit pk
interessiert, dass ein Zufallsgraph
G eine Clique der GrĂśĂe k oder eine unabhĂ¤ngige Menge der GrĂśĂe

n
k besitzt. Da wir nur k k-elementigen Mengen S â V haben, gilt:
 
k
n
nk 2 Âˇ 2k /2
pk 6
Âˇ 2 Âˇ 2â ( 2 ) <
Âˇ
.
k
k! 2k 2 /2
2

Wir setzen k = 2 Âˇ log2 n und erhalten nk = 2k /2 . Da andererseits 2 Âˇ 2k /2 < k! fĂźr k > 4, folgt
somit pk < 1 fĂźr k > 4: Es gibt somit Graphen, die nur Cliquen oder unabhĂ¤ngige Mengen der
GrĂśĂe hĂśchstens 2 log2 n â 1 besitzen. Wir haben somit die Existenz eines Objekts durch ZĂ¤hlen
nachgewiesen.

Bis heute sind keine expliziten Graph-Konstruktionen bekannt, die vergleichbare Ergebnisse liefern!
Andererseits, liefert uns Satz 4.100 keinen explizieten Ramsey-Graphen â er zeigt lediglich nur die
Existens solchen Graphen. Bis heute ist es nur gelungen, expliziete Graphen zu konstruieren, die
â
weder Cliquen noch unabhĂ¤ngige Mengen der GrĂśĂe n besitzen.
Als unser nĂ¤chstes Beispiel betrachten wir den klassischen Satz von TurĂĄn. Ein Gremium V besteht
aus n Personen. Das Problem ist, dass manche von ihnen einanderen kennen. Um die Entscheidung
nicht durch Bekannschaften beeinflĂźssen, will man eine mĂśglicht grĂśĂe Teilmenge S â V der Personen zu finden, so dass keine zwei Personen aus S sich kennen.
Dieses Problem kann man mit Hilfe von Graphen darstellen. Wir haben einen ungerichteten Graph
G = (V, E), wobei Kanten den Bekantschaften entsprechen. Man will eine mĂśglicht grĂśĂe unbhĂ¤ngige
Menge S â V finden.
Das berĂźhmte Satz44 von TurĂĄn sagt, dass jeder Graph mit n Knoten und |E| 6 nk/2 Kanten eine unabhĂ¤ngige Menge S der GrĂśĂe |S| > n/(k + 1) enthalten muss. Der Beweis ist nicht trivial.
Andererseits, man kann âfastâ dasselbe Resultat mit Hilfe der Zufall sehr leicht beweisen.

Satz 4.101. Jeder Graph G = (V, E) mit |V | = n Knoten und |E| 6 nk/2 Kanten muss eine unabhĂ¤ngige Menge S der GrĂśĂe |S| > n/2k enthalten
44Dieser Satz hat die ganze Extremale Graphentheorie initiert.
ÂŠ 2003 by S. Jukna

4.17. DIE PROBABILISTISCHE METHODEâ

245

Beweis. Um die Menge S zu finden, konstruieren wir zuerst eine zufĂ¤llige Menge U â V : Wir nehmen
eine MĂźnze, fĂźr die das Ergebnis Wappen mit Wahrscheinlichkeit p kommt (p ist ein Parameter, den
wir spĂ¤ter bestimmen wollen). Wir werfen dann fĂźr jeden potentiellen Knoten i â V eine faire MĂźnze
und nehmen diesen Knoten i in U, wenn das Ergebnis Wappen ist.
FĂźr jeden Knoten i â V sei X i die Indikatorvariable fĂźr das Ereignis âi â Uâ (i ist gewĂ¤hlt). FĂźr jede
Kante e = {i, j} â E sei Ye die Indikatorvariable
fĂźr das Ereignis âi â U und j â Uâ (beide Endknoten
P
von e sind
gewĂ¤hlt).
Die
Summe
X
:=
X
i âV i ist dann die Anzahl |U | der gewĂ¤hlten Knoten und
P
Y :=
Y
ist
die
Anzahl
der
Kanten,
die in die Menge U gewĂ¤hlten Knoten verbinden. Da
e âE e
fĂźr jeden Knoten i â V gilt: E [X i ] = Pr {i â U } = p, und fĂźr jede Kante e = {i, j} gilt: E [Ye ] =
Pr {i â U und j â U } = p2 , haben wir (nach der linearitĂ¤t des Erwartungswertes)
X
E [X ] =
E [X i ] = p Âˇ |V | = pn
i âV

und
E [Y ] =

X

e âE

E [Ye ] = |E|p2 6

nk 2
p .
2

Die linearitĂ¤t des Erwartungswertes liefert uns wieder:
E [X â Y ] > np â

nk 2
p .
2

Um diesen Ausdruck zu maximieren, wĂ¤hlen wir p = 1/k, und erhalten
E [X â Y ] >

n
n
n
â
=
.
k 2k 2k

Also muss es mindestens eine Auswahl der Menge U â V geben, fĂźr die X â Y > n/2k gilt. Dies
bedeutet, dass die Menge U um mindestens n/2k mehr Knoten als Kanten enthĂ¤lt. Wir kĂśnnen also
diese Kanten vernichten indem wir aus jeder solchen Kante einen Endknoten einfach aus der Menge
U enfernen. Die verbleibende Menge S â U wird immer noch |S| > n/2k Knoten enthalten. Da wir
alle Kanten aus U enfernt haben, ist die Menge S auch unabhĂ¤ngig.

Ein bipartiter Graph G = (L âŞ R, E) heiĂt ein (n, Îą, c)-Expander, wenn |L| = |R| = n und fĂźr alle
S â L mit |S| 6 Îą Âˇ |L| gilt |Î(S)| > c Âˇ |S|, wobei Î(S) die Menge der mit S benachbarten Knoten
ist. Ein solcher Graph ist links d-regulĂ¤r, falls jeder Knoten u â L den Grad d besitzt.
Es ist klar, dass es keinen d-regulĂ¤ren (n, Îą, c)-Expander mit c > d geben kann, da dann |Î(S)| 6 d|S|
fĂźr jede Teilmenge S gilt. Andererseits, braucht man in vielen Anwendungen (wie Tornado Codes) dregulĂ¤re (n, Îą, c)-Expandern mit einem Expansionsgrad c > (1 â ÇŤ )d fĂźr ein kleines ÇŤ > 0. Mit der
probabilistischen Methode kann man zeigen, dass solche Graphen auch tatsĂ¤chlich existieren.
Satz 4.102. FĂźr jedes d > 3 gibt es links d-regulĂ¤re (n, Îą, d â 2)-Expandern mit Îą = âŚ(1/d 4 ).
Beweis. Sei G = (L âŞ R, E) ein durch folgenden Prozess zufĂ¤llig erzeugter bipartiter Graph mit
|L| = |R| = n: Zu jedem Knoten u â L wird zufĂ¤llig gleichverteilt d Nachbarn von u je mit Wahrscheinlichkeit 1/n gewĂ¤hlt.
FĂźr k 6 Îąn sei
pk = Pr {âS â L mit |S| = k und |Î(S)| < (d â 2)k} .
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

246
Um den Satz zu beweisen, reicht es zu zeigen, dass

PÎąn

k=1

pk < 1 gilt.

Fixiere eine beliebige Teilmenge S â L mit |S| = k. Wir stellen uns vor, dass die Knoten in S seine
Nachbarn einnacheinander wĂ¤hlen. Die Knoten in S kĂśnnen hĂśchstens dk verschiedene Nachbarn
wĂ¤hlen. Deshalb folgt aus |Î(S)| < (d â 2)k, dass mindestens 2k Knoten aus S einen bereits von
anderen Knoten in S gewĂ¤hlten Nachbarn wĂ¤hlen mĂźssen. Wir nennen solche Nachbarn populĂ¤r.
Die Wahrscheinlichkeit, dass ein Knoten u â S einen bereits von anderen Knoten aus S gewĂ¤hlten
Nachbarn fĂźr sich aussucht, ist hĂśchstens45
#{bereits gewĂ¤hlten Nachbarn}
dk
6
.
n
n
Deshalb gilt

Pr es gibt mindestens 2k populĂ¤re Nachbarn 6



dk
2k



dk
n

2k

.


Warum? Da es hĂścstens dk
2k MĂśglichkeiten gibt, die 2k Knoten (aus hĂśchstens dk von der Knoten
aus S wĂ¤hlbaren Nachbarn) als âKandidatenâ fĂźr populĂ¤re Nachbarn zu markieren, und (dk/n) 2k die
obere Schranke fĂźr die Wahrscheinlichkeit,
dass alle diese 2k Kandidaten auch tatsĂ¤chlich populĂ¤r

sein werden, ist. Da wir nk MĂśglichkeiten fĂźr die Teilmenge S haben, folgt daraus:46
pk

    2k
n
dk
dk
6
n
k
2k
 en k  edk 2k  dk 2k
6
k
2k
n
 4 k
cd k
=
n

mit c = e3 /4. FĂźr Îą = 1/(cd 4 ) und k 6 Îąn haben wir also, dass stets pk 6 4âk gilt. Deshalb gilt

Pr G ist kein (n, Îą, d â 2)-Expander

6

Îąn
X
k=1

=

pk 6

Îąn
X

k=1
n+1
(1/4)

1â
1 â (1/4)

âk

4

6

n
X
k=0

â16

4âk â 1

4
â 1 6 1/3.
3


45Diese AbschĂ¤tzung ist sehr grob, aber sie reicht uns vollkom aus. Um eine exaktere AbschĂ¤tzung zu bekommen, konnte
man eine Ă¤hnliche Analyze wie in Abschnitt 4.12 durchziehen.
46Hier benutzen wir die AbschĂ¤tzungen (siehe Lemma 1.43):
 n k
k

6

   
n
en k
<
.
k
k

ÂŠ 2003 by S. Jukna

4.18. AUFGABEN

247

4.18 Aufgaben
4. 1. Gegeben sind zwei Ereignisse A und B mit Pr { A} = 0, 7, Pr {B} = 0, 6 und Pr { A âŠ B} = 0, 5. Berechne:


(a) Pr { A âŞ B}
(b) Pr A
(c) Pr B



(d) Pr A âŞ B
(e) Pr A âŠ B
(f) Pr A âŠ B


(g) Pr A âŠ B
(h) Pr ( A âŠ B) âŞ ( A âŠ B)
4. 2. Ein PrĂźfer hat 18 Standardfragen, von denen er in jeder PrĂźfung 6 zufĂ¤llig auswĂ¤hlt, wobei jede Auwahl
die gleiche Wahrscheinlichkeit besitzt. Ein Student kennt die Antwort von genau 10 Fragen. Wie groĂ ist die
Wahrscheinlichkeit, dass er die PrĂźfung besteht, wenn er dazu mindestens drei Fragen richtig beantworten
muss?
4. 3. Von zehn Zahlen sind fĂźnf positiv und fĂźnf negativ. Zwei Zahlen werden zufĂ¤llig ohne ZurĂźcklegen
gezogen und multipliziert. Ist es gĂźnstiger, auf ein positives oder ein negatives Produkt zu setzen?
4. 4. Peter schlĂ¤gt Paul ein Spielchen vor: âDu darfst 3 WĂźrfel werfen. Tretten dabei Sechser auf, so hast du
gewonnen. Wenn keine Sechser vorkommen, habe ich gewonnen.â Paul Ăźberlegt rasch, dass fĂźr jeden WĂźrfel
die Wahrscheinlichkeit 1/6 betrĂ¤gt. Die Wahrscheinlichkeit, dass der erste oder der zweite oder der dritte eine
Sechs aufveisen ist also (1/6) + (1/6) + (1/6) = 1/2. Das Spiel scheint ihm sehr fair zu sein. WĂźrdest Du auch
so Ăźberlegen?
4. 5. (De MĂŠrĂŠâs Paradox)47 Wir wĂźrfeln 3 mal einen SpielwĂźrfel und betrachten zwei Ereignisse
A =
B

=

{die Summe der Augenzahlen ist 11}
{die Summe der Augenzahlen ist 12}

Bestimme die Wahrscheinichkeiten Pr { A} und Pr {B}. Sind sie gleich?

4. 6. Seien A und B zwei unabhĂ¤ngige Ereignisse mit Pr { A} = Pr {B} und Pr { A âŞ B} = 1/2. Bestimme Pr { A}.

4. 7. Wir sagen, dass ein Ereignis A ist fĂźr einen anderen Ereignis B attraktiv (bzw. abstoĂend), falls Pr {B | A } >
Pr {B} (bzw. Pr {B | A } < Pr {B}) gilt. Zeige folgendes:
(a) A ist fĂźr B attraktiv ââ B ist fĂźr A attraktiv.

(b) A ist fĂźr B weder attraktiv noch abstoĂend ââ A und B sind unabhĂ¤ngig.

(c) A ist fĂźr B attraktiv ââ Pr {B | A } > Pr B A .

(d) A ist fĂźr B attraktiv â A ist abstoĂend fĂźr B.

4. 8. Es werden dre verschiedenfĂ¤rbige WĂźrfeln geworfen. Betrachte die folgenden Ereignisse:
A: Es fĂ¤llt mindestens eine 1.
B: Jeder WĂźrfel hat eine andere Augenzahl.
C: Die drei Augenzahlen stimmen Ăźberein.
Berechne Pr { A}, Pr {B}, Pr {C} und Pr { A âŠ B}. Sind A und B unabhĂ¤ngig?


4. 9. Zeige: Wenn Pr { A | B } = Pr { A} ist, dann ist auch Pr A | B = Pr A

4. 10. Wir haben drei MĂźnzen. Eine MĂźnze (die WW-MĂźnze) hat auf beiden Seiten das Wappen, die Zweite
(die KK-MĂźnze) hat auf beiden Seiten den Kopf, und die dritte (die WK-MĂźnze) hat das Wappen auf einer und
den Kopf auf der anderen Seite. Sie ziehen rein zufĂ¤llig eine der drei MĂźnzen, werfen diese MĂźnze, und es
47Diese Frage hat der franzĂśsischer Edelmann De MĂŠrĂŠ an seinem Freund Pascal in 17. Jahrhundert gestellt.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

248

kommt Wappen. (Wir nehmen an, dass (ausser der Markierung) die MĂźnzen fair sind, d.h. jede Seite kann mit
gleicher Wahrscheinlichkeit 1/2 kommen.) Was ist die Wahrscheinlichkeit dafĂźr, dass die WK-MĂźnze gezogen
war? Hinweis: Die Antwort ist nicht 1/2.
4. 11. Ein Spieler wettet auf eine Zahl von 1 bis 6. Drei WĂźrfel werden geworfen und der Spieler erhĂ¤lt 1 oder
2 oder 3 Euro, wenn 1 bzw. 2 bzw. 3 WĂźrfel die gewettete Zahl zeigen. Wenn die gewettete Zahl Ăźberhaupt
nicht erscheint, dann muss der Spieler ein Euro abgeben.
Wieviele Euro gewinnt (oder verliert) der Spieler im Mittel pro Spiel? Ist das Spiel fair?
4. 12. Karl und Lina fĂźhren mit einem GlĂźcksrad ein Spiel durch. Dieses enthĂ¤lt N gleich groĂe Sektoren, die
mit den Zahlen 0 bis N â 1 beschriftet sind. Man vereinbart, dass derjenige gewonnen hat, der zuerst die Zahl
0 erdreht. Dann ist das Spiel beendet. Wir nehmen an, dass Karl beginnt.
Sei p = 1/N die Wahrscheinlichkeit, dass nach einer Drehung das Rad auf 0 stehen bleibt, und sei q = 1 â p.
Sei K das Ereignis, dass Karl in maximal k Runden gewinnt und sei L das Ereignis, dass Lina in maximal k
Runden gewinnt. Um wieviel kleiner sind Linas Gewinnchancen?
4. 13. (Russisches Roulette) Beim russischen Roulette steckt in der Trommel einer Pistole genau eine Kugel.
Der âSpielerâ versetzt die Trommel in Rotation und drĂźckt dann ab. Zwei Gangster duellieren sich dergestalt,
dass sie sich aus 2 m gegenseitig beschieĂen, wobei jeder genau eine Kugel im Magazin hat. Der beleidigte Gangster darf zuerst schieĂen, dann kommt der jeweils nĂ¤chste an die Reihe. Vor jedem Schuss wird das
Magazin in eine zufĂ¤llige Umdrehung versetzt.
Wie groĂ ist die Ăberlebenschance fĂźr den 1. (beleidigten) Gangster bzw. fĂźr den 2. Gangster, wenn man 5
Runden vereinbart und das Magazin 8 Kugeln fassen kann, aber nur eine enthĂ¤lt?
4. 14. Kurt gewinnt gegen Christian 60 % aller Tennis-Spiele; die Wahrscheinlichkeit, dass er einen einzelnen
Satz gewinnt, betrĂ¤gt somit 0, 6.
Mit welcher Wahrscheinlichkeit gewinnt Kurt das Match aus der Serie Best-of-three? (Kurt muss zwei SĂ¤tze
gewinnen, die Reihenfolge der Siege ist egal, aber wenn Kurt die ersten beiden SĂ¤tze gewinnt, dann ist das Spiel
vorbei.)
4. 15. Es gibt 4 MĂźnzen 1, 2, 3, 4 in einer Kiste. Die Wahrscheinlichkeit, dass die i-te MĂźnze den Kopf ergibt ist
1/i. Wir wĂ¤hlen eine der vier MĂźnzen rein zufĂ¤llig und werfen sie solang, bis ein Kopf kommt.
(a) Gebe einen Wahrscheinlichkeitsraum fĂźr dieses Experiment. (Die Summe der Wahrscheinlichkeiten von
Elementarereignissen muss 1 sein!)
(b) Was ist die Wahrschienlichkeit dafĂźr, dass der Kopf erstmals nach dem zweiten Wurf kommt?
(c) Angenommen, Kopf ist genau nach zwei Wurfen gekommen. Was ist dann die Warscheinlichkeit, dass die
i-te MĂźnze gewĂ¤hlt war?
4. 16. 5 Urnen enthalten verschiedenfarbige Kugeln wie folgt:
Urne
1
Anzahl rote
4
Anzahl grĂźne 2

2 3
3 1
1 7

4 5
2 3
5 2

Es wird eine beliebige Urne ausgewĂ¤hlt und ihr eine beliebige Kugel entnommen. Mit welcher Wahrscheinlichkeit wurde die erste Urne gewĂ¤hlt unter der Voraussetzung, dass die gezogene Kugel rot war?
4. 17. Wir haben zwei Urnen. Die erste Urne enthĂ¤lt 10 Kugeln: 4 rote und 6 blaue. Die zweite Urne enthĂ¤lt
16 rote Kugeln und eine unbekante Anzahl b von blauen Kugeln. Wir ziehen rein zufĂ¤llig und unabhĂ¤ngig eine
Kugel aus jeder der beiden Urnen. Die Wahrscheinlichkeit, dass beide Kugeln dieselbe Farbe tragen sei 0, 44.
Bestimme die Anzahl b der blauen Kugeln in der zweiten Urne.
4. 18. DreiĂig Studierende haben sich auf die Klausur in âMathematische Grundlagen der Informatikâ vorbereitet. Die HĂ¤lfte dieser 30 haben die Ăbungsaufgaben regelmĂ¤Ăig bearbeitet, die andere HĂ¤lfte aber nicht. Wir
nehmen an, die Wahrscheinlichkeit eine Aufgabe in der Klausur zu lĂśsen ist p = 0, 8 fĂźr diejenigen, welche
ÂŠ 2003 by S. Jukna

4.18. AUFGABEN

249

die Ăbungsaufgaben bearbeitet haben. FĂźr die anderen sei die Wahrscheinlichkeit immerhin noch q = 0, 6. Wir
nehmen auch an, dass es keine Korrelationen zwischen Fragen gibt: Beantwortet man eine Frage richtig bzw.
falsch, so hat das keinen Einfluss auf andere Antworten.
In der Klausur werden 20 Fragen gestellt.48 Es besteht, wer mindestens 13 davon richtig lĂśst.
(a) Wie groĂ ist die Wahrscheinlichkeit, dass ein zufĂ¤llig herausgegriffener Studierender diese Klausur besteht?
(b) Falls Sie einen zufĂ¤llig gewĂ¤hlten Studierenden treffen und er oder sie gibt an, durchgefallen zu sein, wie
groĂ ist dann die Wahrscheinlichkeit, dass er oder sie die Ăbungsaufgaben regelmĂ¤Ăig bearbeitet hat?
Hinweis: Seien K das Ereignis, die Klausur zu bestehen; Ai , die i-te Aufgabe richtig zu lĂśsen und U, geĂźbt zu

haben. Gegeben sind also die beiden bedingten Wahrscheinlichkeiten Pr { Ai | U } = p = 0, 8 und Pr Ai U =
q = 0, 6 fĂźr alle i = 1, 2, . . . , 20. Da es keine Korrelationen zwischen Fragen gibt, ist z.B. die Wahrscheinlichkeit,
 k
20âk
genau k Fragen (richtig) zu beantworten gleich 20
, wobei r = p, falls man geĂźbt hat, und r = q
k r (1 â r)
sonst.
4. 19. Ein Studentenclub hat n > 2 RĂ¤ume. Ein Student sucht dort seine Freundin. Er weiĂ, dass die Wahrscheinlichkeit, dass die Freundin den Klub besucht, gleich a ist, und die Wahrscheinlichkeit, dass sie in einem
bestimmten Raum aufhĂ¤lt, fĂźr alle RĂ¤ume gleich ist. FĂźr i = 1, . . . , n â 1 sei
â˘ pi = die Wahrscheinlichkeit, das er seine Freundin im Raum i + 1 antrifft, wenn er bereits i RĂ¤ume erfolglos
nach ihr abgesucht hat;
â˘ qi = die Wahrscheinlichkeit, das er seine Freundin Ăźberhaupt im Klub antrifft, wenn er bereits i RĂ¤ume
erfolglos nach ihr abgesucht hat?
Bestimme diese Wahrscheinlichkeiten. Welche von diesen Wahrscheinlichkeiten sind fĂźr wachsendes i wachsend welche sind fallend?
4. 20. Die folgende Daten waren einmal in Wall Street Journal verĂśffentlicht. FĂźr eine i Jahre alte Frau in
der U.S.A. mit i â {20, 30, 40, 50, 60} ist die Wahrscheinlichkeit, an einer bestimmten Krankheit (in der Zeitung war das Krebs) in den nĂ¤chsten 10 Jahren zu erkranken, gleich 0, 5%, 1, 2%, 3, 2%, 6, 4%,10, 8%. FĂźr
dieselben Altesgruppen sind die Wahrscheinlichkeiten, irgendwann an dieser Krankheit zu erkranken, gleich
39, 6%, 39, 5%, 39, 1%, 37,5%, 34,2%. Das sieht merkwĂźrdig aus! Benutze die vorherige Aufgabe, um diese
Datein zu erklĂ¤ren.
4. 21. Ein Sortiment von 20 Teilen gilt als âgutâ, wenn es hĂśchstens 2 defekte Teile enthĂ¤lt, als âschlechtâ, wenn
es mindestens 4 defekte Teile enthĂ¤lt. Weder der KĂ¤ufer noch der VerkĂ¤ufer weiĂ, ob das gegebene Sortiment
gut oder schlecht ist. Deshalb kommen sie Ăźberein, 4 zufĂ¤llig herausgegriffene Teile zu testen. Nur wenn alle 4
in Ordnung sind, findet der Kauf (des ganzen Sortiments) statt. Der VerkĂ¤ufer trĂ¤gt bei diesem Verfahren das
Risiko, ein gutes Sortiment nicht zu verkaufen, der KĂ¤ufer das Risiko, ein schlechtes Sortiment zu kaufen.
Wer trĂ¤gt das grĂśĂere Risiko?
4. 22. Von einem Spiel ist bekannt, dass man mit einer Wahrscheinlichkeit von p = 0, 1 gewinnt. Man spielt so
lange, bis man einen Gewinn erzielt. Dann beendet man seine Teilnahme am Spiel.
Wie lange muss man spielen (Anzahl der Spiele), wenn man mit einer Wahrscheinlichkeit von 0, 75 einen
Gewinn erzielen mĂśchte? Hinweis: Geometrische Verteilung.
4. 23. Sei X : âŚ â N eine diskrete Zufallsvariable mit E [X] > 0. Zeige:
E [X]2
  6 Pr {X , 0} 6 E [X] .
E X2
4. 24. Zeige, dass die Markov-Ungleichung optimal ist. D.h. fĂźr eine natĂźrliche Zahl k finde eine nicht-negative
Zufallsvariable X, so dass Pr {X > k Âˇ E [X]} = 1/k gilt.
48Rein hypothetisch ... Im Wirklichkeit werden wir viel weniger Fragen stellen.
ÂŠ 2003 by S. Jukna

KAPITEL 4. DISKRETE STOCHASTIK

250

Pn
4. 25. Sei X =
i=1 X i eine Summe von Bernoulli-Variablen. Die Variablen X i mĂźssen nicht unbedingt
unabhĂ¤ngig sein! Zeige:
n
  X
E X2 =
Pr {X i = 1} E [X |X i = 1] .
i=1

Hinweis: Zuerst zeige, dass E



X2



=

Pn

i=1 E [X

Âˇ X i ] gilt.

4. 26. Ein vereinfachstes Model der BĂśrse geht davon aus, dass in einem Tag eine Aktie mit dem aktuellen
Preis a mit Wahrscheinlichkeit p um Faktor r > 1 bis zum ar steigen wird und mit Wahrscheinlichkeit 1 â p
bis zum a/r fallen wird.
Angenommen, wir starten mit dem Preis a = 1. Sei X der Preis der Aktie nach d Tagen. Bestimme E [X] und
Var [X].
4. 27.PSein X1 , . . . , X n unabhĂ¤ngige Bernoulli-Variablen mit Pr {X i = 1} = pi and Pr {X i = 0} = 1 â pi . Sei
n
X = i=1
X i (mod 2). Zeige:
"
#
Y
1
1â
(1 â 2pi ) .
Pr {X = 1} =
2
i

Hinweis: Betrachte die Zufallsvarianle Y = Y1 Âˇ Âˇ Âˇ Yn mit Yi = 1 â 2X i . Was ist E [Y ]?

4. 28. Seien f , g : R â R beliebige Funktionen. Zeige: Sind X,Y : âŚ â R zwei unabhĂ¤ngige Zufallsvariablen,
so sind die Zufallsvariablen f (X ) und g(Y ) unabhĂ¤ngig.

Pn
P
4. 29. Seien A1 , . . . , An beliebige Ereignisse. Seien a = i=1
Pr { Ai } und b = i< j Pr Ai âŠ A j . Zeige

a + 2b
Pr A1 Âˇ Âˇ Âˇ An 6
â 1.
a2

Hinweis: Sei X die Anzahl der tatsĂ¤chlich vorkommenden Ereignisse. Benutze Tschebyschevâs Ungleichung,


um Pr {X = 0} 6 a â2 E (X â a) 2 zu zeigen.
4. 30. Sei X : âŚ â {0, 1, . . . , M} eine Zufallsvariable und a = M â E [X]. Zeige, dass
Pr {X > M â b} >

bâa
b

fĂźr jedes 1 6 b 6 M gilt.
4. 31. FĂźr zwei Vektoren u, v â {0, 1} n ist ihr Skalarprodukt als hu, vi =
ein rein ZufĂ¤llig gewĂ¤hlter Vektor in {0, 1} n .

Pn

i=1 ui vi

(mod 2) definiert. Sei nun x

Zeige: Pr {hx, vi = 1} = 1/2 fĂźr jedes v , (0, . . . , 0), und Pr {hx, vi = hx, wi} = 1/2 fĂźr alle v , w.
4. 32. Sei X nicht-negative ganzzahlige Zufallsvariable.
 
Zeige: E X 2 > E [X] und Pr {X = 0} > 1 â E [X].

4. 33. Ist X : âŚ â R eine Zufallsvariable und f : R â R eine Funktion, so ist auchPY := f (X ) eine
Zufallsvariable. Wie sieht ihr Erwatungswert E [Y ] aus? Nach der Definition ist E [Y ] =
y y Âˇ Pr {Y = y}.

 P
Zeige, dass E f (X ) = x f (x) Âˇ Pr {X = x} gilt.
 
4. 34. Zeige, dass im Allgemeinen die Divisionsregel E YX = E[X]
E[Y ] auch fĂźr unabhĂ¤ngigen Zufallsvariablen
X,Y nicht gilt!
4. 35. Sei A â âŚ ein Ereignis und X sei die Indikatorvariable fĂźr A, d.h. X (Ď) = 1 fĂźr Ď â A und X (Ď) = 0

fĂźr Ď < A. Zeige, dass Var [X] = Pr { A} Âˇ Pr A .

4. 36. Ein Mann hat n SchlĂźssel aber nur eine davon passt zu seinem TĂźr. Der Mann probiert die SchlĂźssel
zufĂ¤llig. Sei X die Anzahl der Versuche bis der richtige SchlĂźssel gefunden ist. Bestimme den Erwartungswert
E [X], wenn der Mann den bereits ausprobierten SchlĂźssel
(a) am Bund lĂ¤sst (also kann er ihn noch mal probieren);
ÂŠ 2003 by S. Jukna

4.18. AUFGABEN

251

(b) vom Bund nimmt
4. 37. Seien A und B zwei zufĂ¤llige Teilmengen der Menge [n] = {1, . . . , n} mit Pr {x â A} = Pr {x â B} = p
fĂźr alle x â [n]. Ab welchem p kĂśnnen wir nicht mehr erwarten, dass die Mengen A und B disjunkt sind?

4. 38. Wir verteilen m Bonbons an n Kinder. Jedes der Kinder fĂ¤ngt mit gleicher Wahrscheinlichkeit ein Bonbon. Wie viele Bonbons mĂźssen geworfen werden, bis jedes Kind ein Bonbon gefangen hat?
4. 39. Es ist rutschig und wenn das Kind einen Schritt nach vorn versucht, dann kommt es tatsĂ¤chlich mit einer
Wahrscheinlichkeit 2/3 um einen Schritt nach vorn. Allerdings rutscht es mit Wahrscheinlichkeit 1/3 einen
Schritt zurĂźck. Alle Schritte seien hierbei voneinander unabhĂ¤ngig.

Das Kindergarten sei von dem Kind 100 Schritte weit entfernt. Zeige, dass das Kind nach 500 Schritten mit
wenigstens 90% Wahrscheinlichkeit angekommen ist. SchĂ¤tze hierzu entsprechende Wahrscheinlichkeit mit
Hilfe des Satzes von Tschebyschev ab.

Pn
P
4. 40. Seien A1 , . . . , An beliebige Ereignisse. Seien weiterhin a = i=1
Pr { Ai } und b = i< j Pr Ai âŠ A j .
Zeige:

a + 2b
Pr A1 Âˇ Âˇ Âˇ An 6
â 1.
a2
Hinweis: Sei X die Anzahl der Ereignisse, die tatsĂ¤chlich vorkommen. Benutze die Tschebyschev-Ungleichung,


um zu zeigen, dass Pr {X = 0} 6 a â2 E (X â a) 2 gilt. Benutze die LinearitĂ¤t des Erwartungswertes, um den
rechten Term auszurechnen.
4. 41. Wie oft muss eine faire MĂźnze mindestens geworfen werden, damit mit einer Wahrscheinlichkeit von
mindestens 3/4 die relative HĂ¤ufigkeit von Kopf von erwarteten Wert p = 1/2 um weniger als 0, 1 abweicht?

ÂŠ 2003 by S. Jukna

252

KAPITEL 4. DISKRETE STOCHASTIK

ÂŠ 2003 by S. Jukna

Kapitel 5

Lineare Algebra
Contents
5.1

Lineare VektorrĂ¤ume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254

5.2

Basis und Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256

5.3

Skalarprodukt und Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258

5.4

Dimensionsschranke und ihre Anwendungenâ . . . . . . . . . . . . . . . . . . 260

5.5

Matrizen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263

5.6

Rang einer Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270

5.7

LĂśsbarkeit der linearen Gleichungssysteme . . . . . . . . . . . . . . . . . . . . 272

5.8

GauĂ-Verfahren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275

5.9

Inversen von Matrizen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

5.10 OrthogonalitĂ¤t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
5.11 Determinanten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
5.12 Eigenwerte und Eigenvektoren . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
5.13 Einige Anwendungen des MatrizenkalkĂźlsâ

. . . . . . . . . . . . . . . . . . . 292

5.13.1 MatrizenkalkĂźl unf komplexe Zahlen . . . . . . . . . . . . . . . . . . . . 292
5.13.2 Diskrete Fourier-Transformation . . . . . . . . . . . . . . . . . . . . . . . 293
5.13.3 Fehlerkorrigierende Codes . . . . . . . . . . . . . . . . . . . . . . . . . . 298
5.13.4 Expandergraphen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
5.13.5 Expander-Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
5.13.6 Markov-Ketten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
5.14 Aufgaben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

Falls nichts anderes gesagt ist, bezeichnet F im Folgenden einen beliebigen KĂśrper, deren Elemente
wir âZahlenâ nennen werden.
253

KAPITEL 5. LINEARE ALGEBRA

254

5.1 Lineare VektorrĂ¤ume
Ein Vektor x â Fn ist eine Folge x = (x 1 , . . . , x n ) von Zahlen x i â F. Vektoren kann man komponentenweise addieren und mit einer Zahl 1 (oder Skalar) Îť â F multiplizieren: x+y = (x 1 +y1 , . . . , x n +yn )
und Îťx = (Îť x 1 , . . . , Îť x n ).

z = 3x
z=x+y
x
y

x

Nicht jede Teilmenge von Vektoren in Fn ist unter diesen zwei Operationen abgeschloĂen. AbgeschloĂene Teilmengen heiĂen VektorrĂ¤ume. D.h. eine Teilmenge V â Fn ist ein Vektorraum Ăźber den
KĂśrper F, falls folgendes gilt:
- u â V und Îť â F â Îťu â V ;
- u, v â V â u + v â V .
Insbesondare ist Fn ein Vektorraum, und der Nullvektor 0 = (0, . . . , 0) ist im jeden Vektorraum
enthalten (da 0 Âˇ u = 0 gilt). Deshalb kann man VektorrĂ¤ume bekommen indem man eine beliebige
(nicht leere) Teilmenge A â Fn der Vektoren nimmt und die Menge
( k
)
X
span(A) =
Îť i vi : k â N, v1 , . . . , vk â A, Îť 1 , . . . , Îť k â F
i=1

aller endlichen Linearkombinationen von Vektoren in A betrachtet. Man sagt auch, dass span(A) von
den Vektoren in A erzeugte (oder aufgespannte) Vektorraum ist.



Gilt V = span(A) fĂźr eine endliche Menge A, so sagt man, dass V ein endlich erzeugter Vektorraum ist. In diesem Kapitel werden wir nur solche VektorrĂ¤ume betrachten. D.h. unter âVektorraumâ werden wir immer einen endlich erzeugten Vektorraum verstehen!
Satz 5.1. Sei A eine endliche Menge der Vektoren in Fn . Der von A erzeugte Vektorraum span(A)
bleibt unverĂ¤ndert, wenn man:
1. Einen Vektor v von A mit einer Zahl x , 0 multipliziert.
2. Einen Vektor v von A durch die Summe v + u von v mit einem anderen Vektor u â A ersetzt.
Beweis. Zu 1: Den Koeffizient Îť zu v in jeder Linearkombination von A kann man durch Îť/x ersetzen.
Zu 2: Die Koeffizienten Îť und Âľ zu v und u in jeder Linearkombination von A kann man durch die
Koeffizienten Îť und Âľ â Îť ersetzen.

1Komponentenweise Multiplikation von x mit einer Zahl Îť entspricht dem âSkalierungâ von x â VerlĂ¤ngerung oder
VerkĂźrzung um Faktor Îť.
ÂŠ 2003 by S. Jukna

5.1. LINEARE VEKTORRĂUME

255

Sei A = {v1 , . . . , vn } ein Erzeugungssystem von V , d.h. span(A) = V . Welche Vektoren (wenn Ăźberhaupt) kann man aus A weglassen, ohne die Erzeugunseigenschaft zu zerstĂśren? Ein Vektor vn â A
is âĂźberflĂźssigâ, wenn span(A \ {vn }) = span(A) gilt, also wenn vn in span(v1 , . . . , vnâ1 ) liegt. Dann
gibt es also Zahlen Îť 1 , . . . , Îť nâ1 mit
vn = Îť 1 v1 + . . . + Îť nâ1 vnâ1
bzw.
0 = Îť 1 v1 + . . . + Îť n vn
mit Îť n , 0 (nĂ¤mlich Îť n = â1). Anders ausgedrĂźckt: Der Nullvektor lĂ¤sst sich als Linearkombination
der vi âs darstellen, wobei nicht alle Koeffizienten gleich Null sind. Diese Beobachtung fĂźhrt uns zum
folgenden wichtigen Konzept der linearer Algebra
Definition: Vektoren v1 , . . . , vm â V heiĂen linear unabhĂ¤ngig, wenn aus
Îť 1 v1 + . . . + Îť m vm = 0 mit Îť i â F
stets folgt: Îť 1 = Îť 2 = . . . = Îť m = 0.
Mit anderen Worten, v1 , . . . , vm â V sind linear unabhĂ¤ngig, wenn der Nullvektor 0 = (0, . . . , 0) nur
die triviale Darstellung 0 = 0v1 + . . . + 0vm zulĂ¤sst.

z
x

z
x

yâ

y

Abbildung 5.1: Vektoren x, y, z sind linear abhĂ¤ngig, da z = x + y gilt. Vektoren x, yâ˛, z sind auch linear
abhĂ¤ngig, da z = x + 12 yâ˛ gilt.

â˛ Beispiel 5.2 : Seien2
ďŁŽ

ďŁš
ďŁŽ ďŁš
ďŁŽ ďŁš
ďŁŽ ďŁš
2
0
0
1
ďŁ°
ďŁť
ďŁ°
ďŁť
ďŁ°
ďŁť
ďŁ°
v1 = 0 , v2 = 1 , v3 = 0 , v4 = 0 ďŁť .
0
0
1
1

Dann sind die Vektoren v1 , v3 , v4 linear abhĂ¤ngig, da
ďŁŽ ďŁš
ďŁŽ ďŁš
ďŁŽ ďŁš
1
2
0
1
ďŁ° 0 ďŁť = ÂˇďŁ° 0 ďŁť+1 ÂˇďŁ° 0 ďŁť.
2
1
0
1

Aber die ersten drei Vektoren v1 , v2 , v3 sind linear unabhĂ¤ngig. (Warum?)
2Vektoren schreibt man entweder waagerecht (als âZeilenâ) oder senkrecht (als âSpaltenâ).
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

256

5.2 Basis und Dimension
Ein Erzeugungssystem A fĂźr V = span(A) ist minimal, falls span(A \ {v}) , V fĂźr alle v â A gilt (d.h.
wir kĂśnnen die Menge B nicht verkleinern, ohne die Erzeugungseigenschaft zu zerstĂśren). Solche
minimale Erzeugungssysteme nennt man Basisen von V .
Zum Beispiel die Menge B = {v1 , v2 , v3 } mit
ďŁŽ ďŁš
ďŁŽ ďŁš
ďŁŽ ďŁš
1
0
0
v1 = ďŁ° 0 ďŁť , v2 = ďŁ° 1 ďŁť , v3 = ďŁ° 0 ďŁť
0
0
1
ist eine Basis von V = F3 ; man nennt diese Basis Standardbasis.

Es ist klar, dass ein Vektorraum viele verschiedene Basisen besitzen kann. Es ist deshalb interessant,
dass nichtdestotrotz alle diese Basisen denselben Anzahl von Vektoren haben mĂźssen! Diese wichtige
Eigenschaft der VektorrĂ¤ume liefert uns der folgender Satz.
Satz 5.3. (Basisaustauschsatz von Steinitz) Sei B eine Basis von V und x â V , x , 0. Dann gibt es
ein v â B, so dass (B \ {v}) âŞ {x} auch eine Basis von V ist.
Pn
Beweis. Sei B = {v1 , . . . , vn } und sei x =
i=1 Îť i vi die Darstellung von x. Da x , 0, muss es
mindestens ein k mit Îť k , 0 geben. Nehmen wir o.B.d.A. an, dass k = 1 ist. Wir wollen zeigen, dass
dann B â˛ = {x, v2 , . . . , vn } auch eine Basis von V ist. D.h. wir mĂźssen zeigen dass: (i) span(B â˛) = V ,
und (ii) B â˛ linear unabhĂ¤ngig ist.
Pn
(i) span(B â˛) = V : Aus x = i=1
Îť i vi folgt, dass
n

v1 =

X Îťi
1
x1 â
vi
Îť1
Îť1
i=2

Da sich somit der entfernte Vektor v1 als Linearkombination aus span(B â˛ ) darstellen lĂ¤sst, muss
span(B) in span(B â˛) enthalten sein. Da offensichtlich auch span(B â˛ ) â span(B) gilt (x liegt ja in
span(B)), muss die Gleicheit span(B â˛) = span(B) und damit auch span(B â˛ ) = V gelten.
(ii) B â˛ = {x, v2 . . . , vn } ist linear unabhĂ¤ngig: Sei
0 = Âľ1 x +

n
X

Âľ i vi

(5.1)

i=2

= Âľ1

n
X

Îť i vi +

i=1

n
X

Âľ i vi

i=2

= Âľ1 Îť 1 v1 +

n
X

(Îť 1 + Âľi )vi .

i=2

Da B linear unabhĂ¤ngig ist, mĂźssen die sĂ¤mtliche Koeffizienten gleich 0. Insbesondare muss auch
Âľ1 Îť 1 = 0 gelten. Da aber Îť 1 , 0, muss Âľ1 = 0 sein. Dann gilt jedoch (nach (5.1))
0 =

n
X

Âľ i vi .

i=2

ÂŠ 2003 by S. Jukna

5.2. BASIS UND DIMENSION

257

Da B linear unabhĂ¤ngig ist,3 mĂźssen alle Âľi = 0 sein. Also ist auch B â˛ linear unabhĂ¤ngig.



Korollar 5.4. (Dimension) Ist V ein endlich erzeugter Vektorraum, so besteht jede Basis von V aus
der gleichen Anzahl von Vektoren.
Diese Zahl heiĂt Dimension von V , in Zeichen dim(V ).
Beweis. Seien A = {u1 , . . . , un } und B = {v1 , . . . , vm } zwei Basen von V . Nehmen wir an, dass m < n
gilt. Nach dem Basisaustauschstz kĂśnnen wir m (oder weniger) Vektoren aus A durch die Vektoren
v1 , . . . , vm ersetzen, so dass die neue Menge Aâ˛ immer noch eine Basis ist. Da m kleiner als n ist,
muss mindestens ein Vektor u â A \ B auch in der Menge Aâ˛ bleiben. Da aber Aâ˛ â B âŞ {u} und B
eine Basis war, gilt u â span(B) und damit kann Aâ˛ keine Basis sein. Widerspruch.

Der nĂ¤chster Satz erklĂ¤rt warum eigentlich eine Basis auch âBasisâ genannt wird. Genau wie fĂźr anderen mathematischen Strukturen, heiĂen zwei VektorrĂ¤ume U und W isomorph, falls es eine bijektive
Abbildung f : U â W gibt, so dass f (Îťu) = Îť f (u) und f (u + v) = f (u) + f (v) fĂźr alle Îť â F und
u, v â U gilt.
Satz 5.5. Sei V ein Vektorraum Ăźber einen KĂśrper F.
1. Ist B â V eine Basis von V , so lĂ¤sst sich jedes Vektor u â V , u , 0 auf genau eine Weise als
Linearkombination der Vektoren aus B darstellen.
2. Ist d = dim(V ), so ist V isomorph zu dem Vektorraum Fd .
Beweis. Zu 1: Sei B = {v1 , . . . , vd } eine Basis und
span(B) gibt
Pd0 , u â V beliebig. Wegen V =P
d
es einen Vektor x = (x 1 , . . . , x d ) â Fd mit u = i=1
x i vi . Es gelte auĂerdem u = i=1
yi vi mit
d
(y1 , . . . , yd ) â F . Dann ist
0=uâu=

d
X
i=1

x i vi â

d
X
i=1

yi vi =

d
X
i=1

(x i â yi )vi .

(5.2)

Nun ist B als Basis linear unabhĂ¤ngig, also muss x i = yi fĂźr alle i = 1, . . . , d gelten.
Zu 2: Sei B = {v1 , . . . , vd } eine Basis von V . Wie wir bereits gezeigt
Pd haben, gibt es dann fĂźr jedes
u â V genau einen Vektor f (u) = (x 1 , . . . , x n ) â Fd mit u = i=1
x i vi . Damit ist die Abbildung
f : V â Fd bijektiv. Ausserdem, fĂźr alle u, w â V mit f (u) = (x 1 , . . . , x n ) und f (w) = (y1 , . . . , yn )
gilt:
d
d
d
X
X
X
u+w =
x i vi +
yi vi =
(x i + yi )vi
i=1

i=1

i=1

und somit auch

f (u + w) = (x 1 + y1 , . . . , x d + yd ) = (x 1 , . . . , x d ) + (y1 , . . . , yd ) = f (u) + f (w).
Da fĂźr alle Îť â F die auch Gleichheit f (Îťu) = Îť f (u) (Ăśffensichtlich) gilt, ist f : V â Fd ein
Isomorphismus.

3Und jede (nicht leere) Teilmenge von linear unabhĂ¤ngigen Vektoren muss auch (trivialler Weise) linear unabhĂ¤ngig
sein.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

258

Die Zahlen x i in f (u) = (x 1 , . . . , x n ) heiĂen Koordinaten von Vektor u â V bezĂźglich der Basis
B. Beachte, dass die Koordinaten von der gewĂ¤hlten Basis abhĂ¤ngen: Verschiedene Basisen erzeugen
verschiedene Bijektionen f : V â Fd .
Im Fd mit Standardbasis

ďŁŽ

ďŁŻ
ďŁŻ
e1 = ďŁŻ
ďŁ°

hat man fĂźr ein Vektor u â Fd
ďŁŽ
u1
ďŁŻ u2
ďŁŻ
ďŁŻ ..
ďŁ° .

ud

1
0
..
.
0

ďŁš

ďŁš

ďŁŽ

ďŁş
ďŁŻ
ďŁş
ďŁŻ
ďŁş , e2 = ďŁŻ
ďŁť
ďŁ°
ďŁŽ

ďŁş
ďŁŻ
ďŁş
ďŁŻ
ďŁş = u1 ďŁŻ
ďŁť
ďŁ°

ďŁš

1
0
..
.

ďŁš

0
1
..
.

ďŁş
ďŁŻ
ďŁş
ďŁŻ
ďŁş , . . . , ed = ďŁŻ
ďŁť
ďŁ°

0

ďŁŽ

ďŁş
ďŁŻ
ďŁŻ
ďŁş
ďŁş + u2 ďŁŻ
ďŁť
ďŁ°

0

d.h. in diesem Fall gilt: f (u) = u.

ďŁŽ

0
1
..
.
0

ďŁš

0
0
..
.
1

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

ďŁŽ

ďŁş
ďŁŻ
ďŁŻ
ďŁş
ďŁş + . . . + ud ďŁŻ
ďŁť
ďŁ°

0
0
..
.
1

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

5.3 Skalarprodukt und Norm
Das Skalarprodukt (oder inneres Produkt) zweier Vektoren ist die Zahl
hx, yi = x 1 y1 + x 2 y2 + Âˇ Âˇ Âˇ + x n yn .
Zur Schreibweise: Anstatt von hx, yi schreibt man oft x Âˇ y. Das Skalarprodukt hat folgende Eigenschaften (zeige das!):
1. hx, xi > 0, und hx, xi = 0 genau dann, wenn x = 0.
2. hx + y, zi = hx, zi + hy, zi.
3. hÎťx, yi = Îť hx, yi.
Die LĂ¤nge (oder euklidische Norm) eines Vektors x = (x 1 , . . . , x n ) ist definiert durch:
X
1/2
n
1/2
2
kxk := hx, xi =
xi
.
i=1

Die folgende Ungleichung hat sich als sehr nĂźtzlich erwiesen.
Satz 5.6. (CauchyâSchwarz-Ungleichung) FĂźr alle Vektoren x, y â Rn gilt:
| hx, yi | 6 kxk Âˇ kyk.
D.h.

X
n
i=1

x i yi

2

6

X
n
i=1

x 2i

 X
n
i=1

yi2



.

(5.3)

ÂŠ 2003 by S. Jukna

5.3. SKALARPRODUKT UND NORM

259

Beweis. FĂźr jedes Îť â R gilt:
0 6 hÎťx â y, Îťx â yi = hÎťx, Îťx â yi â hy, Îťx â yi
Wir setzen Îť :=

hx,yi
hx,xi

= Îť 2 hx, xi â 2Îť hx, yi + hy, yi .

und erhalten:
0 6

hx, yi2
hx, yi2
hx, yi2
+
hy,
yi
=
hy,
yi
â
hx,
xi
â
2
hx, xi
hx, xi
hx, xi2

woraus hx, yi2 6 hx, xi hy, yi = kxk 2 Âˇ kyk 2 folgt.



Mit dem Skalarprodukt lĂ¤sst sich auch der Begriff des Winkels zwischen zwei Vektoren erklĂ¤ren. FĂźr
x, y , 0 zeigt die Cauchy-Schwarz Ungleichung, dass | hx, yi | 6 kxk kyk, was wir in der Form
hx, yi
61
â1 6
kxk Âˇ kyk
schreiben kĂśnnen. Es gibt dann genau ein Îą â [0, Ď] mit
hx, yi
= cos(Îą).
(5.4)
kxk Âˇ kyk
Man nennt Îą den Winkel zwischen x und y.
Satz 5.7. Seien x, y â Rn . Dann gilt:
1. kxk > 0, und kxk = 0 ââ x = 0
2. kÎťxk = |Îť|kxk
3. kx + yk 6 kxk + kyk (Dreiecksungleichung)

4. kx + yk 2 = kxk 2 + kyk 2 , falls x und y orthogonal sind, d.h. hx, yi = 0 gilt (Pythagoras-Theorem)
Beweis. (1) ist trivial. (2) gilt, denn
p
p
p
(â) â
kÎťxk =
hÎťx, Îťxi =
Îť 2 hx, xi = Îť 2 Âˇ hx, xi = |Îť| Âˇ kxk
â
wobei (â) aus a2 = |a| fĂźr alle a â R folgt. Zum (3) und (4):
kx + yk 2

= hx + y, x + yi

= hx, xi + hy, yi + 2 hx, yi
=

kxk 2 + kyk 2 + 2 hx, yi
2

2

6 kxk + kyk + 2kxk kyk
=

(damit ist (4) bewiesen)
(Cauchy-Schwarz Ungleichung)

2

(kxk + kyk) .


Diese Eigenschaften charakterisieren unsere Anschauung von LĂ¤nge oder Abstand, deshalb heiĂt auch
dist(x, y) := kx â yk
der Abstand von x und y. Die Dreiecksungleichung fĂźr die euklidische Norm liefert und die Dreiecksungleichung fĂźr die Abstand:
dist(x, z) 6 dist(x, y) + dist(y, z).
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

260

5.4 Dimensionsschranke und ihre Anwendungenâ
Ein wichtiges Korollar aus dem Basisaustauschsatz von Steinitz ist auch die Tatsache, dass keine
linear unabhĂ¤ngige Menge in V mehr als dim(V ) Vektoren haben kann.
Korollar 5.8. (Dimensionsschranke) Sind die Vektoren v1 , . . . , vm linear unabhĂ¤ngig in V , so gilt
m 6 dim(V ).
Auf dieser Eigenschaft der linear unabhnĚgigen Vektoren bassiert sich die sogenannte Methode der
linearer Algebra, die viele Anwendungen in der Diskreten Mathematik gefunden hat. Wir demonstrieren die Methode mit zwei Beispielen.
â˛ Beispiel 5.9 : (âOddtownâ) Eine kleine Stadt Namens âEventownâ4 hat n (erwachsene) Einwohner.
Da in so kleinerer Stadt nicht viel los ist, haben die Einwohner eine AktivitĂ¤t gefunden: sie
probieren mĂśglichst viele verschiedene5 Clubs (oder Vereine) zu bilden. Da zu viele Clubs schwer
zu koordinieren sind, hat das Rathaus eine Regelung herausgegeben:
(i) die Anzahl der Mitglieder in jedem Club muss gerade sein,
(ii) je zwei Clubs mĂźssen auch gerade Anzahl gemeisamer Mitglieder haben.
Frage: Wieviele Clubs kĂśnnen die Einwohner mit dieser Regelung bilden? Die Antwort ist einfach: falls alle Einwohner verheiratet sind, kĂśnnen sie mindestens 2 ân/2â Clubs bilden â es reicht,
dass jeder Mann auch seine Frau in einer Club mitnimmt. Das ist viel zu viel fĂźr eine so kleine
Stadt! Um die Ordnung im Stadt wieder herzustellen, ist das Rathaus gezwungen, irgendwie die
Anzahl der Clubs drastisch zu reduzieren. Es ist aber nicht erlaubt, die Regelung komplett neu
umzuschreiben â erlaubt ist nur ein einziges Wort zu Ă¤ndern. Ist das Ăźberhaupt mĂśglich?
Der BĂźrgemeister hat wĂ¤rend seines Studiums die Vorlesung âMathematische Grundlagenâ besucht und hat das Korollar 5.8 gekonnt. Deshalb hat er den folgenden Vorschlag gemacht: ersetze
einfach das Wort âgeradeâ in (i) durch âungeradeâ. Er behauptet, dass dann die Einwohner hĂśchstens n verschiedene Clubs bilden kĂśnnen! Das Rathaus war vor diesem Vorschlag so begeistert,
das es auch die Name der Stadt von âEventownâ auf âOddtownâ geĂ¤ndert hat. Die Frage ist nun,
ob der BĂźrgermeister Recht hat? Und wenn ja, dann warum?
Das kann man mittels linearer Algebra beweisen. Seien A1 , . . . , Am â {1, . . . , n} alle mĂśgliche
Clubs, die in Oddtown gebildet werden kĂśnnen. Formell sieht die neue Regelung folgender MaĂen aus:
(iâ) fĂźr alle i muss | Ai | ungerade sein,
(ii) fĂźr alle i , j muss | Ai âŠ A j | gerade sein.
Behauptung: m 6 n.
Beweis. Sei vi = (vi1 , vi2 , . . . , vin ) â GF(2) n der Inzidenzvektor fĂźr den Club Ai â {1, . . . , n},
d.h. vi j = 1 genau dann, wenn der j-te Einwohner zum Club Ai gehĂśrt. Wenn wir modulo 2
4engl. âgeradeâ = âevenâ
5âVerschiedeneâ bedeutet hier, dass keine zwei Clubs dieselben Mitglieder hat.
ÂŠ 2003 by S. Jukna

5.4. DIMENSIONSSCHRANKE UND IHRE ANWENDUNGENâ

261

rechnen, dann kann man die Regeln (iâ) und (ii) so umschreiben:

vi , v j =



1 falls i = j
0 sonst.

Wir wollen zeigen, dass dann die Vektoren v1 , . . . , vm linear unabhĂ¤ngig sein mĂźssen: aus der
Dimensionsschranke (Korollar 5.8) folgt dann unmittelbar die Ungleichung m 6 dim(GF(2) n ) =
n.
Pm
Um die unabhĂ¤ngigkeit der Vektoren vi zu zeigen, sei i=1
Îť i vi = 0. Dann gilt fĂźr jedes j =
1, . . . , n:
0 = 0, v j =

m
X
i=1

Îť i vi , v j = Îť j v j , v j = Îť j
| {z }
=1

und damit Îť j = 0 fĂźr alle j. Deshalb sind die Vektoren linear unabhĂ¤ngig und die Ungleichung
m 6 n ist bewiesen.


Wir geben auch eine mehr respektable Anwendung der Dimensionsschranke an. Der folgender Satz,
bekannt als Fisher-Ungleichung ist ein der KernsĂ¤tze in der sogenannten Design Theory. Statistiker
R. A. Fisher hat diesen Satz in 1940 fĂźr den spezielfall, wenn k = 1 und alle Mengen gleich groĂ
sind. SpĂ¤ter, de Bruijn and ErdoĚs (1948), R. C. Bose (1949) und Majumdar (1953) haben den Satz
bis zu folgender allgemeiner Form gebracht. Alle usprĂźngliche Beweise dieses Satzes waren echt
kompliziert.
Es ist deshalb Ăźberraschend, wie einfach kann man diesen Satz mit Hilfe der linearen Algebra beweisen.
Fisher-Ungleichung
Seien A1 , . . . , Am verschiedene Teilmengen von {1, . . . , n} mit der Eigenschaft, dass
| Ai âŠ A j | = k
fĂźr ein festes k und alle i , j gilt. Dann gilt: m 6 n.

Beweis. Seien v1 , . . . , vm â {0, 1} n Inzidenzvektoren von A1 , . . . , Am , d.h. Vektor vi hat Einsen in
Positionen j mit j â Ai und sonst Nullen. Beachte, dass die Skalarprodukt vi , v j nichts anderes als
die KardinalitĂ¤t | Ai âŠ A j | des Durchschnitts von Ai und A j ist.
Unser Ziel ist zu zeigen, dass die Vektoren v1 , . . . , vm linear unabhĂ¤ngig in Rn sind, dann folgt die
Behauptung m 6 dim(Rn ) = n aus der Dimensionsschranke (Korollar 5.8).
Wir fĂźhren einen Widerspruchsbeweis. Nehmen wir
dass die Vektoren v1 , . . . , vm linear abhĂ¤ngig
Pan,
m
sind. Dann gibt es reelle Zahlen Îť 1 , . . . , Îť m mit i=1
Îť i vi = 0 und Îť i , 0 fĂźr mindestens ein i.
Weiterhin gilt

| Ai |
falls i = j
vi , v j =
k
falls i , j
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

262
Es folgt:

0 = h0, 0i =
=

=

X
m
i=1

m
X
i=1
m
X
i=1

=

Îť i vi

m
X
i=1

 X
m
j=1

Îť 2i | Ai | + k Âˇ
Îť 2i

Îť j vj

| Ai | + k Âˇ



X

i=1

Îť 2i hvi , vi i +

2

âkÂˇ

X
m

2

Îťi

Îť 2i (| Ai | â k) + k Âˇ

m
X

X

Îť i Îť j vi , v j

16i, j6m

Îťi Îť j

16i, j6m
X
m
i=1

=

i=1

Îťi

m
X

Îť 2i

i=1

.

Es ist klar, dass | Ai | > k fĂźr alle i gilt und | Ai | = k fĂźr hĂśchstens ein i gelten kann (da sonst wĂźrde
die Eigenschaft | Ai âŠ A j | = k verletzt). Wir wissen, dass nicht alle Zahlen Îť 1 , . . . , Îť m gleich Null
sind. Sind es mindestens zwei davon ungleich Null, so ist bereits die erste Summe ungleich Null. Ist
nur eine von dieser Zahlen ungleich Null, so muss die zweite Summe ungleich Null sein. In beiden
FĂ¤llen bekommen wir ein Widerspruch.


In vielen Anwendungen in der Informatik braucht man einige Objekte mit spezifischen Eigenschaften
expilizit zu konstriuieren. Und die lineare Algebra kann hier oft hilfreich sein. Wir demonstrieren dies
auf einem Beispiel.
FĂźr einen Graphen G = (V, E) sei Ď(G) die grĂśĂte Zahl r, so dass der graph G eine Clique oder
eine unabhĂ¤ngige Menge der GrĂśss e r besitzt. Existenz von Graphen auf n Knoten mit Ď(G) 6
2 log n haben wir bereits in Abschnitt 4.17 mit Hilfe von sogenannten âProbabilistischen Methodeâ
bewiesen. Aber die Konstruktion von solchen (sehr merkwĂźrdigen!) Graphen ist sehr schwierig. Es
ist auch schwierig, Graphen mit Ď(G) 6 nÇŤ mit ÇŤ < 1, zu konstruieren: Bislang sind nur explizite
Graphen G mit Ď(G) = n1/2 bekannt. Solche Graphen sind mit Hilfe der sogenannten âLineareAlgebra-Methodeâ konstruiert. Um diese Methode zu demonstrieren, werden wir nun einen Graphen
mit Ď(G) = n1/3 konstruieren.

Sei n = 3t und betrachte den Graphen G = (V, E), deren Knoten alle 3-elementige Teilmengen von
{1, . . . , t} sind. Zwei Mengen (Knoten) A und B sind adjazent genau dann, wenn | A âŠ B| = 1 gilt.
Satz 5.10. (Nagy 1972) Der Graph G hat n = Î(t 3 ) Knoten und enthĂ¤lt weder eine Clique noch eine
unabhĂ¤ngige Menge mit t + 1 Knoten.

Beweis. Sei A1 , . . . , Am eine Clique in G. Dann gilt | Ai âŠ A j | = 1 fĂźr alle 1 6 i , j 6 m. Nach
Fisherâs Ungleichung muss dann m 6 t gelten.
Sei nun A1 , . . . , Am eine unabhĂ¤ngige Menge in G. Dann gilt | Ai âŠ A j | â {0, 2} fĂźr alle 1 6 i , j 6 m.
D.h. alle | Ai | = 3 sind ungerade und alle | Ai âŠ A j | sind gerade Zahlen. Der Beispiel mit dem OddTown sagt uns, dass auch in diesem Fall m 6 t gelten muss.

ÂŠ 2003 by S. Jukna

5.5. MATRIZEN

263

5.5 Matrizen
Endliche Mengen von Vektoren betrachtet man Ăźblicherweise als âMatrizen.â Eine Matrix ist ein
rechteckiges Zahlenschema. Eine m Ă n Matrix Ăźber einem KĂśrper besteht aus m waagerecht verlaufenden Zeilen und n senkrechten verlaufenden Spalten:
ďŁŽ

ďŁŻ
ďŁŻ
A=ďŁŻ
ďŁ°

a11
a21
..
.

a12
a22
..
.

ďŁš

. . . a1n
. . . a2n
..
.

am1 am2 . . . amn

ďŁş
ďŁş
ďŁş
ďŁť

mit ai j â F. Wenn die Zahlen m und n bekannt sind, schreibt man oft kurz A = (ai j ) oder6 A = (ai, j ).

Die transponierte Matrix von einer m Ă n-Matrix A = (ai j ) ist die n Ă m-Matrix Aâ¤ = (ci j ) mit ci j =
a ji . D.h. man bekommt eine transponierte Matrix AT indem man die Matrix Ăźber die Hauptdiagonale
âumkipptâ: Zeilen von A sind dann die Spalten von AT und Spalten von A sind dann die Zeilen von
Aâ¤ .
Vektoren x â Fn kann man auch als Matrizen betrachten: entweder als einen Zeilenvektor (1 Ă n
Matrix) oder als einen Spaltenvektor (n Ă 1 Matrix).

Eine n Ă n Einheitsmatrix hat die Form (wenn die Dimension n aus dem Kontext klar ist, werden wir
einfach E anstatt En schreiben):
ďŁŽ

ďŁŻ
ďŁŻ
ďŁŻ
En = ďŁŻ
ďŁŻ
ďŁ°

1
0
0
..
.

0
1
0
..
.

0
0
1
..
.

...
...
...
..
.

0
0
0
..
.

0 0 0 ... 1

ďŁš
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁť

Eine m Ă n Matrix A = (ai j ) kann man mit einem Spaltenvektor x der LĂ¤nge n von rechts wie auch
mit einem Zeilenvektor y der LĂ¤nge m von links multiplizieren:
â˘ Ax ist ein Spaltenvektor, deren EintrĂ¤ge die Skalarprodukte von x mit der Zeilen von A sind.
Anschaulich:

=

â˘ yA ist ein Zeilenvektor, deren EintrĂ¤ge die Skalarprodukte von y mit der Spalten von A sind.
Anschaulich:
6Man trennt die Indizes i und j durch die Komma nur dann, wenn man mĂśgliche Verwechlunge vermeiden will. Will
man zum Beispiel das Element in i-ter Zeile und (n â j)-ter Spalte angeben, so schreibt man âai, nâ j â, nicht âainâ j â.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

264

=
â˘ Multipliziert man A von beiden Seiten, so bekommt man eine Zahl yAx, d.h. das Skalarprodukt
von yA und x:
m X
n
X
yAx = hyA, xi = hy, Axi =
a i j yi x j .
i=1 j=1

Nun schauen wir, wie man zwei Matrizen addieren und multiplizieren kann.
â˘ Die Summe zweier m Ă n-Matrizen A = (ai j ) und B = (bi j ) ist die Matrix A + B = (ci, j ) mit
ci j = ai j + bi j .
â˘ Die Îť-Fache (Îť â F) von A = (ai j ) ist die Matrix Îť Âˇ A = (ci, j ) mit ci j = Îťai j .
â˘ Wie sollte man zwei Matrizen multiplizieren? Hier kĂśnnte man verschiedene Definitionen wĂ¤hlen.
Wir werden bald sehen, dass in Anwendungen die folgende (auf erstem Blick sehr unnatĂźrliche)
Definition von grĂśĂer Bedeutung ist. Ist A = (ai j ) eine m Ă n-Matrix und B = (bi j ) eine n Ă rMatrix, so ist ihr Matrixprodukt die m Ă r-Matrix A Âˇ B = (ci, j ) mit7
ci j =

n
X
k=1

aik Âˇ bk j .

Trotz aller Indices gibt es eine einfache Merkregel zur Berechnung von ci j :
j te âte Spalte

i te âte
â
Zeile

j
i

i
m

m

=
n

n

r

r

ci j

= Skalarprodukt der i-ten Zeile der ersten Matrix
mit der j-ten Spalte der zweiten Matrix
n
X
=
aik Âˇ bk j
k=1

7Beachte, dass wir nicht komponentenweise multiplizieren. D.h. ci j ist nicht als ci j = ai j Âˇ bi j definiert!! In diesem Fall
hĂ¤tten wir ein âTraumproduktâ â viele Studenten kĂśnnen nur darĂźber trĂ¤umen. Man muss aber sagen, dass (im Gegenteil
zur allgemeinen Auffassung, dass das Traumprodukt nutzlos ist) auch diese Definition des Matrixprodukts (auch bekannt
als Hadamard-Produkt) interessante Anwendungen (insbesondere in Kombinatorik) haben kann.
ÂŠ 2003 by S. Jukna

5.5. MATRIZEN

265



Daran erkennt man im Ăźbrigen sehr deutlich, dass man nicht irgendwelche Matrizen multiplizieren kann: Die Spaltenzahl der ersten Matrix muss mit der Zeilenzahl der zweiten Matrix
gleich sein!

Wenn man die Spalten von B als Vektoren b1 , . . . , br betrachtet, so ist AB die Matrix, deren Spalten
die Matrix-Vektor Produkte Ab1 , . . . , Abr sind.
Bemerkung 5.11. Der (einziger!) Grund, warum die Multiplikation von Matrizen so âunnatĂźrlichâ
definiert ist, ist folgender. Eine Abbildung L : Fn â Fm heiĂt linear, falls fĂźr alle x, y â Fn und
Îť, Âľ â F gilt
L(Îťx + Âľy) = ÎťL(x) + ÂľL(y).
Man kann zeigen (wir werden das nicht tun), dass es fĂźr jede lineare Abbildung L : Fn â Fm eine
m Ă n Matrix A mit L(x) = A Âˇ x gibt. Also sind lineare Abbildungen nichts anderes als Matrix-Vektor
Produkte.
Seien nun h : Fm â Fk und g : Fn â Fm zwei lineare Abbildungen, und A und B die zur diesen
Abbildungen gehĂśrenden m Ă k und n Ă m Matrizen, d.h. h(y) = Ay und g(x) = Bx. Dann ist
die Komposition f (x) = h(g(x)) durch das Produkt AB diesen beiden Matrizen definiert: f (x) =
A(Bx) = (AB)x. Das ist eine der wichtigsten Eigenschaften des Matrizenprodukts Ăźberhaupt!
Satz 5.12. (Rechenregeln)
Distributivgesetze

(A + B) Âˇ C = A Âˇ C + B Âˇ C
A Âˇ (B + C) = A Âˇ B + A Âˇ C

HomogenitĂ¤t

Îť Âˇ (A Âˇ B) = (Îť Âˇ A) Âˇ B = A Âˇ (Îť Âˇ B)

Assoziativgesetz

A Âˇ (B Âˇ C) = (A Âˇ B) Âˇ C

Transponieren

(A + B) â¤ = Aâ¤ + B â¤
(A Âˇ B) â¤ = B â¤ Âˇ Aâ¤ (beachte die Reihenfolge!)

Einheitsmatrix

Em Âˇ A = A und A Âˇ En = A

Beweis. Ăbungsaufgabe.





Matrizenmultiplikation ist i.A. nicht kommutativ, d.h. A Âˇ B = B Âˇ C gilt i.A. nicht! Gegenbeispiel:

 
 
 
 
 

0 1
1 1
0 1
1 1
1 1
1 0
Âˇ
=
,
=
Âˇ
1 0
0 1
1 1
1 0
0 1
0 1

â˛ Beispiel 5.13 : (Unsterbliche Hasen) Wir betrachten folgendes Modell der Entwicklung einer Hasenpopulation: Im ersten Lebensjahr sind Hasen noch nicht erwachsen. Wenn sie ab dem zweiten
Lebensjahr erwachsen sind, dann wirft jedes Hasenpaar jedes Jahr ein Paar Junge. Annahme:
Hasen sterben nie!
Frage: Wie entwickelt sich die Hasenpopulation im Lauf der Jahre?
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

266

Wir bezeichnen mit an die Anzahl der Hasenpaare im ersten Lebensjahr (junge Hasen), und mit
bn die Anzahl der erwachsenen Hasenpaare, jeweils im Jahr Nummer n. Es gelten die Rekursionsformeln:
1. an+1 = bn : es gibt fĂźr jedes im Jahr n erwachsene Hasenpaar ein Paar Junge im nĂ¤chsten
Jahr.
2. bn+1 = bn + an : zu denn schon im Jahr n erwachenen Hasenpaaren kommen im folgenden
Jahr noch die erwachsen gewordenen Junghasen hinzu.
In Matrixform ist also


an+1
bn+1



= AÂˇ



an
bn



mit A =



0 1
1 1



Die Hasenpopulation im Jahr n wird damit durch








an
anâ1
anâ2
a1
nâ1
= AÂˇ
= AÂˇ AÂˇ
= ... = A
Âˇ
bn
bnâ1
bnâ2
b1
beschrieben.
Weil die Hasen in unserem Model nicht sterben, ist es einfach, die Enwicklung auch auf andere
Art zu beschreiben: Wir fĂźhren hier nur Buch Ăźber die Gesamtzahl cn der Hasenpaare im Jahr
n. Die Zahl bn der im Jahr n erwachsenen Hasen ist einfach die Gesamtzahl cnâ1 der Hasen, die
schon im Jahr zuvor da waren. Damit ergibt sich die Formel
cn+2 = cn+1 + cn ,
den im Jahr n+1 kommen zu den an+1 Hasenpaaren des Vorjahres noch die von den erwachsenen,
also schon im Jahr n vorhandenen Hasepaaren geborenen Junghasen zu. Die Folge c1 , c2 , c3 , c4 , . . .
ist also vollstĂ¤ndig durch die Rekursionsformel und die Angabe von c1 , c2 festgelegt. Die Rekursionsformel lĂ¤sst sich auch als

 
 

0 1
cn
cn+1
=
Âˇ
cn+2
1 1
cn+1
schreiben. Der Spezialfall c1 = c2 = 1 heiĂt auch Fibonacci-Folge. Jedes cn lĂ¤sst sich auch
explizit durch die beeindruckende Formel
â !n
â !n !
1
1+ 5
1â 5
â
cn = â
2
2
5
definieren (siehe Beispiel 3.84). Das kann man auch mit Induktion Ăźber n beweisen (Ăbungsaufgabe).
â˛ Beispiel 5.14 : (Anwendung in Ăkonomie) Wir betrachten eine Anzahl von GĂźtern (Waren,
Dienstleistungen), durchnummeriert von 1 bis n. Einen Vektor x = (x 1 , . . . , x n ) deuten wir als
Mengenangaben fĂźr diese GĂźter.
Durch eine n Ă n-Matrix A = (ai j ) beschreiben wir, wieviel von den jeweiligen GĂźtern benĂśtigt
wird, um eines dieser GĂźter herzustellen. Der Koeffizient ai j gibt an, wieviel von dem Gut mit
der Nummer i bei der Produktion einer Einheit von Gut j verbraucht wird.
ÂŠ 2003 by S. Jukna

5.5. MATRIZEN

267

Beispielsweise betrachten wir die Produktion von Stahl. Wir nummerieren in der Reihenfolge: 1
= Steinkohle, 2 = Stahl. Dann besagt die Matrix8


0 3
A=
0, 1 0
folgendes:
-

Zur Produktion einer Tonne Stahl werden drei Tonnen Steinkohle benĂśtigt.

-

Zur FĂśrderung von einer Tonne Steinkohle werden 100 kg Stahl (in Form der Abnutzung von
GerĂ¤ten) benĂśtigt.

In der Ăkonomie sind Modelle mit weitaus mehr GĂźtern gebrĂ¤uchlich, verwandte Matrizen werden fĂźr ganze Volkswirtschaften erstellt (mit einigen hundert Zeilen und Spalten).
Eine Fragestellung fĂźr das obige Model: Wieviel muss produziert werden, wenn 1000 t Stahl verkauft werden sollen? Im einfachsten Beispiel oben mĂźssen dazu 3000 t Steinkohle bereitgestellt
werden, fĂźr deren FĂśrderung wiederum 300 t Stahl verbraucht werden, zu dessen Produktion 900 t
Steinkohle nĂśtig waren, zu deren FĂśrderung 90 t Stahl gebraucht werden, zu dessen Produktion
270 t Steinkohle ... Insgesamt sind wir soweit schon bei 1390 t Stahl und 4170 t Steinkohle, und
es ist klar, dass sich die Rechnung im Prinzip beliebig fortzetzt, allerdings absehbar mit immer
kleineren, am Ende bedeutungslosen Mengen.
Etwas formaler: Um b â Rn zu verkaufen muss natĂźrlich b produziert werden, aber zusĂ¤tzlich
wird Ab benĂśtigt, zur Produktion von Ab wiederum A2 b, zur Produktion von A2 b zusĂ¤tzlich A3 b,
und so weiter, insgesamt
â
X
2
x := b + Ab + A b + . . . =
Ak b.
k=0

Die exakte Bestimmung von x (ohne den Grenzwert zu berechnen) lĂ¤uft so:
x = b + Ab + A2 b + A3 b + . . .
Ab + A2 b + A3 b + . . .

Ax =
x â Ax = b

Also ergibt sich die gesuchte nĂśtige Gesamtproduktion x als LĂśsung des linearen Greichungssystems
(En â A)x = b.
Nun geben wir einen Beispiel, wie die Matrixmultiplikation in Graphentheorie angewandt sein kann.
In vielen Anwendungen tauchen die folgenden Probleme auf. Gegeben sei ein (gerichteter oder ungerichteter) Graph G = (V, E) und eine natĂźrliche Zahl n > 1. FĂźr zwei Knoten i und j interessiert
man sich, ob es einen Weg von i nach j der LĂ¤nge n gibt. 9 Wenn ja, wie viele solcher Wege gibt es
8Matrizen wie
A=



0, 2
0, 1

3
0, 1



sind qualitativ plausibler (schlieĂlich wird auch bei der Stahlproduktion wieder Stahl verbraucht ...)
9Zur Erinnerung: Ein Weg von u nach v in einem ungerichteten Fraphen ist eine Folge u0 ,u1 , . . . ,ul jeweils benachbarter
Knoten. Beachte, dass der Weg einen Knoten wie auch als auch eine Kante mehrmals durchlaufen kann!
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

268

insgesamt? Dazu kann man gut die Matrizenalgebra verwenden: man nimmt die sogenannte âAdjazenzmatrixâ A = (ai j ) von G und berechnet ihre n-te Potenz An . Wir mĂźssen aber zuerst definieren,
was die Adjazenzmatrix Ăźberhaupt ist.
Sei G ein Graph mit der Knotenmenge V = {1, . . . , n}. Die n Ă n Matrix A = (ai j ) mit
ai j =



1
0

falls i und j benachbart in G sind
sonst

heiĂt Adjazenzmatrix von G. Ist der Graph ungerichtet, so ist seine Adjazenzmatrix symmetrisch.
â˛ Beispiel 5.15 : Ein gerichteter Graph mit seiner Adjazenzmatrix:
2

1

ďŁŽ

0
ďŁŻ 0
A=ďŁŻ
ďŁ° 1
0

3

4

1
0
0
0

0
1
0
0

ďŁš
1
0 ďŁş
ďŁş
1 ďŁť
0

â˛ Beispiel 5.16 : Ein ungerichteter Graph G mit seiner Adjazensmatrix A und die Produktmatrix A2 =
A Âˇ A:
2

1

3

4

ďŁŽ

0
ďŁŻ 1
A=ďŁŻ
ďŁ° 1
0

1
0
1
0

1
1
0
1

ďŁš
0
0 ďŁş
ďŁş
1 ďŁť
0

ďŁŽ

2
ďŁŻ
1
A2 = ďŁŻ
ďŁ° 1
1

1
2
1
1

1
1
3
0

ďŁš
1
1 ďŁş
ďŁş
0 ďŁť
1

Mit Ainj werden wir den Eintrag in der i-ten Zeile und j-ten Spalte von An bezeichnen:
Ainj := Eintrag in der i-ten Zeile und j-ten Spalte von An

Satz 5.17. Sei G ein (gerichteter oder ungerichteter) Graph mit den Knoten 1, . . . , m und A = (ai, j )
seine Adjazenzmatrix. Sei An die n-te Potenz von A. Dann ist fĂźr jede 1 6 i, j 6 m
Ainj = Anzahl der Wege der LĂ¤nge n in G von i nach j
Beweis. Wir beweisen den Satz mittels Induktion Ăźber n.
FĂźr n = 1 ist die Behauptung trivialerweise erfĂźllt, denn es gilt A1 = A und die Adjazenzmatrix zeigt
die Zahl aller Kanten, also aller Wege der LĂ¤nge 1 zwischen zwei Knoten an.
ÂŠ 2003 by S. Jukna

5.5. MATRIZEN

269

Sei die Behauptung bereits fĂźr alle Potenzen Ar , r = 1, . . . , n â 1 bewiesen. Nach der Definition der
Matrixmultiplikation ist der Eintrag Ainj in der i-ten Zeile und j-ten Spalte von An = A Âˇ Anâ1 genau
das Skalarprodukt der i-ten Zeile von A mit der j-ten Spalte von Anâ1 , d.h.
Ainj

=

n
X
k=1

aik Âˇ Aknâ1
j ,

nâ1 bezeichnen.
wobei die Aknâ1
j Koeffizienten der Matrix A

FĂźr jedes k, 1 6 k 6 m, ist nun der Summand aik Âˇ Aknâ1
j genau dann von Null verschieden, wenn
nâ1 ist. Da nach Induktionsvoraussetzung Anâ1 die Zahl
=
A
aik = 1 gilt und demzufolge aik Âˇ Aknâ1
j
kj
kj
der Wege der LĂ¤nge n â 1 angibt, die von k nach j fĂźhren, und sich aufgrund der Existenz der Kante
(i, k) (aik = 1) jeder dieser Wege zu einem Weg der LĂ¤nge n von i nach j fortsetzen lĂ¤sst, trĂ¤gt der
Summand aik Âˇ Aknâ1
j genau die Anzahl der Wege der LĂ¤nge 1 + (n â 1) = n von i Ăźber k nach j zur
Summe Ainj bei. Da Ăźber alle Zwischenknoten k (1 6 k 6 m), summiert wird, gibt Ainj wie behauptet
die Zahl sĂ¤mtlicher Wege der LĂ¤nge n an, die in G von i nach j fĂźhren.

Eine der Grundaufgaben in der linearen Algebra betrifft das LĂśsen linearer Gleichungssysteme. Sind
ďŁŽ

ďŁŻ
ďŁŻ
A=ďŁŻ
ďŁ°

a11
a21
..
.

a12
a22
..
.

. . . a1n
. . . a2n
..
.

am1 am2 . . . amn

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

ďŁŽ

ďŁŻ
ďŁŻ
x=ďŁŻ
ďŁ°

x1
x2
..
.
xn

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

ďŁŽ

ďŁŻ
ďŁŻ
b=ďŁŻ
ďŁ°

b1
b2
..
.
bm

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

dann ist
Ax = b
eine prĂ¤gnante AbkĂźrzung fĂźr das Gleichungssystem mit n Unbekanten und m Gleichungen
a11 x 1 + a12 x 2 + Âˇ Âˇ Âˇ + a1n x n

= b1

a21 x 1 + a22 x 2 + Âˇ Âˇ Âˇ + a2n x n

= b2

am1 x 1 + am2 x 2 + Âˇ Âˇ Âˇ + amn x n

= bm

...

Gesucht ist dann die LĂśsungsmenge aller x â Rn , fĂźr die die m Gleichungen gleichzeitig erfĂźllt sind.
Die wichtigsten Fragen Ăźber lineare (wie auch Ăźber allgemeine) Gleichungssysteme sind:
-

Ist Ax = b Ăźberhaupt lĂśsbar?

-

Falls lĂśsbar, wieviele LĂśsungen x dann Ax = b hat?

-

Falls lĂśsbar, wie kann man die LĂśsungen x von Ax = b finden?

Um diese (wie auch viele andere) Fragen Ăźber Matrizen zu beantworten, erwies sich das Konzep des
âRangsâ von Matrizen als sehr hilfreich.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

270

5.6 Rang einer Matrix
Sei A eine m Ă n Matrix Ăźber einen KĂśrper F:
ďŁŽ
a11
ďŁŻ a21
ďŁŻ
A=ďŁŻ .
ďŁ° ..

a12
a22
..
.

. . . a1n
. . . a2n
..
.

am1 am2 . . . amn

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

Der Spaltenraum von A ist der von der Spalten von A erzeugter Vektorraum
V = { Ax : x â Fn } â Fm .
Der Zeilenraum von A ist der von der Zeilen von A erzeugter Vektorraum
W = {yA : y â Fm } â Fn .
Dann heiĂt dim(V ) der Zeilenrang und dim(W ) der Spaltenrang von A.
Eine wichtige Boebachtung ist, dass (nach Satz 5.1) der Zeilenraum (bzw. Spaltenraum) unter folgenden Elementartransformationen unverĂ¤ndert bleiben:
(i) Permutation von Zeilen (bzw. Spalten);
(ii) Addition eines skalaren Vielfachen einer Zeile (bzw. Spalte) zu einer anderen Zeile (bzw. Spalte).



Aber vorsichtig: Permutation von Spalten (bzw. Zeilen) kann den Zeilenraum (bzw. Spaltenraum) verĂ¤ndern! Um das zu sehen, nehmen wir zum Beispiel die Matrix


0 1 0
A=
0 0 1

und vertauschen die erste mit der zweiten Spalte:


1 0 0
â˛
A =
0 0 1
Dann liegt der Vektor (0, 1, 1) in den Zeilenraum von A aber nicht in den Zeilenraum von Aâ˛ .
Trotzdem, man kann leicht zeigen (Ăbungsaufgabe!), dass die Dimensionen dim(V ) und dim(W )
der VektorrĂ¤ume auch bei der Permutationen der Spalten wie auch Zeilen unverĂ¤ndet bleiben. Diese
Eigenschaft erlaubt uns den folgenden Satz zu zeigen.
Satz 5.18. (Rang) Sei A eine m Ă n Matrix Ăźber R. Dann gilt:
Spaltenrang(A) = Zeilenrang(A).
Diese Zahl heiĂt Rang von A und ist mit rk(A) bezeichnet.

ÂŠ 2003 by S. Jukna

5.6. RANG EINER MATRIX

271

Beweis. Induktion nach n. Basis n = 1 (nur eine Spalte) ist trivial: Der Zeilenrang wie auch der
Spaltenrang sind in diesem Fall beide gleich 1.
Induktionsschritt: Wir nehmen an, dass die Behauptung fĂźr alle matrizen mit n â 1 Spalten gilt. Ist die
gegebene Matrix A , 0, so kann man sie durch Anwendung der Elementartransformationen auf die
Form
ďŁŽ
ďŁš
b11 0 . . . 0
ďŁŻ 0
ďŁş
ďŁŻ
ďŁş
B=ďŁŻ .
ďŁş
â˛
ďŁ° ..
ďŁť
A
0

mit b11 , 0 bringen: Durch Permutationen erreicht man zunĂ¤chst b11 , 0 und annuliert dann die
restlichen Elemente der ersten Zeile und Spalte durch geeignete Additionen. Genauer addiert man
zunĂ¤chst das (âb1 j /b11 )-fache der ersten Spalte zu der j-ten Spalte ( j = 2, . . . , n) und verfĂ¤hrt dann
analog mit den Zeilen.
Bezeichnet man mit V â˛ und W â˛ die entsprechenden VektorrĂ¤ume fĂźr die (m â 1) Ă (n â 1) Matrix Aâ˛,
so folgt
dim(V ) = dim(V â˛ ) + 1, dim(W ) = dim(W â˛ ) + 1
und nach der Induktionsannahme dim(V â˛ ) = dim(W â˛) folgt die Behaubtung.



Matrixmultiplikation erlaubt uns den Rang einer Matrix anders charakterisieren:
Satz 5.19. Sei A eine n Ă m Matrix Ăşber einen KĂśrper F. Dann ist rk(A) die kleinste Zahl r fĂźr die es
eine n Ă r Matrix B und eine r Ă m Matrix C mit A = B Âˇ C gibt.
Beweis. Ist der Spaltenrang von A = (ai j ) gleich r, so gibt es r Spalten, die alle verbleibenden Spalten
erzeugen. O.B.d.A. kĂśnnen wir annehmen, dass dies die ersten r Spalten sind (sonst permutiere sie
entsprechend). Sei B die n Ăr Matrix die aus diesen ersten r Spalten von A besteht. Seien b1 , . . . , bn â
Fr die Zeilen von B. Da fĂźr r + 1 6 j 6 m die j-te Spalte von A eine Linearkombination der ersten
r Spalten sein soll, muss es einen Vektor c j â Fr mit ai j = bi , c j geben. Ist j 6 r, so gilt dasselbe
mit c j = (0, . . . , 0, 1, 0, . . . , 0), wobei die Eins in j-ter Position steht. Damit ist jeder Eintrag ai j von
A als Skalarprodukt zweier Vektoren aus Fr dargestellt:
ai j = bi , c j

(â)

â¤
Betrachten man nun die Vektoren câ¤
1 , . . . , cm als Spalten einer r Ă m Matrix C, so kann man die
Bedingung (â) als A = B Âˇ C schreiben.


Bemerkung 5.20. Satz 5.19 erlaubt uns den Rang mit Hilfe von Skalarprodukt zu beschreiben. FĂźr
eine Matrix A = (ai j ) Ăźber F ist rk(A) die kleinste Zahl r, so dass es Vektoren vi â Fr mit der
Eigenschaft ai j = vi , v j zu den Zeilen und Spalten zugewiesen werden kĂśnnen.
Die folgenden Eigenschaften des Rangs bezĂźglich der Matrixaddition und Matrixmultiplikation sind
zwar nicht scharf, sind aber oft sehr nĂźtzlich.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

272
Satz 5.21. 1. Sind A und B zwei m Ă n-Matrizen, so gilt:

rk(A) â rk(B) 6 rk(A + B) 6 rk(A) + rk(B).

(5.5)

2. Ist A eine m Ă n-Matrix und B eine n Ă p-Matrix, so gilt
rk(A) + rk(B) â n 6 rk(A Âˇ B) 6 min {rk(A), rk(B)} .

(5.6)

Beweis. (5.5) folgt aus der Charakterisierung von Rang durch Skalarprodukt.
Zu (5.6): Der Spaltenraum10 von AÂˇ B ist eine Teilmenge des Spaltenraums von A, und der Zeilenraum
von A Âˇ B ist eine Teilmenge des Zeilenraums von B.

Eine (quadratische!) n Ă n Matrix heiĂt singulĂ¤r, falls rk(A) < n gilt. Gilt rk(A) = n, so sagt man,
dass A einen vollen Rang hat; solche Matrizen nennt man auch regulĂ¤r. Eine nĂźtzliche Merkregel ist:
A ist regulĂ¤r (hat vollen Rang) ââ aus Ax = 0 folgt x = 0.
â˛ Beispiel 5.22 : Jede Dreiecksmatrix
ďŁŽ

ďŁŻ
ďŁŻ
ďŁŻ
A=ďŁŻ
ďŁŻ
ďŁ°

a11
0
0
..
.

â
a22
0
..
.

0

0

â
... â
â
... â
a33 . . . â
..
. . ..
. .
.
0
. . . ann

ďŁš
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁť

wobei aii , 0 fĂźr alle i = 1, . . . , n und mit â gefĂźhlten EintrĂ¤ge aus beliebigen Zahlen bestehen,
hat vollen Rang.

5.7 LĂśsbarkeit der linearen Gleichungssysteme
FĂźr eine m Ă n Matrix A Ăźber einem KĂśrper F, sei
Im A = { Ax : x â Fn } â Fm
der Spaltenraum von A. Man kann sich leicht Ăźberzeugen (Ăbungsaufgabe!), dass Im A ein Vektorraum in Fm ist. Also ist das Gleichungssystem Ax = b lĂśsbar genau dann, wenn der Koeffizientenvektor b in diesem Vektorraum liegt. Das system ist universel lĂśsbar, falls â A = Fm . Das Gleichungssystem ist eindeutig lĂśsbar, falls Ax = b fĂźr genau ein x â Fn gilt.
Satz 5.23. FĂźr eine m Ă n Matrix A ist die Gleichungssystem Ax = b
1.

lĂśsbar ââ rk(A) = rk(A|b)

2.

universell lĂśsbar ââ rk(A) = m

2.

eindeutig lĂśsbar ââ rk(A) = n = m

10D.h. der von Spalten aufgespannte Vektorraum.
ÂŠ 2003 by S. Jukna

5.7. LĂSBARKEIT DER LINEAREN GLEICHUNGSSYSTEME

273

Beweis. Seien v1 , . . . , vn die Spaltenvektoren von A.
Ax = b ist lĂśsbar

ââ
ââ

b â span(v1 , . . . , vn )

ââ

rk(A) = rk(A|b).

ââ

rk(A) = m

ââ

ââ

rk(A) = n und m = n

âx â Fn mit Ax = b

span(v1 , . . . vn ) = span(v1 , . . . , vn , b)

jeder Vektor b â Rm ist eine Linearkombination von v1 , . . . , vn
Ax = b ist universell lĂśsbar.

ââ

{v1 , . . . , vn } ist eine Basis von Fn = Fm

ââ

b lĂ¤sst sich auf genau eine Weise als Linearkombination

ââ

Ax = b ist eindeutig lĂśsbar.

der Vektoren v1 , . . . , vn darstellen (Satz 5.5)


Korollar 5.24. Die Gleichungssystem Ax = b Ăźber R hat entweder (i) genau eine LĂśsung, oder (ii)
unendlich viele LĂśsungen, oder (iii) keine LĂśsung.
In dreidimensionallem Raum R3 definiert jede Gleichung ax + by + cz = d eine Ebene E. Die LĂśsung
fĂźr ein System aus 3 solchen Gleichungen ist genau der Durchschnitt E1 âŠ E2 âŠ E3 der entsprechenden
Ebenen. Diese Ebenen kĂśnnen sich schneiden in entweder (i) einem Punkt (= genau eine LĂśsung),
oder (ii) in einer Gerade (= unendlich viele LĂśsungen). Wenn die Geraden E1 âŠE2 , E1 âŠE3 und E2 âŠE3
parallel sind, dann gibt es keine LĂśsung fĂźr das System. Das kann man wie folgt veranschaulichen:

genau eine Loesung

unendlich viele Loesungen

keine Loesung

Lemma 5.25. (Farkas Lemma) Sei A eine m Ă n Matrix Ăźber einen KĂśrper und b â Fm . Dann ist
die Gleichungssytem Ax = b lĂśsbar ââ das Gleichungssystem yâ¤ A = 0 keine LĂśsung y â Fm mit
hy, bi = 0 hat.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

274
Beweis.
Ax = b

ââ

span(A, b) = span(A)

ââ

span(A) â¤ \ span(A, b)â¤ = â

span(A, b)â¤ = span(A) â¤

ââ

es gibt kein y mit yâ¤ A = 0 und hy, bi = 0.

ââ




Dieses Lemma hat eine interessante logische Struktur: âx P(x) ââ ây ÂŹQ(y). D.h. irgenwas
gibt es genau dann, wenn irgenwas anderes nicht gibt! Solche Aussagen sind in der Mathematik

selten.
Is b = 0 der Nullvektor, so heiĂt das Gleichungssystem Ax = b homogen. Die Menge der LĂśsungen
des homogenen Gleichungssystems Ax = 0 bezeichnet man mit Ker A:
Ker A = {x â Fn : Ax = 0}.
Beachte, dass Ker A ein linearer Unterraum von Fn ist â ein solcher Vektorraum heiĂt ein affiner
Raum.



Ist b , 0, so muss U = {x â Fn : Ax = 0} nicht unbedingt ein Vektorraum sein! Ist zum
Beispiel b + b , b, so gehĂśrt x + x nicht zu U:
A(x + x) = Ax + Ay = b + b , b.

Wie gross ist |Ker A|? Diese Frage kann mit dem folgenden Satz beantworten.
Satz 5.26. (Dimensionsformel fĂźr lineare Abbildungen) FĂźr jede mĂn Matrix A Ăźber einem KĂśrper
F gilt:
dim(Ker A) + dim(Im A) = n.
(5.7)
Beweis. Betrachte die durch L(x) = Ax definierte Abbildung L : Fâ Fm . Diese Abbildung ist linear,
da fĂźr alle x, y â Fn und Îť, Âľ â F die Gleichung L(Îťx + Âľy) = ÎťL(x) + ÂľL(y) gilt.

Ist Im L = {0}, so ist Fn = Ker L und wir sind fertig. Nehmen wir also an, dass Im L , {0}. Sei
w1 , . . . , ws â Fm mit s = dim(Im L) eine Basis von Im L, und nehme s Vektoren v1 , . . . , vs â Fn
fĂźr die gilt L(v1 ) = w1 , . . . , L(vs ) = ws . Sei auch u1 , . . . , ur â Fn eine Basis von Ker L; es gilt also
L(u1 ) = . . . = L(ur ) = 0. Es reicht zu zeigen, dass B = {v1 , . . . , vs , u1 , . . . , ur } eine Basis von Fn
bildet. Wir mĂźssen zeigen: B ist linear unabhĂ¤ngig, und span(B) = Fn .
Behauptung: B ist linear unabhĂ¤ngig. Betrachte eine beliebige Linearkombination
s
X

Îť i vi +

i=1

r
X

Âľjuj = 0

j=1

Da L linear ist, gilt
0=

s
X
i=1

Îť i L(vi ) +

r
X
j=1

=0

s
z }| { X
Âľ j L(u j ) =
Îť i wi
i=1

ÂŠ 2003 by S. Jukna

5.8. GAUSS-VERFAHREN

275

P
Da w1 , . . . , ws linear unabhĂ¤ngig sind, es folgt Îť 1 = . . . = Îť s = 0. Also ist rj=1 Âľ j u j = 0. Aber
v1 , . . . , vs bilden eine Basis von Ker L und sind deshalb linear unabhĂ¤ngig. D.h. es muss auch Âľ1 =
. . . = Âľ s = 0 gelten.
Behauptung: span(B) = Fn . Nehmen wir einen beliebigen Vektor v â Fn . Da L(v) â Im L und die
Vektoren w1 , . . . , ws eine Basis von Im L bilden, kann man L(v) als ihre Linearkombination L(v) =
P
s
i=1 Îť i wi darstellen. Da L linear ist, gilt
L(v) =

s
X

Îť i wi =

i=1

s
X

Îť i L(vi ) = L(x)

wobei x :=

i=1

Ps

i=1

Îť i vi .

Da 0 = L(v) â L(x) = L(v â x), liegt Vektor v â x in Ker L. Da die P
Vektoren u1 , . . . , ur eine Basis von
s
Ker L bilden, kann man v â x als ihre Linearkombination v â x = i=1
Âľi ui darstellen. Damit haben
wir den Vektor v als Linearkombination
v =xâ

s
X
i=1

Âľ i ui =

s
X

Îť i vi +

i=1

s
X

Âľ i ui

i=1

der Vektoren aus B dargestellt und v â span(B) gezeigt.



Korollar 5.27. Sei A eine m Ă n-Matrix Ăźber einem KĂśrper F. Dann ist die Menge L aller LĂśsungen
des homogenen Gleichungssystems Ax = 0 ein Unterraum von Rn mit dim(L) = n â rk(A).
Homogene Gleichungssysteme sind gut, um die lineare UnabhĂ¤ngigkeit bzw. lineare AbhĂ¤ngigkeit
von Vektoren v1 , . . . , vn zu zeigen: Fasse die Vektoren als Spalten einer Matrix A und schaue, welche
LĂśsungen x das Gleichungssystem Ax = 0 hat. Jede LĂśsung x gibt uns eine Linearkombination der
Spalten, die gleich 0 ein muss. Also sind v1 , . . . , vn linear unabhĂ¤ngig genau dann, wenn es keine
weitere LĂśsungen von Ax = 0 ausser x = 0 gibt.

5.8 GauĂ-Verfahren
Um die LĂśsbarkeit von Ax = b zu bestimmen reicht es also die Rangs von A und (A|b) zu vergleichen.
Wie bestimmt man aber den Rang einer Matrix? Diese Frage kann man leicht lĂśsen, indem man die
Matrix in eine sogenannte âZeilenstufenformâ ĂźberfĂźhrt.
Eine Matrix A ist in einer Zeilenstufenform, falls sie die folgende Form hat:
ďŁŽ

ďŁŻ
ďŁŻ
ďŁŻ
ďŁŻ
ďŁŻ
ďŁŻ
A=ďŁŻ
ďŁŻ
ďŁŻ
ďŁŻ
ďŁŻ
ďŁŻ
ďŁ°

0 ... 0 â˘ â â â â
0 ... 0 0 0 â˘ â â
0 ... 0 0 0 0 0 0
..
.. .. .. .. ..
.
. . . . .
0 ÂˇÂˇÂˇ 0 0 0 0 0 0
0 ... 0 0 0 0 0 0
..
.. .. .. .. ..
.
. . . . .
0 ... 0 0 0 0 0 0

â
â

â˘

â
â
â
..
.

..
.
0 0
0 0
.. ..
. .
0 0

... â ...
... â ...
... â ...
..
. . ..
. .
.
â˘ â ...
... 0 ...
..
. . ..
. .
.

â
â
â
..
.

ďŁš

ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
â ďŁş
ďŁş
0 ďŁş
ďŁş
ďŁş
ďŁť
... 0 ... 0
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

276

Die â˘âs bezeichnen von Null verschiedene MatrixeintrĂ¤ge (Pivotelemente) und die mit ââs gefĂźhlte
Zone besteht aus beliebigen Zahlen.
Zum Beispiel


1 2 1
0 0 3



0 0 3
1 2 1




und



1 0 1
0 0 3

ďŁŽ



ďŁš

1 2 3
und ďŁ° 4 5 6 ďŁť
0 7 8

sind in Zeilenstufenform,

aber nicht.

Behauptung 5.28. Ist eine Matrix A in Zeilenstufenform, so gilt
rk(A) = Anzahl der Pivotelemente â˘ in A

Beweis. Seien v1 , . . . , vr die Zeilen von A mit Pivotelementen und sei Îť 1 v1 + Îť 2 v2 + . . . + Îť r vr = 0
eine Linearkombination. Da die Spalte zu dem Pivotelement in der ersten Zeile v1 sonst nur Nullen
hat, muss Îť 1 = 0 gelten. Also gilt Îť 2 v2 + . . . + Îť r vr = 0. Nach demselben Argument muss Îť 2 = 0
gelten, usw. Somit haben wir, dass die Zeilen v1 , . . . , vr linear unabhĂ¤ngig sind und damit muss der
Zeilenrang (und damit auch rk(A)) mindestens r sein. Andererseits, kann der Zeilenrang (und damit
auch rk(A)) nicht grĂśĂer als r sein, da alle andere Zeilen nur aus Nullen bestehen.


Die ĂberfĂźhrung einer beliebigen Matrix A = (ai j ) in Zeilenstufenform geht folgendermaĂen vor:
1. Ist in der ersten Spalte ein Eintrag , 0, so kann man die entsprechende Zeile durch Vertauschung
mit der ersten Zeile an die oberste Position bringen.
2. Danach addiert man Vielfache der ersten Zeile zu den folgenden, so dass Ăźberall sonst in der ersten
Spalte nur noch Nullen stehen.
3. Man wendet darauf das Verfahren auf die Matrix an, die entsteht, wenn man die erste Zeile und
die erste Spalte streicht.
WĂ¤hrend der ĂberfĂźhrung einer Matrix in einer Zeilenstufenform erhalten wir (im Allgemeinen) eine
andere Matrix Aâ˛. Satz 5.1 sagt uns aber, dass der Rang von A bleibt dabei unverĂ¤ndert.

â˛ Beispiel 5.29 :
ďŁŽ

ďŁš
1 3 â4
ďŁŻ 3 9 â2 ďŁş
ďŁş
A=ďŁŻ
ďŁ° 4 12 â6 ďŁť
2 6
2
ÂŠ 2003 by S. Jukna

5.8. GAUSS-VERFAHREN

277
1 3 â4
3 9 â2 z2 â 3z1
4 12 â6 z3 â 4z1
2 6
2 z4 â 3z1
1 3 â4
0 0 10
0 0 10 z3 â z2
0 0 10 z4 â z2
1 3 â4
0 0 10
0 0
0
0 0
0

Deshalb ist der Zeilenrang, und deshalb auch der Rank, von A gleich 2.

Um eine LĂśsung x von Ax = b zu bestimmen, kann man nun folgendermaĂen angehen â dieses einfaches Verfahren ist als GauĂ-Verfahren bekannt. Dieses Verfahren besteht aus folgenden drei Schritten:

Schritt 1: ForwĂ¤rtselinination: Bringe die erweiterte Matrix [A|b] zu einer Zeilenstufenform [Aâ˛ |b â˛].

Schritt 2: LĂśsbarkeitsentscheidung: Hat mindestens eine Zeile von [Aâ˛ |b â˛] ohne Pivotelement einen
Eintrag , 0 in der letzten Spalte (zu b), so ist rk(A|b) > rk(A) = r, und das Gleichungssystem ist nicht lĂśsbar (siehe Satz 5.23). Ist das nicht der Fall, so ist das Gleichungssystem
lĂśsbar und wir kĂśnnen den nĂ¤chsten Schritt machen.

Schritt 3: RĂźckwĂ¤rtssubstitution: Die zu Spalten ohne Pivotelemente gehĂśrenden Variablen sind die
freien Variablen. Man lĂśst dann das Gleichungssystem nach den zu den Pivotelementen
gehĂśrenden abhĂ¤ngigen Variablen aus und bestimmt diese nacheinander in AbhĂ¤ngigkeit
von den freien Variablen.

â˛ Beispiel 5.30 :

ďŁŽ

ďŁš
1 3 â4
3
ďŁŻ 3 9 â2 â11 ďŁş
ďŁş
[A|b] = ďŁŻ
ďŁ° 4 12 â6 â6 ďŁť
2 6
2 â10
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

278

1 3 â4
3
3 9 â2 â11 z2 â 3z1
4 12 â6 â6 z3 â 4z1
2 6
2 â10 z4 â 3z1
1 3 â4
3
0 0 10 â20
0 0 10 â18 z3 â z2
0 0 10 â16 z4 â z2
1 3 â4
3
0 0 10 â20
0 0
0
2
0 0
0
4 z4 â 2z3
1 3 â4
3
0 0 10 â20
0 0
0
2
0 0
0
0
LĂśsbarkeitsentscheidung: Das Gleichungssystem Ax = b ist nicht lĂśsbar, da b3â˛ = 2 , 0 ist.
â˛ Beispiel 5.31 :
ďŁš
1 â4
2
0 2
ďŁŻ 2 â3 â1 â5 14 ďŁş
ďŁş
[A|b] = ďŁŻ
ďŁ° 3 â7
1 â5 16 ďŁť
0
1 â1 â1 2
ďŁŽ

1
2
3
0
1
0
0
0
1
0
0
0

â4
â3
â7
1
â4
5
5
1
â4
1
0
0

2
â1
1
â1
2
â5
â5
â1
2
â1
0
0

0
â5
â5
â1
0
â5
â5
â1
0
â1
0
0

2
14
16
2
2
10
10
2
2
2
0
0

z2 â 2z1
z3 â 3z1
z2 â z4
z2 â z3
z3 â 5z4

LĂśsbarkeitsentscheidung: Hier ist rk(A) = 2 = rk(A|b), also ist das System lĂśsbar.
RĂźckwĂ¤rtssubstitution: Die freien Variablen sind x 3 und x 4 . Wir setzen x 3 = Îť, x 4 = Âľ und
erhalten
x2 = 2 + x3 + x4 = 2 + Îť + Âľ
x 1 = 2 + 4x 2 â 2x 3 = 2 + 4(2 + Îť + Âľ) â 2Îť = 10 + 2Îť + 4Âľ
ÂŠ 2003 by S. Jukna

5.9. INVERSEN VON MATRIZEN

279

Alle LĂśsungen ergeben sich daher als
ďŁŽ
ďŁš ďŁŽ
x1
10 + 2Îť + 4Âľ
ďŁŻ x2 ďŁş ďŁŻ 2 + Îť + Âľ
ďŁş ďŁŻ
ďŁŻ
ďŁ° x3 ďŁť = ďŁ°
Îť
x4
Âľ

ďŁš

ďŁŽ

ďŁš
ďŁŽ
10
ďŁş ďŁŻ 2 ďŁş
ďŁŻ
ďŁş=ďŁŻ
ďŁş
ďŁŻ
ďŁť ďŁ° 0 ďŁť+Îť ÂˇďŁ°
0

ďŁš
ďŁŽ
2
ďŁŻ
1 ďŁş
ďŁş+ ÂľÂˇďŁŻ
ďŁť
ďŁ°
1
0

ďŁš
4
1 ďŁş
ďŁş.
0 ďŁť
1

5.9 Inversen von Matrizen
Eine n Ă n-Matrix A heiĂt invertierbar, wenn es eine n Ă n-Matrix Aâ1 gibt, so dass:
Aâ1 Âˇ A = A Âˇ Aâ1 = En .
Aâ1 heiĂt dann die Inverse von A.
Lemma 5.32. (Eigenschaften von Inversen)
(Aâ1 ) â1 = A
(A Âˇ B) â1 = B â1 Âˇ Aâ1
(Aâ¤ ) â1

=

(Aâ1 ) â¤

Inversion ist involutorisch
Inverse eines Produkts (beachte die Reihenfolge!)
Inverse der Transponierten
= Transponierte der Inversen

A invertierbar ââ rk(A) = n ââ aus Ax = 0 folgt x = 0
Beweis. Die erste Eigenschaft ist trivial: multipliziere beide Seiten mit Aâ1 .
Um die zweite Eigenschaft (A Âˇ B) â1 = B â1 Âˇ Aâ1 zu zeigen, benutzen wir einfach die Definition von
(A Âˇ B) â1 : das muss eine Matrix C (falls eine solche existiert) mit der Eigenschaft (AB)C = C (AB) =
E sein. Falls Aâ1 und B â1 existieren, so kann man C := B â1 Âˇ Aâ1 nehmen.
Die dritte Eigenschaft (Aâ¤ ) â1 = (Aâ1 ) â¤ ist wieder trivial.

Ist rk(A) = n, so bilden die Spalten von A eine Basis von Fn . Deshalb hat das Gleichungssystem
A Âˇ X = B eine LĂśssung X fĂźr beliebige n Ă n Matrix B, und damit auch fĂźr B = E. Ist nun A
invertierbar, so folgt aus Satz 5.21(2): rk(A) > rk(A Âˇ Aâ1 ) = rk(E) = n.

Die inverse Aâ1 einer n Ă n Matrix A (falls sie Ăźberhaupt existier) kann man mit dem sogenannten
GauĂ-Jordan Verfahren bestimmen.
Schritt 0: A mit Einheitsmatrix E nach rechts erweitern: A 7â [A|E].
Schritt 1: VorwĂ¤rtselimination mit elementaren Zeilenumformungen bis zur Zeilenstufenform links
von
Schritt 2: Invertierbarkeitstest
Fall 1. Links von steht eine 0 in der Hauptdiagonale
=â A ist nicht invertierbar, fertig.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

280

Fall 2. Links von steht keine 0 in der Hauptdiagonale, dann
Schritt 3: RĂźckwĂ¤rtselimination mit elementaren Zeilenumformungen bis links die Einheitsmatrix E
steht. Rechts von steht dann Aâ1 .

A

Rueck.

Vorw.

E

Aâ1

E

Das Verfahren sieht irgendwie âmagischâ aus â warum soll am Ende die Inverse rechts von | stehen?
FĂźr gegebene n Ă n Matrix A, gesucht ist eine n Ă n Matrix X mit A Âˇ X = E. Sind x1 , . . . , xn die
Spalten von X , so mĂźssen wir die n Gleichungssysteme
ďŁŽ ďŁš
ďŁŽ ďŁš
ďŁŽ ďŁš
1
0
0
ďŁŻ 0 ďŁş
ďŁŻ 1 ďŁş
ďŁŻ 0 ďŁş
ďŁŻ ďŁş
ďŁŻ ďŁş
ďŁŻ ďŁş
Ax1 = ďŁŻ .. ďŁş , Ax2 = ďŁŻ .. ďŁş , . . . , Ax x = ďŁŻ .. ďŁş
ďŁ° . ďŁť
ďŁ° . ďŁť
ďŁ° . ďŁť
0

0

1

lĂśsen. Und das GauĂâJordan Verfahren lĂśst einfach diese Gleichungssysteme simultan! Da am Ende
links von | die Einheitsmatrix steht, mĂźssen die Spalten der rechts stehenden Matrix die gesuchten
LĂśsungen, d.h. Spalten von X = Aâ1 sein.
â˛ Beispiel 5.33 : Gesucht ist die Inverse zu
A=



A mit Einheitsmatrix E nach rechts erweitern:

1 2
3 4
Ziehe 3-faches der ersten Zeile von 2. Zeile ab:

1
2
0 â2
Teile 2. Zeile durch â2:



1 2
0 1

1 2
3 4




1 0
0 1

1 0
â3 1
1

0

3
2

â 12




Ziehe das 2-fache der 2. Zeile von der ersten ab:


1 0 â2
1
3
1
0 1
2 â2
Also ist
â1

A
Probe:

=



â2
3
2

â1

AÂˇ A

1
â 12
=




1 2
3 4

die Inverse zu
 
â2
Âˇ
3
2

1
â 12



A=

=





1 0
0 1

1 2
3 4




ÂŠ 2003 by S. Jukna

5.9. INVERSEN VON MATRIZEN

281

â˛ Beispiel 5.34 : Gegeben sei die Matrix
ďŁŽ

ďŁš
2 0 1
A=ďŁ° 3 1 2 ďŁť
0 1 1

Gesucht ist eine Matrix X mit

ďŁŽ

ďŁš
1 0 0
AÂˇX = E =ďŁ° 0 1 0 ďŁť
0 0 1

Schritt 0: A mit Matrix B nach rechts erweitern: A 7â [A|E].
Schritt 1: VorwĂ¤rtselimination mit
links von
2 0
3 1
0 1
2 0
0 1
0 1
2 0
0 1
0 0

elementaren Zeilenumformungen bis zur Zeilenstufenform
1
0
1
2
0
1
1
0
0
1
1
0
1/2 â3/2 1
1
0
0
1
1
0
1/2 â3/2 1
1/2 3/2 â1

0
0 â3/2z1
1
0
0
1
âz2
0
0
1

Schritt 2: Invertierbarkeitstest. Links von steht keine 0 in der Hauptdiagonale =â A ist invertierbar.
Schritt 3: RĂźckwĂ¤rtselimination mit elementaren Zeilenumformungen bis links die Einheitsmatrix E steht. Rechts von steht dann Aâ1 .
2
0
0
2
0
0
2
0
0
1
0
0

0
1
0
0
1
0
0
1
0
0
1
0

1
1
0
0
1/2 â3/2 1
0 âz3
1/2 3/2 â1 1
1
1
0
0 â2z3
0
â3
2 â1
1/2 3/2 â1 1
0
â1
2 â2 Âˇ1/2
0
â3
2 â1
1/2 3/2 â1 1
Âˇ2
0
â1
1 â1
0
â3
2 â1
1
3
â2 2

Jetzt steht auf der linken Seite die Einheitsmatrix, also rechts die LĂśsung
ďŁŽ
ďŁš
â1
1 â1
2 â1 ďŁť
Aâ1 = ďŁ° â3
3 â2
2
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

282

5.10 OrthogonalitĂ¤t
Die Vektoren x und y heiĂen orthogonal, falls hx, yi = 0 gilt. Wenn A â V eine Teilmenge eines
Vektorraums V ist, dann ist
AâĽ = {x â V : hx, yi = 0 fĂźr alle y â A}
das orthogonale Komplement von A.
Satz 5.35. FĂźr alle A â V ist AâĽ ein Vektorraum.
Beweis. Ăbungsaufgabe.



Sei V ein endlichdimensionaler Vektorraum Ăźber R und sei v1 , . . . , vn seine Basis. Die Basis heiĂt
orthonormal, wenn

1 falls i = j
vi , v j =
0 falls i , j.
In anderen Worten eine Basis ist orthonormal, wenn seine Vektoren die LĂ¤nge 1 haben und senkrecht
zu einanderem liegen. Die Frage, ob jeder endlicherzeugter Vektorraum V eine Orthonormalbasis
bezitzt lĂ¤sst sich positiv mit folgendem Satz beantworten.
Satz 5.36. Jeder endlicherzeugter Vektorraum besitzt eine Orthonormalbasis.
Dies ist eine direkte Folgerung aus dem folgenden Satz.
Satz 5.37. (GramâSchmidt-Orthogonalisierungsverfahren) Sei W â V ein Unterraum von V und
w1 , . . . , wm Orthonormalbasis von W . Ist W , V , dann gibt es ein v â W â¤ , so dass w1 , . . . , wm , v eine
Orthonormalbasis von span(w1 , . . . , wm , v) ist.
Beweis. WĂ¤hle zuert einen beliebigen Vektor a â V \ W , setze Îť i := ha, wi i und betrachte den Vektor
b := a â

m
X

Îť i wi .

i=1

Da fĂźr jedes i0 â {1, . . . , m} gilt:
b, wi 0

=
=

a, wi 0 â

m
X
i=1

Îť i wi , wi 0 = a, wi 0 â Îť i 0 wi 0 , wi 0
| {z }
=1

a, wi 0 â a, wi 0 = 0,

der Vektor b auf allen w1 , . . . , wm senkrecht steht (und damit sind die Vektoren w1 , . . . , wm , b linear
unabhĂ¤ngig (siehe Aufgabe 36), und Vektor v = kbb k leistet das GewĂźnschte.

ÂŠ 2003 by S. Jukna

5.10. ORTHOGONALITĂT

283

Satz 5.38. Sei v1 , . . . , vn eine Orthonormalbasis von V . Dann gilt fĂźr jedes x â V :
x=

n
X
i=1



hx, vi i vi .

Dieser Satz erklĂ¤rt, warum ist es gut, eine Orthonormalbasis v1 , . . . , vn zu haben: Dann kann
man nĂ¤mlich sehr einfach die Koeffizienten der Koordinatendarstellung von Vektoren x â V
bezĂźglich dieser Basis bestimmen!
Pn
Beweis. Sei x = i=1
Îť i vi die (eindeutige, siehe Satz 5.5) Darstellung von x. Die Koeffizienten Îť i
sind dann leicht zu bestimmen:
X
Îť j v j , vi = Îť i
hx, vi i = Îť i hvi , vi i +
| {z }
| {z }
j,i
=1

=0



Sei V ein Vektorraum. Eine Teilmenge U â V heiĂt Untertraum von V , falls U selbst ein Vektorraum
ist.
Satz 5.39. Ist U â V ein Unterraum, so gilt
dim(U ) + dim(U âĽ ) = dim(V )
und jeder Vektor v â V lĂ¤sst sich eindeutig als die Summe
mit u â U und w â U âĽ

v =u+w

darstellen. Vector u = v â w heisst dann die orthogonale Projektion von v auf U und ist mit u =
projU (v) bezeichnet.
Beweis. Ist U = {0} oder U = V , so ist die Aussage trivial. Nehmen wir deshalb an, dass {0} â U â V
ein echter nicht trivialer Unterraum von V ist. Sei {u1 , . . . , ur } eine orthonormale Basis von U. Wende
das GramâSchmidt-Orthogonalisierungsverfahren an und erweitere sie bis zu einer othonormalen Basis {u1 , . . . , ur , v1 , . . . , vs } von V . Wir behaupten, dass dann B = {v1 , . . . , vs } eine (auch orthonormale)
Basis von U âĽ ist.
Zu zeigen: U âĽ â span(B). Ist x â U âĽ, so gilt
=0

r z }| {
s
s
X
X
X
x=
hx, ui i ui +
x, v j v j =
x, v j v j .
i=1

Zu zeigen: B ist unabhĂ¤ngig. Ist

j=1

Ps

j=1

j=1

Îť j v j = 0, so gilt fĂźr jedes i = 1, . . . , s

0 = h0, vi i =

s
X
j=1

Îť j v j , vi = Îť i hvi , vi i = Îť i .

ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

284
U = { (0,y) : y in R }

w

v
U = { (x,0) : x in R }
u

Abbildung 5.2: u = projU (v)

Projektionen haben die Eigenschaft, dass x = projU (v) derjeniger Vektor in U ist, der die kleinste
Abstand zu Vektor v hat.
Satz 5.40. Sei U â V ein Unterraum, v â V und u = projU (v). Dann gilt: kv â xk > kv â uk fĂźr alle
x â U, x , u.
Beweis. Da u = projU (v), muss es ein w â U âĽ mit v = u + w geben. Dies bedeutet insbesondare,
dass der Vektor v â u = w in U âĽ liegen muss. Andererseits, liegt der Vektor u â x in U, da beide
Vektoren u und x in U liegen. Damit wissen wir, dass die Vektoren v â u und u â x orthogonal sind,
d.h. hv â u, u â xi = 0 gilt. Mit der Anwendund des Pythagoras-Theorems erhalten wir:
kv â xk 2 = k(v â u) + (u â x)k 2
= kv â uk 2 + ku â xk 2
> kv â uk

2

(Pythagoras)
(da x , u)


Ist U â Fn ein Unterraum, so wissen wir bereits (siehe Satz 5.39), dass jeder Vektor v â V sich
eindeutig als die Summe
v=u+w
mit u â U und w â U âĽ
darstellen lĂ¤sst. Der Vector u = v â w heisst dann die orthogonale Projektion von v auf U und ist mit
u = projU (v) bezeichnet. Nun wollen wir die Projektionen auch berechnen kĂśnnen. D.h. wir wollen
eine Matrix A finden, so dass projU (v) = Av gilt.
Satz 5.41. FĂźr jeden Unterraum U â Fn gibt es eine n Ă n Matrix A, so dass A = Aâ¤ (A ist symmetrisch), AAâ¤ = A (A ist idempotent) und fĂźr v â Fn
projU (v) = Av
gilt.
Beweis. Sei B = [b1 , . . . , bk ] mit k = dim(U ) die n Ă k Matrix, deren Spalten b1 , . . . , bk eine Basis
von U bilden. Betrachte die folgende n Ă n Matrix
A = B(B â¤ B) â1 B â¤ .
ÂŠ 2003 by S. Jukna

5.11. DETERMINANTEN

285

Aus (X Âˇ Y ) â¤ = Y â¤ Âˇ X â¤ und (X â¤ ) â¤ = X folgt, dass A eine symmetrische und indempotente Matrix
ist.
Sei nun u = projU (v). Dann gibt es ein (eindeutiger) Vektor w â U â¤ mit v = u + w. Wir kĂśnnen den
P
Vektor u als eine Linearkombination u = Bx = ki=1 x i bi der Basisvektoren darstellen. Wir wollen
den Vektor x = (x 1 , . . . , x k ) der Koeffizienten bestimmen. Da v = u + w, wissen wir, dass
!
k
X
v=
x i bi + w
i=1

gilt. Wir betrachten nun die Skalarprodukte von v mit Basisvektoren:
bj, v =

k
X

x i b j , bi + b j , w =

i=1

k
X

x i b j , bi

i=1

In der Form von Matrizen kĂśnnen wir dies als
ďŁŽ
hb1 , b1 i hb1 , b2 i Âˇ Âˇ Âˇ hb1 , bk i
ďŁŻ hb2 , b1 i hb2 , b2 i Âˇ Âˇ Âˇ hb2 bk i
ďŁŻ
ďŁŻ
..
..
..
ďŁ°
.
.
.
ÂˇÂˇÂˇ
hbk , b1 i hbk , b2 i Âˇ Âˇ Âˇ hbk , bk i

umschreiben. Dies ist in der Form

ďŁš ďŁŽ
ďŁş ďŁŻ
ďŁş ďŁŻ
ďŁşÂˇďŁŻ
ďŁť ďŁ°

x1
x2
..
.
xk

ďŁš

ďŁŽ

ďŁş ďŁŻ
ďŁş ďŁŻ
ďŁş=ďŁŻ
ďŁť ďŁ°

hb1 , vi
hb2 , vi
..
.
hbk , vi

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

(B â¤ B)x = B â¤v.
Da B und v gegeben sind, reicht es diesen Gleichungssystem auf x auflĂśssen. Da die Spalten von B
linear unabhĂ¤ngig sind, hat die Matrix C = B â¤ B den vollen Rank. (Warum? Ăbungsaufgabe!) Damit
ist diese Matrix invertierbar und wir kĂśnnen das Gleichungssystem oben nach x auflĂśssen, indem wir
beide Seiten mit C â1 = (B â¤ B) â1 multiplizieren
x = B(B â¤ B) â1 B â¤ Âˇ v = A Âˇ v.
Zusammen mit u = Bx folgt daraus die Behauptung:
projU (v) = u = Bx = B(B â¤ B) â1 B â¤v = Av.


5.11 Determinanten
In diesem Abschitt betrachten wir nur quadratische Matrizen, d.h. n Ă n-Matrizen. Mit jeder solchen
Matrix A kann man eine Zahl â sogenannte âDeterminanteâ det A in Verbindung bringen. Sei Sn die
Menge aller Permutationen Ď : {1, 2, . . . , n} â {1, 2, . . . , n}. Ist
ďŁš
ďŁŽ
a11 a12 . . . a1n
ďŁŻ a21 a22 . . . a2n ďŁş
ďŁŻ
ďŁş
A = ďŁŻ ..
ďŁş
..
..
ďŁť
ďŁ° .
.
.
an1 an2 . . . ann

ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

286

eine n Ă n-Matrix Ăźber F, und in F gilt 1 + 1 , 0, so ist ihre Determinante det A definiert durch:
X
(5.8)
det A :=
Ď(Ď)a1Ď (1) a2Ď (2) Âˇ Âˇ Âˇ Âˇ Âˇ anĎ (n)
Ď âS n

wobei Ď(Ď) das Signum (+1 oder â1) von
auch rekursiv wie folgt ausrechnen:

Q

16i< j6n (Ď( j)

â Ď(i)) ist. Die Determinante kann man

n = 1:

det A := a11
n
X
n > 1: det A :=
(â1) k+1 ak1 Âˇ det Ak1 (Entwicklung nach der 1. Spalte)
k=1

Hier ist Ak1 eine (n â 1) Ă (n â 1)-Untermatrix von A ohne ersten Spalte und k-ten Zeile.
Die Determinante von A bezeichnet man mit

det A =

a11
a21
..
.

a12
a22
..
.

. . . a1n
. . . a2n
..
.

an1 an2 . . . ann




a1
b1
Zum Beispiel, ist A = [a, b mit Spalten a =
und b =
, so ist
a2
b2


a1 b1
a1 b1
det
= a1 b2 â a2 b1
=
a2 b2
a2 b2
die Determinante von a und b. Die Vektoren a und b sind genau dann linear abhĂ¤ngig, wenn sie auf
einer Geraden liegen (kolinear sind). Anderenfalls spannen a, b ein Parallelogramm auf; dieses hat
die FlĂ¤che
F = kak Âˇ h = kak Âˇ kbk sin Îą
wobei Îą der Winkel zwischen a und b ist.

h = || b|| sin( Îą)

b
Îą

a

t = || b|| cos( Îą)

Wenn wir nun das Quadrat von F betrachten, erhalten wir:
F 2 = kak 2 Âˇ kbk 2 Âˇ sin2 Îą = kak 2 Âˇ kbk 2 Âˇ (1 â cos2 Îą) (Pythagoras)


ha, bi2
2
2
= kak Âˇ kbk Âˇ 1 â
(5.4)
kak 2 Âˇ kbk 2
= kak 2 Âˇ kbk 2 â ha, bi2

= (a12 + a22 )(b21 + b22 ) â (a1 b1 + a2 b2 ) 2
= (a1 b2 â a2 b1 ) 2 .

ÂŠ 2003 by S. Jukna

5.11. DETERMINANTEN

287

Also ist F = | det (a, b)|. Fazit: Determinante von A gibt uns die FlĂ¤che des von den Spalten von A
ausgespannten Parallelogramms. Daraus folgt auch:
det (a, b) = 0 ââ a und b linear abhĂ¤ngig sind.
Behauptung 5.42. Seien A, B beliebige n Ă n-Matrizen und E eine Einhatsmatrix. Dann gilt:
1. det E = 1.
2. det A = 0, wenn A eine Nullzeile enthĂ¤lt.
3. det Îť A = Îť n det A.
4. det Aâ¤ = det A.
5. Entsteht B aus A durch Vertauschung zweier Zeilen oder zweier Spalten, so gilt
det B = â det A.
6.

a

(5.9)

det A Âˇ B = det A Âˇ det B.

a Die

âRegelâ det A + B = det A + det B ist fĂźr n > 2 falsch!

Die ersten vier Eigenschaften folgen unmittelbar aus der Definition 5.8. Die letzten zwei sind weniger
trivial.
Eine der wichtigsten Eigenschaften der Determinante ist ihre linearitĂ¤t.
Satz 5.43. (LinearitĂ¤t von Determinanten) Die Determinante ist linear in den Zeilen, d.h. wenn wir
die r-te Zeile als Vektor ai = (ai1 , . . . , ain ) abkĂźrzen und wenn ai = Îťbi + Âľci ist, so gilt
ďŁŽ
ďŁš
ďŁŽ
ďŁš
ďŁŽ
ďŁš
..
..
..
.
ďŁŻ
ďŁş
ďŁŻ . ďŁş
ďŁŻ . ďŁş
ďŁŻ
ďŁş
ďŁŻ
ďŁş
ďŁş
det ďŁ° Îťbi + Âľci ďŁť = Îť det ďŁ° bi ďŁť + Âľ det ďŁŻ
ďŁ° ci ďŁť
..
..
..
.
.
.

.
wobei hier die Punkte .. andeuten sollen, dass dieser Teil des Vektors bei der Rechnung unverĂ¤ndert
bleibt. Dasselbe gilt auch fĂźr Spalten.
Ist die Matrix A bereits in einer Zeilenstufenform
ďŁŽ
a11 â
ďŁŻ 0
a22
ďŁŻ
ďŁŻ 0
0
A=ďŁŻ
ďŁŻ ..
..
ďŁ° .
.
0

0

â
... â
â
... â
a33 . . . â
..
. . ..
. .
.
0
. . . ann ,

dann gilt offensichtlich:

ďŁš
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁť

det A = a11 Âˇ a22 Âˇ Âˇ Âˇ ann .
Eine natĂźrliche Frage deshalb ist, ob (und wenn ja, dann wie) sich die Determinante verĂ¤ndert, wenn
man die Matrix mit Hilfe von Elementartransformationen zu einer Zeilenstufenform ĂźberfĂźhrt?
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

288

Elementartransformationen sind zwei: (i) Permutation von Zeilen und (ii) Addition eines skalaren
Vielfachen einer Zeile zu einer anderen Zeile. Wir wissen bereits, dass die erste Transformation nur
die Vorzeichen der Determinante verĂ¤ndern kann: Entsteht B aus A durch Vertauschung zweier Zeilen
oder zweier Spalten, so gilt det B = â det A. Was aber mit der zweiten Transformation? Ăberaschenderweise ist diese (zweite) Transformation noch harmlosser: Sie verĂ¤ndert nicht mal die Vorzeichen!
Satz 5.44. Entsteht B aus A durch Addition einer mit Îť multiplizierten Zeile zu einer anderen, so gilt
det B = det A.
Beweis. Sei A eine n Ă n Matrix. Da durch das Transponieren einer Matrix Spalten in Zeilen umgeformt werden und det Aâ¤ = det A gilt, genĂźgt es den Spaltenfall zu betrachten.11
Wird ein Îť-faches der j-ten Spalte a j zu der i-te Spalte ai , i , j addiert (also wird ai durch ai + Îťa j
ersetzt), so erhĂ¤lt man die Matrix
Aâ˛ = [a1 , . . . , ai + Îťa j , . . . , a j , . . . , an ]
Aufgrund der LinearitĂ¤t (Satz 5.43) kann die Determinante aufgeteilt werden in
det Aâ˛ = det [a1 , . . . , ai , . . . , a j , . . . , an ] + det [a1 , . . . , Îťa j , . . . , a j , . . . , an ]
= det A + Îť det [a1 , . . . , a j , . . . , a j , . . . , an ]
Die zweite Matrix enthĂ¤lt zwei identische Spalten und hat somit Determinante Null. Warum? Da nach
(5.9) muss ja
det [a1 , . . . , a j , . . . , a j , . . . , an ] = â det [a1 , . . . , a j , . . . , a j , . . . , an ]
gelten.



Als Korollar bekommen wir einen einfachen Algorithmus zur Berechnung der Determinante.
Korollar 5.45. Wird eine n Ă n Matrix A = (ai j ) durch Elementartransformationen mit insgesamt k
Zeilenvertauschungen zu einer Dreiecksmatrix B = (bi j ) gebracht, so gilt
det A = (â1) k b11 Âˇ b22 Âˇ Âˇ Âˇ bnn .
Insbesondare gilt fĂźr den Betrag:
| det A| = |b11 Âˇ b22 Âˇ Âˇ Âˇ bnn |.
Im Falle einer singulĂ¤ren Matrix A (rk(A) < n) enthĂ¤lt die Zeilenstufenform B = (bi j ) wenigstens
eine Null an der Diagonalle (bii = 0 fĂźr mindestens ein i). Damit erhalten wir den folgenden SingularitĂ¤tskriterium:
Korollar 5.46.
det A = 0 ââ rk(A) < n
11Nur um die Schreibweise zu vereinfachen.
ÂŠ 2003 by S. Jukna

5.12. EIGENWERTE UND EIGENVEKTOREN

289

Ausser das Determinanten die SingularitĂ¤t bzw. RegularitĂ¤t der Matrizen wiederspiegeln, kann man
sie auch fĂźr die LĂśsung linearer Gleichungssystemen Ax = b mit regulĂ¤ren Koeffizientmatrizen A
benutzen. In diesem Fall existiert ja Aâ1 und Ax = b ist dann universell und eindeutig lĂśsbar und die
LĂśsung ist x = Aâ1 b.
Satz 5.47. (Cramerâsche Regel) Ist A eine regulĂ¤re n Ă n-Matrix, so werden die Komponenten x i
der eindeutig bestimmten LĂśsung von Ax = b gegeben durch
xi =

det Ai (b)
det A

fĂźr

i = 1, . . . , n,

wobei Ai (b) die durch ersetzen der i-ten Spalte von A durch b enstandene Matrix ist.

5.12 Eigenwerte und Eigenvektoren
Definition: Sei A eine n Ă n-Matrix Ăźber R. Ein Skalar Îť â R ist ein Eigenwert von A, falls es ein
Vektor x â Rn mit x , 0 und Ax = Îťx gibt; der Vektor x selbst heiĂt Eigenvektor zum Eigenwert Îť.
Wegen Îťx = ÎťEx kann man das System Ax = Îťx auch in der Form (A â ÎťE)x = 0 mit
ďŁŽ
ďŁš
a11 â Îť
a12
...
a1n
ďŁŻ a21
a22 â Îť . . . a2n x n ďŁş
ďŁş
ďŁŻ
A â ÎťE = ďŁŻ
ďŁş
..
..
..
ďŁ°
ďŁť
.
.
.
an1

. . . ann â Îť

an2

schreiben.

Man kann leicht zeigen (siehe Aufgabe 41), dass Eigenvektoren zu verschiedenen Eigenverten linear
unabhĂ¤ngig sein mĂźssen. Daraus unmittelbar folgt, dass es zu jeder n Ă n Matrix hĂśchstens n verschiedene Eigenwerte gibt. (Warum? Siehe Korollar 5.8)
Um Eigenverte einer n Ă n Matrix A zu bestimmen, kann man Determinanten benutzen. NĂ¤mlich, ist
Îť Eigenwert von A genau dann, wenn die Gleichungssystem (A â ÎťE)x = 0 eine nichtriviale LĂśsung
x , 0 hat, und das ist genau dann der Fall, wenn
det (A â ÎťE) = 0

(5.10)

p(Îť) = det (A â ÎťE)

(5.11)

ist (siehe Korollar 5.46). Das Polynom

heiĂt das charakteristischen Polynoms von A. Sind Îť 1 , . . . , Îť n die Eigenwerte von A, so sind sie die
Nullstellen dieses Polynoms und es gilt
det (A â ÎťE) = (â1) n (Îť â Îť 1 ) Âˇ Âˇ Âˇ (Îť â Îť n ).

(5.12)

Daraus ergeben sich einige Verbindungen zwischen Eigenwerten und der Determinante wie auch der
âSpurâ einer Matrix. Die Spur (engl. âtraceâ) einer n Ă n Matrix A = (ai j ) ist die Summe
Tr(A) =

n
X

aii

i=1

der Diagonalelemente.

ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

290

Satz 5.48. Ist A eine n Ă n Matrix mit Eigenwerten Îť 1 , . . . , Îť n , so gilt:
det A =

n
Y

Îťi ,

i=1

Tr(A) =

n
X

Îťi .

i=1

Beweis. Um die erste Gleichung zu erhalten, setze einfach Îť = 0 in (5.12).
Um die zweite Gleichung zu beweisen, schreibe die rechte Seite von (5.12) als ein Polynom (in Îť)
und beachte, dass der Koeffizient zu Îť nâ1 ist gleich
(â1) n

X

i = 1n â Îť i = (â1) n+1 (Îť 1 + Îť 2 + . . . + Îť n ),

wobei das Koeffizient zu Îť nâ1 in det (A â ÎťE) ist gleich (â1) nâ1 (a11 + a22 + . . . + ann ).



Da wir uns nur Ăźber Nullstellen des charakteristischen Polynoms det (A â ÎťE) kĂźmmern, reicht es die
Matrix A â ÎťE zu einer Zeilenstufenform B = (bi j ) zu reduzieren (Anzahl der Zeilenvertauschungen
ist dabei uns absolut unwichtig!) und die Gleichung
b11 Âˇ b22 Âˇ Âˇ Âˇ bnn = 0
nach Îť auslĂśsen. Da det (A â ÎťE) ein Polynom (mit Îť als seine Variable) des Grades n ist, so kann A
hĂśchstens n verschiedene Eigenwerte haben. (Wir haben diesen Fakt bereits oben erwĂ¤hnt.)
â˛ Beispiel 5.49 :
A=

3âÎť
â1
â1 3 â Îť
= (Îť â 2)(Îť â 4)

det (A â ÎťE) =



3 â1
â1
3



= (3 â Îť)(3 â Îť) â (â1)(â1) = (Îť 2 â 6Îť + 9) â 1

Somit sind Îť = 2 und Îť = 4 die LĂśsungen von det (A â ÎťE), also - die Eigenwerte von A.
â˛ Beispiel 5.50 :

ďŁŽ

ďŁš
0 1 1
A = ďŁ° 1 0 1 ďŁť,
1 1 0

ďŁŽ

ďŁš
âÎť 1
1
A â ÎťE = ďŁ° 1 âÎť 1 ďŁť
1
1 âÎť

Um | det (A â ÎťE)| auszurechnen, transformieren wir die Matrix A â ÎťE auf einer Zeilenstffenform. Zuerst vertauschen wir die 1. und 3. Spalten :
ďŁŽ

ďŁš
1
1 âÎť
ďŁ° 1 âÎť
1 ďŁť
âÎť
1
1
ÂŠ 2003 by S. Jukna

5.12. EIGENWERTE UND EIGENVEKTOREN

291

Danach ziehen wir die 1. Zeile aus der 2. Zeile und addieren das Îť-fache von der 1. Zeile zu der
3. Zeile:
ďŁŽ
ďŁš
1
1
âÎť
ďŁ° 0 âÎť â 1
Îť+1 ďŁť
0
Îť + 1 âÎť 2 + 1
AnschlieĂend addieren wir die 2. Zeile zu der 3. Zeile und erhalten die Matrix, die bereits in einer
Zeilenstufenform ist:
ďŁŽ
ďŁš
1
1
âÎť
ďŁ° 0 âÎť â 1
Îť+1 ďŁť
2
0
âÎť + Îť + 1

Daraus ergibt sich die Gleichung:

| det (A â ÎťE)| = |1 Âˇ (Îť + 1) Âˇ (âÎť 2 + Îť + 2)| = 0.
Aus dieser Gleichung erhalten wir, dass die Matrix A die beiden Eigenwerte â1 und 2 besitzt,
wobei â1 zweifacher Eigenwert ist.
Zwei n Ă n Matrizen R ynd S heiĂen Ă¤hnlich, falls es eine invertierbare Matrix P mit
R = Pâ1 SP
gibt.
Satz 5.51. Ăhnliche Matrizen besitzen denselben Spektrum, d.h. die Mengen ihrer Eigenwerte sind
gleich.
Beweis.
det (R â ÎťE) = det (Pâ1 SP â ÎťPâ1 P)
= det (Pâ1 (S â ÎťE)P)

= det Pâ1 Âˇ det (S â ÎťE) Âˇ det P
= det (S â ÎťE) Âˇ det (Pâ1 P)
= det (S â ÎťE).


Eine Diagonalmatrix ist eine n Ă n Matrix D = (d i j ) mit d i j = 0 fĂźr alle i , 0, d.h. D = E Âˇ d mit
d = (d11 , d22 , . . . , d nn ).
Eine n Ă n Matrix ist diagonalizierbar, falls sie zu einer Diagonalmatrix Ă¤hnlich ist. Solche Matrizen sind gut, da sie hanbare Dartellungen haben. Leider ist nicht jede Matrix diagonalizierbar. DafĂźr
braucht man, dass Eigenwerte linear unabhĂ¤ngig sind. Da Eigenvektoren zu verschiedenen Eigenverten linear unabhĂ¤ngig sein mĂźssen (siehe Aufgabe 41), reicht es (fĂźr die Diagonalizierbarkeit), dass
alle Eigenwerte verschieden sind.
Satz 5.52. Sei A eine n Ă n Matrix. Sind die Eigenvektoren von A linear unabhĂ¤ngig, so its A diagonalizierbar.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

292

Beweis. Sei Î eine n Ă n Diagonalmatrix, deren DiagonaleintrĂ¤ge die Eigenwerte Îť 1 , . . . , Îť n von A
sind, und sei V = [v1 , . . . , vn ] die n Ă n Matrix, deren Spalten die Eigenvektoren sind. Aus Avi = Îť i vi
fĂźr alle i folgt
AÂˇV

= A Âˇ [v1 , . . . , vn ] = [Îť 1 v1 , . . . , Îť n vn ] = [v1 , . . . , vn ] Âˇ Î = V Âˇ Î.

Die inverse V â1 existiert genau dann, wenn rk(V ) = n ist, d.h. wenn v1 , . . . , vn linear unabhĂ¤ngig
sind. Und in diesem Fall haben wir A = V â1 ÎV .

Mit der Induktion kann man leicht zeigen, dass
Ak = V Îk V â1
fĂźr alle k = 0, 1, . . . gilt. AuĂerdem ist Îk eine Diagonalmatrix mit Îť 1k , . . . , Îť kn auf der Diagonale.
Diese Beobachtungen geben uns eine geschlĂśĂene Form fĂźr die Potenzen Ak .

5.13 Einige Anwendungen des MatrizenkalkĂźlsâ
Im diesen Abschnitt werden wir auf ein paar Beispiele demonstrieren, wie MatrizenkalkĂźl in manchen
Situationen helfen kann.

5.13.1 MatrizenkalkĂźl unf komplexe Zahlen
Komplexe Zahlen bilden einen Ring, indem die Gleichung x 2 = â1 eine LĂśsung i =
i2 = â1 gilt.

â

â1 hat, d.h.

TatsĂ¤chlich kennen wir schon einen anderen Ring R, in dem die Gleichung x 2 = â1 lĂśsbar ist! Das ist
der Ring von 2 Ă 2-Matrizen mit Ăźblichen Matrizenaddition und Multiplikation. Setzen wir nun


0 â1
I :=
1
0


1 0
E :=
0 1
so erhalten wir

 
 

0 â1
1 0
I =
Âˇ
=
= E.
1
0
0 1


a 0
Der Ring R enthĂ¤lt zwar R nicht direkt, aber die Matrizen aE =
mit a â R verhalten sich
0 a
beim Rechnen wie die reellen Zahlen selbst, d.h. {aE : a â R} ist ein zu R isomorpher UnterkĂśrper
in R. FĂźr diesen konkreten Ring R besteht also die vorhin ausgewĂ¤hlte Teilmenge C genau aus allen
Matrizen


a âb
aE + bI =
b a
2



0 â1
1
0

mit a, b â R. Mit der Indentifikation E 7â 1 und I 7â i sind wir wieder bei der Ăźblichen Schreibweise


a âb
a + ib 7â
.
b a
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

293

Wozu ist diese Matrixdarstellung der komplexen Zahlen gut? Sie erleichtert die Additions- und Multiplikationsregeln solcher Zahlen zu erinnern:

 

a âb
c âd
(a + ib) + (c + id) =
+
b a
d c


a + c â(b + d)
=
b+d
a+c
= (a + c) + i(b + d)
und
(a + ib) Âˇ (c + id) =
=




a âb
b a

 

c âd
Âˇ
d c

ac â bd â(ad + bc)
ad + bc
ac â bd



= (ac â bd) + i(ad + bc)


a âb
Ist die komplexe Zahl z = a + ib als z = aE + bI =
dargestellt, so stellt die Matrix
b
a


a b
z := aE â bI =
âb a


a âb
die konjugierte Zahl dar. Ist z =
, so ist
b
a
"
#
a
b
z
â1
|z | 2
|z | 2
z =
= 2
â |Zb|2 |za|2
|z|
mit |z| =

â

a2 + b2 die multiplikative Inverse von z, da
# 

 " a

b
0 â1
a
b
2
2
â1
|z |
|z |
zÂˇz =
Âˇ
=
=I
1
0
âb a
â |Zb|2 |za|2

gilt.

5.13.2 Diskrete Fourier-Transformation
Wie kann man zwei Polynome vom Grad n Ăźber einem KĂśrper F mĂśglichst schnell multiplizieren? Es
ist klar, dass O(n2 ) arithmetischen Operationen reichen aus. Geht es aber schneller? Auf dem ersten
Blick sollte es nicht gehen, da jedes Paar von Koeffizienten multipliziert sein muss. Wie wir bald
sehen werden, trĂźgt der Schein! Wenn man diese Frage genauer anschaut, reichen sogar O(n log n)
Operationen vollkom aus!
ZunĂ¤chst machen wir ein paar Bemerkungen Ăźber Polynome.
1.

Jedes Polynom a(x) = a0 + a1 x + a2 x 2 + Âˇ Âˇ Âˇ + anâ1 x nâ1 kann man als Koeffizientenvektor
a = (a0 , a1 , . . . , anâ1 ) betrachten. Damit ist fĂźr jedes x â F der Wert a(x) nichts anderes als das
Skalarprodukt des Koeffizientenvektors a mit dem Potenzenvektor x = (1, x, x 2 , . . . , x nâ1 ).
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

294
2.

Jedes Polynom a(x) vom Grad n â 1 ist durch seine Werte y1 = a(x 1 ), . . . , yn = a(x n ) auf
beliebigen n verschiedenen Punkten x 1 , . . . , x n eindeutig bestimmt.

Das Ziel der sogenannten âdiskreten Fourier Transformationâ ist das Rechenen mit Polynomen durch
effiziente Matrix-Operationen zu ersetzen. Man will nĂ¤hmlich eine n Ă n Matrix Mn bestimmen, so
dass

1.

Die Matrix-Vektor Produkte Mn Âˇ x und Mnâ1 Âˇ x viel weniger als n2 arithmetische Oprationen
brauchen.

2.

FĂźr ein Polynom a(x) = a0 + a1 x + a2 x 2 + Âˇ Âˇ Âˇ + anâ1 x nâ1 ist y = Mn Âˇ a die Auswertung
y1 = a(x 1 ), y2 = a(x 2 ), . . . , yn = a(x n )
von a(x) auf n verschiedenen Punkten x 1 , . . . , x n .

In der diskreten Fourier-Transformation wĂ¤hlt man diese Evaluirungspunkte x 1 , . . . , x n sehr spezifisch
aus. Man nimmt nĂ¤hmlich die sogenannte n-te Einheitswurzeln, d.h. die LĂśsungen der Gleichung
x n = 1 Ăźber F.
Eine primitive n-te Einheitswurzel ist eine Zahl Ď n â F \ {0}, so dass Ď nn = 1 und Ď kn , 1 fĂźr alle
k = 1, 2, . . . , n â 1 gilt.
â˛ Beispiel 5.53 : Der KĂśrper F = R der reelen Zahlen hat nur Âą1 als Einheitswurzeln, weil aus x n = 1
auch |x| = 1 folgt.
â˛ Beispiel 5.54 : In dem KĂśrper C der komplexen Zahlen sind die primitive n-te Einheitswurzeln
genau die Zahlen Ď0n , Ď1n , . . . , Ď nnâ1 mit

Ďn = e

2iĎ
n

= cos

2Ďk
2Ď
+ i sin .
n
n

Die Werte Ď0n , Ď1n , . . . , Ď nnâ1 liegen auf dem Einheitskreis und bilden bezĂźglich der Multiplikation
eine Gruppe mit n Elementen. Die Abbildung 5.3 zeigt die primitive 8-te Einheitswurzel Ď8 :=
2iĎ
e 8 und ihre Potenzen. Man rechnet leicht nach, dass Ď82 = i eine primitive 4-te Einheitswurzel
ist.

â˛ Beispiel 5.55 : Ist F = Fq ein endlicher KĂśrper mit q Elementen, so gibt es (nach dem sogenannten
Satz von Artin) eine primitive n-te Einheitswurzel genau dann, wenn n ein Teiler von q â 1 ist.
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

295

Im
âť

Ď8 = e

2iĎ
8

â˛ Re

Ď87 = e

14iĎ
8

Abbildung 5.3: Die primitive 8-te Einheitswurzel
Lemma 5.56. Sei Ď n eine primitive n-te Einheitswurzel in einem KĂśrper F.
1. Die Zahlen 1, Ď n , Ď2n , . . . , Ď nnâ1 sind verschieden und es gilt:
1 + Ď n + Ď2n + . . . + Ď nnâ1 = 0.
2. Ist d = ggT(k, n) der grĂśĂter gemeinsammer Teiler von k und n, so ist
Ď kn = Ď n/d
eine primitive (n/d)-te Einheitswurzel. Insbesondare, ist Ďâ1
n eine primitive n-te Einheitswurzel.
3. Is n = 2m, so gilt
Ď kn = âĎ nm+k
fĂźr alle k = 0, 1, . . . , m.
Beweis. Zu (1): Angenommen gebe es 0 6 s < r 6 n â 1 mit Ď ns = Ďrn . Dann sollte auch Ďrnâs = 1
gelten, was aber unmĂśglich ist, da Ďkn , 1 fĂźr alle k = 1, 2, . . . , n â 1 gelten muss.
Pnâ1 k
Ď n eine geometrische Reihe ist, erhalten wir
Da k=0
nâ1
X
k=0

Ď kn =

Ď nn â 1
= 0.
Ďn â 1

Zu (2): Sei m = n/d. Wegen n | km haben wir (Ď kn ) m = 1. Nehmen wir an, dass (Ď kn ) j = 1 fĂźr
ein j â {1, . . . , m â 1} gilt. Dann muss k j durch n teilbar sein und damit muss auch (k/d) j durch
m = n/d teilbar sein. Da aber m = n/d und k/d teilerfremd sind, muss (nach Lemma 2.8) auch j
durch m = n/d teilbar sein, ein Widerspruch.
Zu (3): Es gilt (Ď k+m
) 2 = (Ď kn ) 2 Âˇ (Ď nn ) 2 = (Ď kn ) 2 , da Ď nn = 1 gilt. Weiter ist Ď k+m
, Ď kn , denn sonst
n
n
m
wĂ¤re Ď n = 1 was der Tatsache widerspricht, dass Ď n eine primitive n-te Einheitswurzel ist. Da die
Quadrate von Ď k+m
und Ď kn Ăźbereinstimmen, aber Ď k+m
, Ď kn gilt, muss Ď k+m
= âĎ kn gelten.
n
n
n
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

296


Man will nun ein gegebenes Polynom
a(x) = a0 + a1 x + a2 x 2 + Âˇ Âˇ Âˇ + anâ1 x nâ1
auf n verschiedenen Punkten
1, Ď n , Ď2n , . . . , Ď nnâ1
auswerten, d.h. alle n Werte
yi = a(Ď in )

i = 0, 1, . . . , n â 1

bestimmen. Die Hauptidee der sogenannten diskreten Fourier-Transformation berĂźht sich auf der
Gleichung
a(x) = aeven (x 2 ) + x Âˇ aodd (x 2 )
(5.13)
mit
aeven (x) = a0 + a2 x + a4 x 2 + a6 x 3 + Âˇ Âˇ Âˇ + a2k x k + Âˇ Âˇ Âˇ + anâ2 x n/2â1

aodd (x) = a1 + a3 x + a5 x 2 + a7 x 3 + Âˇ Âˇ Âˇ + a2k+1 x k + Âˇ Âˇ Âˇ + anâ1 x n/2â1 .

Will man nun ein Polynom a(x) auf der i-ten Potenz Ď in der n-ten primitiven Einheitswurzel Ď n
auswerten, so reicht es12 die Polynome aeven (x) und aodd (x) auf der i-ten Potenz Ď in/2 der (n/2)-ten
primitiven Einheitswurzel Ď n/2 auswerten:
a(Ď in ) = aeven (Ď in/2 ) + Ď in Âˇ aodd (Ď in/2 )

(5.14)

D.h. anstatt ein Polynom des Grades n â 1 an n Punkten 1, Ď n , Ď2n , . . . , Ď nnâ1 auszuwerten, reicht es
n/2â1
zwei Polynome des Grades n/2 â 1 an n/2 Punkten 1, Ď n/2 , Ď2n/2 , . . . , Ď n/2
auszuwerten.
Diese Beobachtung liefert uns einen rekursiven Auswertungsverfahren, den kann man auch als ein
Matrix-Vektor Produkt kurz beschreiben. Dazu betrachten wir die Matrix (die sogenannte FourierMatrix)
ďŁś
ďŁŤ
1 1
1
... 1
ďŁŹ 1 Ďn
Ď2n
. . . Ď nnâ1 ďŁˇ
ďŁˇ
ďŁŹ
4
ďŁˇ
ďŁŹ 1 Ď2n
Ďn
. . . Ď2(nâ1)
n
ďŁˇ
ďŁŹ
3
6
3(nâ1)
DFTn = ďŁŹ 1 Ď
ďŁˇ
Ďn
. . . Ďn
n
ďŁˇ
ďŁŹ
ďŁŹ .. ..
ďŁˇ
..
..
ďŁ¸
ďŁ­ . .
.
.
1 Ď nnâ1 Ď2(nâ1)
n

. . . Ď n(nâ1)

2

Die diskrete Fourier-Transformation eines Vektors x â Fn ist durch y = DFTn Âˇ x definiert. Oft
bezeichnet man diese Operation als
y = DFTn (x).
Beachte, dass dann y = DFTn (a) genau die Auswertung des Polynoms a(x) auf n verschiedenen
Punkten
1, Ď n , Ď2n , . . . , Ď nnâ1
ergibt.
12Da Ď 2n = Ď n/2 gilt.
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

297

FĂźr einen Vektor x = (x 0 , x 1 , . . . , x nâ1 ) sei xeven = (x 0 , x 2 , . . . , x nâ2 ) und xodd = (x 1 , a3 , . . . , x nâ1 ).
Aus der Beobachtung (5.14) â zusammen mit der (aus Lemma 5.56(3) folgender) Beobachtung, dass
gilt â bekommt man unmittelbar die
fĂźr n2 6 i < n die Gleichheit Ď in = Ď n(iân/2)+n/2 = âĎ iân/2
n
folgende rekursive Gleichung fĂźr die Berechnung der diskreten Fourier-Transformation DFTn (x):
Satz 5.57. Sei n gerade. Dann gilt

 

DFTn/2 ân/2 Âˇ DFTn/2
xeven
DFTn (x) =
Âˇ
DFTn/2 âân/2 Âˇ DFTn/2
xodd
wobei ân/2 die (n/2) Ă (n/2) Diagonalmatrix mit den EintrĂ¤gen 1, Ď n , Ď2n , . . . , Ď nn/2â1 auf der Diagonale ist.
Sei nun T (n) die Anzahl der arithmetischen Operationen, die man fĂźr die Berechnung der diskreten
Fourier-Transformation DFTn (x) braucht. Da sich die VektorlĂ¤nge in jedem Schritt halbiert, gilt
n
T (n) 6 2 Âˇ T
+ O(n)
2

woraus nach Masters-Theorem (Satz 3.94) T (n) = O(n log n) folgt. Also ist dieses Verfahren um
âŚ(n/ log n) schneller als die trivialle Berechnung von DFTn (x) mit O(n2 ) Operationen.
Die Matrix DFTn hat einige sehr interessante Eigenschaften, die die Berechnung von DFTn (a) wie
auch die Berechnung von der Inverse DFTn â1 sehr erleichtern.
Lemma 5.58. Die Inverse DFTn â1 von DFTn ist gegeben durch
(DFTn â1 )i, j =

1
Âˇj
Âˇ Ďâi
n
n

Beweis. Wir multiplizieren die i-te Zeile von DFTn mit der j-ter Spalte von DFTn â1 und erhalten den
Skalarprodukt
nâ1
nâ1
nâ1
âk Âˇ j
(iâ j) X
X
1 X (iâ j) Âˇk Ď n
i Âˇk Ď n
= Âˇ
Âˇ
Ďn Âˇ
Ďn
=
Ď kn .
n
n
n
k=0

k=0

Wenn i = j, dann ist dieser Skalarprodukt gleich
gleich 0 nach Lemma 5.56(1).

k=0

1
n (1

+ 1 + Âˇ Âˇ Âˇ + 1) = 1 und wenn i , j, dann ist er


Wir wissen bereits, dass die diskrete Fourier-Transformation y = DFTn (a) schnell berechenbar ist.
Diese Eigenschaft ist in vielen Anwendungen benutzt. Wir zeigen nur, wie man damit Polynome
schnell multiplizieren kann. Sei
c(x) = c0 + c1 x + c2 x 2 + Âˇ Âˇ Âˇ + c2nâ2 x 2nâ2 = a(x) Âˇ b(x)
Produkt zweier Polynome
a(x) = a0 + a1 x + a2 x 2 + Âˇ Âˇ Âˇ + anâ1 x nâ1 ,
b(x) = b0 + b1 x + b2 x 2 + Âˇ Âˇ Âˇ + bnâ1 x nâ1 .

ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

298

Um das Produktpolynom c(x) = a(x) Âˇ b(x) zu bestimmen, reicht es den Koeffizienten-Vektor c zu
bestimmen. Wir kĂśnnen o.B.d.A annehmen, dass alle Koeffizienten-Vektoren a, b und c die LĂ¤nge 2n
haben â dazu reicht es die Vektoren nach rechts mit n Nullen zu erweitern.
Satz 5.59.
c = DFT2n â1 (DFT2n (a) Âˇ DFT2n (b))
wobei Âˇ die komponentenweise Multiplikation der Vektoren bezeichnet.
Beweis. Vektoren y = DFT2n (a) und z = DFT2n (b) geben uns die Auswertunge der Polynome a(x)
2 , . . . , Ď 2nâ1 eines primitiven (2n)und b(x) auf 2n verschiedenen Punkten â den Potenzen 1, Ď2n , Ď2n
2n
ten Einheitswurzeln Ď2n . Damit ergibt das Vektor w = y Âˇ z die Auswertung des Produktpolynoms
c(x) = a(x) Âˇ b(x) auf diesen 2n verschiedenen Punkten. Da der Grad von c(x) kleiner als die Anzahl
dieser Punkte is, muss der Koeffizienten-Vektor c eindeuting durch w bestimmt sein, und kann als
c = DFT2n â1 Âˇ w berechnet sein.


5.13.3 Fehlerkorrigierende Codes
FĂźr die Frage, wozu es denn gut ist, die VektorrĂ¤ume auch Ăźber endlichen KĂśrpern zu haben, ist die
Codierungstheorie ein wichtiges Beispiel. Da dies Gegenstand einer eigener Vorlesung ist, besprechen
wir hier nur Grundfragen zur AnknĂźpfung an lineare Algebra.
Nun werden an jede Art der NachrichtĂźbertragung zwei gegensĂ¤tzliche Forderungen gestellt: Sie soll
einerseits so wirtschaftlich (d.h. schnell) wie mĂśglich sein und andererseits so sicher wie mĂśglich sein.
Mit Sicherheit ist dabei die Vermeidung von Ăbertragungsfehlern gemeint, nicht die AbhĂśrsicherung,
letztere ist das Feld der Kryptographie, die spezielle Arten der Codierung verwendet (wir haben bereits
ein solches Verfahren â die RSA-Codes â im Abschnitt 2.4.1 gelernt).
Die allgemeine Situation besteht aus zwei Spielern: Alice (die Senderin) und Bob (der EmpfĂ¤nger).
Alice will Nachrichten an Bob verschicken. Dazu wĂ¤hlen Alice und Bob eine geeignete Teilmenge
der binĂ¤ren Strings C â {0, 1} n aus; C nennt man das Code. Dann kodiert Alice ihre Nachricht als ein
String y = (y1 , . . . , yn ) aus C. WĂ¤hrend der Ăbertragung kĂśnnen einige Symbolen yi geĂ¤ndert sein.
WĂźrde man davon ausgehen, dass alle n Symbolen geĂ¤ndert sein kĂśnnten, wĂ¤re dann nichts mehr
zu machen â keine Kodierung kĂśnnte dann helfen. Also geht man davon aus, dass hĂśchtens bis zu
irgendwelcher Anzahl t von Symbolen im String y geĂ¤ndert sein kĂśnnten.
Bob bekommt nun einen String x = (x 1 , . . . , x n ), der sich mĂśglicherweise in bis zu t Positionen von
der Originalnachricht y unterscheidet. Es soll Bob mĂśglich sein, die Originalnachricht y aus x wieder
(eindeutig!) zu rekonstruiren.
Um die Situation genauer zu beschreiben, brauchen wir den Begriff der âHammingdistanzâ zwischen
Strings. Die Hammingdistanz d(x, y) zwischen Strings x, y ist die Anzahl der Stellen, in der diese
Strings sich unterscheiden: d(x, y) = |{i : x i , yi }|. Das Code C heiĂt t-fehlerkorregierend, falls die
minimale Distanz
d(C) := min{d(x, y) : x, y â C und x , y}
mindestens 2t +1 betrĂ¤gt. Die Bedeutung hier ist klar: Ist d(C) > 2t +1, dann kann die Hammingkugel
B(x, t) = {z : z â F2n mit d(x, z) 6 t}
nur einen Codewort enthalten.
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

299

Das Codewort selbst

11
00
00
11
00 Worte mit 1 Fehler
11

11
00
00
11
00
11
11
00
00
11
00
11

11
00
00
11

Worte mit 2 Fehler

11
00
00
11

Da wĂ¤hrend der Ăbertragung hĂśchsten t Bits von y (Alices Nachricht) geĂ¤ndert wurde, muss y das
einzige Codewort in B(x, t) sein. Das Problem dabei ist, dieses einzige Codewort zu finden.
Eine MĂśglichkeit
wĂ¤re fĂźr alle Vektoren z â B(x, t) zu testen, ob z in C liegt. Es sind aber |B(x, t)| =

P
t
n
t solchen Vektoren, und jeder von ihnen muss mit |C | Vektoren aus C verglichen werden.
â
n
i=0 i
Das braucht insgesamt â nt |C | Vergleiche. Wenn Alice viele verschiedene Nachrichten verschicken
will, dann muss |C | groĂ sein (da jede Nachricht ihr eigenes Codewort braucht), und die Dekodierung
wird dann sehr zeitaufwĂ¤ndig!
Man muss also einen Code C finden, so dass:
1. C groĂ genug ist
2. d(C) > 2t + 1 gilt
3. Dekodierung (wie auch Kodierung) relativ leicht sind.
Um diese Ziele zu erreichen, betrachten wir die Strings als Vektoren in dem Vektorraum F2n , wobei
F2 = GF(2) der KĂśrper von Charakteristik 2 ist (Addition und Multiplikation modulo 2). Als Code
wĂ¤hlen wir einen Vektorraum C â F2n , so dass:
(i) die Dimension dim(C) = k groĂ genug ist (und damit |C | = 2k auch groĂ ist), und
(ii) jeder Vektor x â C, x , 0 mindestens 2t + 1 von Null verschiedene Bits hat.
Das (ii) Ă¤quivalent zu d(C) > 2t + 1 ist, folgt aus dem folgenden Behauptung. FĂźr ein Vektor x â C
sei
|x| = x 1 + x 2 + . . . + x n
die Anzahl der Einsen in x.
Behauptung 5.60. Sei C â Fn ein linearer Code, |C | > 2 und sei
w(C) = min{|x| : x â C und x , 0}.
Dann gilt: d(C) = w(C).
Beweis. d(C) 6 w(C): Sei x â C, x , 0 mit |x| = w(C). Da 0 â C und x , 0, haben wir:
d(C) 6 d(x, 0) = |x| = w(C).
d(C) > w(C): Seien nun x und y zwei verschiedene Vektoren aus C mit d(x, y) = d(C). Da C
ein Vektorraum ist, gehĂśrt auch der Vektor 13 x â y zu C. Da x â y , 0 (die Vektoren x, y sind ja
verschieden), haben wir d(C) = d(x, y) = |x â y| > w(C).

13Wie es Ăźblich ist, bezeichnen wir die Addition in GF(2) durch â, d.h. x â y = x + y mod 2.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

300

Es bleibt also nur zu zeigen, wie man mit Hilfe eines linearen Codes C effizient kodieren und dekodieren kann. Dazu benutzt man sogenannte Generator- und Kontrollmatrizen.
1. Eine Generatormatrix (oder Erzeugermatrix) G des Codes ist eine k Ă n-Matrix, deren Zeilen eine
Basis von C bilden; also gilt C = {yâ¤ G : y â Fk }.
2. Eine Kontrollmatrix (oder PrĂźfmatrix) H des Codes ist eine (n â k) Ă n-Matrix, deren Zeilen eine
Basis von C âĽ bilden; also gilt C = {x â Fn : Hx = 0}.
Die Zeilen von G und die Zeilen von H sind also paarweise orthogonal. Der Name14 âKontrollmatrixâ
erklĂ¤rt sich von selbst: Da (C âĽ ) âĽ = C ist, gehĂśrt ein x â Fn genau dann zu C, wenn H Âˇ x = 0 gilt. Es
gilt also: Ist C â Fn ein linearer Code mit der Kontrollmatrix H, so gilt fĂźr alle x â Fn
x â C ââ H Âˇ x = 0
D.h. Codeworte sind genau die Vektoren, die senkrecht zu allen Zeilen von H liegen.
Sind die Matrizen G und H gegeben, so ist die Kodierung (fĂźr Alice) sehr einfach: Um ihre Nachricht
v â F2k zu kodiern, multipliziert sie die Generatormatrix G von links mit vâ¤ und verschicht das
Codewort y = vâ¤ Âˇ G.
Bob bekommt einen Vektor x, der sich mĂśglicherweise in bis zu t Bits von der Originalnachricht y
unterscheidet. Um ihm zu dekodiern, berechent Bob den Vektor s(x) := H Âˇx (das sogenannte Syndrom
von x) und benutzt den folgenden Fakt. Sei d(x, C) = min{d(x, y) : y â C}.
Lemma 5.61. FĂźr jedes x â F2n mit d(x, C) 6 t gibt es genau einen Vektor a â B(0, t) mit s(x) = s(a)
und x â a â C.
Beweis. Da wir 6 t Fehlern haben kĂśnnen, wissen wir, dass es mindestens ein a â B(0, t) mit xâa â C
gibt. Dann ist 0 = H (x â a) = Hx â Ha und, da wir in dem KĂśrper GF (2) arbeiten, die Gleichheit
Hx = Ha folgt.
Um die Eindeutigkeit von a zu beweisen, nehmen wir an, dass es auch ein anderes b â B(0, t) mit
b , a und x â b â C gibt. Seien u = x â a und v = x â b. Da C linear ist, muss auch der Vektor
u â v = b â a in C liegen. Da aber b â a , 0, 0 â C und C t-fehlerkorregierend ist, muss dann
d(b â a, 0) > 2t + 1 gelten Andererseits gilt:
d(b â a, 0) = d(b, a) 6 d(b, 0) + d(0, a) 6 2t,
ein Widerspruch mit d(C) > 2t + 1.



Also reicht es Bob in einer (im voraus vorbereiteten) Liste L den einzigen String a â B(0, t) mit
s(a) = s(x) zu finden: Dann ist x â a der x am nĂ¤chsten ligendes Codewort und deshalb muss y = x â a
genau die von Alice geschickte Nachricht sein.
Zusammengefasst lĂ¤uft die Kommunikation folgendermaĂen ab:
1. Alice kodiert ihre Nachricht v â {0, 1} k als y = vâ¤ G und verschickt sie.
2. Bob bekommt einen Vektor x, der kann von y in bis zu t Bits unterscheiden. Er sucht dann den
einzigen Vektor a â {0, 1} n mit |a| 6 t Einsen, fĂźr den H Âˇ a = H Âˇ x gilt. Dann ist y = x â a die von
Alice geschickte (kodierte) Nachricht.
14Auf englisch âparity-check matrixâ
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

301

3. Bob weiss, dass die von Alice geschikte Originalnachricht v eine LĂśsung z des Gleichungssystems
zâ¤ G = y ist. Er lĂśst15 also dieses Gleichungssystem, um eine LĂśsung z zu bekommen. Da die Zeilen
von G eine Basis von C bilden, weiss Bob (nach Satz 5.5), dass dann z = v geltem muss. Er hat
also die von Alice geschickte Nachricht rekonstruiert.
â˛ Beispiel 5.62 : F = GF (2)
C = span({ (00000), (11010), (01101), (10111)})
d(C) = 3 =â C ist t-fehlerkorregierend mit t = 1. Kontrollmatrix:
ďŁŽ

ďŁš
1 0 0 1 0
H=ďŁ° 0 1 0 1 1 ďŁť
0 0 1 0 1
Die folgende Tabelle gibt alle Vektoren aus B(0, 1) und die zugehĂśrigen Syndrome wieder:
a
s(a)
00000 000
10000 100
L = 01000 010
00100 001
00010 110
00001 011
Ist etwa x = (11110) die eintreffende Botschaft, so gehĂśrt hierzu das Syndrom s(x) = H Âˇ x =
(001) mit dem eindeutigen a = (00100) und wir erhalten y = x â a = (11010) als Nachricht.
â˛ Beispiel 5.63 : Das sogenannte Hammingcode C â GF (2) n ist ein lineares Code mit n = 2r â 1
und Dimension k = n â r. Seine Kontrollmatrix H ist eine r Ă (2r â 1) Matrix, deren Spalten
alle binĂ¤re Codes der Zahlen 1, 2, . . . , n sind. Dieses Code ist 1-fehlerkorregierend. (Warum?
r
Ăbungsaufgabe.) In diesem Fall ist |C | = 2nâr = 22 â1 â r aber die Syndrom-Liste L ist sehr
3
kurz: |L| 6 n + 1 = 2r . FĂźr r = 3 ist |C | > 22 â1 â 3 = 27 â 3 = 124 â 3 = 121 und |L| 6 2r = 8.
Die entsprechenden Generator- und Kontrollmatrizen sind (verifiziere, dass die Zeilen von G und
die Zeilen von H paarweise orthogonal sind!)
ďŁŽ

1
ďŁŻ 0
G=ďŁŻ
ďŁ° 0
0

0
1
0
0

0
0
1
0

0
0
0
1

0
1
1
1

1
0
1
1

ďŁš
1
1 ďŁş
ďŁş
0 ďŁť
1

ďŁŽ

ďŁš
0 1 1 1 1 0 0
H=ďŁ° 1 0 1 1 0 1 0 ďŁť
1 1 0 1 0 0 1

5.13.4 Expandergraphen
Eigenwerte haben viele Anwendungen in der Diskreten Mathematik und in der Informatik. Und der
Grund dafĂźr ist, dass die Eigenwerte der Adjazenzmatrizen ungerichteter Graphen G = (V, E) einige
15Zum Beispiel mit dem GauĂ-Verfahren (siehe Abschnitt 5.8).
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

302

wichtige Eigenschaften (wie der âExpansionsgradâ) wiederspiegeln. Graphen mit grĂśĂem Expansionsgrad sind in vielen Gebieten anwendbar: Kodierungstheorie (z.B. Tornado Codes), KomplexitĂ¤tstheorie (z.B. gute Zufallsgeneratoren), SchaltkreiskomplexitĂ¤t (um gute untere Schranken zu beweisen), Konstruktion von WWW-Suchmaschinen, usw.
Sei G = (V, E) ein ungerichteter Graph und S â V . Mit

Î(S) = {v â V : uv â E fĂźr ein u â S}

bezeichnen wir die Menge aller Nachbarn von S, d.h. die Menge aller Knoten, die mit mindestens
einer Knoten aus S benachbart sind. Ein graph ist d-regulĂ¤r, falls jeder Knoten genau d Nachbarn hat.
Definition: Sei G ein d-regulĂ¤rer Graph G = (V, E) mit |V | = n Knoten. Dann ist G ein (n, d, c)Expander, falls
|Î(S) \ S| > c|S|

fĂźr jede Teilmenge S â V mit |S| 6 12 |V | gilt.

NatĂźrlich ist jeder zusammenhĂ¤ngender d-regulĂ¤rer Graph ein (n, d, c)-Expander fĂźr irgendein c > 0.
Andererseits, der vollstĂ¤ndiger Graph K n ist n-regulĂ¤r und sehr stark expandiert, da |Î(S) \ S| >
n â |S| gilt. In Anwendungen aber braucht man, dass der Graph so âdĂźnnâ wie mĂśglich ist, d.h. die
Anzahl |E| der Kanten muss so klein wie mĂśglich sein. Insbesondere, braucht man expliziete Familien
von (n, d, c)-Expandern mit n â â, wobei beide Parameter d und c > 0 konstant(!) sind. Diese
âExpansionseigenschaftâ (jede Menge hat viele echten Nachbarn, obwohl der Graph sehr dĂźnn ist)
spielt eine wichtige Role in verschiedenen Feldern der diskreten Mathematik und Informatik.
Die Existenz von Expandergraphen kann man relativ leicht mittels sogenannter âprobabilistischen
Methodeâ nachweisen (siehe Abschnitt 4.17). Es gibt auch einige explizite Konstruktionen von Expandergraphen. Hier wir erwĂ¤hnen nur zwei davon.
Konstruktion 1: Als Knotenmenge nehmen wir die Menge V = Zm Ă Zm . Jeder der Knoten (x, y) ist
verbunden mit 4 Knoten (x + y, y), (x â y, y), (x, y + y), (x, x â y) (alle Operationen sind modulo m).
Konstruktion 2: Als Knotenmenge nehmen wir V = Z p , wobei p eine Primzahl ist. Jeder der Knoten
x ist verbunden mit 3 Knoten x + 1, x â 1, x â1 (hier wiederum ist x Âą 1 modulo p und x â1 ist die
multiplikative Inverse von x in Z p ); der Knoten x = 0 ist mit den Knoten p â 1, 0 und 1 verbunden.
Es ist aber oft sehr schwier nachzuweisen, dass ein bestimmter Graph auch ein Expander ist. In einem
solchen Nachweis spielen die Eigenwerte der Adjazenzmatrix eine grĂśĂe Rolle.
Sei G = (V, E) ein ungerichteter Graph mit der Knotenmenge V , |V | = n, und sei A = (auv ) seine
Adjazenzmatrix, d.h.

auv =



1
0

falls uv â E
falls uv < E.

Da der Graph ungerichtet ist, gilt: uv â E ââ vu â E. Also ist die Adjazenzmatrix symmetrisch
(Aâ¤ = A gilt). FĂźr solchen Graphen gilt folgendes (ohne Beweis):
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

303

Satz 5.64. Sei A eine symmetrische n Ă n-Matrix Ăźber R. Dan gilt:
1. A hat n reellen Eigenwerte Îť 1 > Îť 2 > . . . > Îť n .
2. Es gibt Eigenvektoren, die eine Orthonormalbasis von Rn bilden, wobei der Eigenvektor zu Îť 1 ist
1
v1 = â 1 =
n

1
1 
â ,. . . , â .
n
n

3. Die ersten zwei Eigenwerte sind gegeben durch:


hx, Axi
n
Îť 1 = max
: x â R \ {0} ,
kxk 2


hx, Axi
n
: x â R \ {0} und hx, 1i = 0 .
Îť 2 = max
kxk 2
FĂźr S,T â V sei

e(S,T ) = |{uv : u â S, v â T und uv â E}|

die Anzahl aller Kanten, die Knoten in S mit Knoten in T verbinden. Beachte, dass die Mengen S und
T nicht unbedingt disjunkt sein mĂźssen.
Lemma 5.65. Sei G = (V, E) ein ungerichteter d-regulĂ¤rer Graph auf n = |V | Knoten. Sei Îť = Îť 2
der zweitgrĂśĂte Eigenwert von G. Dann gilt fĂźr jede Zerlegung V = S âŞ T:
e(S,T ) >

(d â Îť)|S||T |
n

(5.15)

Beweis. Sei s = |S| und t = |T | = n â s. Betrachte den Vektor x : V â R mit

ât
falls u â S,
xu =
s
falls u â T.
Der Vektor x hat also die Form:
|S | mal
|T | mal
z
}|
{ z }| {
x = (ât, ât, . . . , ât, s, s, . . . , s)

Daraus folgt, dass Vektor x senkrecht zum Vektor 1 steht: Dann ist
X
X
hx, 1i =
(ât) +
s = s(ât) + ts = 0.
u âS

u âT

Ausserdem gilt
kxk 2 = hx, xi =

X

(ât) 2 +

X

s2 = st 2 + ts2 = st(s + t) = stn,

u âT

u âS

woraus nach Satz 5.64 die Ungleichung
hx, Axi 6 Îť kxk 2 = Îťstn

(5.16)
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

304
folgt. Andererseits, gilt

hx, Axi =

X

u âV

xu

 X

v:uv âE


X
xv = 2
xu x v .
uv âE

Nach der Definition von x gilt fĂźr jede Kante uv â E:
ďŁą 2
falls u, v â S,
ďŁ˛ t
2
s
falls u, v â T,
xu x v =
ďŁł
âst
falls u â S und v â T.
Daraus folgt:

2

X

xu x v

uv âE



= 2 t 2 Âˇ eS + s2 Âˇ eT â st Âˇ e(S,T )

= t 2 Âˇ (2eS ) + s2 Âˇ (2eT ) â 2st Âˇ e(S,T ),

wobei eS bzw. eT die Anzahl der Kanten bezeichnet, deren beide Endknoten in S bzw. in T liegen.
Wir wollen nun die Terme eS und eT durch e(S,T ) ersetzen, und dazu benutzen wir die RegularitĂ¤t.
Wenn wir die Grade aller Knoten in S aufsummieren, dann kommt 2eS + e(S,T ) raus (wir mĂźssen
2eS anstatt eS nehmen, da jede Kante mit beiden Enknoten in S nicht 1 sondern 2 zu dieser Summe
beitrĂ¤gt). Andererseits, ist diese Summe gleich d|S| = ds, da der Graph d-regulĂ¤r ist. Deshalb gilt
2eS + e(S,T ) = ds oder Ă¤quivalent: 2eS = ds â e(S,T ). Genauso gilt 2eT = dt â e(S,T ). Wir setzen
diese Werte in die obige Gleichung und erhalten
hx, Axi = t 2 Âˇ (2eS ) + s2 Âˇ (2eT ) â 2st Âˇ e(S,T )

= t 2 Âˇ (ds â e(S,T )) + s2 Âˇ (dt â e(S,T )) â 2st Âˇ e(S,T )
= (t 2 ds + s2 dt) â e(S,T ) Âˇ (t 2 + 2st + s2 )

= std(t + s) â e(S,T ) Âˇ (t + s) 2
= stdn â e(S,T ) Âˇ n2 ,

(da s + t = n gilt).

Zusammen mit (5.16) ergibt dies
e(S,T ) =

dstdn â Îťstn (d â Îť)st
dstdn â hx, Axi
>
=
,
2
n
n2
n

wie erwĂźnscht.



Eine wichtige Konsequenz aus Lemma 5.65 ist, dass d-regulĂ¤re Graphen mit Îť < d auch gute Expandern sind.
Korollar 5.66. Sei G = (V, E) ein ungerichteter d-regulĂ¤rer Graph mit n Knoten, und sei Îť = Îť 2 der
zweiter Eigenwert seiner Adjazenzmatrix. Dann ist G ein (n, d, c)-Expander mit
c>

dâÎť
.
2d
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

305

Beweis. Sei S â V mit |S| 6 n/2 und sei S = V \ S. Eine Kante uv â E mit u â S kann zwischen S
und S liegen, nur wenn der andere Endpunkt v in Î(S) \ S liegt. Deshalb gilt e(S, S) 6 d Âˇ |Î(S) \ S|.
Zusammen mit (5.15) ergibt dies


(d â Îť)|S|(n â |S|)
dâÎť
|S|
dâÎť
|Î(S) \ S| >
>
Âˇ |S| 1 â
>
Âˇ |S|.
dn
d
n
2d


5.13.5 Expander-Codes
Expandergraphen erlauben uns sehr effiziente fehlerkorrigierende Codes zu entwerfen (sogenannte
âTornado Codesâ). Hier beschreiben wir nur die Hauptidee dieser Codes.
Sei G = (LâŞR, E) ein bipartiter Graph mit |L| = n, |R| = m und E â LĂR. Der Code C â {0, 1} n zum
Graphen G ist wie folgt definiert. Jedem Knoten u â L ist eine Variable x u zugeordnet. Ist x â {0, 1} n
eine Belegung dieser Variablen, so heiĂt der Knoten v â R zufrieden mit x, falls
X
x u mod 2 = 0
u âÎ(v)

gilt (siehe Abbildung 5.4). Dann ist der Code zum Graphen G definiert durch:
C = {x â {0, 1} n : alle Knoten in R sind mit x zufrieden}.
Es ist klar, dass das Code C linear ist, d.h. der Vektor x â y fĂźr alle x, y â C in Code liegt. AuĂerdem ist
das Code durch eine lineare Gleichungssystem mit m Gleichungen und n Variablen definiert, woraus
|C | > 2nâm folgt.
L

R

1

v1
0
1

v2

0

Abbildung 5.4: Vektor x = (1010). Knoten v2 ist mit x zufrieden, Knoten v1 aber nicht.
Sei dist(C) der minimale Hamming-Abstand zwischen zwei verschiedenen Vektoren in C. Ein bipartiter Graph G = (L âŞ R, E) heiĂt links d-regulĂ¤r, falls jeder Knoten u â L denselben Grad d hat.
Lemma 5.67. Sei G ein links d-regulĂ¤rer (Îą, c)-Expander mit c > 12 d und sei C â {0, 1} n der Code
zu G. Dann gilt:
dist(C) > Îąn.
Beweis. Wir nehmen an, dass dist(C) 6 Îąn gilt. Dann muss es einen Vektor x â C mit |x| 6 Îąn
Einsen geben (siehe Behauptung 5.60). Sei nun S = {u â L : x u = 1}. Dann gilt |S| 6 dist(C) 6 Îąn.
ÂŠ 2003 by S. Jukna

306

KAPITEL 5. LINEARE ALGEBRA

Da G ein (Îą, d/2)-Expander ist, gilt damit auch |Î(S)| > 12 d|S|. Wir behaupten, dass es mindestens
einen Knoten v0 â Î(S) mit genau einen Nachbar in S geben muss. WĂ¤re das nĂ¤hmlich nicht der Fall,
so hĂ¤tten wir
d
2 Âˇ Î(S) > 2 Âˇ |S| = d|S|
2
Kanten zwischen S und Î(S), was unmĂśglich ist, da jeder Knoten in S Grad d hat.
Da
P aber x u = 0 fĂźr alle u < S, ist genau ein von der Bits x u mit u â Î(v0 ) gleich 1. Deshalb ist
u âÎ(v 0 ) x u = 1 und der Knoten v0 kann nicht zufrieden mit dem Vektor x sein, ein Widerspruch mit
x â C.

Nach Lemma 5.67 kĂśnnen Expander Codes ziemlich viel (bis zu Îąn/2) Ăbertragungsfehler korregieren. Viel wichtiger aber ist, dass der Dekodierungs-Algorithmus fĂźr solche Codes verblĂźfend einfach
ist.
Wir sagen, dass ein Knoten u â L von x kritisch ist, falls mehr als die HĂ¤lfte der Nachbarn von u
unzufrieden mit dem aktuellen Vektor x sind.
WHILE âu â L und u kritisch DO ersetze das Bit x u durch x u â 1.
Im Verlauf des Algorithmus werden verschiedene Bits geflippt. Es ist klar, dass in jedem Schritt
(falls er unternomen wird!) wird sich die Anzahl der unzufriedenen Knoten in R verkleinern. D.h. der
Algorithmus wird bestimmt terminieren. Das Problem aber ist, dass der resultierender Vektor x muss
nicht unbedingt in C liegen! Zum Beispiel, wenn G der im Abbildung 5.4 dargestellter Graph ist und
x = (1010) ist, so wird der Algorithmus keinen Bit von x flippen, da keiner der Knoten in L kritisch
ist. Der Vektor x aber gehĂśhrt nicht zum C (da v1 unzufrieden mit x ist), aber der Algorithmus gibt x
aus.
Es ist deshalb interessant, dass der Algorithmus bereits korrekt funktionieren wird, wenn der graph G
ein Expander ist!
Lemma 5.68. Sei G ein links d-regulĂ¤rer (Îą, c)-Expander mit c > 34 d und sei C â {0, 1} n der
Code zu G. Seien y â C und yâ˛ â {0, 1} n . Ist dist(y, yâ˛ ) 6 Îąn/2, so wird der Algorithmus die
Originalnachricht y aus dem (beschĂ¤digten) Vektor yâ˛ in hĂśchstens |R| = m Schritten rekonstruieren.
Beweis. Wir betrachten den allgemeinen Schritt. Sei x der bis zum diesen Schritt vom Algorithmus
erzeugte Vektor, und sei S = {u â L : x u , yu } die Menge der aktuellen âFehlerbitsâ. Ausserdem,
sei Z bzw. U die Menge aller Nachbarn von Knoten in S, die zufrieden bzw. unzufrieden mit dem
(aktuellen) Vektor x sind.16
Zuerst zeigen wir, dass jeder Nachbarn von S, der mit dem Vektor x zufrieden ist, muss mindestens
zwei Nachbarn in S haben, d.h
|S âŠ Î(v)| > 2
(5.17)
fĂźr alle v â Î(S) gilt. Angenommen, es gibt einen Knoten v â Z, der nur einen Nachbarn u in S hat.
Da y â C, muss der Knoten v zufrieden mit y sein, d.h. die Anzahl der Einsen in {yu : u â Î(v)}
muss gerade sein. Aber auf diesen Bits unterscheidet x vom y an genau einer Stelle x u . Deshalb kann
v nicht zufrieden mit x sein, ein Widerspruch mit v â Z.
16Beachte, dass der Algorihmus nur die Mengen U und Z in jedem Schritt kennt, die Menge S ist ihm unbekannt.
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

307

Wir behaupten nun folgendes:
Solange 0 < |S| 6 Îąn gilt, wird der Algorithmus mindestens ein Bit flippen.
Da |S| 6 Îąn, muss S um Faktor > 34 d expandieren, woraus die Ungleichung
| Z | + |U | = |Î(S)| >

3
d|S|
4

(5.18)

folgt. Betrachte nun die Menge aller d|S| Kanten zwischen S und Î(S) = Z âŞ U. Mindestens |U | von
diesen Kanten sind inzident mit den Knoten in U, und nach (5.17) mindestens 2| Z | diesen Kanten
mĂźssen inzident mit den Knoten in Z sein. Deshalb gilt
2| Z | + |U | 6 d|S|.

(5.19)

Aus (5.18) und (5.19) folgt
d|S| â |U | > 2| Z | > 2



3
d|S| â |U |
4



oder Ă¤quivalent
|U | >

1
d|S|.
2

(5.20)

Also, mehr als 12 d|S| der Nachbarn der |S| Knoten sind unzufrieden. Deshalb muss es einen Knoten
u â S geben, so dass u mehr als d/2 unzufriedenen Nachbarn hat, und der Algorithmus flippt den Bit
xu .
Es bleibt also zu zeigen, dass die Invariante |S| 6 Îąn stets gilt (bis S leer ist). Am Anfang haben wir
|S| = dist(y, yâ˛ ) 6 Îąn/2, und deshalb auch
|U | 6 |Î(S)| 6

1
Îądn.
2

(5.21)

Ausserdem, muss diese Ungleichung in jedem Schritt gelten, da sich in jedem Schritt (falls er geschiet)
die Menge U verklienert. Warum? Durch Flippen eines Bits x u sind nur die Nachbarn von u bertoffen
und nach dem Flippen haben wir mehr zufriedene davon als unzufriedene. Also muss auch |S| 6 Îąn
in jedem Schritt gelten, denn sonst (wenn |S| > Îąn) hĂ¤tten wir nach (5.20) |U | > 12 d|S| > 12 Îądn, ein
Widerspruch mit (5.21).
Damit ist Lemma 5.68 bewiesen.



5.13.6 Markov-Ketten
In diesem Abschnitt betrachten wir eine Situation, wo drei uns bereits bekannte Gebiete â Stochastik,
MatrizenkalkĂźl und Analysis â sich zusammen trefen.
Man hat ein System, das in jedem Zeitpunkt in einem der m ZustĂ¤nde V = {1, . . . , m} sein kann. Sei
pi j die Wahrscheinlichkeit, dass das System im nĂ¤chsten Schritt zum Zustand j Ăźbergeht, falls es jetzt
in Zustand i ist. Da die pi j âs Wahrscheinlichkeiten sein sollen, muss 0 6 pi j 6 1 fĂźr alle i, j und
m
X

pi j = 1

j=1

ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

308

fĂźr alle i gelten. Wir haben also eine unendliche Folge der Zufallsvariablen <X n >, die Werte in
{1, 2, . . . , m} annehmen. Der Index n entspricht den Zeiteinheiten (=Schritten) und X n stellt den Zustand des Systems nach n Schritten dar. Der wichtigste Unterschied der Markov-Ketten17 von allgemeinen stochastischen Prozessen ist ihre âGedĂ¤chtnislosigkeitâ: In jedem Schritt i hĂ¤ngen die Ereignisse âX n+1 = jâ nur von der Ereignissen âX n = iâ ab. D.h. die Wahrscheinlichkeit, in welchen
Zustand das System (unser Experiment) im (n + 1)-ten Schritt Ăźbergehen wird, hĂ¤ngt nur von dem
Zustand ab, in dem das System sich gerade (im Schritt n) befindet (und hĂ¤ngt nicht von der Vergangenheit ab). Mathematisch lĂ¤sst sich diese Eigenschaft so auszudrucken:
Pr {X n+1 = j | X n = i, X nâ1 = k, . . . , X0 = l } = Pr {X n+1 = j | X n = i } = pi j .
Das System kann man auch als ein gerichteten Graph G = (V, E) darstellen, wobei die Kante von i
nach j (falls sie vorhanden ist18) ist mit der Wahrscheinlichkeit pi j markiert; die Zahl pi j ist also die
Wahrscheinlichkeit, dass beginnend in Knoten (= Zustand) i das System in einem Schritt zum Knoten
(= Zustand) j Ăźbergehen wird. Die Matrix
ďŁŽ

ďŁŻ
ďŁŻ
P=ďŁŻ
ďŁ°

p11
p21
..
.

p12
p22
..
.

. . . p1m
. . . p2m
..
.

pm1

pm2 . . . pmm

ďŁš
ďŁş
ďŁş
ďŁş
ďŁť

heiĂt die Ăbergangsmatrix. Ein solches System heiĂt Markov-Kette.
Satz 5.69. Ist P die Ăbergangsmatrix einer Markov-Kette, so ist Pinj die Wahrscheinlichkeit, in n
Schritten den Knoten j aus Knoten i zu erreichen.
Beweis. VĂśllig analog mit dem Beweis vom Satz 5.17.



â˛ Beispiel 5.70 : Ein Land irgendwo am Rande der Welt ist durchaus schĂśn, nur mit dem Wetter haben
die Einwohner Pech gehabt: sie haben nie zwei Tage nacheinander die Sonne. Falls sie die Sonne
haben, dann wird bestimmt am nĂ¤chsten Tag keine Sonne sein, sondern es wird entweder regnen
oder schneien, und zwar mit gleicher Wahrscheinlichkeit 1/2. Wenn es gerade regnet oder schneit,
dann gibt es eine 1/2 Chance, dass genau so auch am nĂ¤chsten Tag sein wird. Wenn am den Tag
schneit oder regnet, dann ist die Chance, am nĂ¤chsten Tag Sonne zu haben, nur 1/4:

Sonne
1/4

Regen
1/2

1/2

1/2

1/4

1/4

Schnee
1/4

1/2

17Engl. Markov chains.
18Die nicht vorhandene Kanten (i, j) tragen also die Wahrscheinlichkeit pi j = 0.
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

309

Damit haben wir drei mĂśgliche ZustĂ¤nde 1 =âregnetâ, 2 = âsonnigâ, 3 = âschneitâ und die Ăbergangsmatrix sieht folgender MaĂen aus:
ďŁŽ
ďŁš
regnet
1/2 1/4 1/4
P = sonnig ďŁ° 1/2
0 1/2 ďŁť
schneit
1/4 1/4 1/2
Wenn man die Potenzen P n fĂźr n = 2, 3, . . . betrachtet, bekommt man sehr bald (bereits bei n = 7)
die Matrix
ďŁŽ
ďŁš
0, 4 0, 2 0, 4
P7 = ďŁ° 0, 4 0, 2 0, 4 ďŁť
0, 4 0, 2 0, 4

D.h. langfristig gesehen (mit Abstand von mindestens einer Woche) wird sich das Wetterverhalten
stabilisieren: egal welches Wetter heute ist, wird in einer Woche nur mit Wahrscheinlichkeit 0, 2
sonnig sein, und mit gleicher Wahrscheinlichkeit 0, 4 wird entweder regnen oder schneien.
Eine interessante Klasse sind Markov-Ketten mit sogenannten âabsorbierendenâ ZustĂ¤nden. Ein Zustand i ist absorbierend, falls pii = 1 gilt. D.h. kommt einmal das System in Zustand i, so kann
es den Zustand nicht mehr verlassen. Wenn zum Besipiel man mit einer Markov-Kette ein System
von Lebenswesen modeliert, dann hat dieses System einen absorbierenden Zustand âTodâ (wie in
dem Beispiel mit dem Frosch und dem Storch im Abschnitt 4.15). Die nicht absorbierende ZustĂ¤nge
heiĂen auch vorĂźbergehende ZustĂ¤nde.
Eine Markov-Kette heiĂt absorbierend, wenn sie folgenden zwei Bedingungen erfĂźllt:
1. Die Kette hat mindestens einen absorbierenden Zustand.
2. Es ist mĂśglich, aus jedem nicht absorbierenden Zustand einen absorbierenden Zustand zu erreichen
(vielleicht in mehr als einem Schritt).
1
2
0.6

0.2

P=

0.3 0.6 0.1
0
1 0
0.6 0.2 02

0.6
1

3
0.2

0.1

0.3

Abbildung 5.5: Eine absorbierende Markov-Kette mit 3 ZustĂ¤nden und seine Ăbergansmatrix P. Der
Zustand 2 ist absorbierend.

â˛ Beispiel 5.71 : (Spaziergang eines Betrunkenen) Ein stark betrunkener Man spaziert entlang des
aus vier Strecken bestehenden Park Avenue:
1/2
1

1

2
1/2

3
1/2

1/2

1/2

4

5

1

1/2
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

310

Wenn er an einer der Ecken 2, 3 oder 4 ist, so geht er nach links oder nach rechts mit gleicher
Wahrscheinkichkeit 1/2. Er bewegt sich so bis er entweder die Ecke 5 (das ist eine Bar) oder die
Ecke 1 (da ist seine Wohnung) erreicht; falls er eine dieser zwei Ecken erreicht, bleibt er da. Wir
kĂśnnen dieses System durch folgende Ăbergangsmatrix darstellen:
ďŁŽ

ďŁŻ
ďŁŻ
P=ďŁŻ
ďŁŻ
ďŁ°

1
0
0
0
0
1/2 0 1/2 0
0
0 1/2 0 1/2 0
0
0 1/2 0 1/2
0
0
0
0
1

ďŁš
ďŁş
ďŁş
ďŁş
ďŁş
ďŁť

Die Kette ist absorbierend, da von nicht absorbierenden ZustĂ¤nden 2, 3 und 4 kann man die
absorbierende ZustĂ¤nde 1 und 5 erreichen.
Typische Fragen fĂźr gegebene absorbierende Markov-Kette sind:
1. Ist das System im nicht absorbierenden Zustand i, wie lange wird es durchschnittlich dauern bis
das System in einen der absorbierenden ZustĂ¤nde Ăźbergeht?
2. Ist das System im nicht absorbierenden Zustand i, mit welcher Wahrscheinlichkeit wird dann das
System in einen absorbierenden Zustand j Ăźbergehen?
Diese Frage kann man mit Hilfe der Matrizenalgebra relativ leicht beantworten.
Sei P die Ăbergangasmatrix einer absorbierenden Markov-Kette. Nummeriere die ZustĂ¤nde so, dass
nicht-absorbierende ZustĂ¤nde zuerst kommen. Falls die Kette r absorbierenden und s nicht-absorbierenden
ZustĂ¤nde hat, dann sieht die kanonische Form der Ăberhangsmatrix wie folgt aus:

P=



Q R
0 E



(5.22)

Hier ist E die r Ă r Einheitsmatrix und 0 ist die r Ă s Nullmatrix. Beachte, dass die n-te Potenz P n
von P die folgende Form hat (nachrechnen!):
n

P =



Qn
0

â
E



Damit ist Q inj die Wahrscheinlichkeit, in n Schritten den vorĂźbergehenden Zustand j aus vorĂźbergehendem Zustand i zu erreichen. ZunĂ¤chst zeigen wir, dass diese Wahrscheinlichkeiten mit wachsendem n gegen Null streben.19
Lemma 5.72. In einer absorbierenden Markov-Kette gilt lim Q n = 0. D.h. mit Wahrscheinlichkeit 1
nââ
wird die Kette aus jedem vorĂźbergehendem Zustand zu einem absorbierendem Zustand Ăźbergehen.
19Den Begriff der Konvergenz kann man leicht auf die Folgen von Matrizen Ăźbetragen. Dazu reicht es die ÇŤ-Umgebung
UÇŤ (A) einer Matrix A = (ai j ) als die Menge aller Matrizen B = (bi j ) mit |bi j â ai j | < ÇŤ fĂźr alle i, j definieren.
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

311

Beweis. Aus jedem vorĂźbergehenden Zustand i ist es mĂśglich einen absorbierenden Zustand zu erreichen. Sei t i die minimale Anzahl der Schritte aus Zustand i einen absorbierenden Zustand zu erreichen. Sei pi die Wahrscheinlichkeit, dass das System aus Zustand i in t i Schritten einen absorbierenden Zustand nicht erreichen wird. Dann gilt pi < 1. Sei t die grĂśĂte Zahl aus t 1 , . . . , t s und p die
grĂśĂte Zahl aus p1 , . . . , ps . Die Wahrscheinlichkeit, in t Schritte nicht absorbiert zu sein, ist 6 p; fĂźr
2t Schritten ist diese Wahrscheinlichkeit 6 p2 , usw. Da p < 1, strebt die Folge (Q it : i = 1, 2, . . .)
gegen der Nullmatrix 0 und damit muss 20 auch die Folge (Q n : n = 1, 2, . . .) gegen der Nullmatrix 0
streben.

Satz 5.73. Sei P die Ăbergangsmatrix einer absorbierenden Markov-Kette mit n ZustĂ¤nden und sei
Q die nicht absorbierende Teilmatrix von P. Dann hat die Matrix E â Q die Inverse N = (E â Q) â1
und es gilt: N = E + Q + Q2 + . . .. Sei weiterhin (v1 , . . . , vs ) die i-te Zeile von N. Dann gilt:
s
X

vj

=

die erwartete Anzahl der Schritte bis die Kette in einen

j=1

absorbierenden Zustand landet, wenn sie in Zustand i startet

Die Matrix N = (E â Q) â1 nennt man die fundamentale Matrix der Markov-Kette.
Beweis. Sei (E â Q)x = 0, d.h. x = Q Âˇ x. Daraus folgt: x = Q n x fĂźr alle n = 1, 2, . . .. Da lim Q n = 0
nââ
ist, muss lim Q n x = 0 und somit auch x = 0 gelten. D.h. das Gleichungssystem (E â Q)x = 0 kann
nââ

nur triviale LĂśsung x = 0 haben woraus folgt, dass die Matrix E âQ invertierbar ist. Sei N = (E âQ) â1
die inverse Matrix von E â Q. Beachte dass
(E â Q)(E + Q + Q2 + . . . + Q n ) = E â Q n+1
gilt. Multiplizieren wir beide Seiten mit N = (E â Q) â1 , so erhalten wir
E + Q + Q2 + . . . + Q n = N (E â Q n+1 ).

Strebt nun n gegen â, so strebt nach Lemma 5.72 die Folge der Matrizen Q n+1 gegen der Nullmatrix
0. Somit gilt:
N = E + Q + Q2 + . . .
d.h. die Reihe Q0 +Q1 +Q2 + . . . mit Q0 = E konvergiert (und sogar absolut, da EintrĂ¤ge nicht-negativ
sind) gegen den Grezwert N:
n
X
N = lim
Qk
nââ

k=0

Es bleibt zu zeigen, dass Ni j genau die erwartete Anzahl
P der Besuche
Ps des Zustands j ist, wenn dass
System in Zustand i startet. Somit ist die Summe sj=1 v j =
j=1 Ni j die erwartete Anzahl der
Schritte bis die Kette in einen absorbierenden Zustand landet, wenn sie in Zustand i startet.

Seien nun i, j beliebig (aber fest) und sei X k die Indikatorvariable fĂźr das Ereignis, dass das System
den Zustand j in genau k Schritten erreicht, wenn es im Zustand i startet. Die Zufallsvariable Yn :=
20Die Folge (Q n : n = 1, 2, . . .) ist monoton fallende Folge von Matrizen.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

312

X1 + X2 + . . . + X n beschreibt also die Anzahl der Besuche des Zustands j in ersten n Schritten, wenn
dass System in Zustand i startet. Dann gilt Pr {X k = 1} = Q kij , Pr {X k = 0} = 1 â Q kij und
E [Yn ] = E

" n
X
k=0

#

Xk =

n
X

E [X k ] =

k=0

n
X
k=0

Pr {X k = 1} = Q0i j + Q1i j + Q2i j + . . . + Q inj .

Andererseits ist der Grenzwert lim E [Yn ] genau die erwartete Anzahl der Besuche des Zustands j,
nââ
wenn dass System in Zustand i startet. Nach der unendlichen LinearitĂ¤t des Erwartungswertes gilt:
" â #
â
X
X
lim E [Yn ] = E
Yn =
E [Yn ] = Q0i j + Q1i j + Q2i j + . . . = Ni j
nââ

n=0

n=0


Die nĂ¤chste Frage ist, mit welcher Wahrscheinlichkeit wird das System aus einem vorĂźbergehenden
Zustand i in einen absorbierenden Zustand j Ăźbergehen? Die Antwort ist im folgenden Satz gegeben.
21
Satz 5.74. Sei B = N Âˇ R. Dann ist Bi j die Wahrscheinlichkeit, dass das System im Zustand j
absorbiert wird, wenn es in vorĂźbergehenden Zustand i startet.
Beweis. Da N = E + Q + Q2 + . . ., haben wir B = N R = E R + QR + Q2 R + . . .. Es gilt also:22
Bi j

=

â X
m
X
n=0 k=1

=

m X
â
X
k=1 n=0

=

m
X
k=1

n
Q ik
Âˇ Rk j
n
Q ik
Âˇ Rk j

Nik Âˇ Rk j

= N Ri j .

â˛ Beispiel 5.75 : (Geometrische Verteilung) Wir wiederholen das Bernoulli-Experiment X1 , X2 , . . .
mit Erfolgswahrscheinlichkeit p , 0 vielmals und wollen die Erwartete Anzahl der MiĂerfolge
(= Anzahl der Nullen) vor dem ersten Erfolg (vor der ersten Eins) bestimmen. Die entsprechende
Zufallsvariable ist Y = min{T : XT = 1}. Das Experiment kann man als die folgende absorbierende Markov-Kette darstellen:
1âp

0

p

1

1

21Hier ist R die s Ă r Teilmatrix aus der kanonischen Form (5.22) der Ăbergangsmatrix P.
22Da Q0 = E
ÂŠ 2003 by S. Jukna

5.13. EINIGE ANWENDUNGEN DES MATRIZENKALKĂLSâ

313

Die entsprechende Ăbergangsmatrix (bereits in einer kanonischer Form) ist
P=



1âp p
0
1



Somit ist Q = 1 â p, E â Q = 1 â (1 â p) = p und N = (E â Q) â1 = p1 . Ausserdem gilt:
N Âˇ R = p1 Âˇ p = 1, d.h. dass System wird mit Wahrscheinlichkeit 1 den Zustand 1 (Erfolg)
erreichen.
â˛ Beispiel 5.76 : (Runs) Wir wiederholen wiederum das Bernoulli-Experiment X1 , X2 , . . . mit MiĂerfolgswahrscheinlichkeit p , 0 vielmals und wollen die Erwartete Anzahl der Versuche vor dem
ersten Erfolg bestimmen. Versuche sind Zahlen 0 und 1, wobei eine Null mit Wahrscheinlichkeit
p kommt. Als Erfolg nehmen wir ein Run 111 aus drei nacheinandere folgenden Einsen. Das
Experiment kann man als die folgende absorbierende Marko-Kette darstellen:

p
p
1

p

1âp

2

1âp

3

1âp

4
Erfolg

Die entsprechende Ăbergangsmatrix (bereits in einer kanonischer Form) ist
ďŁŽ

ďŁš
0
p 1âp
0
ďŁŻ p
0
1âp
0 ďŁş
ďŁş
P=ďŁŻ
ďŁ° p
0
0
1âp ďŁť
0
0
0
1
Die Matrix Q ist also
ďŁŽ

ďŁš
p 1âp
0
0
1âp ďŁť
Q=ďŁ° p
p
0
0

Nun benutzen wir das Program maple um die Inverse N = (E â Q) â1 auszurechnen:23
ďŁŽ

ďŁŻ
ďŁŻ
ďŁŻ
N =ďŁŻ
ďŁŻ
ďŁŻ
ďŁ°

1
1
1
+
â
â1 + 3 p â 3 p2 + p3 1 â 2 p + p2 â1 + p
p (p â 2)
1
1
+
â
â1 + 3 p â 3 p2 + p3 1 â 2 p + p2 â1 + p
p
p
1
â
+
â
2
3
2
â1 + 3 p â 3 p + p
1â2p+ p
â1 + p
â

ďŁš
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁş
ďŁť

23Wir kĂśnnen das tun, da wir bereits wissen, wie man die Inversen berechnen kann, und wir kĂśnnen diese Routinearbeit
dem Computer Ăźberlassen.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

314
und

ďŁŽ

ďŁš
p2 â 3 p + 3
ďŁŻ â (â1 + p) (1 â 2 p + p2 ) ďŁş
ďŁŻ
ďŁş
ďŁş
ďŁŻ
pâ2
ďŁş
ďŁŻ
N Âˇ1=ďŁŻ
ďŁş
ďŁŻ (â1 + p) (1 â 2 p + p2 ) ďŁş
ďŁŻ
ďŁş
ďŁť
ďŁ°
1
â
(â1 + p) (1 â 2 p + p2 )
Somit ist die erwartete Anzahl der Versuche (beginnend im Zustand 0), bis ein Run 111 kommt,
gleich (die erste Zeile von N Âˇ 1):
â

p2 â 3 p + 3
1 â (1 â p) 3
=
,
(â1 + p) (1 â 2 p + p2 )
p(1 â p) 3

eine Formel, die wir bereits mit Hilfe von Waldâs Theorem (fĂźr beliebig langen Runs) bewiesen
haben (siehe Beispiel 4.94).
â˛ Beispiel 5.77 : (Spaziergang eines Betrunkenen â Fortsetzung) Die kanonische Form fĂźr die Matrix dieser Markov-Kette ist:
ďŁŽ

ďŁŻ
ďŁŻ
P=ďŁŻ
ďŁŻ
ďŁ°

Also ist

ďŁš
0 1/2 0 1/2 0
1/2 0 1/2 0
0 ďŁş
ďŁş
0 1/2 0
0 1/2 ďŁş
ďŁş
0
0
0
1
0 ďŁť
0
0
0
0
1
ďŁŽ

ďŁš
0 1/2
0
0 1/2 ďŁť
Q = ďŁ° 1/2
0 1/2
0

und

ďŁŽ

ďŁš
1 â1/2
0
1 â1/2 ďŁť
E â Q = ďŁ° â1/2
0 â1/2
0

Berechnet man nun (EâQ) â1 (z.B. mit GauĂ-Jordan Verfahren, siehe Abschnitt 5.9), so bekommt
man
ďŁŽ
ďŁš
3/2 1 1/2
1 ďŁť
N = (E â Q) â1 = ďŁ° 1 2
1/2 1 3/2
Damit ist

ďŁŽ
ďŁš ďŁŽ ďŁš ďŁŽ ďŁš
2
3/2 1 1/2
1
3
1 ďŁťÂˇďŁ° 1 ďŁť=ďŁ° 4 ďŁť
N Âˇ1= 3 ďŁ° 1 2
4
1/2 1 3/2
1
3

Wenn der Betrunkene in einem der ZustĂ¤nde 2, 3 oder 4 startet, dann wird er im Durchschnitt 3, 4
oder 3 Schritte machen, bis er absorbiert (in der Kneipe oder zu Hause) wird.
1/2
1

1

2
1/2

3
1/2

1/2

1/2

4

5

1

1/2
ÂŠ 2003 by S. Jukna

5.14. AUFGABEN

315

Und mit welchen Wahrscheinlichkeiten wird der Betrunkene absorbiert? Aus der kanonischen
Form der Ăbergansgsmatrix fĂźr diese Markov-Kette haben wir:
ďŁŽ
ďŁš
2
1/2 0
0 ďŁť
R= 3 ďŁ° 0
4
0 1/2
Dann ist das Produkt B = N R gleich

ďŁŽ

ďŁš ďŁŽ
ďŁš
3/2 1 1/2
1/2 0
B = NR = ďŁ° 1 2 1 ďŁť Âˇ ďŁ° 0
0 ďŁť
1/2 1 3/2
0 1/2
=

ďŁŽ
ďŁš
2
3/4 1/4
3 ďŁ° 1/2 1/2 ďŁť
4
1/4 3/4

Diese Matrix gibt uns die Wahrscheinlichkeiten, mit deren der Berunkene im Zustand 1 (zu Hause) oder im Zustand 5 (die Kneipe) absorbiert wird, wenn er im einen der ZustĂ¤nde 2, 3 oder 4
startet. Startet er z.B. im Zustand 2, so wird er nur mit Wahrscheinlichkeit 3/4 (und nicht mit
Wahrscheinlichkeit 1/2, wie man vermuten kĂśnnte!) nach Hause kommen und immerhin mit
Wahrscheinlichkeit 1/4 die Kneipe erreichen.

5.14 Aufgaben
5. 1. Sei V ein Vektorraum Ăźber einem KĂśrper F mit char(F) , 2. Seien u und v zwei linear unabhĂ¤ngige
Vektoren in V . Zeige, dass dann auch die Vektoren x = u â v und y = u + v linear unabhĂ¤ngig sind.
5. 2. Es sei F ein KĂśrper und A eine (beliebige) Menge. Die Menge F A aller Funktionen f : X â F mit der
Addition
( f + g)(x) := f (x) + f (y)
und der Skalarmultiplikation mit einer reellen Zahl Îť gemĂ¤Ă
Îť f (x) := Îť Âˇ f (x)

âx â X

bildet einen Vektorraum Ăźber dem KĂśrper F.
(a) (UnabhĂ¤ngigkeits-Kriterium) Seien nun f 1 , . . . , f m : A â F Funktionen, fĂźr die es Elemente a1 , . . . , a m
mit den folgenden Eigenschaften gibt:
(i) f i (ai ) , 0 fĂźr alle 1 6 i 6 m,
(ii) f i (a j ) = 0 fĂźr alle 1 6 j < i 6 m.
Zeige, dass dann f 1 , . . . , f m als Vektoren in F A linear unabhĂ¤ngig sind.
(b) Zeige, dass fie Funktionen f , g, h linear unabhĂ¤ngig (in Vektorraum RR ) sind, wobei
f (x) = e x , g(x) = x 4 , h(x) = 4x
â â
5. 3. Beweise oder widerlege: Die drei Vektoren 1, 2, 3 im Vektorraum R Ăźber dem KĂśrper Q sind linear
unabhĂ¤ngig. Sind die drei Vektoren linear unabhĂ¤ngig Ăźber dem KĂśrper R?
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

316

5. 4. Sei A eine m Ă n Matrix, und b â Rm ein Vektor, b , 0. Ist dann die Menge U = {x : x â Rn , Ax = 0}
ein Vektorraum? Ist dann die Menge V = {x : x â Rn , Ax = b} ein Vektorraum?
5. 5. Sei L : R3 â R3 die Abbildung definiert durch die Matrix
ďŁŽ
ďŁš
1 2 â1
1 ďŁť
A=ďŁ° 0 1
1 1 â2

Bestimme Basen und Dimensionen von Im L und Ker L.

Hinweis: Im L ist der Spaltenraum von A â dim(Im L) ist der Zeilenrang von der transponierten Matrix Aâ¤ ;
diese Matrix bekommt man, wenn man die Spalten von A als Zeilen schreibt. Um den Zeilenrang (und damit
auch den Rang) einer Matrix zu bestimmen, transformiert man die Matrix zu einer Zeilenstufenform.
5. 6. Die Abbildung L : R3 â R3 sei definiert durch
ďŁŤ
ďŁś ďŁŤ
ďŁś
x
x + 2y â z
ďŁ¸
y+z
LďŁ­ y ďŁ¸ = ďŁ­
z
x + y â 2z

Bestimme Basen und Dimensionen der UntervektorrĂ¤ume Im L und Ker L.

5. 7. Sei L : V â W eine lineare Abbildung. Dann gilt offensichtlich: L ist surjektiv ââ Im L = W . Zeige,
dass
L ist injektiv ââ Ker L = {0}
5. 8. Ein Vater und seine beide SĂśhne sind zusammen hundert Jahre alt, der Vater ist doppelt so alt wie sein
Ă¤ltester Sohn und dreissig Jahre Ă¤lter als sein jĂźngster. Wie alt ist der Vater?
5. 9. Wir wollen einen Obstsalat aus Apfeln, Bananen und Orangen. Die NĂ¤rstoffanteile seien in folgender
Tabelle angegeben:
EiweiĂ Fett Kohlenhydrate
Apfel
0, 3
0, 6
15
Bananen
1, 1
0, 2
22
Orangen
1, 0
0, 2
12
Stelle einen Obstsalat zusammen, der insgesamt 9 g EiweiĂ, 5 g Fett und 194 g Kohlenhydrate enthĂ¤lt.
5. 10. LĂśse die folgenden Gleichungssysteme:
ďŁŽ
ďŁš ďŁŽ
ďŁš ďŁŽ
ďŁš
1 2 0
x
4
ďŁ° 3 1 1 ďŁťÂˇďŁ° y ďŁť=ďŁ° 0 ďŁť
1 1 2
z
3
ďŁŽ

6 0
ďŁ° 3 2
1 0

ďŁŽ

0 2
ďŁ° 2 1
4 2

ďŁš ďŁŽ
ďŁš
ďŁš ďŁŽ
1
x
1
0 ďŁťÂˇďŁ° y ďŁť=ďŁ° 0 ďŁť
1
z
2

ďŁš ďŁŽ
ďŁš ďŁŽ
ďŁš
2
x
1
0 ďŁťÂˇďŁ° y ďŁť=ďŁ° 1 ďŁť
0
z
2

5. 11. LĂśse die Gleichung:


â4
âx

x
4

2

=



â1 0
0 â1



ÂŠ 2003 by S. Jukna

5.14. AUFGABEN

317

5. 12. FĂźr welche Werte von a hat das folgende
mehrere LĂśsungen?
x +
x +
ax +
5. 13. Berechne



1
0

1
1

n

Gleichungssystem (i) keine bzw. (ii) genau eine bzw. (iii)
y
ay
y

+
+
+

az
z
z

=1
=1
=1

, wobei n eine natĂźrliche Zahl sein soll.

5. 14. Bestimme die inverse Matrizen von:
A=



1
2

5. 15. Gegeben ist die Matrix A =

ďŁŽ

ďŁš
ďŁŽ
0
1 2
â1 ďŁť , C = ďŁ° 2 4
0
3 5

â1 1
, B=ďŁ° 0 1
0 1

2
5





1 3
4 â3



. Gesucht ist ein Vektor x =

5. 16. Ermittle den Rang folgender Matrizen:
ďŁŽ

1 â1
ďŁŻ â2 â1
A=ďŁŻ
ďŁ° 0 â3
â7
1
5. 17. Es sei A =



ďŁš

3
4
10
â1

1 2
3 â4

2
3 ďŁş
ďŁş
7 ďŁť
0


ďŁŽ

ďŁŻ
B=ďŁŻ
ďŁ°

1
2
1
â1

5
1
14
4

3
0
9
3

4
3
9
1

ďŁš

2
1 ďŁş
ďŁş
5 ďŁť
1



x1
x2

ďŁš
3
5 ďŁť
6

ďŁŽ

ďŁŻ
ďŁŻ
C=ďŁŻ
ďŁŻ
ďŁ°

, so dass gilt: Ax = x.

1
3
5
1
2
2
4
2
2 â4

0
2
1
1
1

1
3
0
4
2

4
1
3
2
6

ďŁš
ďŁş
ďŁş
ďŁş
ďŁş
ďŁť

. Berechne

(a) A2 + 3A â 10E

(b) 2A2 â 3A + 5E


1 2
5. 18. Es sei A =
. Berechne
4 â3
(a) A2 + 2A â 11E

(b) 2A3 â 4A + 5E
5. 19. Bestimme die Inverse Aâ1 von

falls sie existiert.

ďŁŽ

1 2
A=ďŁ° 0 1
1 0

ďŁš
0
1 ďŁť
2

5. 20. Sei B = P APâ1 , wobei A und P invertierbare n Ă n Matrizen sind. Finde B â1 .

5. 21. Seien A und B invertierbare n Ă n Matrizen Ăźber R. Zeige, dass dann die Spalten von Aâ1 B den ganzen
Vektorraum Rn erzeugen.
5. 22. Nehmen wir an, dass die letzte Spalte von AB eine Nullspalte 0 ist, aber die Matrix B selbst keine
Nulspalte enthĂ¤lt. Was kann man dann Ăźber die Spalten von A sagen?
5. 23. Zeige: Wenn die Spalten von B linear abhĂ¤ngig sind, dann sind auch die Spalten von AB linear abhĂ¤ngig.
5. 24. Sei C A = En (die n Ă n Einheitsmatrix). Zeige, dass dann die Gleichungssystem Ax = 0 nur die triviale
LĂśsung x = 0 haben kann.
ÂŠ 2003 by S. Jukna

KAPITEL 5. LINEARE ALGEBRA

318

5. 25. Sei AD = Em (die m Ă m Einheitsmatrix). Zeige, dass dann die Gleichungssystem Ax = b fĂźr alle
b â Rm lĂśsbar ist.
5. 26. Sei (B â C)D = 0, wobei B und C m Ă n Matrizen sind, und die Matrix D invertierbar ist. Zeige, dass
dann B = C gelten muss.
5. 27. Berechne die Inverse Aâ1 von

falls sie existiert.
5. 28. Berechne die Eigenwerte von

ďŁŽ

0
A=ďŁ° 4
3
ďŁŽ

1
A=ďŁ° 0
0

ďŁš
1 â1
â3
4 ďŁť
â3
4
ďŁš
â3 5
1 0 ďŁť
â2 2

5. 29. Sei G = (V, E) ein ungerichteter Graph mit V = {1, . . . , n}. Ein Dreieck in G ist eine Menge {i, j, k}
aus drei Knoten die paarweise adjazent sind, d.h. {i, j} â E, {i, k} â E und { j, k} â E. Sei A = (ai j ) die
Adjazenzmatrix von G und A2 = (bi, j ) Zeige, dass G ein Dreieck genau dann besitzt, wenn es ein Paar 1 6 i <
j 6 n mit ai j , 0 und bi j , 0 gibt.
Fazit: Da die Multiplikation zweier Matrizen relativ schnell ausgerechnet sein kann (in Zeit ungefĂ¤hr n2 ), haben
wir damit einen Algorithmus zur bestimmung
der Dreiecksfreiheit von Graphen, der wesentlich schneller als

der trivialer Algoritmus (der alle n3 â n3 MĂśglichkeiten ausprobiert) lĂ¤uft.
5. 30. Definiere die Matrizen H2m , m = 2, 4, 8, . . . rekursiv wie folgt:




1
1
Hm
Hm
H2 =
,
H2m =
.
1 â1
Hm âHm

Zeige, dass die Matrizen H2m regulĂ¤r sind, d.h. vollen Rang haben.
5. 31. Sei F ein KĂśrper und M n die Menge aller n Ă n-Matrizen Ăźber F. Zeige, dass dann (M n , +, Âˇ) ein Ring
ist.
5. 32. Die Disjunktheitsmatrix (engl. disjointness matrix) ist eine 2n Ă 2n 0-1 Matrix Dn , deren Zeilen und
Spalten mit Teilmengen einer n-elementigen Menge markiert sind, und
Dn ( A, B) = 1

ââ

A âŠ B = â.

Zeige, dass Dn Ăźber jeden KĂśrper F den vollen Rang 2n hat. Hinweis: Benutze die Induktion Ăźber n zusammen
mit der folgender rekursiver Konstruktion von Dn :




1 1
Dnâ1 Dnâ1
D1 =
,
Dn =
1 0
Dnâ1
0
5. 33. Die âSchneidungsmatrixâ (engl. intersection matrix) ist eine (2n â 1) Ă (2n â 1) 0-1 Matrix Q n deren
Zeilen und Spalten mit nicht leeren Teilmengen einer n-elementigen Menge markiert sind, und Q n ( A, B) = 1
ââ A âŠ B , â. Zeige, dass diese Matrix auch den vollen Rang Ăźber jeden KĂśrper F hat. Hinweis: Sei In
die 2n Ă 2n Matrix, die nur aus Einsen besteht. Dann ist In â Dn genau die Matrix Q n mit einer zusĂ¤tzlicher
Nullzeile und einer zusĂ¤tzlicher Nullspalte.
5. 34. Sei A eine m Ă n Matrix Ăźber dem KĂśrper F = GF (2) (nur zwei Elemente 0 und 1 mit der Addition und
Multiplikation modulo 2) und span( A) sei ihr Zeilenraum. Ein Vektor x â Fn heiĂt gerade bzw. ungerade, falls
die Anzahl |x| = x 1 + . . . + x n der Einsen in x gerade bzw. ungerade ist. Sei 1 = (1, . . . , 1). Zeige folgendes:
1 < span( A) genau dann, wenn es einen ungeraden Vektor x â Fn mit A Âˇ x = 0 gibt.
ÂŠ 2003 by S. Jukna

5.14. AUFGABEN

319

5. 35. (Putnam Exam, 1991) Seien A und B zwei verschiedene n Ă n-Matrizen Ăźber R mit A3 = B 3 und A2 B =
B 2 A. Zeige, dass dann die Matrix A2 + B 2 keine Inverse haben kann. Hinweis: Betrachte ( A2 + B 2 )( A â B).

5. 36. Seien v1 , . . . , vk â V \ {0} mit vi , v j = 0 fĂźr alle i , j. Zeige, dass dann v1 , . . . , vk linear unabhĂ¤ngig
sind.
5. 37. Zeige, dass fĂźr alle x, y â Rn gilt:
(a) kx + yk 2 + kx â yk 2 = 2(kxk 2 + kyk 2 ).
(b) Ist kxk = kyk, so sind die Vektoren x + y und x â y orthogonal.

5. 38. Benutze die CauchyâSchwarz Ungleichung um folgendes zu Zeigen: Ist u = (u1 , . . . ,un ) ein Vektor in
â
Rn , dann gilt: |u| 6 nÂˇ kuk, wobei |u| := |u1 |+. . .+|un | und |ui | der Betrag von ui ist. Hinweis: Betrachte den
â
Vektor v = (v1 , . . . , vn ) mit vi = 1 falls ui > 0, und vi = â1 sonst. Beachte, dass |u| = hu, vi und kvk = n.
5. 39. Seien x 1 , . . . , x n reelle Zahlen und Ď : [n] â [n] sei eine Permutation von {1, . . . , n}. Zeige, dass dann
Pn
Pn 2
i=1 x i Âˇ x Ď (i) 6
i=1 x i . Hinweis: CauchyâSchwarz Ungleichung.
5. 40. Sei V von den Vektoren v1 = (1, 1, â1), v2 = (â1, 2, 2) und v3 = (1, 4, 0) erzeugter Unterraum von R3 .
Wende den GauĂâSchmidt-Orthogonalisierungsverfahren, um die Orthonormalbasis von V zu bestimmen.

5. 41. Zeige, dass Eigenvektoren zu verschiedenen Eigenverten linear unabhĂ¤ngig sein mĂźssen. Hinweis: Als
erstes zeige folgendes: Ist x = Îąy, so kĂśnnen x, y nicht Eigenvektoren zu verschiedenen Eigenwerten sein.
5. 42. Beweise oder wiederlege: Wenn alle EintrĂ¤ge einer quadratischen Matrix A nicht negativ sind, dann sind
auch alle Eigenwerte von A nicht negativ.
5. 43. Sei A eine reelwertige n Ă n Matrix und sei B = A Âˇ Aâ¤ . Zeige, dass alle Eigenwerte von B nicht negativ
sind.
5. 44. Eine n Ă n Matrix heiĂt symmetrisch, falls Aâ¤ = A gilt. Zeige, dass Eigenvektoren zu verschiedenen
Eigenwerten einer symmetrischen Matrix orthogonal sind.
5. 45. Sei C â Fn ein lineares Code der Dimension k = dim(C). Sei G = [Ek | A] seine Generatormatrix. Zeige,


dass dann die Kontrollmatrix die Form H = âAâ¤ |Enâk hat.
5. 46. Die Vandermonde-Matrix (engl. Vandermonde matrix) ist eine n Ă n Matrix X n , deren i-te Zeile die Form
(1, x i , x 2i , . . . , x inâ1 ) mit x i â F hat:
ďŁŤ

ďŁŹ
ďŁŹ
Xn = ďŁŹ
ďŁ­

1
1
..
.

x1
x2
..
.

x 21
x 22
..
.

...
...

x 1nâ1
x 2nâ1
..
.

1

xn

x 2n

...

x nâ1
n

ďŁś
ďŁˇ
ďŁˇ
ďŁˇ
ďŁ¸

Zeige, dass
det X n =

Y

16i< j6n

(x j â x i ).

Hinweis: Induktion Ăźber n. Multipliziere jede Spalte mit x 1 und substragiere sie von der (nĂ¤chsten) rechten
Spalte, beginnend mit der rechten Spalte. Das sollte det(X n ) = (x n â x 1 ) Âˇ Âˇ Âˇ (x 2 â x 1 ) det(X nâ1 ) liefern.
5. 47. Sei V ein Vektorraum der Dimension n Ăźber einem KĂśrper F, und sei W â V . Man sagt, dass die
Vektoren aus W in allgemener Lage (in general position) sind, falls jede(!) n = dim(V ) Vektoren aus W linear
unabhĂ¤ngig sind. Sei V = Fn mit |F| > n und sei W die Menge aller Vektoren von der Form

m(a) := 1, a, a 2, . . . , a nâ1 â Fn

mit

a â F.

Zeige, dass die Vektoren aus W in allgemeiner Lage sind.

ÂŠ 2003 by S. Jukna

320

KAPITEL 5. LINEARE ALGEBRA

5. 48. Eine n Ă n Matrix A = (ai, j ) heiĂt streng diagonal dominant, wenn
|ai,i | > |ai,1 | + . . . + |ai,iâ1 | + |ai,i+1 | + . . . + |ai, n |
fĂźr alle Zeilem i = 1, . . . , n gilt. Zeige, dass soche Matrizen regulĂ¤r sind, d.h. den vollen Rang haben.

ÂŠ 2003 by S. Jukna

