Skript zur Vorlesung
Mathematik I/II fuĚr Inf, WInf
Wintersemester 2010/11, Sommersemester 2011
Robert Haller-Dintelmann
7. Juli 2011

Inhaltsverzeichnis
I. Mathematik I
1. Grundbegriffe
1.1. Aussagen . . . . . . . . . . . . . . .
1.1.1. Aussagen . . . . . . . . . . .
1.1.2. Aussageformen . . . . . . . .
1.1.3. All- und Existenzquantor . . .
1.1.4. VerknuĚpfung von Aussagen .
1.2. Mengen . . . . . . . . . . . . . . . .
1.3. Relationen . . . . . . . . . . . . . . .
1.3.1. Ordnungsrelationen . . . . . .
1.3.2. AĚquivalenzrelationen . . . . .
1.4. Abbildungen . . . . . . . . . . . . . .
1.5. Beweisprinzipien . . . . . . . . . . .
1.5.1. Der direkte Beweis . . . . . .
1.5.2. Beweis durch Kontraposition .
1.5.3. Beweis durch Widerspruch . .
1.5.4. VollstaĚndige Induktion uĚber N

1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
2.1. Rechnen in Z, Primzahlen und Teiler . . . . . . . . . . . . . . .
2.1.1. Modulare Arithmetik . . . . . . . . . . . . . . . . . . . .
2.1.2. Der Euklidische Algorithmus . . . . . . . . . . . . . . . .
2.1.3. Der kleine Satz von Fermat . . . . . . . . . . . . . . . .
2.2. Die Mathematik hinter Public-Key-Verfahren der Kryptographie
2.3. Gruppen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1. Untergruppen . . . . . . . . . . . . . . . . . . . . . . . .
2.3.2. Gruppenhomomorphismen . . . . . . . . . . . . . . . . .
2.4. Ringe und KoĚrper . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1. Ringe . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.2. KoĚrper . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5. Der KoĚrper der komplexen Zahlen . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

3
3
3
3
4
4
5
8
9
11
13
15
15
16
16
17

.
.
.
.
.
.
.
.
.
.
.
.

19
19
20
21
24
25
27
30
33
35
35
37
40

i

Inhaltsverzeichnis
3. Lineare Algebra
3.1. VektorraĚume . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1. Das Axiomensystem und Beispiele . . . . . . . .
3.1.2. Exkurs: Axiomensysteme . . . . . . . . . . . . .
3.1.3. Die Summenschreibweise . . . . . . . . . . . . .
3.2. UntervektorraĚume, Basis und Dimension . . . . . . . .
3.2.1. UntervektorraĚume . . . . . . . . . . . . . . . . .
3.2.2. Lineare UnabhaĚngigkeit und Basen . . . . . . .
3.3. Der Faktorraum . . . . . . . . . . . . . . . . . . . . . .
3.4. Normierte RaĚume . . . . . . . . . . . . . . . . . . . . .
3.5. Geometrie im Rn . . . . . . . . . . . . . . . . . . . . .
3.6. Lineare Abbildungen . . . . . . . . . . . . . . . . . . .
3.7. Matrizen und lineare Abbildungen . . . . . . . . . . . .
3.7.1. Matrixrechnung . . . . . . . . . . . . . . . . . .
3.7.2. Die Abbildungsmatrix einer linearen Abbildung
3.8. Lineare Gleichungssysteme . . . . . . . . . . . . . . . .
3.8.1. LoĚsbarkeitstheorie . . . . . . . . . . . . . . . . .
3.8.2. Der GauĂ-Algorithmus . . . . . . . . . . . . . .
3.9. Basiswechsel . . . . . . . . . . . . . . . . . . . . . . . .
3.10. Determinanten . . . . . . . . . . . . . . . . . . . . . .
3.11. Eigenwerttheorie . . . . . . . . . . . . . . . . . . . . .
4. Analysis â Teil I: Konvergenz und Stetigkeit
4.1. Die reellen Zahlen . . . . . . . . . . . . . . . . . . . .
4.2. Wurzeln, FakultaĚten und Binomialkoeffizienten . . . .
4.3. Konvergenz von Folgen . . . . . . . . . . . . . . . . .
4.3.1. Der Konvergenzbegriff und wichtige Beispiele
4.3.2. Konvergenzkriterien . . . . . . . . . . . . . .
4.3.3. Teilfolgen und HaĚufungswerte . . . . . . . . .
4.4. Asymptotik . . . . . . . . . . . . . . . . . . . . . . .
4.5. Reihen . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5.1. Absolute Konvergenz . . . . . . . . . . . . . .
4.5.2. Das Cauchy-Produkt . . . . . . . . . . . . . .
4.6. Konvergenz in normierten RaĚumen . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

47
47
47
52
54
55
55
58
63
66
72
78
87
87
91
97
98
100
104
110
116

.
.
.
.
.
.
.
.
.
.
.

125
125
127
130
131
138
140
141
145
148
152
154

II. Mathematik II
4.7. Stetigkeit reeller Funktionen . . . . . . . . . .
4.7.1. Der Grenzwertbegriff fuĚr Funktionen .
4.7.2. Stetigkeit . . . . . . . . . . . . . . . .
4.7.3. Eigenschaften stetiger Funktionen . . .
4.8. Stetigkeit von Funktionen mehrerer Variablen
4.9. Potenzreihen . . . . . . . . . . . . . . . . . . .

ii

163
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

165
165
168
172
174
180

Inhaltsverzeichnis
4.10. Wichtige Funktionen . . . . . . . . . . . . . .
4.10.1. Exponentialfunktion und Logarithmus
4.10.2. Trigonometrische Funktionen . . . . .
4.10.3. Die Polardarstellung komplexer Zahlen
4.10.4. Hyperbolische Funktionen . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

187
187
189
194
195

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

197
197
197
200
205
208
218
220
227
235
238
238
245
248
255
258

.
.
.
.
.
.
.
.
.
.

267
267
270
270
273
274
277
278
280
284
289

7. Allgemeine Algebra
7.1. Allgemeine Algebren . . . . . . . . . . . . . . . . . . . . . . . . .
7.2. Unteralgebren und Erzeugnis . . . . . . . . . . . . . . . . . . . . .
7.3. Homomorphismen und Isomorphsimen . . . . . . . . . . . . . . .

291
291
294
295

Tabelle der griechischen Buchstaben

299

Index

300

5. Analysis â Teil II: Differential- und Integralrechnung
5.1. Differenzierbarkeit von Funktionen in einer Variablen
5.1.1. Der Ableitungsbegriff . . . . . . . . . . . . . .
5.1.2. Ableitungsregeln . . . . . . . . . . . . . . . .
5.1.3. HoĚhere Ableitungen . . . . . . . . . . . . . . .
5.2. Eigenschaften differenzierbarer Funktionen . . . . . .
5.3. Extremwerte . . . . . . . . . . . . . . . . . . . . . . .
5.4. Partielle Ableitungen . . . . . . . . . . . . . . . . . .
5.5. Totale Differenzierbarkeit . . . . . . . . . . . . . . . .
5.6. Extremwertprobleme in mehreren Variablen . . . . .
5.7. Integration in R . . . . . . . . . . . . . . . . . . . . .
5.7.1. Definition des bestimmten Integrals . . . . . .
5.7.2. Stammfunktionen und der Hauptsatz . . . . .
5.8. Integrationstechniken . . . . . . . . . . . . . . . . . .
5.9. Uneigentliche Integrale . . . . . . . . . . . . . . . . .
5.10. Fourierreihen . . . . . . . . . . . . . . . . . . . . . .
6. GewoĚhnliche Differentialgleichungen
6.1. Problemstellung und Motivation . . . . . . . . . . . .
6.2. Elementare LoĚsungsmethoden . . . . . . . . . . . . .
6.2.1. Getrennte VeraĚnderliche . . . . . . . . . . . .
6.2.2. Homogene Differentialgleichungen . . . . . . .
6.2.3. Lineare Differentialgleichungen erster Ordnung
6.3. Systeme von Differentialgleichungen . . . . . . . . . .
6.3.1. Lineare Systeme . . . . . . . . . . . . . . . . .
6.3.2. Lineare Systeme mit konstanten Koeffizienten
6.4. Differentialgleichungen hoĚherer Ordnung . . . . . . .
6.5. Existenz- und Eindeutigkeitsresultate . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

iii

Teil I.
Mathematik I

1

1. Grundbegriffe
1.1. Aussagen
1.1.1. Aussagen
Eine Aussage ist ein in verstaĚndlicher Sprache formulierter Satz, der entweder
wahr (w) oder falsch (f) ist.
Beispiel 1.1.1. Hier sind fuĚnf Aussagen:
A1 : 3 ist eine ungerade Zahl. (w)
A2 : Die Erde ist eine Scheibe. (f)
A3 : Es regnet gerade in Madrid. (?)
A4 : Jede natuĚrliche Zahl ist gerade. (f)
A5 : 3 ist eine Primzahl. (w)
Keine Aussage ist: Guten Morgen.â
â

1.1.2. Aussageformen
Eine Aussageform ist ein Satz mit einer oder mehreren Variablen, der bei Belegung der Variablen durch eine konkreten Wert eine Aussage wird.
Beispiel 1.1.2. Hier sind vier Aussageformen:
E1 (x) : x + 10 = 5.
E2 (x) : x2 âĽ 0.
E3 (n) : n ist gerade.
E4 (x, y) : 3x â 4y 6= 10.

3

1. Grundbegriffe

1.1.3. All- und Existenzquantor
Sei E(x) eine Aussageform und M eine Menge von moĚglichen x. Dann bedeutet
âx â M : E(x)

FuĚr alle x aus M ist E(x) wahrâ.
â

Man nennt â den Allquantor.
Weiter bedeutet
âx â M : E(x)

Es existiert ein x aus M, fuĚr das E(x) wahr istâ.
â

Man nennt â den Existenzquantor.
Man beachte, dass durch das Vorstellen eines Quantors auf diese Weise aus einer
Aussageform eine Aussage wird. Hat die Aussageform mehrere Variablen braucht
es natuĚrlich auch mehrere Quantoren.
Beispiel 1.1.3. Aus obigen Aussageformen koĚnnen wir z.B. die folgenden Aussagen machen:
(a) âx â R : E2 (x), d.h. âx â R : x2 âĽ 0. (w)
(b) ân â N : E3 (n) entspricht genau A4 . (f)
(c) ân â N : E3 (n), d.h. es gibt eine gerade natuĚrliche Zahl. (w)
Warnung 1.1.4. Es existiert ein xâ bedeutet nicht Es existiert genau ein xâ.
â
â
Ist die Aussage âx â M : E(x) wahr, so kann es durchaus mehrere x geben, fuĚr
die E(x) wahr wird!

1.1.4. VerknuĚpfung von Aussagen
Seien A und B zwei Aussagen. Dann koĚnnen wir daraus verschiedene neue Aussagen machen.
Konjunktion ( undâ): Zeichen: â§
â
Aâ§B :

Sowohl A als auch B sind wahr.

Disjunktion ( oderâ): Zeichen: â¨
â
Aâ¨B :

A ist wahr oder B ist wahr.

Negation ( nichtâ): Zeichen: ÂŹ
â
ÂŹA :

4

A gilt nicht.

1.2. Mengen
Implikation ( wenn . . . , dannâ): Zeichen: =â
â
A =â B : Wenn A gilt dann auch B.
Aus A folgt B.
A impliziert B.
AĚquivalenz ( genau dann, wennâ): Zeichen: ââ
â
A ââ B : A gilt genau dann, wenn B gilt.
A und B sind aĚquivalent.
Warnung 1.1.5. (a) â¨ ist nicht entweder . . . oder â, d.h. Aâ¨B ist auch wahr,
â
wenn sowohl A als auch B wahr sind.
(b) GewoĚhnungsbeduĚrftig ist zunaĚchst folgendes: Wenn A falsch ist, dann ist
A =â B in jedem Fall wahr. Anders ausgedruĚckt: Aus einer falschen Aussage kann man alles folgern. Man sieht das auch an der Wahrheitstafel der
Implikation
A B
w w
w f
f w
f f

A =â B
w
f
w
w

Bemerkung 1.1.6. Als kleine UĚbung machen wir uns noch klar, dass die Aussage
C := (A =â B) ââ (ÂŹB =â ÂŹA) immer wahr ist:
A
w
w
f
f

B
w
f
w
f

A =â B
w
f
w
w

ÂŹA
f
f
w
w

ÂŹB
f
w
f
w

ÂŹB =â ÂŹA
w
f
w
w

C
w
w
w
w

Das bedeutet, dass der Wahrheitsgehalt der Aussagen A =â B und ÂŹB =â
ÂŹA immer der selbe ist. Zum Nachweis von A =â B ist wahrâ kann man
â
also gleichbedeutend auch ÂŹB =â ÂŹA ist wahrâ beweisen. Das ist dann ein
â
sogenannter Beweis durch Kontraposition und ist manchmal einfacher als ein
direkter Beweis, vgl. Abschnitt 1.5.

1.2. Mengen
Beispiele von Mengen sind: Die Menge aller Studierenden in einem HoĚrsaal, ein
Dreieck (als Punktmenge der Ebene), die Menge aller Dreiecke in der Ebene oder

5

1. Grundbegriffe
die Mengen N,1 Z, Q, R, also die Mengen der natuĚrlichen, ganzen, rationalen,
bzw. reellen Zahlen. Den Begriff der Menge definieren wir hier nicht, sondern
legen ihn naiv zu Grunde; wir stellen uns damit auf den Standpunkt der naiven
(und nicht der axiomatischen) Mengenlehre.
Wenn wir Mengen bilden, ist unser Ausgangspunkt immer eine gegebene, unter
UmstaĚnden sehr groĂen Grundmenge G, aus der Elemente ausgesondert und zu
neuen Mengen zusammengefasst werden. Auf diese Weise vermeidet man Bildungen wie die Menge aller Mengenâ, die zu WiderspruĚchen fuĚhren.
â
Mengen kann man, solange sie klein genug sind, einfach durch das AufzaĚhlen ihrer
Elemente angeben, z.B.
M1 = {0, 1, 2, 3, 4, 5}.

Es ist aber haĚufig angenehmer, sie durch die Angabe einer definierenden Eigenschaft, die genau fuĚr die Elemente der Menge, und nur fuĚr diese, wahr ist, zu
beschreiben. FuĚr unsere Menge M1 koĚnnte das so aussehen:
M1 = {x â N : x < 6} oder M1 = {x â N : x â 6 ist keine natuĚrliche Zahl}.
Allgemein schreibt man
M = {x â G : E(x)},

wobei G die Grundmenge ist, aus der die Elemente der Menge M ausgesondert
werden sollen und E(x) eine Aussageform.
Definition 1.2.1. Seien M und N Mengen. Wir schreiben a â M, falls a ein
Element von M ist und, falls dem nicht so ist, a 6â M.
Ist jedes Element von N auch in M enthalten, so schreiben wir N â M und
sagen N ist eine Teilmenge von M. Weiter nennt man in diesem Fall M eine
Obermenge von N und schreibt M â N. Solche Teilmengenbeziehungen werden
oft auch als Inklusion bezeichnet.
Schlussendlich schreiben wir â fuĚr die leere Menge, d.h. die Menge, die kein
Element enthaĚlt.
Bemerkung 1.2.2. FuĚr zwei Mengen M und N gilt M = N genau dann, wenn
M â N und N â M gilt.
Definition 1.2.3. Seien M und N Mengen in einer Grundmenge G. Dann ist
(a)
(b)
(c)
(d)
(e)
1

6

M âŞ N := {x â G : x â M â¨ x â N}
M âŠ N := {x â G : x â M â§ x â N}
M c := {x â G : x 6â M}
M \ N := {x â M : x 6â N}
M Ă N := {(x, y) : x â M, y â N}

Vereinigung von M und N ,
Schnitt von M und N ,
Komplement von M in G,
Mengendifferenz von M und N,
kartesisches Produkt von M und N .

In dieser Vorlesung ist N := {0, 1, 2, 3, . . . }. FuĚr die natuĚrlichen Zahlen ohne Null schreiben
wir Nâ := {1, 2, 3, 4, . . . }.

1.2. Mengen
Bemerkung 1.2.4. Damit ist ebenfalls fuĚr eine endliche Anzahl von Mengen
A1 , A2 , . . . , An das n-fache kartesische Produkt

A1 Ă A2 Ă Âˇ Âˇ Âˇ Ă An = (a1 , a2 , . . . , an ) : aj â Aj fuĚr j = 1, 2, . . . , n .
definiert.

Satz 1.2.5. Seien A, B und C Mengen. Dann gilt
(a) A âŞ B = B âŞ A und A âŠ B = B âŠ A.

(Kommutativgesetze)

(b) (A âŞ B) âŞ C = A âŞ (B âŞ C) und (A âŠ B) âŠ C = A âŠ (B âŠ C).
(Assoziativgesetze)

(c) A âŞ (B âŠ C) = (A âŞ B) âŠ (A âŞ C) und A âŠ (B âŞ C) = (A âŠ B) âŞ (A âŠ C)
(Distributivgesetze),

(d) (A âŞ B)c = Ac âŠ B c und (A âŠ B)c = Ac âŞ B c

(Regeln von De Morgan).

Beweis. Wir behandeln hier das erste Distributivgesetz und die erste Regel von
De Morgan, die weiteren verbleiben als UĚbungsaufgabe.
FuĚr das Distributivgesetz zeigen wir zuerst (vgl. Bemerkung 1.2.2)
A âŞ (B âŠ C) â (A âŞ B) âŠ (A âŞ C),
und zwar folgendermaĂen: Sei x â AâŞ(BâŠC). Dann ist also x â A oder x â BâŠC.
Betrachten wir zunaĚchst den Fall x â A. Dann gilt natuĚrlich auch x â A âŞ B und
x â A âŞ C, denn diese Mengen sind ja groĚĂer als A. Also ist x â (A âŞ B) âŠ (A âŞ C)
und wir sind fertig. Betrachten wir also den Fall x â B âŠ C. Dann ist x â B und
x â C, also gilt wieder x â A âŞ B und x â A âŞ C, dieses Mal, weil x sowohl in
B als auch in C liegt. Daraus folgt wieder x â (A âŞ B) âŠ (A âŞ C) und wir haben
A âŞ (B âŠ C) â (A âŞ B) âŠ (A âŞ C) gezeigt.
Um die im ersten Distributivgesetz behauptete Gleichheit zu zeigen, muĚssen wir
nun noch die umgekehrte Inklusion
(A âŞ B) âŠ (A âŞ C) â A âŞ (B âŠ C)
zeigen. Dazu sei x â (A âŞ B) âŠ (A âŞ C). Dann ist x sowohl in A âŞ B, als auch in
A âŞ C. Wir betrachten die beiden FaĚlle x â A und x 6â A. (Man beachte, dass wir
dann alle denkbaren FaĚlle x â G beruĚcksichtigt haben!) Ist x â A, so haben wir
sofort auch x â A âŞ (B âŠ C), was unser Ziel war. Es bleibt also der Fall x 6â A.
Da dann x in A âŞ B ist, ohne in A zu sein, muss x zwangslaĚufig in B sein, denn
wie sollte es sonst da hineinkommen? Genauso folgt x â C aus x â A âŞ C. Also
ist x in B âŠ C und damit auch x â A âŞ (B âŠ C) und wir haben auch die zweite
Inklusion und damit die Gleichheit
(A âŞ B) âŠ (A âŞ C) = A âŞ (B âŠ C)

7

1. Grundbegriffe
gezeigt.
FuĚr die erste De Morganâsche Regel zeigen wir wieder zuerst
(A âŞ B)c â Ac âŠ B c .
Sei dazu x â (A âŞ B)c . Dann ist x 6â (A âŞ B), d.h. x ist nicht in der Vereinigung
von A und B. Damit kann x weder in A noch in B sein, denn sonst wuĚrde es ja in
dieser Vereinigung liegen. Es ist also x 6â A und x 6â B, d.h. x â Ac und x â B c ,
was schlieĂlich x â Ac âŠ B c nach sich zieht.
Die zweite Inklusion
(A âŞ B)c â Ac âŠ B c

geht folgendermaĂen: Es sei x â Ac âŠ B c . Dann ist x â Ac und x â B c . Also ist
x nicht in A und nicht in B, es ist also auch nicht in der Vereinigung von A und
B, was gerade x â (A âŞ B)c bedeutet.

Definition 1.2.6. Eine Menge M heiĂt endlich, falls sie endlich viele Elemente
besitzt. In diesem Fall schreiben wir |M| fuĚr die Anzahl der Elemente von M.
Bemerkung 1.2.7. Seien A und B endliche Mengen, dann gilt |AĂB| = |A|Âˇ|B|.
Warum? Es gilt A Ă B = {(a, b) : a â A, b â B}. FuĚr die Wahl der a â A in
der ersten Komponente hat man |A| MoĚglichkeiten. Ist dann a â A gewaĚhlt, so
gibt es fuĚr jede dieser Wahlen wieder |B| MoĚglichkeiten ein b â B zuzulosen.
Zusammen ergibt das |A| Âˇ |B| MoĚglichkeiten, d.h. es gilt |A Ă B| = |A| Âˇ |B|.
UĚbungsaufgabe 1.2.8. Es seien A und B endliche Mengen. Zeigen Sie:
|A âŞ B| = |A| + |B| â |A âŠ B|.
Definition 1.2.9. Ist M eine Menge, so heiĂt
P(M) := {N : N Teilmenge von M}
Potenzmenge von M.

Beispiel 1.2.10. Es ist P({0, 1}) = â, {0}, {1}, {0, 1} .

1.3. Relationen
Definition 1.3.1. Sei X eine Menge. Eine Teilmenge R â X Ă X heiĂt (zweistellige) Relation auf X. Man schreibt xRy, falls das Tupel (x, y) â R liegt und
sagt x steht in Relation zu yâ.
â
Beispiel 1.3.2. (a) â¤ in N: Dann ist R = {(n, m) â N Ă N : n â¤ m} und x
steht genau dann mit y in Relation, wenn x â¤ y gilt.

8

1.3. Relationen
(b) Nehmen Sie als X die Menge aller Internetseiten, so koĚnnen Sie durch die
Setzung R := {(x, y) : x verlinkt nach y} eine Relation auf X definieren,
die die Verlinkungsstruktur codiert.
Definition 1.3.3. Sei X eine Menge. Eine Relation R auf X heiĂt
(a) reflexiv, falls xRx fuĚr jedes x â X gilt.
(b) symmetrisch, falls fuĚr alle x, y â X mit xRy auch yRx gilt.
(c) antisymmetrisch, falls fuĚr alle x, y â X, fuĚr die xRy und yRx gilt, x = y
folgt.
(d) transitiv, falls fuĚr alle x, y, z â X mit xRy und yRz auch xRz gilt.
(e) AĚquivalenzrelation, falls R reflexiv, symmetrisch und transitiv ist. In diesem Fall schreibt man meist âźâ statt Râ.
â
â
(f ) Ordnungsrelation, falls R reflexiv, antisymmetrisch und transitiv ist. Man
schreibt dann meist â¤â statt Râ.
â
â
Ist â¤ eine Ordnungsrelation auf X, so heiĂt X partiell geordnet.

1.3.1. Ordnungsrelationen
Beispiel 1.3.4. Ordnungsrelationen sind z.B.
(a) â¤â in N, Z, Q, R.
â
(b) die lexikographische Ordnung.
(c) Ist M eine Menge, so ist â eine Ordnungsrelation auf P(M).
Bemerkung 1.3.5. (a) Hat man eine Ordnungsrelation â¤ auf einer Menge X,
so kann es immer noch sein, dass es Elemente x, y â X gibt, die unvergleichbar sind, fuĚr die also weder x â¤ y noch y â¤ x gilt, vgl. z.B. Beispiel 1.3.4 (c).
Gilt fuĚr eine Ordnungsrelation zusaĚtzlich
FuĚr alle x, y â X gilt x â¤ y oder y â¤ x,
so heiĂt â¤ eine Totalordnung und die Menge X dann total geordnet.
(b) Ist (X, â¤) eine partiell (total) geordnete Menge, so ist auch jede Teilmenge
Y von X durch â¤ partiell (total) geordnet.
(c) Sei (X, â¤) eine partiell geordnete Menge. Wir schreiben
x âĽ y, falls y â¤ x,
x < y, falls x â¤ y und x 6= y,
x > y, falls y < x.

9

1. Grundbegriffe
Definition 1.3.6. Sei (X, â¤) eine partiell geordnete Menge und Y â X
(a) g â X heiĂt groĚĂtes Element von X, falls x â¤ g fuĚr alle x â X.

k â X heiĂt kleinstes Element von X, falls k â¤ x fuĚr alle x â X.

(b) s â X heiĂt obere Schranke von Y , falls y â¤ s fuĚr alle y â Y .

t â X heiĂt untere Schranke von Y , falls t â¤ y fuĚr alle y â Y .

Satz 1.3.7. Sei (X, â¤) eine partiell geordnete Menge. Dann hat X hoĚchstens ein
grĚoĂtes und hoĚchstens ein kleinstes Element.
Beweis. Seien g1 und g2 groĚĂte Elemente von X. Da g1 groĚĂtes Element ist, gilt
g2 â¤ g1 . Da aber auch g2 ein groĚĂtes Element ist, haben wir auch g1 â¤ g2 . Also
ist wegen der Antisymmetrie von Ordnungsrelationen g1 = g2 .
FuĚr die kleinsten Elemente fuĚhrt ein analoges Argument zum Ziel.
Definition 1.3.8. Es sei (X, â¤) eine partiell geordnete Menge und Y â X.
(a) Hat S := {s â X : s obere Schranke von Y } ein kleinstes Element s0 , so
heiĂt sup Y := s0 Supremum von Y .
Hat T := {t â X : t untere Schranke von Y } ein groĚĂtes Element t0 , so
heiĂt inf Y := t0 Infimum von Y .
(b) Gilt s0 = sup(Y ) â Y , so heiĂt s0 Maximum von Y ; Bezeichnung max Y .
Gilt t0 = inf(Y ) â Y , so heiĂt t0 Minimum von Y ; Bezeichnung min Y .

Merkregel:

Das Supremum ist die kleinste obere Schranke.
Das Infimum ist die groĚĂte untere Schranke.

Beispiel 1.3.9. (a) Q+ := {x â Q : x > 0} hat in Q, versehen mit der uĚblichen
Ordnung, kein groĚĂtes und kein kleinstes Element. Wohl hat diese Menge
aber untere Schranken, z.B. â7, â43 oder 0. Die groĚĂte untere Schranke
und damit inf Q+ ist 0. Dieses ist aber kein Minimum, denn 0 6â Q+ .
(b) {x â Q : x2 < 2} hat in Q obere Schranken, z.B. 2 oder 37, aber â
kein
Supremum, denn die Menge der oberen Schrankenâist {q â Q : q âĽ 2}
und diese Menge hat kein kleinstes Element, denn 2 6â Q.
(c) In N mit der uĚblichen Ordnung hat jede Teilmenge ein Minimum und jede
endliche Teilmenge ein Maximum.
(d) In (P({0, 1, 2}), â) hat die Teilmenge M := {â, {0}} obere Schranken, z.B.
{0}, {0, 1} und {0, 2}. Dabei ist {0} die kleinste obere Schranke, also das
Supremum, das in diesem Fall, wegen {0} â M, auch das Maximum ist.

Hat N := â, {0}, {1} ein Supremum, Infimum, Maximum, bzw. Minimum?

10

1.3. Relationen

1.3.2. AĚquivalenzrelationen
(a) =â in N, Z, Q, R
â
(b) gleicher Vorname, Pulloverfarbe, ArmlaĚnge in Menschengruppen

Beispiel 1.3.10.

(c) Verwandtschaftsbeziehungen
Beispiel 1.3.11. Sei n â Nâ fest gewaĚhlt. Wir definieren die Relation âźn auf Z
durch
a âźn b ââ a â b ist Vielfaches von n ââ âk â Z : a â b = k Âˇ n,

a, b â Z.

Wir werden gleich zeigen, dass âźn eine AĚquivalenzrelation auf Z ist. Vorher sei
noch vermerkt, dass man statt a âźn b oft a âĄ b (mod n) schreibt, gelesen: a ist
â
kongruent b modulo nâ.
Beispielsweise ist
19 âĄ 9 (mod 5), denn 19 â 9 = 10 ist Vielfaches von 5,
23 âĄ 1 (mod 2),
17 âĄ 3 (mod 7).
Nun zum Nachweis, dass âźn AĚquivalenzrelation ist:

1. ReflexivitaĚt: Sei a â Z. Dann ist a â a = 0 = 0 Âˇ n ein Vielfaches von n, also
gilt a âźn a.
2. Symmetrie: Seien a, b â Z mit a âźn b. Dann gibt es ein k â Z mit a â b = k Âˇ n.
Also gilt b â a = (âk) Âˇ n. Nun ist auch âk â Z. Also gibt es ein â â Z mit
b â a = â Âˇ n, d.h. b âźn a.
3. TransitivitaĚt: Seien a, b, c â Z mit a âźn b und b âźn c. Das bedeutet, dass es
zwei Zahlen k, â â Z gibt mit a â b = k Âˇ n und b â c = â Âˇ n. Damit ist
a â c = a â b + b â c = k Âˇ n + â Âˇ n = (k + â) Âˇ n.
Da auch k + â â Z ist, folgt damit a âźn c.
Satz 1.3.12. Sei âź eine AĚquivalenzrelation auf einer Menge X 6= â. Wir definieren fuĚr jedes a â X die AĚquivalenzklasse aĚ als
aĚ := {x â X : x âź a}.
Dann gilt
(a) aĚ 6= â fuĚr jedes a â X.
(b) FuĚr alle a, b â X mit aĚ 6= bĚ gilt aĚ âŠ bĚ = â.
S
(c) aâX aĚ = X, d.h. die Vereinigung aller AĚquivalenzklassen ist gleich X.
11

1. Grundbegriffe
Beweis.

(a) Sei a â X. Wegen der ReflexivitaĚt von âź, gilt a âź a, also ist a â aĚ.

(b) Wir beweisen die Aussage per Kontraposition (vgl. Bemerkung 1.1.6), d.h.
wir zeigen: aĚ âŠ bĚ 6= â =â aĚ = bĚ.

Wenn aĚ âŠ bĚ 6= â ist, so gibt es ein Element x aus dieser Menge. FuĚr dieses x
gilt dann sowohl x âź a, als auch x âź b. Wegen der Symmetrie von âź, haben
wir also a âź x und x âź b und damit folgt aus der TransitivitaĚt a âź b.

Sei nun y â aĚ. Dann ist y âź a und da wir auch a âź b haben, folgt wieder
mit der TransitivitaĚt von âź die Beziehung y âź b. Das bedeutet y â bĚ und
wir haben damit aĚ â bĚ gezeigt.
Startet man mit einem z â bĚ, so zeigt man genauso z â aĚ und bekommt
bĚ â aĚ.

Zusammen ist also aĚ = bĚ und wir sind fertig.

S
(c) ZunaĚchst gilt fuĚr alle a â X natuĚrlich aĚ â X, also ist auch aâX aĚ â X.
Wir muĚssen nur noch die umgekehrte Inklusion zeigen.
S
Sei b â X. Dann ist b â bĚ nach (a), also ist auch b â aâX aĚ und wir haben
die umgekehrte Inklusion.
Bemerkung 1.3.13. Satz 1.3.12 bedeutet, dass die AĚquivalenzrelation âź eine
Zerlegung von X in die AĚquivalenzklassen erzeugt, die die Elemente von X nach
der durch âź beschriebenen Eigenschaft sortiert.
Die Menge
X/âź := {aĚ : a â X}
aller AĚquivalenzklassen heiĂt Faktormenge von X bezuĚglich âź.

Beispiel 1.3.14. Als Beispiel betrachten wir wieder âźn auf Z aus Beispiel 1.3.11.
Dann ist fuĚr jedes a â Z
aĚ = {b â Z : a âźn b} = {b â Z : âk â Z mit a â b = n Âˇ k}
= {b â Z : b = a â nk fuĚr ein k â Z} = {b â Z : b = a + nk fuĚr ein k â Z}
= {a + nk : k â Z} =: a + n Âˇ Z.
FuĚr n = 3 gilt also beispielsweise
0Ě = 0 + 3 Âˇ Z = {. . . , â6, â3, 0, 3, 6, . . . }

1Ě = 1 + 3 Âˇ Z = {. . . , â5, â2, 1, 4, 7, . . . }

2Ě = 2 + 3 Âˇ Z = {. . . , â4, â1, 2, 5, 8, . . . }

3Ě = 3 + 3 Âˇ Z = {. . . , â3, 0, 3, 6, 9, . . . } = 0Ě.
Also ist
Z/âź3 = {0Ě, 1Ě, 2Ě}

12

(durch 3 teilbare Zahlen)
(Rest 1 beim teilen durch 3)
(Rest 2 beim teilen durch 3)

1.4. Abbildungen
eine Zerlegung von Z, die die ganzen Zahlen nach ihrem Rest beim Teilen durch
â 1 und in aĚ sind
drei sortiert. Genauso enthaĚlt Z/âźn die n Elemente 0Ě, 1Ě, . . . , n]
jeweils alle die ganzen Zahlen enthalten, die beim Teilen durch n den Rest a
haben.
Man schreibt meist kurz Zn statt Z/âźn .
Wir werden uns diesem Thema im Abschnitt 2.1 noch genauer widmen.

1.4. Abbildungen
Definition 1.4.1. Seien A und B Mengen und jedem Element a â A sei genau ein Element f (a) â B zugeordnet. Diese Zuordnung heiĂt Abbildung oder
Funktion f . Man schreibt
(
AâB
f:
a 7â f (a).

und nennt A den Definitionsbereich, B den Zielbereich, sowie a 7â f (a) die
Funktionsvorschrift von f .
Weiter heiĂt die Menge f (A) := {f (a) : a â A} â B das Bild und die Menge
{(a, f (a)) : a â A} â A Ă B der Graph von f .
Ist schlieĂlich C â B, so bezeichnet man mit f â1 (C) := {a â A : f (a) â C} â A
das Urbild von C unter f .
Beispiel 1.4.2.

(a) Bekannt sind Funktionen wie
(
(
[0, â) â [0, â)
RâR
oder
g:
f:
â
2
x 7â x.
x 7â x

(
NĂNâN
(b) Auch + :
(a, b) 7â a + b

, d.h. die Addition in N, ist eine Abbildung.

(
AâA
(c) Auf jeder Menge A kann man die IdentitaĚt, d.h. id :
a 7â a

definieren.

(d) Ist (
X eine Menge mit einer auf X erklaĚrten AĚquivalenzrelation âź, so ist
X â X/âź
die sogenannte kanonische Abbildung.
Î˝:
a 7â aĚ
Definition 1.4.3. Seien A, B, C Mengen und f : A â B, sowie g : B â C
Funktionen. Dann heiĂt
(
AâC
gâŚf :
a 7â (g âŚ f )(a) := g(f (a))
Verkettung von f und g. Man liest g âŚ f als g nach fâ.
â

13

1. Grundbegriffe
Definition 1.4.4. Eine Funktion f : A â B heiĂt
(a) surjektiv, wenn f (A) = B.

(b) injektiv, wenn fuĚr alle x, y â A aus f (x) = f (y) schon x = y folgt.
(c) bijektiv, wenn f surjektiv und injektiv ist.
Satz 1.4.5. Eine Funktion f : A â B ist genau dann bijektiv, wenn fuĚr jedes
b â B genau ein a â A existiert mit f (a) = b. In diesem Fall existiert eine
Abbildung f â1 : B â A, so dass
f â1 (f (a)) = a

fuĚr alle a â A

und

f (f â1(b)) = b fuĚr alle b â B

gilt.
Beweis. 1. Schritt: Wir zeigen: f bijektiv =â fuĚr alle b â B existiert genau ein
a â A mit f (a) = b.

Da f surjektiv ist, gibt es zu jedem b â B mindestens ein a â A mit f (a) = b.
Nehmen wir an, es gaĚbe mehr als eins, d.h. es gaĚbe a1 , a2 â A mit f (a1 ) = f (a2 ) =
b, so folgt aus der InjektivitaĚt von f sofort a1 = a2 , es kann also nur genau ein
solches a â A geben.

2. Schritt: Wir zeigen: FuĚr alle b â B existiert genau ein a â A mit f (a) = b =â f
bijektiv.

Nach Voraussetzung sind alle b â B in f (A) enthalten, also ist f surjektiv. Seien
nun a1 , a2 â A mit f (a1 ) = f (a2 ) gegeben. Da jedes b â B nur genau ein Urbild
hat, muss dann a1 = a2 sein, d.h. f ist auch injektiv.
3. Schritt: Wir zeigen: f bijektiv =â es existiert f â1 : B â A mit f â1 (f (a)) = a
fuĚr alle a â A und f (f â1 (b)) = b fuĚr alle b â B.

FuĚr jedes b â B definieren wir f â1 (b) := a, wobei a â A das nach dem ersten
Schritt eindeutig bestimmte Element mit f (a) = b ist. Dann ist f â1 (f (a)) das
Element von A, das in f eingesetzt f (a) ergibt, also f â1 (f (a)) = a fuĚr alle a â A.
Sei nun b â B. Dann ist f â1 (b) das Element von A mit f (f â1 (b)) = b und wir
sind fertig.
Definition 1.4.6. Es seien A, B zwei Mengen und f : A â B bijektiv. Dann
heiĂt die Abbildung f â1 aus Satz 1.4.5 Umkehrfunktion von f .
Beispiel 1.4.7. Die Funktion f : R â R mit f (x) = x2 (vgl. Beispiel 1.4.2 (a))
ist nicht injektiv, denn es gilt f (1) = 12 = 1 = (â1)2 = f (â1), aber 1 6= â1. Sie
ist auch nicht surjektiv, denn â1 6â f (R).
Betrachtet man fË : R â [0, â) mit fË(x) = x2 , so ist diese nun surjektiv, denn
f (R) = {x2 : x â R} = [0, â), aber genau so wie oben nicht injektiv.
Ë
Ë
Geht man jedoch zu fË : [0, â) â [0, â) mit fË(x) = x2 uĚber, so ist diese injektiv
Ë
und surjektiv, d.h. bijektiv. Die nach Satz 1.4.5 existierende Umkehrfunktion fËâ1
ist genau die Funktion g aus Beispiel 1.4.2 (a), d.h. die Wurzelfunktion.

14

1.5. Beweisprinzipien
Definition 1.4.8. Sei f : A â B eine Funktion und M â A. Dann heiĂt
f |M

(
M âB
:
x 7â f (x)

die EinschraĚnkung von f auf M.
UĚbungsaufgabe 1.4.9. Beweisen Sie: Sind f : A â B und g : B â C bijektive
Funktionen, so ist auch g âŚ f : A â C bijektiv.

1.5. Beweisprinzipien
In einem Beweis ist die Aufgabe aus einer Aussage A, der Voraussetzung, eine
Aussage B, die Behauptung, zu folgern. Anders ausgedruĚckt: Man muss nachweisen, dass die Aussage A =â B wahr ist. Selbst, wenn der Satz, der zu beweisen
ist, eine AĚquivalenz, d.h. eine Aussage der Form A ââ B postuliert, wird der
Beweis fast immer in die Teilbeweise A =â B und B =â A aufgeteilt, vgl. den
Beweis von Satz 1.4.5.
Dieser Abschnitt stellt moĚgliche Beweismethoden zusammen und liefert jeweils
ein kurzes Beispiel. Einige davon haben wir in den vorheringen Kapiteln schon
geshen, einige sind neu.

1.5.1. Der direkte Beweis
Der direkte Beweis hat folgende Form:
Voraussetzung:
Behauptung:
Beweis:

Aussage A
Aussage B
Sei A erfuĚllt. Dann gilt . . . bla bla bla und deswegen
. . . . Also gilt auch B.

Bisherige Beispiele fuĚr direkte Beweise waren die Beweise von (a) und (c) aus
Satz 1.3.12 und die von Satz 1.2.5. Hier ist ein weiteres:
Beispiel 1.5.1. Voraussetzung: Seien n, m â N gerade Zahlen.
Behauptung: Dann ist auch n + m gerade.
Beweis: Seien n und m gerade Zahlen. Dann gibt es â, k â N mit n = 2â und
m = 2k. Mit diesen â, k gilt dann n + m = 2â + 2k = 2(â + k). Mit â und k ist
auch p := â + k â N. Also haben wir gezeigt, dass es ein p â N mit n + m = 2p
gibt. Damit ist n + m gerade.

15

1. Grundbegriffe

1.5.2. Beweis durch Kontraposition
Der indirekte Beweis oder auch Beweis durch Kontraposition hat folgende Form:
Voraussetzung:
Behauptung:
Beweis:

Aussage A
Aussage B
Es gelte ÂŹB. Dann gilt . . . bla bla bla und deswegen
. . . . Also ist auch A falsch.

Durch diese BeweisfuĚhrung ÂŹB =â ÂŹA ist auch die Aussage A =â B wahr, vgl.
Bemerkung 1.1.6. Ein Beispiel fuĚr einen Beweis durch Kontraposition haben wir
bereits bei Satz 1.3.12 (b) gesehen. Ein weiteres kurzes Beispiel ist folgendes:
Beispiel 1.5.2. Voraussetzung: Sei n â N mit n2 gerade.
Behauptung: Dann ist auch n gerade.
Beweis: Sein n â N so, dass die Behauptung nicht gilt, d.h. n sei ungerade. Dann
gibt es ein k â N mit n = 2k + 1 und es gilt n2 = (2k + 1)2 = 4k 2 + 4k + 1 =
2(2k 2 + 2k) + 1. Damit haben wir â := 2k 2 + 2k â N gefunden mit n2 = 2â + 1.
Dann ist n2 ebenfalls ungerade und die Voraussetzung falsch.

1.5.3. Beweis durch Widerspruch
Der Beweis durch Widerspruch ist eng verwandt mit der Kontraposition. Seine
uĚbliche Form ist die folgende:
Voraussetzung:
Behauptung:
Beweis:

Aussage A
Aussage B
Es gelte A. Angenommen B waĚre falsch. Dann gilt
. . . bla bla bla und deswegen . . . . Also ergaĚbe sich ein
Widerspruch. Damit war die Annahme falsch und es
gilt B.

Eines
der typischen ersten Beispiele fuĚr diese Beweistechnik ist der Beweis, dass
â
2 irrational ist.
â
Beispiel 1.5.3. Behauptung:
Die
Zahl
2 ist nicht rational.
â
Beweis: Annahme: 2 ist rational.
â
Dann gibt es n, m â N mit 2 = n/m. AuĂerdem koĚnnen wir annehmen, dass
dieser Bruch bereits maximal gekuĚrzt ist, d.h. wir koĚnnen die Zahlen n und m
â 2
teilerfremd waĚhlen. Es gilt 2 = 2 = n2 /m2 , d.h.
n2 = 2m2 .

(1.1)

Aus dieser Gleichheit bekommen wir jetzt insbesondere, dass die Zahl n2 eine
gerade Zahl ist und nach Beispiel 1.5.2 ist dann auch n gerade. Also gibt es ein

16

1.5. Beweisprinzipien
k â N mit n = 2k und wir erhalten wieder mit (1.1), dass 2m2 = (2k)2 = 4k 2 ,
d.h. m2 = 2k 2 ist.
Also ist auch m2 gerade und damit wie oben m gerade und wir haben einen
Widerspruch, denn nun sind n undâm teilerfremd und beide gerade.
Also war die Annahme falsch und 2 ist nicht rational.

1.5.4. VollstaĚndige Induktion uĚber N
Die vollstaĚndige Induktion ist ein Beweisverfahren, das dazu dient, die Richtigkeit
einer Aussageform E(n) fuĚr alle natuĚrlichen Zahlen n nachzuweisen. Es sieht so
aus:
Voraussetzung:
Behauptung:
Beweis:

Aussage A
FuĚr alle n â N gilt E(n)
Induktionsanfang: Es gilt A und bla bla bla, also gilt
auch E(0).
Induktionsvoraussetzung: FuĚr ein n â N gelte E(n).
Induktionsschluss: E(n) und A sind wahr, also ist . . . bla
bla bla und deswegen . . . . Damit gilt auch E(n + 1).

Bemerkung 1.5.4. Das Verfahren funktioniert allgemeiner auch um zu zeigen,
dass eine Aussage E(n) fuĚr alle n âĽ n0 , fuĚr ein n0 â N, also ab einem gewissen n0
fuĚr alle groĚĂeren n gilt. Dann muss der Induktionsanfang den Nachweis erbringen,
dass E(n0 ) wahr ist.
AuĂerdem werden Sie in der Vorlesung Formale Grundlagen der Informatikâ
â
noch weitere Verallgemeinerungen dieser Methodik auf andere Strukturen als N
kennen lernen.
n(n + 1)
Beispiel 1.5.5. Behauptung: FuĚr alle n â Nâ gilt 1 + 2 + Âˇ Âˇ Âˇ + n =
.
2
Beweis: Induktionsanfang: Auf der linken Seite der behaupteten Gleichheit steht
fuĚr n = 1 einfach 1 und auf der rechten Seite steht 1 Âˇ (1 + 1)/2 = 2/2 = 1. Also
stimmt diese fuĚr n = 1.
n(n + 1)
.
Induktionsvoraussetzung: FuĚr ein n â Nâ gelte 1 + 2 + Âˇ Âˇ Âˇ + n =
2
Induktionsschritt: Es ist 1 + 2 + Âˇ Âˇ Âˇ + (n + 1) = (1 + 2 + Âˇ Âˇ Âˇ + n) + (n + 1), also
erhalten wir mit der Induktionsvoraussetzung
n(n + 1)
n(n + 1) + 2(n + 1)
+ (n + 1) =
2
2

(n + 1) (n + 1) + 1
(n + 2)(n + 1)
=
=
,
2
2

1 + 2 + Âˇ Âˇ Âˇ + (n + 1) =

was die behauptete Gleichheit fuĚr n + 1 zeigt.
Da das Prinzip der Induktion bisher noch nicht vorkam, hier noch ein Beispiel.

17

1. Grundbegriffe
Beispiel 1.5.6. Behauptung: FuĚr jede endliche Menge M gilt |P(M)| = 2|M | .
Beweis: Wir fuĚhren eine Induktion nach der MaĚchtigkeit der Menge M.
Induktionsanfang: Ist |M| = 0, so muss M = â sein. Dann ist P(M) = P(â) =
{â} und wir haben |P(M)| = 1 = 20 = 2|M | . Die Behauptung stimmt also fuĚr
|M| = 0.
Induktionsvoraussetzung: FuĚr ein n â N gilt fuĚr alle Mengen mit n Elementen
|P(M)| = 2|M | .
Induktionsschritt: Sei M eine Menge mit n + 1 Elementen. Dann hat M mindestens ein Element. Sei also ein x â M fest gewaĚhlt. Wir betrachten nun
N := M \ {x}. Dann hat N genau n Elemente, nach der Induktionsvoraussetzung
gilt also |P(N)| = 2|N | = 2n .
Es gilt aber

P(M) = P(N) âŞ A âŞ {x} : A â P(N) .
(1.2)
Um das einzusehen, beweisen wir zunaĚchst die Inklusion ââ. Sei also B â P(M).
â
Dann ist entweder x â B oder x 6â B. Im zweiten Fall ist B auch Teilmenge von N
also in P(N) und damit in der Menge auf der rechten Seite in (1.2). Ist x â B, so
ist BĚ := B \{x} â P(N) und damit B = BĚ âŞ{x} wiederum in dem Mengensystem
auf der rechten Seite von (1.2) enthalten. Die Inklusion ââ ist klar, denn wegen
â
x â M und N â M ist jedes Element des rechten Mengensystems eine Teilmenge
von M.
Mit der Hilfe von (1.2) sind wir nun bald am Ziel. Wichtig ist noch die Beobachtung, dass wegen x 6â N

P(N) âŠ A âŞ {x} : A â P(N) = â
gilt, denn damit folgt mit UĚbungsaufgabe 1.2.8 und der Induktionsvoraussetzung

|P(M)| = P(N) âŞ A âŞ {x} : A â P(N)

= |P(N)| + A âŞ {x} : A â P(N)
= |P(N)| + |P(N)| = 2|P(N)| = 2 Âˇ 2|N | = 2 Âˇ 2n = 2n+1

und wir haben die Behauptung mit |M| = n + 1 gezeigt.

18

2. Algebraische Strukturen:
Gruppen, Ringe, KoĚrper
Nach dem vorhergehenden Abschnitt, der vor allem die Sprache der Mathematik
einfuĚhren sollte, wollen wir nun richtigâ anfangen. Ein haĚufiges MissverstaĚndnis
â
uĚber Mathematik, ist Mathematik = Rechnenâ, das werden Sie schon gemerkt
â
haben, so viel gerechnet haben wir bisher nicht. Eher mathematisch ist die folgende Frage: Was ist das uĚberhaupt: rechnenâ? Was machen wir, wenn wir
â
rechnen? Was ist die dahinterliegende allgemeine Struktur? Dieser Frage wollen
wir ein bisschen nachgehen und uns verschiedene Rechenstrukturen anschauen.
Der Weg wird nicht ganz geradlinig sein, sondern wir werden den einen oder anderen Abstecher, z.B. zum RSA-Algorithmus aus der Public-Key-VerschluĚsselung
machen, aber die Grundfrage dieses Abschnitts ist obiges was ist rechnen?â
â
Beginnen wollen wir auf vertrautem Grund, dem Rechnen mit ganzen Zahlen.

2.1. Rechnen in Z, Primzahlen und Teiler
Definition 2.1.1. Es seien a, b â Z und p â N.
(a) Man sagt p teilt a und schreibt p|a, falls ein m â Z existiert mit a = m Âˇ p.
(b) Eine natuĚrliche Zahl p > 1 heiĂt Primzahl, wenn p nur durch p und 1
teilbar ist.
(c) Die Zahl ggT(a, b) := max{q â N : q|a und q|b} heiĂt groĚĂter gemeinsamer
Teiler von a und b.
Satz 2.1.2 (Division mit Rest). Seien a â Z und b â Nâ . Dann gibt es eindeutig
bestimmte Zahlen q â Z und r â {0, 1, . . . , b â 1} mit a = q Âˇ b + r.
Beweis. Wir betrachten nur den Fall a âĽ 0, der Beweis im Fall a < 0 verlaĚuft
analog. Zu vorgegebenen a und b betrachten wir die Menge
M := {s â N : s Âˇ b â¤ a}.
Dann ist M â {0, 1, . . . , a}, denn fuĚr alle s â M gilt s = s Âˇ 1 â¤ s Âˇ b â¤ a. Damit
existiert q := max M als groĚĂte ganze Zahl, fuĚr die noch q Âˇ b â¤ a gilt. Mit diesem
q setzen wir nun r := a â q Âˇ b. Dann ist in jedem Fall a = q Âˇ b + r.

19

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Zum Nachweis, dass r â {0, 1, . . . , b â 1} gilt, uĚberlegen wir uns zunaĚchst, dass
wegen q Âˇ b â¤ a auch r = a â q Âˇ b âĽ 0 gilt. Wir muĚssen also noch zeigen, dass
r < b ist. Nehmen wir an, es waĚre r âĽ b, so folgt
(q + 1) Âˇ b = qb + b â¤ qb + r = a
und damit waĚre q + 1 â M, was im Widerspruch zur Konstruktion von q als
groĚĂtem Element von M steht.
Es bleibt noch die Eindeutigkeit zu zeigen. Seien dazu q1 , q2 â Z und r1 , r2 â
{0, 1, . . . , b â 1} mit a = q1 Âˇ b + r1 = q2 Âˇ b + r2 . Dann gilt
(q1 â q2 ) Âˇ b = r2 â r1 .
Insbesondere teilt damit b die Zahl r2 âr1 . Nun liegt aber r2 âr1 zwischen â(bâ1)
und b â 1 und die einzige Zahl in diesem Bereich, die durch b teilbar ist, ist Null.
Also gilt r2 â r1 = 0, d.h. r2 = r1 . Damit ist aber q1 Âˇ b = q2 Âˇ b und wegen b 6= 0
erhalten wir auch q1 = q2 und sind fertig.
Definition 2.1.3. Seien a â Z, b â Nâ und q und r seien die eindeutig bestimmten Zahlen aus Satz 2.1.2. Dann heiĂt q Quotient und r Rest der Division von a
und b. Man schreibt
jak
und
r = a mod b.
q=
b
UĚbungsaufgabe 2.1.4. Zeigen Sie:
(a) FuĚr jedes a â Z und b â Nâ ist die Zahl a mod b das eindeutige r â
{0, 1, . . . , b â 1} mit b|(a â r).
(b) a|b ââ b mod a = 0.

2.1.1. Modulare Arithmetik
In diesem ganzen Abschnitt sei n â Nâ eine feste Zahl.
Satz 2.1.5. FuĚr alle a, b â Z gilt


(a) (a + b) mod n = (a mod n) + (b mod n) mod n.

(b) (a Âˇ b) mod n = (a mod n) Âˇ (b mod n) mod n.
(c) ab mod n = (a mod n)b mod n.

Beweis. Seien k = âa/nâ und â = âb/nâ, d.h.
a = kn + a mod n

20

und

b = ân + b mod n.

2.1. Rechnen in Z, Primzahlen und Teiler
(a) Mit obiger Notation gilt
a + b = kn + a mod n + ân + b mod n = (k + â)n + a mod n + b mod n.
Also ist

(a + b) mod n = (a mod n) + (b mod n) mod n,

denn (k + â)n ist durch n teilbar.
(b) UĚbung
(c) Unter Verwendung von (b) gilt

b
ab mod n = (a
| Âˇ a Âˇ{z. . . Âˇ a}) mod n = (a mod n) mod n.
b Mal

Bemerkung 2.1.6. Obiges Resultat bedeutet, dass man, wann immer am Ende
einer Rechnung nur der Rest modulo n interessiert, auch nach jedem Rechenschritt schon die Zwischenergebnisse modulo n reduzieren kann. Das ist insbesondere im Zusammenhang mit Fragen der Effizienz von Algorithmen und damit
fuĚr die Geschwindigkeit von Computerprogrammen von Interesse.
Beispiel 2.1.7. Wir waĚhlen mal n = 7 und berechnen
(9 â 15) Âˇ 23 + 705

322

mod 7

= (9 mod 7 â 15 mod 7) Âˇ (23 mod 7) + 705 mod 7
322
= (2 â 1) Âˇ 2 + 5
mod 7 = 7322 mod 7
= 0322 mod 7 = 0.

322

mod 7

Wir haben also (einfach und von Hand!) herausgefunden, dass ((9 â 15) Âˇ 23 +
705)322 durch 7 teilbar ist.
Als UĚbung koĚnnen Sie zeigen, dass die Zahl 3444 + 4333 durch 5 teilbar ist.

2.1.2. Der Euklidische Algorithmus
Das erste Ziel dieses Abschnittes ist die algorithmische Bestimmung von ggT(a, b)
fuĚr gegebene a, b â Nâ .
Lemma 2.1.8. Seien a, b â Nâ mit a âĽ b. Dann gilt
(a) ggT(a, b) = ggT(b, a mod b)
(b) b|a =â ggT(a, b) = b.

21

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Beweis. (a) Sei d := ggT(a, b). Dann gilt nach Definition d|a und d|b, also ist
a mod d = 0 und b mod d = 0, vgl. UĚbungsaufgabe 2.1.4 Damit gilt nach
Definition der Division mit Rest und unter Mitwirkung von Satz 2.1.5
h
jak i
(a mod b) mod d = a â
b mod d
b j k

a
= a mod d â
mod d (b mod d) = 0.
b

Wir finden also d|(a mod b). Damit ist d schon mal gemeinsamer Teiler von
b und a mod b. Es bleibt noch zu zeigen, dass d der groĚĂte solche ist.
Sei also c ein gemeinsamer Teiler von b und a mod b. Dann gilt wegen c|b
und c|(a mod b) auch c|(kb+a mod b) fuĚr jedes k â Z. Insbesondere koĚnnen
wir k = âa/bâ waĚhlen. Dann ist kb + a mod b = a und wir bekommen c|a.
Also ist c auch ein gemeinsamer Teiler von a und b. Dieser kann nicht groĚĂer
sein als d = ggT(a, b), also gilt c â¤ d und wir sind fertig.

(b) Es gilt immer b|b und da b nach Voraussetzung auch a teilt, ist b schon
mal ein gemeinsamer Teiler von a und b. Sei c ein weiterer gemeinsamer
Teiler von a und b. Dann gilt wegen c|b sofort c â¤ b, womit b der groĚĂte
gemeinsame Teiler von a und b ist.
Beispiel 2.1.9. Wir bestimmen den groĚĂten gemeinsamen Teiler von 128 und
36. Es ist
(a)

128 mod 36 = 20, da 3 Âˇ 36 = 108, also ggT(128, 36) = ggT(36, 20)
(a)

36 mod 20 = 16, da 1 Âˇ 20 = 20, also ggT(128, 36) = ggT(20, 16)
(a)

20 mod 16 = 4, da 1 Âˇ 16 = 16, also ggT(128, 36) = ggT(16, 4)
(a)

(b)

16 mod 4 = 0, da 4 Âˇ 4 = 16, also ggT(128, 36) = ggT(4, 0) = 4.
Schematisch:

a
b
128 36
36 20
20 16
16 4
4 0

Beim UĚbergang von einer Zeile zur naĚchsten uĚbertraĚgt man jeweils die rechte Zahl
in die linke Spalte und fuĚllt die rechte Spalte mit dem Rest der beim Teilen mit
Rest uĚbrigbleibt. Man wiederholt dieses bis in der rechten Spalte einer Zeile Null
steht, die Zahl in der linken Spalte dieser Zeile ist dann der groĚĂte gemeinsame
Teiler.

22

2.1. Rechnen in Z, Primzahlen und Teiler
Dieses Verfahren ist sehr wichtig und hat darum auch einen eigenen Namen.
Satz 2.1.10 (Euklidischer Algorithmus). Seien a, b â Nâ mit a > b. Der Algorithmus
Euklid(a, b)
IF b = 0 THEN return a
ELSE return Euklid(b, a mod b)
terminiert nach endlich vielen Schritten und liefert ggT(a, b).
Beweis. FuĚr jede Ausgangswahl von a, b â Nâ gilt 0 â¤ a mod b < b, also wird
das zweite Argument des Aufrufs in jedem Schritt echt kleiner. Damit muss es in
endlich vielen Schritten nach Null kommen, d.h. der Algorithmus terminiert.
Seien an , bn die Eingangswerte beim n-ten Aufruf von Euklid. Terminiert der
Algorithmus nach n Schritten, d.h. ist bn = 0, so bedeutet das anâ1 mod bnâ1 = 0,
d.h. bnâ1 |anâ1 . Damit ist nach Lemma 2.1.8 (b)
ggT(anâ1 , bnâ1 ) = bnâ1 = an = Euklid(a, b).
Andererseits ist nach (a) des selben Lemmas
ggT(anâ1 , bnâ1 ) = ggT(anâ2 , bnâ2 ) = Âˇ Âˇ Âˇ = ggT(a0 , b0 ) = ggT(a, b).
Satz 2.1.11 (Erweiterter Euklidischer Algorithmus). Seien a, b â Nâ mit a > b.
Der Algorithmus
Erw-Euklid(a, b)
IF b = 0 THEN return (a, 1, 0)
ELSE DO
(d, x, y) := Erw-Euklid(b, a mod b)
return (d, y, x â âa/bâ Âˇ y)
OD
terminiert nach endlich vielen Schritten und liefert (d, k, â) = Erw-Euklid(a, b)
mit d = ggT(a, b) und die Zahlen k und â erfuĚllen die Beziehung
d = ggT(a, b) = ka + âb.
Wir wollen diesen Satz hier nicht beweisen, sondern nur kurz erwaĚhnen, dass der
Algorithmus zur Bestimmung von d genau der selbe ist wie im einfachen Euklidschen Algorithmus, man also den Beweis, dass dies der ggT ist und dass der
Algorithmus nach endlich vielen Schritten terminiert von oben uĚbernehmen kann.
Die zweite Aussage uĚber die Zahlen k und â beweist man schlieĂlich per Induktion nach der Anzahl der rekursiven Aufrufe, aber das soll hier nicht ausgefuĚhrt
werden.

23

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Beispiel 2.1.12. Wir starten den erweiterten Euklid mit a = 72 und b = 5. Wie
oben bekommen wir ein Schema:
a
72
5
2
1

b âa/bâ k â
5
14 -2 29 = 1 â 14 Âˇ (â2)
2
2 1 â2 = 0 â 2 Âˇ 1
1
2 0
1= 1â2Âˇ0
0
1
0

Man rechnet dabei zunaĚchst die ersten zwei Spalten wie in Beispiel 2.1.9 und
protokolliert zusaĚtzlich in der dritten Spalte jeweils den Quotienten der ersten
beiden Spalten mit. Damit bekommt man schon mal den ggT in der linken unteren
Ecke, hier ist er 1.
Nun schreibt man 1 und 0 in die unterste Zeile der k- und â-Spalte und rechnet
von unten wieder rauf. Dabei kommt in die k-Spalte jeweils der Wert aus der
â-Spalte in der Zeile drunter und in die â-Spalte kommt x â q Âˇ y, wobei x der
Wert aus der k-Spalte der Zeile darunter, q der Quotient aus der aktuellen Zeile
und y der Wert aus der â-Spalte der Zeile drunter ist.
Das Endergebnis besteht nun aus dem ggT der beiden Zahlen und aus den EintraĚgen bei k und â in der ersten Zeile. FuĚr diese beiden Zahlen gilt dann (vgl.
den Satz) ggT(a, b) = ka + âb.
In obigem Beispiel haben wir tatsaĚchlich
ka + âb = (â2) Âˇ 72 + 29 Âˇ 5 = â144 + 145 = 1 = ggT(72, 5).
Beispiel 2.1.13. Erw-Euklid liefert auch eine Methode, um Gleichungen zu loĚsen
der Form: Gegeben a und n mit ggT(a, n) = 1, finde x mit ax âĄ 1(mod n). Ist
naĚmlich (d, k, â) = Erw-Euklid(n, a), so gilt d = 1 und 1 = kn + âa, also
1 mod n = (kn + âa) mod n = âa mod n.
Also ist x = â eine LoĚsung.
Als Beispiel betrachte man 93x âĄ 1 (mod 100). Wegen Erw-Euklid(100, 93) =
(1, 40, â43) ist x = â43 mod 100 = 57 eine LoĚsung. TatsaĚchlich ist 57 Âˇ 93 =
5301 âĄ 1 (mod 100).

2.1.3. Der kleine Satz von Fermat
Satz 2.1.14 (Kleiner Satz von Fermat). FuĚr alle Primzahlen p und alle a â N
gilt ap âĄ a (mod p).
Beweis. Wir fuĚhren eine Induktion nach a.
Induktionsanfang: FuĚr a = 0 gilt 0p = 0 âĄ 0 (mod p).
Induktionsvoraussetzung: FuĚr ein a â N gelte ap âĄ a (mod p).

24

2.2. Die Mathematik hinter Public-Key-Verfahren der Kryptographie
Induktionsschluss: Nach der allgemeinen binomischen Formel gilt


 
 
p
p pâ2
p pâ1
p
p
a + 1,
a
+ÂˇÂˇÂˇ+
a
+
(a + 1) = a +
pâ1
2
1

(2.1)

wobei fuĚr jedes k â {1, 2, . . . , p â 1}
 
p Âˇ (p â 1) Âˇ . . . Âˇ (p â k + 1)
p
âN
=
1 Âˇ 2 Âˇ ...Âˇ k
k
ist. Da k in jedem Fall kleiner als p ist, und p eine Primzahl, muss dieser Ausdruck
fuĚr jedes k â {1, 2, . . . , p â 1} durch p teilbar sein. Damit liefert (2.1)

(a + 1)p mod p = ap + 0 Âˇ apâ1 + 0 Âˇ apâ2 + Âˇ Âˇ Âˇ + 0 Âˇ a + 1 mod p = (ap + 1) mod p.
Nach der Induktionsvoraussetzung ist ap mod p = a, also haben wir schlieĂlich
(a + 1)p mod p = (a + 1) mod p,
womit die Behauptung fuĚr a + 1 gezeigt ist.
Korollar 2.1.15. Ist p Primzahl und a â N eine Zahl, die nicht von p geteilt
wird, so gilt apâ1 âĄ 1 (mod p).
Beweis. Nach Satz 2.1.14 gilt ap âĄ a (mod p), es gibt also ein k â Z mit ap =
kp + a, womit a(apâ1 â 1) = kp folgt. Damit teilt p das Produkt a(apâ1 â 1). Da p
eine Primzahl ist, muss nun entweder p|a oder p|(apâ1 â1) gelten. Ersteres ist nach
Voraussetzung gerade ausgeschlossen, also gilt zweiteres, d.h. (apâ1 â 1) mod p =
0. Das liefert apâ1 mod p = 1 mod p, also die Behauptung.

2.2. Die Mathematik hinter Public-Key-Verfahren
der Kryptographie
Das Ausgangssituation der Kryptographie ist, dass jemand, der uĚblicherweise
Bob genannt wird, jemand anderes, uĚblicherweise Alice, eine Nachricht zukommen lassen will, ohne dass diese von anderen Personen gelesen werden kann. Die
Inkarnation des BoĚsen, die versucht an die Nachricht zu gelangen, wird dabei
uĚblicherweise Eve genannt.
Das Grunddilemma lautet: Bob koĚnnte die Nachricht verschluĚsseln, doch dazu
muĚssen sich Bob und Alice zunaĚchst uĚber den SchluĚssel einigen und wie macht
man das so, dass Eve nicht die Kommunikation uĚber den SchluĚssel abfaĚngt? Eine
LoĚsung liefert die modulare Arithmetik; wie, das wollen wir hier kurz anhand des
RSA-Algorithmus beschreiben. Dieser ist benannt nach den Entwicklern Roland
Rivest, Adi Shamir und Leonard Adleman und er ist einer der grundlegenden

25

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Public-Key-Verfahren, d.h. VerschluĚsselungsverfahren, bei denen Alice den VerschluĚsselungsschluĚssel einfach oĚffentlich zugaĚnglich macht.
Die StaĚrke des Algorithmus steht und faĚllt damit, dass es kein effizientes Verfahren
zum Zerlegen groĂer natuĚrlicher Zahlen in ihre Primfaktoren gibt.
Der Algorithmus braucht drei Schritte, die einmalig zur Vorbereitung von Alice
ausgefuĚhrt werden muĚssen:
1. Alice waĚhlt zwei (groĂe) Primzahlen p und q mit p 6= q und berechnet n = p Âˇ q
und N = (p â 1) Âˇ (q â 1).
2. Alice waĚhlt ein e â N mit ggT(e, N) = 1 und bestimmt dann ein x â N mit
ex âĄ 1 (mod N), vgl. Beispiel 2.1.13.
3. Alice schickt unverschluĚsselt und frei zugaĚnglich das Zahlenpaar (n, e) an Bob,
das ist ihr sogenannter Public Key.
VerschluĚsseln und EntschluĚsseln einer Nachricht M â N mit M < n geht dann
so:
VerschluĚsseln: Bob rechnet M â˛ := M e mod n und schickt das Ergebnis an
Alice.
EntschluĚsseln: Alice rechnet M â˛â˛ := (M â˛ )x mod n.
Zum EntschluĚsseln verwendet Alice ihren sogenannten Private Key (n, x). Dieser
ist nur ihr bekannt und um x aus dem public key (n, e) zu berechnen, braĚuchte
man N, d.h. p und q und damit die Primfaktorzerlegung von n.
Es bleibt uns noch zu zeigen, dass Alice auch wirklich Bobs Nachricht lesen kann,
d.h. dass M â˛â˛ = M gilt.
Satz 2.2.1. Mit obigen Bezeichnungen gilt M â˛â˛ = M ex mod n = M fuĚr alle
M < n.
Beweis. Es ist nach Konstruktion ex âĄ 1 (mod N), wobei N = (p â 1)(q â 1) ist.
Also gibt es ein k â N mit ex = 1 + k(p â 1)(q â 1), woraus
M ex = M Âˇ M (pâ1)(qâ1)k = M Âˇ (M pâ1 )(qâ1)k
folgt. Nun betrachten wir zwei FaĚlle: Ist M 6âĄ 0 (mod p), so liefert der kleine
Satz von Fermat, vgl. Korollar 2.1.15, M pâ1 mod p = 1, und damit ist
M ex mod p = (M mod p) Âˇ (M pâ1 mod p)(qâ1)k mod p = M mod p.
Ist dagegen M ein Vielfaches von p, so gilt ebenfalls M ex mod p = M mod p,
denn dann steht auf beiden Seiten der Gleichung Null.

26

2.3. Gruppen
Mit der selben Argumentation fuĚr q statt p bekommen wir auch
M ex = M Âˇ (M qâ1 )(pâ1)k
und damit
M ex mod q = (M mod q) Âˇ (M qâ1 mod q)(pâ1)k mod q = M mod q.
Zusammengenommen gibt es also zwei Zahlen k1 , k2 â N mit M ex = M + k1 p =
M + k2 q, woraus insbesondere k1 p = k2 q folgt. Nun sind aber p und q zwei
verschiedene Primzahlen. Das bedeutet p|k2 und wir bekommen noch ein k3 â N
mit k2 = k3 p. Damit haben wir nun endguĚltig
M ex mod n = (M + k3 pq) mod n = (M + k3 n) mod n = M mod n = M.

2.3. Gruppen
In diesem Abschnitt beginnen wir mit der abstrakten Beschreibung des Rechnens.
Wir beschraĚnken uns dazu zunaĚchst auf nur eine Rechenoperation. Diese kann
ein Plus, ein Mal oder noch etwas anderes sein. Deshalb brauchen wir ein neues
nicht mit Assoziationen beladenes Zeichen, als das wir im Folgenden meistens ââ
â
nehmen.
Definition 2.3.1. Eine Gruppe ist eine Menge G 6= â mit einer Abbildung
( VerknuĚpfung) â : G Ă G â G, so dass gilt
(a) FuĚr alle a, b, c â G gilt a â (b â c) = (a â b) â c.

(AssoziativitaĚt)

(n) Es gibt ein n â G, so dass fuĚr alle a â G gilt n â a = a und a â n = a.
(Existenz eines neutralen Elements)
(i) Zu jedem a â G gibt es ein a â G, so dass a â a = n und a â a = n gilt.
(Existenz des inversen Elements)
Gilt zusaĚtzlich noch
(k) FuĚr alle a, b â G ist a â b = b â a

(KommutativitaĚt),

so heiĂt die Gruppe G abelsch.
Beispiel 2.3.2.
nicht N.

(a) Z mit der uĚblichen Addition ist eine abelsche Gruppe, aber

(b) Q mit der uĚblichen Addition und Q \ {0} mit der uĚblichen Multiplikation
sind abelsche Gruppen.

27

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
(c) Sei M eine beliebige nichtleere Menge und
F := {f : M â M bijektiv}.
Dann ist F mit der Verkettung âŚâ als VerknuĚpfung eine Gruppe. Man
â
nennt diese die Permutationsgruppe von M.
Man beachte, dass diese Gruppe i.A. nicht abelsch ist. Machen Sie sich das
an dem Beispiel M = R und f (x) = x3 , g(x) = x + 1 klar.
Um einzusehen, dass (F, âŚ) eine Gruppe ist, muĚssen wir uns zunaĚchst uĚberlegen, dass âŚ eine vernuĚnftige VerknuĚpfung auf F ist, d.h. dass fuĚr alle
f, g â F auch f âŚ g â F , also bijektiv, ist. Das war aber gerade die Aussage
von UĚbungsaufgabe 1.4.9.
Nun muĚssen wir noch (a), (n) und (i) zeigen. Zum Nachweis von (a) rechnen
wir fuĚr drei Funktionen f, g, h â F , dass fuĚr alle x â M gilt






f âŚ(gâŚh) (x) = f (gâŚh)(x) = f g(h(x)) = (f âŚg)(h(x)) = (f âŚg)âŚh (x).
Das bedeutet aber gerade, dass f âŚ (g âŚ h) = (f âŚ g) âŚ h ist.

Ein neutrales Element koĚnnen wir explizit angeben, naĚmlich die IdentitaĚt
id : M â M mit id(x) = x fuĚr alle x â M. Es gilt naĚmlich fuĚr alle f â F
und jedes x â M
(id âŚ f )(x) = id(f (x)) = f (x) und (f âŚ id)(x) = f (id(x)) = f (x).
Damit haben wir id âŚ f = f und f âŚ id = f und somit gezeigt, dass id ein
neutrales Element in F ist.
SchlieĂlich ist jedes f â F bijektiv, besitzt also eine Umkehrfunktion f â1 ,
die wiederum bijektiv, also ein Element von F ist. FuĚr diese gilt f âŚf â1 = id
und f â1 âŚ f = id, vgl. Satz 1.4.5, also ist jeweils f â1 das inverse Element
zu f .
(d) Betrachtet man eine geometrische Figur in der Ebene und alle Spiegelungen
und Drehungen der Ebene, die die Figur auf sich selbst abbilden, so erhaĚlt
man die sogenannte Symmetriegruppe der Figur. Nimmt man beispielsweise ein Quadrat Q, vgl. Abbildung 2.1, so ist die Symmetriegruppe gerade
gegeben durch

G = Spiegelung an a, Spiegelung an b, Spiegelung an c, Spiegelung an d,
Drehung um 90âŚ , Drehung um 180âŚ , Drehung um 270âŚ , id

Machen Sie sich klar, was die VerknuĚpfung in dieser Gruppe ist und dass
es sich mit dieser tatsaĚchlich um eine Gruppe handelt. Was ergibt die Spiegelung an a verknuĚpft mit der Spiegelung an b? Ist diese Gruppe abelsch?

28

2.3. Gruppen

c

b

d

a

Abbildung 2.1.: Das Quadrat Q mit den Symmetrielinien a, b, c und d
(e) Zu n â Nâ betrachten wir wieder die Menge Zn = {0Ě, 1Ě, . . . , n]
â 1} der
Restklassen modulo n. Definieren wir fuĚr aĚ, bĚ â Zn
aĚ + bĚ := a]
+b
so ist Zn mit diesem +â eine abelsche Gruppe.
â
Bevor wir den Nachweis der Axiome (a), (n), (i) und (k) als UĚbungsaufgabe
stehen lassen koĚnnen, sollte zumindest geklaĚrt werden, dass obiges Plus eine
vernuĚnftige VerknuĚpfung ist, d.h. dass die Definition von den gewaĚhlten
RepraĚsentanten a, bzw. b unabhaĚngig ist.
Seien dazu a1 , a2 , b1 , b2 â Z mit aĚ1 = aĚ2 und bĚ1 = bĚ2 . Dann gilt a1 âĄ
a2 (mod n) und b1 âĄ b2 (mod n) und wir haben mit Satz 2.1.5 auch
(a1 + b1 ) mod n = a1 mod n + b1 mod n = a2 mod n + b2 mod n
= (a2 + b2 ) mod n,
^
oder anders ausgedruĚckt a^
1 + b1 = a2 + b2 .
Satz 2.3.3. Sei (G, â) eine Gruppe. Dann gilt
(a) G enthaĚlt nur ein neutrales Element.
(b) Zu jedem a â G gibt es genau ein Inverses.
(c) FuĚr gegebene a, b, c, d â G sind die Gleichungen a â x = b und x â c = d
jeweils eindeutig loĚsbar.
Beweis. (a) Seien n1 , n2 â G neutrale Elemente. Dann gilt durch zweimalige
Anwendung von (n)
n1 = n1 â n2 = n2 .

29

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
(b) Sei a â G. Die Existenz eines inversen Elements zu a garantiert uns (i), zu
zeigen bleibt die Eindeutigkeit. Dazu seien b1 und b2 inverse Elemente von
a. Dann gilt
(n)

(i)

(a)

(i)

(n)

b1 = b1 â n = b1 â (a â b2 ) = (b1 â a) â b2 = n â b2 = b2 .
(c) Wir betrachten nur die erste Gleichung, das Argument fuĚr die zweite verlaĚuft analog. Zum Nachweis der Existenz einer LoĚsung geben wir einfach
eine an, naĚmlich x = a â b, denn fuĚr dieses x gilt
(a)

(i)

(n)

a â x = a â (a â b) = (a â a) â b = n â b = b.
Um Eindeutigkeit zu zeigen, nehmen wir wieder zwei LoĚsungen x1 und x2
her. Dann gilt a â x1 = b = a â x2 und damit auch a â (a â x1 ) = a â (a â x2 ).
Mit (a) folgt daraus (a â a) â x1 = (a â a) â x2 , was, (i) folgend, n â x1 = n â x2
bedeutet. Werfen wir nun noch Axiom (n) dazu, liefert das x1 = x2 und wir
sind fertig.
UĚbungsaufgabe 2.3.4. Es sei (G, â) eine Gruppe mit neutralem Element n.
Zeigen Sie:
(a) FuĚr alle g â G gilt g = g.
(b) Es ist n = n.
(c) Ist G endlich, so gibt es ein k â N mit g â g â Âˇ Âˇ Âˇ â g = n.
|
{z
}
k Mal

2.3.1. Untergruppen

Beispiel 2.3.5. Wir betrachten die Menge 2Z = {. . . , â6, â4, â2, 0, 2, 4, 6, . . . }.
Dann ist 2Z mit der uĚblichen Addition aus Z wieder eine Gruppe, denn fuĚr je 2
Elemente aus 2Z ist deren Summe wieder in 2Z, das neutrale Element 0 ist in 2Z
und zu jedem z â 2Z ist auch das inverse Element âz â 2Z.
Damit ist also (2Z, +) eine Teilmenge der Gruppe (Z, +), die selbst wieder eine
Gruppe ist. Solche PhaĚnomene gibt es sehr oft und wir wollen dies im Folgenden
ein wenig untersuchen.
Definition 2.3.6. Eine Teilmenge U einer Gruppe (G, â) heiĂt Untergruppe von
G, falls auch (U, â) eine Gruppe ist.
Beispiel 2.3.7. (a) Die Teilmengen G und {n} sind Untergruppen einer jeden
Gruppe G. Man nennt diese die trivialen Untergruppen von G.

30

2.3. Gruppen
(b) Erinnern wir uns noch einmal an die Symmetriegruppe des Quadrates aus
Beispiel 2.3.2 (d), so hat diese neben den trivialen Untergruppen noch die
Untergruppe, die aus den drei Drehungen zusammen mit der IdentitaĚt, die
man auch als Drehung um 0âŚ auffassen kann, besteht. Indem Sie sich das
klar machen, koĚnnen Sie schon einiges GefuĚhl fuĚr Gruppen gewinnen.
Bilden auch die Spiegelungen eine Untergruppe?
Satz 2.3.8 (Untergruppenkriterium). Eine Teilmenge U einer Gruppe (G, â) ist
genau dann eine Untergruppe von G, wenn
(UG1) U 6= â und
(UG2) fuĚr alle a, b â U ist auch a â b â U.
Beweis. ââ Ist U eine Untergruppe, so muss U eine Gruppe sein und damit
â
zumindest ein neutrales Element enthalten, also ist U nicht leer und wir
haben (UG1).
Wir zeigen als naĚchstes, dass das neutrale Element von U, das wir mit nU
bezeichnen, gleich dem neutralen Element nG von G ist. Dazu bezeichne nU
das Inverse zu nU in G. Dann gilt nU = nU â nU , also
nG = nU â nU = (nU â nU ) â nU = nU â (nU â nU ) = nU â nG = nU .
Seien nun a, b â U und sei b das inverse Element von b in G. Da U eine
Gruppe ist, hat b auch ein inverses Element bĚ in U. Sind die beiden wirklich
verschieden? Nein, denn wegen bâ bĚ = bĚâb = nU = nG ist bĚ auch ein Inverses
von b in G und dieses ist in der Gruppe G eindeutig, vgl. Satz 2.3.3 (b).
Also gilt b = bĚ â U. Da U eine Gruppe ist, muss schlussendlich mit a und
b auch a â b â U sein und wir sind fertig.
ââ Sei U â G so, dass (UG1) und (UG2) gelten. Wir muĚssen zeigen, dass U
â
dann eine Gruppe ist, d.h. dass â : U Ă U â U gilt, und dass die Axiome
(a), (n) und (i) erfuĚllt sind.
ZunaĚchst ist â auf U assoziativ, da dies auf G gilt. Wir haben also (a).

Weiter gibt es wegen (UG1) auf jeden Fall irgendein a â U. Wegen (UG2)
ist dann auch a â a = n â U. Nun ist jedes Element b â U auch in G und
dort gilt n â b = b und b â n = b, also gilt das auch in U und wir haben (n)
gezeigt.
Zum Nachweis von (i) sei nun a â U gegeben. Da nach obigen UĚberlegungen
das neutrale Element n von G ebenfalls in U liegen muss, gilt wiederum nach
(UG2) nun n â a = a â U.

Es bleibt zu zeigen, dass â eine vernuĚnftige VerknuĚpfung auf U ist, die
Elemente aus U zu Elementen aus U verknuĚpft. Seien dazu a, b â U. Wie

31

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
wir oben schon gezeigt haben, ist dann auch b â U und wegen (UG2) und
dem Resultat von UĚbungsaufgabe 2.3.4 (a) haben wir a â b = a â b â U.
Lemma 2.3.9. Sei (G, â) eine Gruppe, I eine beliebige Indexmenge und Uj sei
fuĚr jedes j â I eine Untergruppe von G. Dann ist auch der Schnitt all dieser
Untergruppen, d.h.
\

Uj = x â G : x â Uj fuĚr jedes j â I ,
jâI

eine Untergruppe von G.

Beweis. Wir wenden das Untergruppenkriterium an. Da alle Uj Untergruppen
sind, muss jede dieser Gruppen das neutrale Element n von G enthalten, vgl. den
Beweis von SatzT2.3.8, also ist dieses auch im Schnitt aller Uj , j â I, enthalten
und wir haben jâI Uj 6= â.
T
Zum Nachweis von (UG2) seien a, b â jâI Uj . Das bedeutet, dass diese beiden
fuĚr jedes j â I in der Untergruppe Uj enthalten sind. Dann liefert aber Satz
T 2.3.8
sofort a â b â Uj und zwar fuĚr jedes j â I. Also haben wir auch a â b â jâI Uj
und sind fertig.
Definition 2.3.10. Sei G eine Gruppe und M â G. Dann heiĂt
\
hMi :=
U
U Untergruppe von G
U âM

Erzeugnis von M oder die von M erzeugte Untergruppe.
Bemerkung 2.3.11. Man beachte, dass nach Lemma 2.3.9 das Erzeugnis hMi
immer eine Untergruppe von G ist. TatsaĚchlich ist hMi die kleinste Untergruppe
von G, in der M ganz enthalten ist.
Insbesondere gilt M = hMi ââ M Untergruppe von G.
Beispiel 2.3.12. Betrachten wir in der Gruppe (Z, +) die Teilmenge M = {2},
so gilt hMi = 2Z, denn zum Einen muĚssen natuĚrlich die Zahlen 2, â2, 0, 2 + 2,
â2 â 2, 2 + 2 + 2, â2 â 2 â 2, . . . drin sein, d.h. 2Z â hMi. Zum Anderen ist 2Z
eine Untergruppe von Z, vgl. Beispiel 2.3.5, also haben wir auch hMi â 2Z und
damit Gleichheit.
Zur VerkuĚrzung der Notation fuĚhren wir noch die folgende Schreibweise fuĚr ein
Gruppenelement g einer Gruppe (G, â) und eine Zahl k â Z ein:
ďŁą
ďŁ´
g â g â g â Âˇ Âˇ Âˇ â g , falls k > 0,
ďŁ´
ďŁ´
|
{z
}
ďŁ´
ďŁ´
ďŁ˛
k Mal
g k := n,
falls k = 0,
ďŁ´
ďŁ´
ďŁ´
g â g â g â Âˇ Âˇ Âˇ â g , falls k < 0.
ďŁ´
ďŁ´
{z
}
ďŁł|
k Mal

32

2.3. Gruppen
UĚbungsaufgabe 2.3.13.

(a) Bestimmen Sie h{3, 6}i und h{3, 2}i in (Z, +).

(b) Zeigen Sie: Ist (G, â) Gruppe und g â G, so gilt h{g}i = {g k : k â Z}.

2.3.2. Gruppenhomomorphismen
Definition 2.3.14. (a) Es seien (G, â) und (H, â) Gruppen. Eine Abbildung
f : G â H heiĂt (Gruppen-)Homomorphismus, falls
f (g1 â g2 ) = f (g1 ) â f (g2)

fuĚr alle g1 , g2 â G

gilt.
(b) Ein bijektiver Gruppenhomomorphismus heiĂt (Gruppen-)Isomorphismus.
(c) Zwei Gruppen G und H, fuĚr die ein Isomorphismus f : G â H existiert,
heiĂen isomorph.
Beispiel 2.3.15.

(a) Die Abbildung
(
(Z, +) â (Z, +)
f:
k
7â 4k

ist ein Homomorphismus, denn fuĚr alle k, â â Z gilt
f (k + â) = 4(k + â) = 4k + 4â = f (k) + f (â).
Allerdings ist f kein Isomorphismus, denn f ist nicht surjektiv.
Zeigen Sie, dass f : (Z, +) â (4Z, +) ein Isomorphismus ist.
(b) Die Abbildung

(
(R, +) â (R \ {0}, Âˇ)
g:
x
7â 2x

ist ebenfalls ein Homomorphismus, denn fuĚr alle x1 , x2 â R gilt
g(x1 + x2 ) = 2x1 +x2 = 2x1 Âˇ 2x2 = g(x1 ) Âˇ g(x2 ).
Ist g ein Isomorphismus?
UĚbungsaufgabe 2.3.16. Zeigen Sie: Ist (G, â) eine Gruppe und g â G ein
beliebiges Gruppenelement, so ist
(
G âG
Ďg :
h 7â g â h â g
ein Homomorphismus.
Finden Sie weiter Beispiele von Gruppen G und Elementen g â G, so dass Ďg
einmal ein Isomorphismus ist und einmal nicht.

33

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Satz 2.3.17. Es seien (G, â) und (H, â) Gruppen mit neutralen Elementen nG
bzw. nH und f : G â H ein Homomorphismus. Dann gilt
(a) f (nG ) = nH .
(b) f (g) = f (g) fuĚr jedes g â G.
(c) f (G) ist eine Gruppe, d.h. eine Untergruppe von H.
(d) Ist G abelsch, so ist auch f (G) abelsch.
Beweis. (a) Dank (n) haben wir nG = nG â nG . Also ist wegen der Homomorphieeigenschaft von f auch f (nG ) = f (nG â nG ) = f (nG ) â f (nG ). Weiter
hat f (nG ) wie jedes Gruppenelement wegen (i) ein Inverses f (nG ) in H.
Damit gilt

(i)
nH = f (nG ) â f (nG ) = f (nG ) â f (nG ) â f (nG )

(i)
(n)
(a)
= f (nG ) â f (nG ) â f (nG ) = nH â f (nG ) = f (nG ).

(b) Sei g â G. Dann gilt mit der Homomorphieeigenschaft von f und Teil (a)
des Beweises.
f (g) â f (g) = f (g â g) = f (nG ) = nH ,
sowie
f (g) â f (g) = f (g â g) = f (nG ) = nH .
Also ist f (g) = f (g).
(c) Wir wenden das Untergruppenkriterium an. Wegen Teil (a) des Beweises
gilt nH = f (nG ) â f (G), also ist f (G) 6= â und wir haben (UG1). Zum
Nachweis von (UG2) seien h1 , h2 â f (G) gegeben. Dann gibt es g1 , g2 â G
mit f (g1 ) = h1 und f (g2 ) = h2 . Mit diesen haben wir dank Teil (b)
h1 â h2 = f (g1 ) â f (g2) = f (g1 ) â f (g2 ) = f (g1 â g2 ) â f (G).
(d) Sei nun G abelsch und seien h1 , h2 â f (G). Dann gibt es wieder g1 , g2 â G
mit f (g1 ) = h1 und f (g2 ) = h2 und wir bekommen
(k)

h1 â h2 = f (g1 ) â f (g2 ) = f (g1 â g2 ) = f (g2 â g1 ) = f (g2 ) â f (g1 ) = h2 â h1 .

Definition 2.3.18. Es seien (G, â) und (H, â) Gruppen mit neutralen Elementen
nG bzw. nH und f : G â H ein Homomorphismus. Dann heiĂt ker(f ) := {g â
G : f (g) = nH } Kern von f .

34

2.4. Ringe und KoĚrper
Beispiel 2.3.19. Wir betrachten
(
(Z4 , +) â (Z4 , +)
f:
f
nĚ
7â 2n.
Diese Abbildung ist wegen

^
g g
f (nĚ1 + nĚ2 ) = f (n^
Ë2 )
1 + n2 ) = 2(n1 + n2 ) = 2n1 + 2n2 = f (nĚ1 ) + f (n

ein Homomorphismus mit
f (0Ě) = 0Ě,

f (1Ě) = 2Ě,

f (2Ě) = 4Ě = 0Ě,

f (3Ě) = 6Ě = 2Ě.

In diesem Fall ist also ker(f ) = {0Ě, 2Ě}.
Satz 2.3.20. Die Menge ker(f ) ist immer eine Untergruppe von G.
Beweis. Es ist immer f (nG ) = nH , vgl. Satz 2.3.17 (a), also ist nG â ker(f ) und
damit ker(f ) 6= â. Seien nun g1 , g2 â ker(f ). Dann gilt
f (g1 â g2 ) = f (g1 ) â f (g2) = f (g1) â f (g2 ) = nH â nH = nH â nH = nH .
Also ist auch g1 â g2 â ker(f ) und die Behauptung folgt aus dem Untergruppenkriterium.
Insbesondere haben wir damit gesehen, dass {0Ě, 2Ě} eine Untergruppe von (Z4 , +)
ist.

2.4. Ringe und KoĚrper
2.4.1. Ringe
Definition 2.4.1. (a) Eine Menge R mit zwei VerknuĚpfungen + : R â R und
Âˇ : R â R heiĂt Ring, falls die folgenden Bedingungen erfuĚllt sind:
â˘ (R, +) ist eine abelsche Gruppe.

â˘ âa, b, c â R : a Âˇ (b Âˇ c) = (a Âˇ b) Âˇ c, d.h. Âˇâ ist assoziativ.
â
â˘ âa, b, c â R : a Âˇ (b + c) = a Âˇ b + a Âˇ c und (a + b) Âˇ c = a Âˇ c + b Âˇ c, d.h.
die beiden VerknuĚpfungen erfuĚllen die Distributivgesetze.
(b) Das neutrale Element der Gruppe (R, +) heiĂt Nullelement, Symbol: 0.
(c) Existiert ein Element 1 â R mit a Âˇ 1 = 1 Âˇ a = a fuĚr jedes a â R, so heiĂt
1 Einselement von R und man nennt dann R einen Ring mit Eins.

35

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
(d) Ist zusaĚtzlich die VerknuĚpfung Âˇâ auf R kommutativ, so nennt man R einen
â
kommutativen Ring.
Beispiel 2.4.2.

(a) (Z, +, Âˇ), (Q, +, Âˇ) sind kommutative Ringe mit Eins.

(b) R[x], d.h. die Menge aller Polynome in einer Variablen uĚber R, ist ein kommutativer Ring mit dem Einselement, das durch das konstante Polynom 1
gegeben ist.
(c) Sei n â N mit n âĽ 2. Definieren wir auf Zn eine Multiplikation durch
aĚ Âˇ bĚ = ag
Âˇ b, so ist diese nach Satz 2.1.5 (b) wohldefiniert.

Wir wollen nun zeigen, dass (Zn , +, Âˇ) sogar ein kommutativer Ring mit Eins
ist. Dazu erinnern wir uns zunaĚchst, dass wir in Beispiel 2.3.2 (e) bereits
festgestellt haben, dass (Zn , +) eine abelsche Gruppe ist. Die AssoziativitaĚt und die KommutativitaĚt von Âˇâ, sowie die Distributivgesetze koĚnnen
â
wir leicht auf die entsprechenden Eigenschaften von Z zuruĚckspielen. Hier
fuĚhren wir beispielhaft nur das Assoziativgesetz vor. FuĚr alle kĚ, âĚ, mĚ â Zn
gilt
(kĚ Âˇ âĚ) Âˇ mĚ = (kg
Âˇ â) Âˇ mĚ = (k^
Âˇ â) Âˇ m = k Âˇ^
(â Âˇ m) = kĚ Âˇ (âĚ Âˇ mĚ).

SchlieĂlich bleibt uns noch das Einselement zu indetifizieren, aber auch das
ist nicht sonderlich schwer, denn fuĚr alle kĚ â Zn gilt 1Ě Âˇ kĚ = 1g
Âˇ k = kĚ, also
ist 1Ě das Einselement von (Zn , +, Âˇ).

Was bei der Multiplikation in Zn fuĚr konkrete (kleine) Werte von n passiert,
kann man sich gut mit Multiplikationstafeln klar machen. Hier sind die fuĚr
n=5
0Ě
1Ě
2Ě
3Ě
4Ě

n=4

0Ě

1Ě 2Ě

3Ě 4Ě

0Ě
0Ě
0Ě
0Ě
0Ě

0Ě
1Ě
2Ě
3Ě
4Ě

0Ě
3Ě
1Ě
4Ě
2Ě

0Ě
2Ě
4Ě
1Ě
3Ě

0Ě
4Ě
3Ě
2Ě
1Ě

0Ě
1Ě
2Ě
3Ě

0Ě 1Ě

2Ě 3Ě

0Ě
0Ě
0Ě
0Ě

0Ě
2Ě
0Ě
2Ě

0Ě
1Ě
2Ě
3Ě

0Ě
3Ě
2Ě
1Ě

Schreibweise 2.4.3. Sei (R, +, Âˇ) ein Ring.
(a) Das zu r â R additiv inverse Element bezeichnet man mit âr und fuĚr
r, s â R schreibt man r â s statt r + (âs).
(b) Oft laĚsst man das Âˇâ weg und schreibt rs statt r Âˇ s fuĚr r, s â R.
â
Satz 2.4.4. Sei (R, +, Âˇ) ein Ring. Dann gelten die folgenden Aussagen:
(a) FuĚr jedes r â R gilt 0 Âˇ r = r Âˇ 0 = 0.

36

2.4. Ringe und KoĚrper
(b) FuĚr alle r, s â R gilt (âr) Âˇ s = r Âˇ (âs) = â(r Âˇ s) und (âr) Âˇ (âs) = rs.
(c) FuĚr jede Wahl von r, s, t â R gilt r(s â t) = rs â rt.
Beweis. (a) Sei r â R. Dann gilt wegen (n) und dank des Distributivgesetzes
r Âˇ 0 = r Âˇ (0 + 0) = r Âˇ 0 + r Âˇ 0. Da (R, +) eine Gruppe ist, besitzt r Âˇ 0 ein
additives Inverses â(r Âˇ 0). Mit diesem gilt
0 = r Âˇ 0 â r Âˇ 0 = (r Âˇ 0 + r Âˇ 0) â r Âˇ 0 = r Âˇ 0 + (r Âˇ 0 â r Âˇ 0) = r Âˇ 0.
(b), (c) UĚbung.
Analog zur Situation bei Gruppen definieren wir Homomorphismen und Isomorphismen von Ringen, als die Abbildungen, die die beiden VerknuĚpfungen von
Ringen respektieren.
Definition 2.4.5. Seien (R, +, Âˇ), (S, â, â) Ringe.
(a) Eine Abbildung f : R â S heiĂt (Ring-)Homomorphismus, falls fuĚr alle
r, s â R gilt
f (r + s) = f (r) â f (s)

und

f (r Âˇ s) = f (r) â f (s).

(2.2)

(b) Sind R und S Ringe mit Eins und sind 1R und 1S die beiden Einselelemente,
so fordert man zusaĚtzlich zu (2.2)
f (1R ) = 1S .
(c) Einen bijektiver Ringhomomorphismus nennt man (Ring-)Isomorphismus
und sagt in diesem Fall, dass die beiden Ringe R und S isomorph sind.
Bemerkung 2.4.6. Wie bei Gruppen gilt auch fuĚr Ringe, dass das Bild eines
Rings unter einem Ringhomomorphismus immer wieder ein Ring ist.

2.4.2. KoĚrper
Die letzte klassische Rechenart, die wir jetzt noch nicht untersucht haben, ist
das Teilen. Zum Teilen brauchen wir auf jeden Fall eine Eins, wir sollten also
mit einem Ring mit Eins R starten und uns uĚberlegen, was wir weiterhin zum
Teilen brauchen. Teilen durch ein r â R bedeutet Multiplizieren mit 1/r, aber
was ist das, 1/r? Das ist ein Element, das, wenn wir es mit r multiplizieren das
Einselement ergibt. Wegen Satz 2.4.4 (a) heiĂt das von vornherein, dass wir Teilen
durch Null komplett vergessen koĚnnen. Um ansonsten freizuĚgig teilen zu koĚnnen,
fordern wir also, dass es zu jedem r â R \ {0} ein Element r â1 â R gibt mit
r Âˇ r â1 = r â1 Âˇ r = 1.
Nun erhebt sich natuĚrlich die Frage: Gibt es solche Ringe? Ja, z.B. Q und R sehen
gut aus. Wie ist es mit unseren anderen Beispielen?

37

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Z: Nein, sicher nicht, denn es gibt kein k â Z mit k Âˇ 2 = 1.
R[x]: Ebensowenig, denn fuĚr welches Polynom P gilt P (x) Âˇ x2 = 1?
Zn : Hier ist die Sache weniger klar und haĚngt von n ab (wie genau werden wir
weiter unten sehen). Wir betrachten die Beispiele n = 4 und n = 5, vgl.
Beispiel 2.4.2 (c).
FuĚr n = 4 gilt 2Ě Âˇ 2Ě = 4Ě = 0Ě, was schon mal befremdlich aussieht. Nehmen
wir nun an, es gaĚbe ein Element 2Ěâ1 â Z4 , also ein n â {0, 1, 2, 3} mit
nĚ = 2Ěâ1 , so folgt
0Ě = 0g
Âˇ n = 0Ě Âˇ nĚ = 2Ě Âˇ 2Ě Âˇ 2Ěâ1 = 2Ě,

was ein sauberer Widerspruch ist, also kann man in Z4 nicht durch 2Ě teilen.
In Z5 sieht das schon anders aus. Aus der Multiplikationstabelle in Beispiel 2.4.2 (c) liest man ab:
1Ěâ1 = 1Ě,

2Ěâ1 = 3Ě,

3Ěâ1 = 2Ě,

4Ěâ1 = 4Ě.

Dem befremdlichen Verhalten von Z4 oben geben wir zunaĚchst einen Namen.
Definition 2.4.7. Sei (R, +, Âˇ) ein Ring. Gibt es Zahlen r, s â R\{0} mit rs = 0,
so heiĂt r ein linker und s ein rechter Nullteiler.
Wir wollen nun den besonders schoĚnen Ringen, in denen wir durch alles auĂer
der Null teilen koĚnnen, einen eigenen Namen geben.
Definition 2.4.8. Ein kommutativer Ring mit Eins K, in dem zusaĚtzlich (K \
{0}, Âˇ) eine abelsche Gruppe ist, heiĂt KoĚrper.
Beispiele von KoĚrpern sind die rationalen Zahlen Q, sowie die reellen Zahlen R
und wie wir oben gesehen haben Z5 .
Bemerkung 2.4.9. Da es in der Defintion des KoĚrpers etwas versteckt ist, sei
an dieser Stelle explizit darauf hingewiesen, dass in jedem KoĚrper 1 6= 0 gelten
muss, denn die Eins ist das neutrale Element der Gruppe (K \ {0}, Âˇ) und kann
damit nicht Null sein.
Wir wollen nun zeigen, dass es in KoĚrpern keine Nullteiler geben kann.
Satz 2.4.10. Ist K ein KoĚrper, so gilt fuĚr alle x, y â K
x Âˇ y = 0 =â x = 0 oder y = 0.

38

2.4. Ringe und KoĚrper
Beweis. Seien x, y â K mit xÂˇy = 0. Ist x = 0 sind wir fertig, sei also x 6= 0. Dann
gibt es das multiplikative Inverse xâ1 â K und wir haben mit UnterstuĚtzung von
Satz 2.4.4 (a)
y = 1 Âˇ y = (xâ1 Âˇ x) Âˇ y = xâ1 Âˇ (x Âˇ y) = xâ1 Âˇ 0 = 0.

Satz 2.4.11. Der Ring (Zn , +, Âˇ) ist genau dann ein KoĚrper, wenn n prim ist.
Beweis. ââ Sei n nicht prim. Dann gibt es p, r â {2, 3, . . . , n â 1} mit n = pr.
â
Das bedeutet aber pĚ Âˇ rĚ = pr
e = nĚ = 0Ě und da pĚ und rĚ beide nicht gleich 0Ě
sind, hat Zn also Nullteiler und kann kein KoĚrper sein.

ââ Zum Nachweis der RuĚckrichtung beobachten wir zunaĚchst, dass (Zn , +, Âˇ)
â
nach Beispiel 2.4.2 (c) ein kommutativer Ring mit Eins ist, es bleibt also nur
zu zeigen, dass jedes Element von Zn , das nicht Null ist, ein mulitplikatives
Inverses besitzt. Sei also a â {1, 2, . . . , n â 1} gegeben.
Da n prim ist und a < n gilt, bekommen wir aus dem kleinen Satz von
Fermat, vgl. Korollar 2.1.15, dass anâ1 âĄ 1 (mod n) ist. Das bedeutet in
nâ2 â Z , so gilt
nâ1 = 1Ě. Betrachten wir nun das Element bĚ := ag
Zn gilt ag
n
nâ2 = a^
nâ1 = 1Ě.
aĚ Âˇ bĚ = aĚ Âˇ ag
Âˇ anâ2 = ag

nâ2 das gesuchte inverse Element und wir sind fertig.
Also ist aĚâ1 = bĚ = ag

Definition 2.4.12. Seien (K, +, Âˇ) und (L, â, â) KoĚrper mit Einselementen 1K
und 1L .

(a) Ein Ringhomomorphismus f : K â L (mit f (1K ) = 1L , vgl. Definition 2.4.5 (b)) heiĂt (KoĚrper-)Homomorphismus.
(b) Ist f zusaĚtzlich bijektiv, so heiĂt f (KoĚrper-)Isomorphismus, und man nennt
dann K und L isomorph.
(c) Ist schlieĂlich f : K â K ein Isomorphismus, so nennt man f einen
(KoĚrper-)Automorphismus von K.
Bemerkung 2.4.13. Wie bei Gruppen und Ringen gilt auch hier, dass fuĚr jeden
KoĚrperhomomorphismus f : K â L die Menge f (K) ein KoĚrper ist.
Beispiel 2.4.14. (a) Jeder KoĚrper K hat den Automorphismus id : K â K,
dies ist der sogenannte triviale KoĚrperautomorphismus.
(b) Ein KoĚrperhomomorphismus ist z.B. id : Q â R.

39

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
Sind (K, +, Âˇ) und (L, â, â) KoĚrper und f : K â L ein Homomorphismus, so
ist wenig uĚberraschend, dass f (0K ) = 0L und f (1K ) = 1L gilt, denn f ist ja
insbesondere auch jeweils ein Gruppenhomomorphismus von (K, +) nach (L, â),
bzw. von (K \ {0}, Âˇ) nach (L \ {0}, â). Auf den ersten Blick weniger zu erwarten
ist folgendes Resultat.
Satz 2.4.15. Jeder KoĚrperhomomorphismus f : K â L ist injektiv.
Beweis. Wir zeigen zunaĚchst, dass f â1 ({0L }) = {0K } ist, d.h. nur das Nullelement von K wird auf das Nullelement von L abgebildet. Dazu nehmen wir an, es
gaĚbe ein x â K mit x 6= 0K und f (x) = 0L . Wegen x 6= 0K gibt es dann xâ1 â K
mit x Âˇ xâ1 = 1K . Also ist
1L = f (1K ) = f (x Âˇ xâ1 ) = f (x) â f (xâ1 ) = 0L â f (xâ1 ) = 0L
und das ist in einem KoĚrper nicht moĚglich.
Seien nun x1 , x2 â K mit f (x1 ) = f (x2 ) gegeben. Dann gilt

f (x1 â x2 ) = f x1 + (âx2 ) = f (x1 ) â f (âx2 ) = f (x1 ) â (âf (x2 ))
= f (x1 ) â f (x2 ) = 0L .
Also muss nach obigen Erkenntnissen x1 â x2 = 0K und damit x1 = x2 sein. Das
bedeutet aber gerade, dass f injektiv ist.
Definition 2.4.16. Ist (K, +, Âˇ) ein KoĚrper, auf dem eine Totalordnung â¤â
â
gegeben ist, so dass
â˘ âa, b, c â K : a â¤ b =â a + c â¤ b + c und
â˘ âa, b, c â K : (a â¤ b und 0K â¤ c) =â ac â¤ bc
gelten, so heiĂt (K, +, Âˇ, â¤) angeordneter KoĚrper.

Ein Paradebeispiel fuĚr einen angeordneten KoĚrper ist Q.

UĚbungsaufgabe 2.4.17. Ist (K, +, Âˇ, â¤) ein angeordneter KoĚrper, so gilt
(a) FuĚr alle a â K mit a > 0 gilt âa < 0.
(b) FuĚr alle a â K gilt a2 âĽ 0.

2.5. Der KoĚrper der komplexen Zahlen
Definition 2.5.1. Wir definieren auf der Menge R2 = R Ă R eine Addition â
und eine Multiplikation â, indem wir fuĚr (x1 , y1) und (x2 , y2) â R2 setzen:
(x1 , y1 ) â (x2 , y2 ) = (x1 + x2 , y1 + y2 ) und
(x1 , y1 ) â (x2 , y2 ) = (x1 x2 â y1 y2 , x1 y2 + y1 x2 ).

40

2.5. Der KoĚrper der komplexen Zahlen
Satz 2.5.2. (R2 , â, â) ist ein KoĚrper.

Beweis. ZunaĚchst ist festzustellen, dass â und â wohldefinierte VerknuĚpfungen
sind, da sie jeweils zwei Elementen von R2 wieder Elemente von R2 zuordnen.
Wir wenden uns also dem Nachweis zu, dass (R2 , â) eine abelsche Gruppe ist.
FuĚr alle (x1 , y1), (x2 , y2 ), (x3 , y3 ) â R2 gilt dank der AssoziativitaĚt bzw. KommutativitaĚt von R


(x1 , y1) â (x2 , y2 ) â (x3 , y3 ) = (x1 + x2 ) + x3 , (y1 + y2 ) + y3

= x1 + (x2 + x3 ), y1 + (y2 + y3 )

= (x1 , y1) â (x2 , y2 ) â (x3 , y3)
und

(x1 , y1 ) â (x2 , y2 ) = (x1 + x2 , y1 + y2 ) = (x2 + x1 , y2 + y1 ) = (x2 , y2) â (x1 , y1 ).

Weiterhin ist (0, 0) das additive neutrale Element, denn fuĚr jedes (x, y) â R2 gilt
(x, y) â (0, 0) = (x + 0, y + 0) = (x, y).

SchlieĂlich ist das zu (x, y) â R2 additiv inverse Element gegeben durch (âx, ây),
denn (x, y) â (âx, ây) = (x â x, y â y) = (0, 0).
Die naĚchste Etappe ist der Nachweis, dass (R2 \ {(0, 0)}, â) eine abelsche Gruppe
ist. Die AssoziativitaĚt und die KommutativitaĚt findet man wieder durch eine
geradlinige Rechnung, die allerdings leicht laĚnglich wird. Wir wollen hier deshalb
darauf verzichten (Weniger freundlich ausgedruĚckt: Der Autor kneift. . . ). Das
multiplikative neutrale Element ist in diesem Fall gegeben durch (1, 0), denn fuĚr
jedes (x, y) â R2 gilt nach Definition der Multiplikation
(x, y) â (1, 0) = (x Âˇ 1 â y Âˇ 0, x Âˇ 0 + y Âˇ 1) = (x, y).

Ganz so einfach zu erraten ist das multiplikativ inverse Element nicht (aber Sie
werden in wenigen Seiten wissen, wie man es sich merken kann). Wir geben uns
ây
x
also ein (x, y) â R2 mit (x, y) 6= (0, 0) vor und behaupten, dass ( x2 +y
2 , x2 +y 2 ) das
Inverse ist. Bevor wir das nachrechnen, beachte man noch, dass dieser Ausdruck
tatsaĚchlich fuĚr alle (x, y) 6= (0, 0) definiert ist, da der Nenner nur Null wird, wenn
x und y beide Null sind. TatsaĚchlich haben wir
 x
ây   x2
ây 2
âxy
xy 
(x, y) â 2
,
=
â
,
+
= (1, 0).
x + y 2 x2 + y 2
x2 + y 2 x2 + y 2 x2 + y 2 x2 + y 2
Damit bleibt uns zum KoĚrpergluĚck nur noch ein Distributivgesetz nachzurechnen.
Seien also (x1 , y1 ), (x2 , y2 ), (x3 , y3 ) â R2 . Dann gilt

(x1 , y1 ) â (x2 , y2 ) â (x3 , y3 ) = (x1 , y1 ) â (x2 + x3 , y2 + y3 )

= x1 (x2 + x3 ) â y1 (y2 + y3 ), x1 (y2 + y3 ) + y1 (x2 + x3 )

= x1 x2 + x1 x3 â y1 y2 â y1 y3 , x1 y2 + x1 y3 + x2 y1 + x3 y1


= x1 x2 â y1 y2 , x1 y2 + x2 y1 â x1 x3 â y1 y3 , x1 y3 + x3 y1
= (x1 , y1 ) â (x2 , y2 ) â (x1 , y1 ) â (x3 , y3).

41

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper

Definition 2.5.3. (R2 , â, â) heiĂt KoĚrper der komplexen Zahlen und wird uĚblicherweise mit C bezeichnet.
(
R âC
ist ein KoĚrperhomomorphismus.
Satz 2.5.4. Die Abbildung f :
x 7â (x, 0)
Beweis. Es gilt fuĚr alle x, y â R
f (x + y) = (x + y, 0) = (x, 0) â (y, 0) = f (x) â f (y)
und
f (xy) = (xy, 0) = (xy â 0 Âˇ 0, x Âˇ 0 + 0 Âˇ y) = (x, 0) â (y, 0).

Da schlieĂlich noch f (1) = (1, 0) = 1C ist, sind wir schon fertig.

Bemerkung 2.5.5. Dank Satz 2.4.15 ist obiger Homomorphismus f : R â
{(x, y) â R2 : y = 0} bijektiv. Die reellen Zahlen koĚnnen daher mit der Identifikation x =
b (x, 0) als TeilkoĚrper der komplexen Zahlen aufgefasst werden. Wir
haben also unseren Zahlraum noch mal erweitert.
Beispiel 2.5.6. Wir berechnen zwei wesentliche Produkte komplexer Zahlen. Es
ist
(0, 1) â (0, 1) = (0 Âˇ 0 â 1 Âˇ 1, 0 Âˇ 1 + 1 Âˇ 0) = (â1, 0) =
b â1

und fuĚr alle y â R gilt

(y, 0) â (0, 1) = (y Âˇ 0 â 0 Âˇ 1, y Âˇ 1 + 0 Âˇ 0) = (0, y).

Bemerkung 2.5.7. Setzt man i := (0, 1), so gilt nach obiger Rechnung i2 =
(â1, 0) =
b â1. Damit koĚnnen wir eine andere, intuitiver zu verwendende Schreibweise der komplexen Zahlen einfuĚhren. Wir machen uns dazu zu nutze, dass fuĚr
(x, y) â C mit obigen Identifikationen gilt
(x, y) = (x, 0) â (0, y) = (x, 0) â (1, 0) â (y, 0) â (0, 1) =
b x Âˇ 1 + y Âˇ i.

Die komplexe Addition und Multiplikation berechnet sich dann wegen
(x1 + y1 i) + (x2 + y2 i) = (x1 + x2 ) + (y1 + y2 )i und
(x1 + y1 i) Âˇ (x2 + y2 i) = x1 x2 + y1 y2 i2 + x1 y2 i + x2 y1 i
= (x1 x2 â y1 y2 ) + (x1 y2 + x2 y1 )i,

indem man wie wir es aus R gewohnt sind rechnet und unterwegs immer i2 = â1
beachtet.
Wir werden deshalb in Zukunft auf die Kringel um Plus und Mal verzichten und
die gewohnten Symbole verwenden.
Die fuĚr die komplexen Zahlen fundamentale Zahl i nennt man auch die imaginaĚre
Einheit.

42

2.5. Der KoĚrper der komplexen Zahlen
Definition 2.5.8. Sei z â C und seien x, y â R so, dass z = (x, y) = x + yi ist.
Dann heiĂt
Re(z) := x Realteil von z und
Im(z) := y ImaginaĚrteil von z.
Ist y = 0, so nennt man z reell und ist x = 0, so heiĂt z rein imaginaĚr.
Bemerkung 2.5.9. (a) ZunaĚchst als Warnung vor einem haĚufigen Fehler der
Hinweis, dass der ImaginaĚrteil einer komplexen Zahl immer reell ist. Es ist
z.B. Im(3 + 2i) = 2 und nicht 2i.
(b) Da die komplexen Zahlen aus R2 hervorgehen, kann man sie sich gut in der
sogenannten komplexen Zahlenebene, auch GauĂâsche Zahlenebene genannt,
vgl. Abbildung 2.2, veranschaulichen.

Im(z)

z

i

|z|

1

Re(z)
_
z

Abbildung 2.2.: Die komplexe Zahlenebene
Definition 2.5.10. Sei z = x + yi â C mit x, y â R. Dann heiĂt.
z := x â yi
p
|z| := x2 + y 2

Satz 2.5.11.

zu z konjugiert komplexe Zahl und
Betrag von z

(a) FuĚr jedes z â C gilt z = z.

(b) Die Abbildung z 7â z ist ein nichttrivialer KoĚrperautomorphismus von C.
(c) Es ist z + z = 2 Âˇ Re(z) und z â z = 2 Âˇ Im(z)i.
(d) Ein z â C ist genau dann reell, wenn z = z gilt.
Beweis.

(a) z = x + yi = x â yi = x + yi = z.

43

2. Algebraische Strukturen: Gruppen, Ringe, KoĚrper
(b) ZunaĚchst ist 1 = 1 = 1C . Weiter gilt fuĚr alle z = x + yi und w = u + vi aus
C mit x, y, u, v â R
z + w = x + yi + u + vi = x + u + (y + v)i = x + u â (y + v)i
= x â yi + u â vi = z + w
und
zw = (x + yi) Âˇ (u + vi) = xu â yv + (xv + yu)i = xu â yv â (xv + yu)i
= xu â (ây)(âv) + x(âv)i + (ây)ui = (x + (ây)i) Âˇ (u + (âv)i)
= (x â yi) Âˇ (u â vi) = z Âˇ w.
Also ist die Konjugation schon mal ein KoĚrperhomomorphismus. Nach
Satz 2.4.15 ist dieser auch injektiv, wir brauchen also zum Nachweis, dass es
sich um einen Automorphismus handelt, nur noch die SurjektivitaĚt. Doch
diese folgt direkt aus (a), denn zu jedem z â C ist demnach z ein Urbild
unter der Konjugation.
SchlieĂlich ist die Konjugation nicht der triviale KoĚrperautomorphismus,
denn i = âi 6= i.
(c) z + z = x + yi + x â yi = 2x = 2 Âˇ Re(z) und z â z = x + yi â (x â yi) =
2yi = 2 Âˇ Im(z)i.
(d) Ist z â C reell, so gilt z = x + 0 Âˇ i fuĚr ein x â R. Also ist in diesem Fall
z = x + 0 Âˇ i = x â 0 Âˇ i = x = z.

Gilt umgekehrt z = z, so ist mit obiger Rechnung Im(z) = (z â z)/(2i) = 0,
also ist z = Re(z) reell.

Satz 2.5.12. FuĚr alle z, z1 , z2 â C gilt
(a) |z| = |z|.
(b) z Âˇ z = |z|2 .
(c) z â1 =

z
|z|2

falls z 6= 0.

(d) Re(z) â¤ |z| und Im(z) â¤ |z|.
(e) |z| â R und |z| âĽ 0 und (|z| = 0 ââ z = 0).
(f ) |z1 Âˇ z2 | = |z1 | Âˇ |z2 |.
(g) |z1 + z2 | â¤ |z1 | + |z2 |.
Beweis. UĚbung

44

(Dreiecksungleichung)

2.5. Der KoĚrper der komplexen Zahlen
Mit dem Wissen aus (c) dieses Satzes erklaĚrt sich nun auch ruĚckwirkend die
zunaĚchst unintuitive Wahl des multiplikativen Inversen im Beweis von Satz 2.5.2.
Satz 2.5.13. Es gibt keine Totalordnung auf C, die C zu einem angeordneten
KoĚrper macht.
Beweis. Wir nehmen an, es gaĚbe eine solche Totalordnung â¤â. Nach UĚbungsâ
aufgabe 2.4.17 (b) gilt dann z 2 âĽ 0 fuĚr jedes z â C. Speziell fuĚr z = â1 erhalten
wir also 1 = (â1)2 âĽ 0 und mit z = i erhalten wir â1 = i2 âĽ 0. Das ist nun ein
Widerspruch zu UĚbungsaufgabe 2.4.17 (a). Also kann es solch eine Totalordnung
nicht geben.
Satz 2.5.14 (Fundamentalsatz der Algebra). Es sei n â Nâ und p(z) = an z n +
anâ1 z nâ1 + Âˇ Âˇ Âˇ + a1 z + a0 ein Ploynom mit aj â C fuĚr j = 0, 1, . . . , n und an 6= 0.
Dann hat p eine Nullstelle in C.
Insbesondere zerfaĚllt jedes komplexe Polynom uĚber C in Linearfaktoren.
Bemerkung 2.5.15. Der Fundamentalsatz der Algebra bedeutet, dass jede polynomiale Gleichung uĚber C loĚsbar ist (ja, auĂer 3 = 5 und AĚhnlichem natuĚrlich. . . ).
Der entsprechende SchoĚnheitsfehler von R, wo z.B. die Gleichung x2 +1 = 0 keine
LoĚsung besitzt, ist damit durch die Zahlerweiterung nach C behoben. Wir werden
diese Eigenschaft von C noch sehr zu schaĚtzen lernen.

45

3. Lineare Algebra
3.1. VektorraĚume
3.1.1. Das Axiomensystem und Beispiele
Definition 3.1.1. (a) Sei V eine Menge und K ein KoĚrper. Weiter seien zwei
VerknuĚpfungen
+ : V Ă V â V,
Âˇ :KĂV âV

(Vektoraddition)
(Skalar-Multiplikation)

gegeben. Die Menge V mit diesen beiden VerknuĚpfungen heiĂt dann Vektorraum uĚber K oder auch K-Vektorraum, falls die folgenden Axiome erfuĚllt
sind:
(V1) (V, +) ist eine abelsche Gruppe.
(V2) âv â V : 1 Âˇ v = v.

(V3) âv â V âÎą, Î˛ â K : (ÎąÎ˛) Âˇ v = Îą Âˇ (Î˛ Âˇ v).

(V4) âv â V âÎą, Î˛ â K : (Îą + Î˛) Âˇ v = Îą Âˇ v + Î˛ Âˇ v.

(V5) âv, w â V âÎą â K : Îą Âˇ (v + w) = Îą Âˇ v + Îą Âˇ w.
(b) Das neutrale Element der Gruppe (V, +) wird als Nullvektor bezeichnet und
die Elemente des zugrundeliegenden KoĚrpers K nennt man Skalare.
Ist speziell K = R, bzw. K = C, so spricht man von einem reellen, bzw.
komplexen Vektorraum.
VektorraĚume spielen in vielen Bereichen der Mathematik eine fundamentale Rolle.
Wir wollen das mit einem Stapel verschiedener Beispiele andeuten.
Beispiel 3.1.2.

(a) Der Raum K n der n-Tupel

Sei K ein KoĚrper und n â Nâ . Dann ist
ďŁź
ďŁąďŁŤ ďŁś
ďŁ´
ďŁ´
x
1
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁ˝
ďŁ˛ďŁŹ x2 ďŁˇ
ďŁŹ ďŁˇ
n
:
x
â
K
fuĚr
j
=
1,
2,
.
.
.
,
n
=
K := K
Ă
K
Ă
Âˇ
Âˇ
Âˇ
Ă
K
ďŁŹ
ďŁˇ
.
j
{z
} ďŁ´
|
ďŁ´
ďŁ­ .. ďŁ¸
ďŁ´
ďŁ´
ďŁ´
ďŁ´
n Mal
ďŁž
ďŁł x
n
47

3. Lineare Algebra
 x 1   y1 
.. , .. â K n und Îą â K durch
mit den fuĚr
.
.
xn

yn

ďŁś
ďŁś ďŁŤ ďŁś ďŁŤ
x1 + y1
y1
x1
ďŁŹx2 ďŁˇ ďŁŹ y2 ďŁˇ ďŁŹ x2 + y2 ďŁˇ
ďŁˇ
ďŁŹ ďŁˇ ďŁŹ ďŁˇ ďŁŹ
ďŁŹ .. ďŁˇ + ďŁŹ .. ďŁˇ = ďŁŹ .. ďŁˇ
ďŁ­.ďŁ¸ ďŁ­.ďŁ¸ ďŁ­ . ďŁ¸
ďŁŤ

xn

yn

xn + yn

ďŁś
ďŁś ďŁŤ
Îąx1
x1
ďŁŹ x2 ďŁˇ ďŁŹ Îąx2 ďŁˇ
ďŁˇ
ďŁŹ ďŁˇ ďŁŹ
und Îą Âˇ ďŁŹ .. ďŁˇ = ďŁŹ .. ďŁˇ
ďŁ­.ďŁ¸ ďŁ­ . ďŁ¸
ďŁŤ

xn

Îąxn

0
gegebenen VerknuĚpfungen ein K-Vektorraum mit ... als Nullvektor. Das
0

Nachrechnen der Axiome ist eine leichte UĚbung.

Im Falle K = R ist Rn der sogenannte reelle Standardvektorraum.
Aus GruĚnden, die erst spaĚter klar werden werden, ist es sinnvoll die Elemente von K n als Spalten zu schreiben. Da das aber in der schriftlichen
Darstellung manchmal sehr viel Platz verbraucht, fuĚhren wir die Notation
ďŁś
x1
ďŁŹ x2 ďŁˇ
ďŁŹ ďŁˇ
(x1 , x2 , . . . , xn )T := ďŁŹ .. ďŁˇ
ďŁ­.ďŁ¸
ďŁŤ

xn

und

ďŁśT
x1
ďŁŹ x2 ďŁˇ
ďŁŹ ďŁˇ
ďŁŹ .. ďŁˇ := (x1 , x2 , . . . , xn )
ďŁ­.ďŁ¸
ďŁŤ

xn

ein. Das Tâ macht also formal aus einem Zeilenvektor einen Spaltenvektor
â
und umgekehrt. Man liest xT als x transponiertâ.
â
(b) Der Raum der p Ă n-Matrizen

Seien K ein KoĚrper und p, n â Nâ . Dann ist K pĂn der Vektorraum aller
Matrizen mit p Zeilen und n Spalten, d.h.
ďŁąďŁŤ
ďŁź
ďŁś
ďŁ´
ďŁ´
Îą11 Îą12 . . . Îą1n
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁ˛ďŁŹ Îą21 Îą22 . . . Îą2n ďŁˇ
ďŁ˝
ďŁŹ
ďŁˇ
pĂn
K
:= ďŁŹ ..
.. . .
. ďŁˇ : Îąj,k â K, j = 1, . . . , p, k = 1, . . . , n .
ďŁ´
ďŁ´
ďŁ­ .
. .. ďŁ¸
.
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁł Îą Îą ... Îą
ďŁž
p1
p2
pn

Eine Matrix ist also ein rechteckiges Schema aus pn Elementen aus K.
Wie kann man nun mit solchen Monstern rechnen und wozu ist das gut?
Die Antwort auf die zweite Frage werde ich Ihnen im weiteren Verlauf der
Vorlesung naĚher bringen, zunaĚchst wollen wir fuĚr die Matrizen eine Addition
und eine Skalar-Multiplikation definieren.
Seien also A = (Îąjk )j=1,...,p,k=1,...,n und B = (Î˛jk )j=1,...,p,k=1,...,n Matrizen aus
K pĂn , sowie Îť â K. Dann definieren wir beide VerknuĚpfungen im Prinzip

48

3.1. VektorraĚume
wie in (a) komponentenweise durch
A + B = (Îąjk )j=1,...,p,k=1,...,n + (Î˛jk )j=1,...,p,k=1,...,n := (Îąjk + Î˛jk )j=1,...,p,k=1,...,n
ďŁŤ
ďŁś
Îą11 + Î˛11 Îą12 + Î˛12 . . . Îą1n + Î˛1n
ďŁŹ Îą21 + Î˛21 Îą22 + Î˛22 . . . Îą2n + Î˛2n ďŁˇ
ďŁŹ
ďŁˇ
=ďŁŹ
ďŁˇ
..
..
..
.
.
ďŁ­
ďŁ¸
.
.
.
.
Îąp1 + Î˛p1 Îąp2 + Î˛p2 . . . Îąpn + Î˛pn
und

ÎťA = Îť(Îąjk )j=1,...,p,k=1,...,n
ďŁŤ
ÎťÎą11 ÎťÎą12 . . .
ďŁŹ ÎťÎą21 ÎťÎą22 . . .
ďŁŹ
= ďŁŹ ..
..
..
ďŁ­ .
.
.
ÎťÎąp1 ÎťÎąp2 . . .

:= (ÎťÎąjk )j=1,...,p,k=1,...,n
ďŁś
ÎťÎą1n
ÎťÎą2n ďŁˇ
ďŁˇ
.. ďŁˇ .
. ďŁ¸
ÎťÎąpn

Der Nullvektor, der hier auch Nullmatrix genannt wird, ist die Matrix, deren
EintraĚge alle Null sind. Das Nachrechnen der Axiome ist hier naturgemaĚĂ
etwas muĚhsamer als in (a) aber genauso elementar.
Hier ist noch ein konkretes Beispiel fuĚr das Rechnen mit Matrizen uĚber Q,
bzw. R, bzw. C.
ďŁŤ
ďŁś
ďŁŤ
ďŁś
3
1
2
â2 0
3
ďŁ­ â2 5
1 ďŁ¸ + 2 ďŁ­ 1 â2 â2 ďŁ¸
1 â1 â1
1
0
0
ďŁŤ
ďŁś ďŁŤ
ďŁś ďŁŤ
ďŁś
3
1
2
â4 0
6
â1 1
8
1 ďŁ¸ + ďŁ­ 2 â4 â4 ďŁ¸ = ďŁ­ 0
1 â3 ďŁ¸ .
= ďŁ­ â2 5
1 â1 â1
2
0
0
3 â1 â1
(c) FunktionenraĚume
Sei K ein KoĚrper und M eine Menge. Die Menge Abb(M, K) aller Funktionen von M nach K ist mit den fuĚr f, g â Abb(M, K) und Îą â K definierten
VerknuĚpfungen
(
(
M âK
M âK
und
Îąf :
f +g :
x 7â Îąf (x)
x 7â f (x) + g(x)
ein K-Vektorraum.
Dies wollen wir exemplarisch ausfuĚhrlich beweisen. ZunaĚchst beobachten
wir, das die beiden oben definierten VerknuĚpfungen in dem Sinne wohldefiniert sind, dass sie als Ergebnis jeweils immer wieder ein Element von
Abb(M, K) liefern. Es bleiben also die Axiome (V1) bis (V5) zu zeigen.

49

3. Lineare Algebra
(V 1) ZunaĚchst ist die VerknuĚpfung +â assoziativ, denn fuĚr je drei Funkâ
tionen f, g, h â Abb(M, K) gilt dank der AssoziativitaĚt der Addition
in K fuĚr alle x â M



(f + g) + h (x) = (f + g)(x) + h(x) = f (x) + g(x) + h(x)

= f (x) + g(x) + h(x) = f (x) + (g + h)(x)


= f + (g + h) (x).
Also ist (f + g) + h = f + (g + h).

Genauso bekommt man die KommutativitaĚt wegen
(f + g)(x) = f (x) + g(x) = g(x) + f (x) = (g + f )(x).
Als neutrales Element der Addition identifizieren wir die Nullabbildung o : M â K mit o(x) = 0 fuĚr alle x â M. Mit dieser gilt naĚmlich
fuĚr alle f â Abb(M, K) und alle x â M
(f + o)(x) = f (x) + o(x) = f (x) + 0 = f (x),
also ist f + o = f fuĚr jedes f â Abb(M, K).

SchlieĂlich findet man zu jedem f â Abb(M, K) das additiv inverse
Element âf : M â K mit (âf )(x) = âf (x), x â M, denn fuĚr dieses
gilt fuĚr jedes x â M
(f + (âf ))(x) = f (x) + (âf )(x) = f (x) â f (x) = 0 = o(x),
und damit haben wir f + (âf ) = o.

(V 2) Sei f â Abb(M, K). Dann gilt (1 Âˇ f )(x) = 1 Âˇ f (x) = f (x) fuĚr alle
x â M, also ist 1 Âˇ f = f .
(V 3) Seien Îą, Î˛ â K und f â Abb(M, K). Dann gilt fuĚr jedes x â M unter
Ausnutzung der AssoziativitaĚt der Multiplikation in K


 

(ÎąÎ˛) Âˇ f (x) = (ÎąÎ˛)f (x) = Îą(Î˛f (x)) = Îą (Î˛ Âˇ f )(x) = Îą Âˇ (Î˛ Âˇ f ) (x)
und damit (ÎąÎ˛) Âˇ f = Îą Âˇ (Î˛ Âˇ f ).

(V 4) Seien Îą, Î˛ â K und f â Abb(M, K). Dann gilt fuĚr jedes x â M unter
Ausnutzung des Distributivgesetzes in K


(Îą + Î˛) Âˇ f (x) = (Îą + Î˛)f (x) = Îąf (x) + Î˛f (x)


= (Îą Âˇ f )(x) + (Î˛ Âˇ f )(x) = Îą Âˇ f + Î˛ Âˇ f (x).
Das liefert wieder (Îą + Î˛) Âˇ f = Îą Âˇ f + Î˛ Âˇ f .

50

3.1. VektorraĚume
(V 5) Es seien Îą â K und f, g â Abb(M, K). Dann gilt fuĚr jedes x â M
wieder dank des Distributivgesetzes



Îą Âˇ (f + g) (x) = Îą(f + g)(x) = Îą f (x) + g(x) = Îąf (x) + Îąg(x)


= (Îą Âˇ f )(x) + (Îą Âˇ g)(x) = Îą Âˇ f + Îą Âˇ g (x)
und wir bekommen Îą Âˇ (f + g) = Îą Âˇ f + Îą Âˇ g wie gefordert.

(d) Der Raum aller Folgen in K
Als Spezialfall von (c) erhalten wir mit M = N den Raum F = Abb(N, K)
aller Folgen in K. FuĚr ein Element a â F schreibt man fuĚr das Bild von
n â N unter a statt a(n) uĚblicherweise an und gibt die Abbildung a als
unendliche Listeâ der Bilder (a1 , a2 , a3 , . . . ) an. Beispiele fuĚr Folgen in R
â
sind
 1 1 1
  1 
1, , , , . . . =
,
2 3 4
n + 1 nâN
(1, 2, 4, 8, 16, . . . ) = (2n )nâN ,

(1, â1, 1, â1, 1, â1, 1, . . . ) = (â1)n nâN .

In dieser Schreibweise lesen sich die VerknuĚpfungen aus F = Abb(N, K)
mit a, b â F und Îą â K so:
a + b = (an )nâN + (bn )nâN = (a1 , a2 , a3 , . . . ) + (b1 , b2 , b3 , . . . )
:= (a1 + b1 , a2 + b2 , a3 + b3 , . . . ) = (an + bn )nâN und
Îą Âˇ a = Îą Âˇ (an )nâN = Îą Âˇ (a1 , a2 , a3 , . . . ) := (Îąa1 , Îąa2 , Îąa3 , . . . ) = (Îąan )nâN .
(e) Der Raum aller endlichen Folgen
Betrachtet man die Teilmenge
c00 := {a â F : an 6= 0 nur fuĚr endlich viele n â N},
von F aus (d), so ist auch diese mit den VerknuĚpfungen aus F ein KVektorraum.
(k)

Elemente von c00 sind z.B. fuĚr jedes k â N die Folgen e(k) mit ej = 1 fuĚr
j = k und Null sonst, d.h.
e(0) = (1, 0, 0, 0, 0, 0, 0, . . . ),

e(1) = (0, 1, 0, 0, 0, 0, 0, 0, . . . ),

e(2) = (0, 0, 1, 0, 0, 0, 0, . . . ),

e(3) = (0, 0, 0, 1, 0, 0, 0, 0, . . . ),

usw.

Verwendet man fuĚr j, k â Z das sogenannte Kronecker-Delta, d.h. die
Schreibweise
(
1, falls j = k,
Î´jk :=
0, falls j 6= k,

51

3. Lineare Algebra
so kann man diese speziellen Elemente kurz beschreiben durch
e(k) = (Î´jk )jâN ,

k â N.

Bemerkung 3.1.3. In jedem Vektorraum V gelten fuĚr die abelsche Gruppe
(V, +) natuĚrlich alle Ergebnisse aus dem Abschnitt uĚber Gruppen. Insbesonere ist
also der Nullvektor und das additive Inverse jeweils eindeutig bestimmt. Ebenso
uĚbernehmen wir fuĚr u, v â G die Schreibweise u â v fuĚr u + (âv). SchlieĂlich
bemerken wir noch, dass nach unseren Erkenntnissen uĚber Gruppen die Gleichung
a + x = b fuĚr jede Vorgabe von a, b â V in V eindeutig loĚsbar ist.
Satz 3.1.4. Es sei V ein K-Vektorraum mit Nullvektor 0V . Dann gilt fuĚr jedes
Îą â K und alle v â V
(a) Îą Âˇ v = 0V ââ (Îą = 0 oder v = 0V ).
(b) (âÎą) Âˇ v = â(Îą Âˇ v), insbesondere ist (â1) Âˇ v = âv.
Beweis. Wir beweisen nur (a), der Teil (b) verbleibt als UĚbung.
Zum Nachweis von ââ in (a) sei zunaĚchst Îą = 0. Dann gilt
â
(V2)

(V4)

(V2)

v = 1 Âˇ v = (1 + 0) Âˇ v = 1 Âˇ v + 0 Âˇ v = v + 0 Âˇ v.
Nach Bemerkung 3.1.3 hat die Gleichung v = v + x genau eine LoĚsung in V . Da
0V nach (V1) eine LoĚsung ist, muss also 0 Âˇ v = 0V sein, wie gewuĚnscht.
Sei nun v = 0V . Wir beobachten, dass fuĚr jedes Îą â K und w â V gilt
(V1)

(V5)

Îą Âˇ w = Îą Âˇ (w + 0V ) = Îą Âˇ w + Îą Âˇ 0V .
Auch die eindeutig loĚsbare Gleichung Îą Âˇ w = Îą Âˇ w + x hat wieder x = 0V als
LoĚsung. Also ist Îą Âˇ 0V = 0V .
Es bleibt noch ââ zu zeigen. Seien also Îą â K und v â V mit Îą Âˇ v = 0V
â
gegeben. Ist Îą = 0 so sind wir fertig, wir betrachten also den Fall Îą 6= 0. Dann
gilt
(V2)

Îą 6= 0

(V3)

ââ

v = 1 Âˇ v = (Îąâ1 Îą) Âˇ v = Îąâ1 Âˇ (Îą Âˇ v) = Îąâ1 Âˇ 0V â= 0V .

3.1.2. Exkurs: MinimalitaĚt und Widerspruchsfreiheit von
Axiomensystemen
Neue algebraische Strukturen, wie Gruppen, Ringe, KoĚrper und VektorraĚume
haben wir jeweils uĚber die Formulierung eines Axiomensystems, also eines Satzes
von a priori als wahr angenommenen Aussagen, definiert. Nun kann man auf
diese Weise natuĚrlich erst einmal so gut wie alles definieren, es stellt sich nur
die Frage was sinnvoll ist. Dabei soll es hier jetzt nicht darum gehen, welche
Axiomensysteme sich in der Mathematik und auĂerhalb als besonders nuĚtzlich

52

3.1. VektorraĚume
erweisen, sondern um die Frage, wann ein Axiomensystem aus mathematischer
Sicht vernuĚnftig gebildet ist.
Der wichtigste Begriff ist in diesem Zusammenhang die Widerspruchsfreiheit.
Man nennt ein Axiomensystem widerspruĚchlich, falls es eine Aussage A gibt, so
dass sowohl A als auch ÂŹA aus dem System bewiesen werden koĚnnen. Ist dies
nicht der Fall, so nennt man das Axiomensystem widerspruchsfrei.
Widerspruchsfreiheit kann man am besten beweisen, indem man ein Modell der
durch das Axiomensystem definierten Struktur angibt, also ein moĚglichst konkretes Beispiel, das das Axiomensystem erfuĚllt. Dies haben wir fuĚr alle bisher
behandelten Axiomensysteme jeweils mehr oder weniger getan, diese sind alle
widerspruchsfrei.
Eine weitere Eigenschaft, die fuĚr ein Axiomensystem nicht unverzichtbar ist, die
man aber meist moĚchte, ist die MinimalitaĚt. Dabei heiĂt ein solches System minimal , wenn keins der Axiome des Systems aus den uĚbrigen Axiomen bewiesen
werden kann.
Ist ein Axiomensystem nicht minimal, so wird meist das uĚberfluĚssigeâ Axiom
â
aus dem System entfernt, das dadurch seinen mathematischen Gehalt ja nicht
aĚndert.
Schauen wir uns unter diesem Aspekt unser Axiomensystem des Vektorraums
noch einmal an, so stellen wir fest, dass dieses nicht minimal ist, denn es gilt der
folgende Satz.
Satz 3.1.5. Die KommutativitaĚt von (V, +) folgt aus den anderen VektorraumAxiomen.
Beweis. Seien a, b â V . Dann gilt
(V1)

(V1)

(V1)

a + b = 0 + a + b = b + (âb) + a + b = b + a + (âa) + (âb) + a + b
3.1.4

= b + a + (â1) Âˇ a + (â1) Âˇ b + a + b


3.1.4
= b + a + (â1) Âˇ (a + b) + (a + b) = b + a + â(a + b) + (a + b)

(V5)

(V1)

= b + a.

Damit der Beweis wirklich vollstaĚndig ist, muĚssen Sie natuĚrlich alle noch nachschauen, dass Sie auch beim Beweis von Satz 3.1.4 (b) die KommutativitaĚt von
(V, +) nicht verwendet haben.
Wir wollen noch beispielhaft zeigen, dass das Axiom (V2) nicht verzichtbar ist.
Wie macht man, das? Man gibt ein Modell an, dass alle Axiome (V1), (V3), (V4)
und (V5) erfuĚllt, aber (V2) verletzt.
Beispiel 3.1.6. Wir nehmen K = R und betrachten die Gruppe (Z, +). Dann
definieren wir die Skalar-Multiplikation â : R Ă Z â Z mit Îą â v = 0 fuĚr jede
Wahl von Îą â R und v â Z. Man sieht sehr schnell ein, dass dann alle Axiome
auĂer (V2) erfuĚllt, aber dieses verletzt ist.

53

3. Lineare Algebra

3.1.3. Die Summenschreibweise
In VektorraĚumen wird viel addiert und wir werden in den naĚchsten Kapiteln
Unmengen Summen mit einer variablen Anzahl, also z.B. n, Summanden haben.
Dazu fuĚhren wir folgende sehr praktische Notation ein, die Sie sich unbedingt
angewoĚhnen sollten.
Definition 3.1.7. Sei n â N und a0 , a1 , a2 , . . . , an seien Elemente einer kommutativen additiven Struktur, also z.B. eines Vektorraums, KoĚrpers oder Rings,
oder auch einer abelschen Gruppe, deren VerknuĚpfung additiv geschrieben wird.
Dann schreibt man
n
X
aj := a0 + a1 + a2 + Âˇ Âˇ Âˇ + an .
j=0

Die Variable, die die Summanden hochzaĚhlt, in obigem Beispiel j, heiĂt Summationsindex.
In Erweiterung obiger Definition schreibt man auch
9
X
j=3

j

3

4

5

2 = 2 +2 +2 +ÂˇÂˇÂˇ+2

9

oder

â
X

xj = x + x2 + x3 + x4 + . . .

j=1

mit hoffentlich intuitiv klarer Bedeutung. Zumindest sollte jeder/m, die/der schon
mal eine Schleife programmiert hat, klar sein was hier passiert.
Im folgenden Beispiel kann man einige oft verwendete Rechenregeln fuĚr das Summenzeichen finden.
Beispiel 3.1.8.
9
X
k=3

(a)
5

5

5

5

5

(k â 3) = 0 + 1 + 2 + Âˇ Âˇ Âˇ + 6 =

6
X

k5

(Indexshift)

k=0

(b) Seien n â Nâ , V ein K-Vektorraum und Îą â K, sowie a1 , a2 , . . . , an â V .
Dann ist
ÎąÂˇ

n
X
k=1

ak = Îą Âˇ (a1 + a2 + Âˇ Âˇ Âˇ + an ) = Îąa1 + Îąa2 + Âˇ Âˇ Âˇ + Îąan =

n
X

ÎąÎąk .

k=1

So einfach ist Ausmultiplizieren und Ausklammern mit dem Summenzeichen.

54

3.2. UntervektorraĚume, Basis und Dimension

3.2. UntervektorraĚume, Basis und Dimension
3.2.1. UntervektorraĚume
Definition 3.2.1. Sei V ein K-Vektorraum. Eine Teilmenge U von V heiĂt
Untervektorraum von V , falls U mit den VerknuĚpfungen von V ebenfalls ein KVektorraum ist.
Bemerkung 3.2.2. Die Teilmengen {0V } und V sind in jedem Vektorraum V
UntervektorraĚume.
Satz 3.2.3 (Untervektorraumkriterium). Eine Teilmenge U eines Vektorraums
V ist genau dann ein Untervektorraum von V , wenn
(UVR1) U 6= â und
(UVR2) âa, b â U âÎť, Âľ â K : Îťa + Âľb â U.
gelten.
Beweis. ââ Sei U ein Untervektorraum von V . Da damit U ein Vektorraum ist,
â
muss U nach (V1) zumindest einen Nullvektor enthalten, also gilt (UVR1).
Seien zum Nachweis von (UVR2) nun a, b â U und Îť, Âľ â K. Da U mit
den VerknuĚpfungen aus V ein K-Vektorraum ist, muss Âˇ : K Ă U â U und
+ : U Ă U â U gelten. Damit sind zunaĚchst Îťa und Âľb in U und dann auch
Îťa + Âľb.
ââ Wir muĚssen zeigen, dass wir aus der Information, dass U eine Teilmenge
â
eines K-Vektorraums ist, und dass (UVR1) und (UVR2) gelten, schon nachweisen koĚnnen, dass U selbst ein K-Vektorraum ist. Setzen wir in (UVR2)
Îť = Âľ = 1, so erhalten wir, dass fuĚr alle a, b â U gilt a + b â U, also
ist + : U Ă U â U schon mal eine vernuĚnftige VerknuĚpfung auf U. Setzt
man Âľ := 0 und b := a, so erhaĚlt man das selbe Resultat fuĚr die SkalarMultiplikation.
Der Nachweis der AssoziativitaĚt und der KommutativitaĚt von +â, sowie
â
von (V2)â(V5) ergibt sich jeweils direkt aus den entsprechenden Eigenschaften von V . Was wirklich zu zeigen bleibt, ist die Existenz eines neutralen
Elements und der additiv inversen Elemente in (U, +).
Nach (UVR1) ist U 6= â, also gibt es irgendein u â U. Mit Hilfe von (UVR2)
muss dann auch 0 Âˇ u = 0V â U sein und dieses Element ist natuĚrlich auch
in U ein neutrales, denn es gilt ja sogar v + 0V = v fuĚr alle v â V .
Sei nun a â U gegeben. Dann ist wiederum nach (UVR2) auch das Element
âa = (â1) Âˇ a â U und wir sind fertig.

55

3. Lineare Algebra
Beispiel 3.2.4. (a) Der Vektorraum c00 der endlichen Folgen in K, den wir in
Beispiel 3.1.2 (e) kennengelernt haben, ist ein Untervektorraum des Raums
aller Folgen aus Beispiel 3.1.2 (d).
(b) In Abb(R, R) betrachten wir die Menge U aller Funktionen f : R â R,
fuĚr die f (0) = 0 gilt und weisen nach, das dies ein Untervektorraum ist.
ZunaĚchst hat die Funktion f : R â R mit f (x) = x3 â Ďx2 + 15x diese
Eigenschaft, also ist U nicht leer, d.h. (UVR1) gilt. Seien nun f, g â U und
Îť, Âľ â R. Dann gilt
(Îťf + Âľg)(0) = Îťf (0) + Âľg(0) = Îť Âˇ 0 + Âľ Âˇ 0 = 0,
also ist auch Îťf + Âľg â U und wir haben (UVR2).
(c) Im Standardvektorraum R2 kann man alle UntervektorraĚume angeben. Neben den immer vorhandenen {0} und R2 sind das genau die durch Ursprungsgeraden beschriebenen Mengen.
Definition 3.2.5. Seien V ein K-Vektorraum, n â Nâ und a1 , a2 , . . . , an â V ,
sowie Îą1 , Îą2 , . . . , Îąn â K. Dann heiĂt
n
X

Îąj aj

j=1

eine Linearkombination von a1 , a2 , . . . , an .
Beispiel 3.2.6. In R2 ist der Vektor (1, 6)T eine Linearkombination von a1 =
(1, 2)T , a2 = (0, 1)T und a3 = (0, 2)T , z.B. mit
(1, 6)T = a1 â 2a2 + 3a3

oder

(1, 6)T = a1 + 4a2 + 0a3 .

Definition 3.2.7. Sei V ein K-Vektorraum und M â V . Dann heiĂt

hMi := v â V : v ist Linearkombination von Vektoren aus M
n
o
n
X
â
= v â V : ân â N âÎą1 , . . . , Îąn â K âm1 , . . . mn â M mit v =
Îąj mj
j=1

lineare HuĚlle von M.
Ist M = {m1 , m2 , . . . , mn } endlich, so schreibt man auch hm1 , m2 , . . . , mn i statt
h{m1 , m2 , . . . , mn }i.
SchlieĂlich setzen wir hâi = {0}.
Satz 3.2.8. Sei V ein K-Vektorraum und M â V . Dann ist hMi ein Untervektorraum von V .

56

3.2. UntervektorraĚume, Basis und Dimension
Beweis. Ist M = â, so ist hMi = {0} und damit ein Untervektorraum. Wir
betrachten also den Fall M 6= â. Da jedes Element m â M sich als die Linearkombination 1 Âˇ m mit Elementen aus M schreiben laĚsst, ist immer M â hMi und
damit insbesondere auch hMi =
6 â. Zum Nachweis von (UVR2) seien u, v â hMi
und Îť, Âľ â K. Dann existieren n, p â Nâ , sowie Îą1 , . . . , Îąn , Î˛1 , . . . , Î˛p â K und
a1 , . . . , an , b1 , . . . bp â M mit
u=

n
X

Îąj aj

und

v=

j=1

p
X

Î˛k bk .

k=1

Also ist
Îťu + Âľv = Îť

n
X

Îąj aj + Âľ

j=1

p
X
k=1

p
n
X
X
(ÂľÎ˛k )bk
Î˛k bk =
(ÎťÎąj )aj +
j=1

k=1

und dieses ist wiederum eine Linearkombination von endlich vielen Elementen
a1 , . . . , an , b1 , . . . , bp â M, also ist Îťu + Âľv â hMi und damit ist (UVR2) erfuĚllt.
UĚbungsaufgabe 3.2.9. Seien V ein K-Vektorraum und M, M1 , M2 Teilmengen
von V . Dann gilt
(a) M1 â M2 =â hM1 i â hM2 i.
(b) M = hMi ââ M Untervektorraum von V .
UĚbungsaufgabe 3.2.10. In Definition 2.3.10 im Abschnitt uĚber Gruppen haben
wir das (Gruppen-)Erzeugnis definiert, dessen Idee sehr an das der linearen HuĚlle
erinnert: Finde eine moĚglichst kleine Unterstruktur, die die gegebene Teilmenge
aber komplett enthaĚlt. Dort war das Vorgehen allerdings ein anderes. Ziel dieser
Aufgabe ist es, zu sehen, dass das dort verwendete Verfahren ebenfalls zur Definition der linearen HuĚlle verwendet werden kann und das dabei auch genau das
selbe herauskommt. Beweisen sie dazu die beiden folgenden Aussagen fuĚr einen
K-Vektorraum V .
(a) Ist
T U â P(V ) ein Mengensystem von UntervektorraĚumen von V , so ist auch
U âU U ein Untervektorraum von V .

(b) Sei M â V und

U := {U â P(V ) : M â U und U Untervektorraum von V }.
Dann gilt

\

U âU

U = hMi.

57

3. Lineare Algebra

3.2.2. Lineare UnabhaĚngigkeit und Basen
Definition 3.2.11. Sei V ein K-Vektorraum und M â V .
(a) Die Menge M heiĂt linear abhaĚngig, falls es eine nichttriviale Linearkombination des Nullvektors aus Elementen von M gibt, d.h. wenn es ein n â Nâ ,
Vektoren v1 , v2 , . . . , vn â P
M und Koeffizienten Îą1 , Îą2 , . . . , Îąn â K gibt, die
nicht alle Null sind, mit nj=1 Îąj vj = 0V .
(b) Ist M nicht linear abhaĚngig, so nennt man M linear unabhaĚngig.

Beispiel 3.2.12.

(a) Die Teilmenge von R2 , gegeben durch
     
2
1
5
,
,
0
1
1

ist linear abhaĚngig, denn
 
 
   
5
1
2
0
1Âˇ
â1Âˇ
â2Âˇ
=
,
1
1
0
0
(b) Betrachtet man in R2 hingegen die Menge
   
0
1
,
,
1
0
so ist diese linear unabhaĚngig, denn aus
   
 
0
0
1
=
+Î˛Âˇ
ÎąÂˇ
0
1
0
folgt Îą = 0 und Î˛ = 0.
(c) Im R-Vektorraum Abb(R, R) betrachten wir die Menge {f, g} := {x 7â
x2 , x 7â x}. Diese ist linear unabhaĚngig, denn hat man Îą, Î˛ â R mit Îąf +
Î˛g = o, so folgt Îąx2 + Î˛x = 0 fuĚr jedes x â R. Setzt man speziell x = 1,
bzw. x = â1 ein, so erhaĚlt man Îą + Î˛ = 0, bzw. Îą â Î˛ = 0. Addition der
beiden Gleichungen liefert dann 2Îą = 0 und damit zuerst Îą = 0 und dann
Î˛ = 0.
(d) In jedem Vektorraum ist {0} linear abhaĚngig, denn 1 Âˇ 0 = 0 ist eine nichttriviale Linearkombination des Nullvektors.
Bemerkung 3.2.13. Linearkombinationen sind immer endliche Summen, d.h.
eine Teilmenge M eines Vektorraums ist genau dann linear unabhaĚngig, wenn fuĚr
jede Wahl von n â Nâ und von Vektoren v1 , . . . , vn â M gilt
n
X
j=1

58

Îąj vj = 0 =â Îą1 = Îą2 = Âˇ Âˇ Âˇ = Îąn = 0.

3.2. UntervektorraĚume, Basis und Dimension
Satz 3.2.14. Es seien V ein K-Vektorraum, n â Nâ und v1 , . . . , vn â V . Dann
gilt
(a) Die Vektoren v1 , . . . , vn sind genau dann linear abhaĚngig, wenn einer von
ihnen eine Linearkombination der n â 1 anderen ist.
(b) Ist p â¤ n und sind v1 , . . . , vn linear unabhaĚngig, so sind auch v1 , . . . , vp
linear unabhaĚngig.
(c) Ist p â¤ n und sind v1 , . . . , vp linear abhaĚngig, so sind auch v1 , . . . , vn linear
abhaĚngig.
(d) Bildet man n + 1 Linearkombinationen w1 , . . . , wn+1 aus v1 , . . . , vn , so sind
w1 , . . . , wn+1 linear abhaĚngig.
Beweis. (a) Wir beweisen zunaĚchst =ââ. Seien dazu v1 , . . . , vn â V linear
â
abhaĚngig.
Dann existieren Îą1 , . . . , Îąn â K, die nicht alle Null sind, so dass
Pn
j=1 Îąj vj = 0 gilt. Sei j0 â {1, . . . , n} so gewaĚhlt, dass Îąj0 6= 0 ist. Dann
haben wir
n
X
Îąj0 vj0 +
Îąj vj = 0.
j=1
j6=j0

Also ergibt sich dank Îąj0 6= 0
vj0 = â

n
n
X
1 X
âÎąj
vj ,
Îąj vj =
Îąj0 j=1
Îąj 0
j=1
j6=j0

j6=j0

was eine Linearkombination der n â 1 uĚbrigen Vektoren ist.

Zum Nachweis der umgekehrten Implikation â=â, sei j0 ein Index, fuĚr den
â
vj0 eine Linearkombination der uĚbrigen Vektoren ist. Das bedeutet, dass es
Îą1 , . . . , Îąj0 â1 , Îąj0 +1 , . . . , Îąn â K gibt mit
vj0 =

n
X
j=1
j6=j0

Îąj vj ,

also ist 1 Âˇ vj0 â

n
X

Îąj vj = 0

j=1
j6=j0

und wir haben wegen 1 6= 0 eine nichttriviale Linearkombination des Nullvektors gefunden. Also sind v1 , . . . , vn linear abhaĚngig.
(b) UĚbung
(c) UĚbung
(d) ohne Beweis

59

3. Lineare Algebra
Definition 3.2.15. Sei V ein K-Vektorraum. Eine Teilmenge B â V heiĂt Basis
von V , falls
(B1) B ist linear unabhaĚngig und
(B2) hBi = V , d.h. B erzeugt V

gelten.

Beispiel 3.2.16. (a) Die Menge {(1, 0)T , (0, 1)T } â R2 aus Beispiel 3.2.12 (b)
ist eine Basis von R2 , denn erstens ist sie nach diesem Beipiel linear unabhaĚngig und zweitens gilt fuĚr alle (Îą, Î˛) â R2
 
 
 
0
1
Îą
.
+Î˛Âˇ
=ÎąÂˇ
1
0
Î˛
(b) Genauso ist auch die Teilmenge von Rn , die gegeben ist durch
ďŁąďŁŤ ďŁś ďŁŤ ďŁś
ďŁŤ ďŁśďŁź
ďŁ´
0 ďŁ´
0
1
ďŁ´
ďŁ´
ďŁ´
ďŁˇďŁ´
ďŁ´
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁ´
ďŁ´
0
1
0
ďŁ´
ďŁŹ ďŁˇďŁ´
ďŁŹ ďŁˇ ďŁŹ ďŁˇ
ďŁ´
ďŁ´
ďŁ˝
ďŁŹ 0 ďŁˇďŁ´
ďŁ˛ďŁŹ 0 ďŁˇ ďŁŹ 0 ďŁˇ
ďŁŹ ďŁˇ
ďŁŹ ďŁˇ ďŁŹ ďŁˇ
ďŁŹ .. ďŁˇ , ďŁŹ .. ďŁˇ , . . . , ďŁŹ .. ďŁˇ ,
ďŁŹ . ďŁˇďŁ´
ďŁŹ . ďŁˇ ďŁŹ . ďŁˇ
ďŁ´
ďŁ´
ďŁ´
ďŁŹ ďŁˇďŁ´
ďŁˇ ďŁŹ ďŁˇ
ďŁ´
ďŁ´
ďŁ´ďŁŹ
ďŁ´
ďŁ­ 0 ďŁ¸ďŁ´
ďŁ­ 0 ďŁ¸ ďŁ­ 0 ďŁ¸
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁł 0
1 ďŁž
0
eine Basis des Rn , die sogenannte Standardbasis.

(c) Im Raum c00 der endlichen Folgen betrachten wir fuĚr k â N die Vektoren
e(k) = (Î´jk )jâN , vgl. Beispiel 3.1.2 (e). Dann ist B := {e(k) : k â N} eine
Basis von c00 . Um das einzusehen, beobachten wir zunaĚchst, dass man jede
endliche Folge (Îą1 , Îą2 , . . . , Îąn , 0, 0, . . . ) als Linearkombination
(Îą1 , Îą2 , . . . , Îąn , 0, 0, . . . ) =

n
X

Îąk e(k)

k=1

dieser speziellen Folgen schreiben kann. Zum Nachweis der lineaeren UnabhaĚngigkeit seien n â N, sowie k1 , k2 , . . . , kn â N mit k1 < k2 < k3 < Âˇ Âˇ Âˇ <
kn paarweise verschiedene Indizes und Îą1 , . . . , Îąn â K so, dass
n
X

Îąâ e(kâ ) = 0 = (0)jâN

â=1

gilt. Das heiĂt

(0, 0, 0, . . . ) = 0, . . . , 0, Îą1 , 0, . . . , 0, Îą2 , 0 . . . . . . . . . 0, Îąn , 0, 0, . . . )
und wir sehen Îą1 = Îą2 = Âˇ Âˇ Âˇ = Îąn = 0.

Man beachte, dass wir hier ein Beispiel einer Basis haben, die unendlich
viele Elemente hat.

60

3.2. UntervektorraĚume, Basis und Dimension
(d) Der Nullraum {0} hat immer die Basis â.
Den folgenden sehr nuĚtzlichen Satz wollen wir hier ohne Beweis verwenden.
Satz 3.2.17 (Basissatz, BasisergaĚnzungssatz).
(a) Jeder Vektorraum hat eine Basis.
(b) Ist V ein Vektorraum und M â V linear unabhaĚngig, so gibt es eine Basis
B von V mit M â B.
Eine wichtige Anwendung dieses Resultats ist die folgende Beobachtung.
Satz 3.2.18. Sei V ein Vektorraum und B eine Basis von V mit n â N Elementen. Dann hat jede Basis von V genau n Elemente.
Beweis. Es sei B = {v1 , . . . , vn } und es sei Bâ˛ = {w1 , . . . , wp } eine weitere Basis
von V mit p â N Elementen. Wir nehmen nun an, es waĚre p > n und betrachten die dann vorhandenen Elemente w1 , . . . , wn , wn+1 â Bâ˛ . Da B eine Basis ist,
muĚssen diese alle Linearkombinationen der Vektoren v1 , . . . , vn sein. Das bedeutet
aber nach Satz 3.2.14 (d), dass w1 , . . . , wn+1 linear abhaĚngig sein muĚssen und wir
haben einen Widerspruch, da Bâ˛ eine Basis von V sein soll.
Nehmen wir umgekehrt p < n an, so sind mit den gleichen Argumenten wie oben
die p + 1 Elemente v1 , . . . , vp+1 â B Linearkombinationen der w1 , . . . , wp und
deshalb ist B linear abhaĚngig, was wieder auf einen Widerspruch fuĚhrt.
Damit bleibt nur p = n uĚbrig.
Definition 3.2.19. Es sei V ein Vektorraum. Besitzt V eine n-elementige Basis,
so sagt man V hat die Dimension n und schreibt dim(V ) = n.
Besitzt V keine endliche Basis, so nennt man V unendlichdimensional.
Wir betrachten zur Illustration noch einmal die VektorraĚume aus Beispiel 3.1.2.
Beispiel 3.2.20. (a) Der Standardvektorraum K n : Es gilt dim(K n ) = n, vgl.
Beispiel 3.2.16 (b).
(b) p Ă n-Matrizen: Hier ist dim(K pĂn ) = pn.
(c) FunktionenraĚume: Wir werden spaĚter im Abschnitt 3.6 sehen, dass fuĚr endliche Mengen M gilt dim(Abb(M, K)) = |M|.
(d) FolgenraĚume: Wie man an Beispiel 3.2.16 (c) sieht, ist c00 unendlichdimensional. Selbiges steht dann auch fuĚr den Raum aller Folgen zu vermuten.
KoĚnnen Sie die hinter dieser Vermutung stehende abstrakte Aussage beweisen?
UĚbungsaufgabe 3.2.21. Ist n â Nâ und V ein n-dimensionaler Vektorraum, so
ist jede linear unabhaĚngige Teilmenge von V mit n Elementen eine Basis von V .

61

3. Lineare Algebra
Satz 3.2.22. Seien n â Nâ , sowie V ein n-dimensionaler K-Vektorraum und
B = {b1 , . . . , bn } eine Basis von VP. Dann gibt es fuĚr jedes v â V eindeutig
bestimmte Îą1 , . . . , Îąn â K mit v = nj=1 Îąj bj .
Beweis. Sei
Pnv â V beliebig vorgegeben. Die Existenz passender Îą1 , . . . , Îąn â K
mit v = j=1 Îąj bj folgt sofort daraus, dass B eine Basis von V ist. Zu zeigen
bleibt die Eindeutigkeit. Seien also zusaĚtzlich Î˛1 , . . . , Î˛n â K, fuĚr die ebenfalls
n
X

Î˛j bj = v =

j=1

n
X

Îąj bj

j=1

gilt. Dann ist insbesondere
0=

n
X
j=1

Î˛j bj â

n
X
j=1

Îąj bj =

n
X
j=1

(Î˛j â Îąj )bj .

Da B eine Basis ist, sind die Vektoren b1 , . . . , bn linear unabhaĚngig, so dass aus
obiger Gleichheit Îąj â Î˛j = 0 und damit Îąj = Î˛j fuĚr alle j = 1, . . . , n gilt.
Definition 3.2.23. Seien n â Nâ , V ein n-dimensionaler K-Vektorraum, B eine
Basis von V und v â VP
. Die nach Satz 3.2.22 eindeutig bestimmten Elemente
Îą1 , . . . , Îąn â K mit v = nj=1 Îąj bj heiĂen Koordinaten von v bezuĚglich B.
Weiter heiĂt der Vektor (Îą1 , . . . , Îąn )T â K n Koordinatenvektor von v bezuĚglich
B. Wir werden diesen haĚufiger mit ~v bezeichnen.
Warnung 3.2.24. Der Koordinatenvektor eines Vektors liegt nur nach der Wahl
einer konkreten Basis fest. AĚndert man die Basis, aĚndern sich auch alle Koordinatenvektoren. Die Schreibweise ~v fuĚr den Koordinatenvektor von v ist also nur
angebracht, wenn aus dem Zusammenhang vollkommen klar ist, bezuĚglich welcher
Basis dieser zu bilden ist. In allen anderen FaĚllen ist eine genauere Schreibweise
notwendig.
Beispiel 3.2.25. (a) Im Raum Abb(R, R) betrachten wir U := hf1 , f2 i, wobei
f1 , f2 : R â R mit f1 (x) = x und f2 (x) = x2 gegeben seien. Die beiden
Vektoren f1 und f2 sind linear unabhaĚngig nach Beispiel 3.2.12 (c), also
bilden sie eine Basis von U.
Das Element g â U mit g(x) = 3x2 + x fuĚr x â R hat bezuĚglich dieser Basis
den Koordinatenvektor ~g = (1, 3)T â R2 , da g = 1 Âˇ f1 + 3 Âˇ f2 gilt.
(b) Wir betrachten R2 mit der Basis, die durch die beiden Vektoren b1 := (1, 1)T
und b2 := (â1, 1)T gegeben ist. Der Vektor e1 = (1, 0)T â R2 hat dann
bezuĚglich dieser Basis den Koordinatenvektor ~e1 = (1/2, â1/2)T , denn es
gilt
 
 
 
1 â1
1 1
1
â
.
=
0
2 1
2 1

62

3.3. Der Faktorraum
Bemerkung 3.2.26. Durch die Bestimmung von Koordinaten scheint jeder ndimensionale K-Vektorraum in gewisser Weise mit dem Raum K n uebereinzustimmen, wenn wir jeden Vektor mit seinem Koordinatenvektor identifizieren.
Diese Intuition werden wir im Abschnitt 3.6 mathematisch rigoros machen.

3.3. Der Faktorraum
Lemma 3.3.1. Es sei V ein K-Vektorraum und U ein Untervektorraum von V .
Die Relation, die fuĚr v, w â V gegeben ist durch
v âź w ââ v â w â U,
ist eine AĚquivalenzrelation auf V .
Beweis. ReflexivitaĚt: Sei v â V . Dann gilt v â v = 0 â U, also ist v âź v.

Symmetrie: Seien v, w â V mit v âź w gegeben. Dann ist v â w â U. Da U ein
Untervektorraum ist, ist dann auch w â v = (â1)(v â w) â U und das bedeutet
w âź v.

TransitivitaĚt: Es seien v, w, x â V mit v âź w und w âź x. Das bedeutet v â w â
U und w â x â U. Da U ein Untervektorraum ist, gilt insbesondere dann auch
v â x = (v â w) + (w â x) â U. Also haben wir v âź x und sind fertig.
Nun erinnern wir uns an ein Ergebnis unserer Betrachtungen in Abschnitt 1.3.2,
den Satz 1.3.12:
Ist âź eine AĚquivalenzrelation auf einer Menge V , so bilden die AĚquivalenzklassen
vĚ von v â V mit
vĚ = {w â V : w âź v}
eine Zerlegung von V , d.h. die Vereingung aller AĚquivalenzklassen ist ganz V und
je zwei verschiedene solcher Klassen sind disjunkt.

Bemerkung 3.3.2. (a) Wie sieht nun die AĚquivalenzklasse eines Elements v â
V bezuĚglich obiger AĚquivalenzrelation aus? Wir uĚberlegen uns folgendes:
w â vĚ ââ w â v â U ââ âu â U : w â v = u
ââ âu â U : w = v + u ââ w â v + U,
wobei v + U := {v + u : u â U} ist. Anschaulich ist damit vĚ der um v
verschobene Unterraum U.
Am besten kann man sich dies an einem eindimensionalen Unterraum U
des R2 veranschaulichen, vgl. Abbildung 3.1.
(b) Es ist immer 0Ě = U, denn zum Einen gilt fuĚr jedes u â U auch uâ0 = u â U
und damit u âź 0, d.h. u â 0Ě. Zum Anderen ist fuĚr jedes v â 0Ě nach
Definition v = v â 0 â U.

63

3. Lineare Algebra
(x+v) ~
x~
v~
~
U=0

x+v

(âv)~
v

x

âv

Abbildung 3.1.: Die AĚquivalenzklassen in R2 /U
Nun kommt die uĚberraschende Wendung. Wenn wir uns die Menge der durch den
obigen Prozess gegebenen AĚquivalenzklassen anschauen, so koĚnnen wir diese auf
eine kanonische Weise selbst wieder zu einem Vektorraum machen.
Satz 3.3.3. Es sei V ein K-Vektorraum, U ein Untervektorraum von V und âź
sei definiert wie in Lemma 3.3.1. Dann ist die Menge
V /U := V /âź = {vĚ : v â V }
mit den fuĚr vĚ, wĚ â V /U und Îą â K durch
vĚ + wĚ := v^
+w

und

definierten VerknuĚpfungen ein K-Vektorraum.

Îą Âˇ vĚ := Îą
fv

Beweis. Der entscheidende Punkt im Beweis ist die Wohldefiniertheit der beiden
VerknuĚpfungen, das bedeutet in diesem Zusammenhang wir muĚssen RepraĚsentantenunabhaĚngigkeit zeigen.
Seien also vĚ, wĚ â V /U und seien v0 â vĚ, sowie w0 â wĚ. Dann ist v â v0 â U und
w âw0 â U. Da U ein Untervektorraum ist, bedeutet das insbesondere, dass auch
die Summe v â v0 + w â w0 in U ist und damit auch (v + w) â (v0 + w0 ) â U
gilt. Also ist v + w âź v0 + w0 , was nach Satz 1.3.12 (b) gerade v^
+ w = v^
0 + w0
bedeutet. Das liefert nun
vĚ + wĚ = v^
+ w = v^
0 + w0 = vĚ0 + wĚ0 .
Um den entsprechenden Nachweis fuĚr die Skalar-Multiplikation zu fuĚhren, geben
wir ein Îą â K und ein vĚ â V /U vor. FuĚr jedes v0 â vĚ gilt dann wieder v â v0 â U

64

3.3. Der Faktorraum
und da U ein Untervektorraum ist, ist auch Îą(v â v0 ) â U. Das liefert Îąv â Îąv0 â
U, was gerade Îąv âź Îąv0 und damit Îą
fv = Îą
g
v0 bedeutet. Das liefert zum Abschluss
wieder
ÎąvĚ = Îą
fv = Îą
g
v0 = ÎąvĚ0 .

Da nun die Wohldefiniertheit der VerknuĚpfungen gezeigt ist, muĚssen wir noch
die Vektorraumaxiome nachweisen. Diese lassen sich jedoch ohne Probleme von
V auf V /U uĚbertragen. Der Nullvektor in V /U ist dabei 0Ě und das Inverse âvĚ
f Wir fuĚhren die UĚbertragung exemplarisch
zu vĚ â V /U ist gegeben durch âv.
anhand der KommutativitaĚt der Vektorraumaddition und (V4) vor.
Seien also fuĚr den Nachweis der KommutativitaĚt vĚ, wĚ â V /U. Dann gilt dank der
KommutativitaĚt in (V, +)
^
vĚ + wĚ = v^
+w =w
+ v = wĚ + vĚ.
Zum Nachweis von (V4) seien Îą, Î˛ â K und vĚ â V /U. Dann haben wir
f = Îą Âˇ vĚ + Î˛ Âˇ vĚ.
^
(Îą + Î˛) Âˇ vĚ = (Îą^
+ Î˛)v = Îąv
+ Î˛v = Îą
fv + Î˛v

Definition 3.3.4. Der Raum V /U in obigem Satz heiĂt Faktorraum von V nach
U oder auch Quotientenraum. Die Schreibweise V /U wird V (faktorisiert) nach
â
Uâ gelesen.
Satz 3.3.5. Sei V ein n-dimensionaler K-Vektorraum und U ein m-dimensionaler Untervektorraum von V . Dann gilt dim(V /U) = n â m.
Beweis. Nach Satz 3.2.17 hat U eine Basis, sei also B = {b1 , . . . , bm } eine solche.
Nach dem gleichen Satz koĚnnen wir diese nun zu einer Basis von V erweitern, es
gibt also bm+1 , bm+2 , . . . , bn â V , so dass Bâ˛ = {b1 , . . . , bn } eine Basis von V ist.
Wir zeigen nun, dass BĚ := {bĚm+1 , . . . , bĚn } eine Basis von V /U ist. Dazu uĚberzeugen wir uns zunaĚchst, dass diese Menge ganz V /U erzeugt.
PnSei also vĚ â V /U. Da
â˛
B eine Basis von V ist, gibt es Îą1 , . . . , Îąn â K mit v = j=1 Îąj bj . Damit gilt
n
n
n
^
X
X
X
g
vĚ =
Îąj bj =
Îąj bĚj .
Îąj bj =
j=1

j=1

j=1

Da b1 , . . . , bm â U sind, gilt fuĚr alle diese bĚ1 = Âˇ Âˇ Âˇ = bĚm = 0Ě und wir verbleiben
mit
n
X
vĚ =
Îąj bĚj ,
j=m+1

was gerade bedeutet, dass vĚ in der linearen HuĚlle von BĚ liegt.

65

3. Lineare Algebra
Zum Nachweis der linearen UnabhaĚngigkeit seien Îąm+1 , . . . , Îąn â K gegeben mit
P
n
j=m+1 Îąj bĚj = 0Ě. Dann gilt wie oben
0Ě =

n
X

Îąj bĚj =

j=m+1

n
^
X
Îąj bj ,

j=m+1

P
d.h. nj=m+1 Îąj bj â U.
Nun haben wir mit B = {b1 , . . . , bm } eine Basis von U, also gibt es nun Koeffizienten Îą1 , . . . , Îąm â K mit
n
X

j=m+1

Îąj bj =

m
X

Îąj bj ,

d.h.

j=1

n
X

j=m+1

Îąj bj â

m
X

Îąj bj = 0.

j=1

Nun ist aber die Menge {b1 , . . . , bn } wiederum eine Basis, diesmal von V , insbesondere sind diese n Vektoren linear unabhaĚngig. Da wir in der letzten Gleichung
aus diesen aber den Nullvektor kombiniert haben, muss Îą1 = Âˇ Âˇ Âˇ = Îąm = Îąm+1 =
Âˇ Âˇ Âˇ = Îąn = 0 gelten und wir sind fertig.

3.4. Normierte RaĚume
Das Ziel dieses Abschnittes ist das Messen von LaĚngen und AbstaĚnden in VektorraĚumen. Dazu betrachten wir in diesem Abschnitt nur reelle VektorraĚume. Alle
Begriffe und Ergebnisse lassen sich auf komplexe VektorraĚume uĚbertragen, wobei sie allerdings zum Teil leicht angepasst werden muĚssen. Der UĚbersichtlichkeit
halber wollen wir hier darauf verzichten.
Definition 3.4.1. Es sei V ein R-Vektorraum. Eine Abbildung k Âˇ k : V â R
heiĂt Norm, falls
(N1) âv â V : kvk âĽ 0 und (kvk = 0 ââ v = 0).
(N2) âÎą â R âv â V : kÎąvk = |Îą|kvk.
(N3) âv, w â V : kv + wk â¤ kvk + kwk.

(Definitheit)

(HomogenitaĚt)
(Dreiecksungleichung)

Ein Vektorraum mit einer Norm heiĂt normierter Raum.
Beispiel 3.4.2.

(a) Der Betrag | Âˇ | in R ist eine Norm.

Das ergibt sich aus den Eigenschaften des Betrags in C in Satz 2.5.12.
(b) Unser Alltagsbegriff von LaĚnge ist der Euklidische Abstand, der z.B. in der
Ebene R2 gegeben ist durch die Euklidische Norm oder auch 2-Norm
q
kxk2 := x21 + x22 ,
x = (x1 , x2 )T â R2 ,
66

3.4. Normierte RaĚume
oder allgemein in Rn durch
v
uX
u n 2
kxk2 := t
xj ,

x = (x1 , x2 , . . . , xn )T â Rn .

j=1

Beim Nachweis, dass dies eine Norm ist, sind die Nachweise von (N1) und
(N2) einfach, dagegen stellt sich der Nachweis von (N3) auf direktem Wege
als sehr sperrig und rechenintensiv heraus. Versuchen Sie sich ruhig einmal
ein bisschen daran. Dass kÂˇk2 eine Norm auf Rn ist, werden wir in KuĚrze aus
einem deutlich allgemeineren Resultat geschenkt bekommen. Wir koĚnnen
also auf die lange Rechnung verzichten.
(c) Es gibt in Rn aber auch noch andere Normen. Wir betrachten in R2 die
Abbildung k Âˇ k1 : R2 â R mit
kxk1 = |x1 | + |x2 |,

x = (x1 , x2 )T â R2 ,

die sogenannte 1-Norm. Diese ist eine Norm, denn
(N1) FuĚr jedes x = (x1 , x2 )T â R2 ist kxk1 = |x1 | + |x2 | âĽ 0 und ist x 6= 0,
so muss x1 6= 0 oder x2 6= 0 gelten. Also ist in diesem Fall |x1 | > 0
oder |x2 | > 0 und wir erhalten kxk1 > 0.
Damit folgt die Definitheit per Kontraposition.

(N2) Seien Îą â R und x â R2 . Dann gilt
kÎąxk1 = k(Îąx1 , Îąx2 )T k1 = |Îąx1 | + |Îąx2 | = |Îą|(|x1 | + |x2 |) = |Îą|kxk1 .
(N3) Seien x, y â R2 . Dann gilt mit Hilfe von (a)
kx + yk1 = k(x1 + y1 , x2 + y2 )T k1 = |x1 + y1 | + |x2 + y2 |
â¤ |x1 | + |y1| + |x2 | + |y2| = kxk1 + kyk1.
Auch die 1-Norm gibt es fuĚr jedes n â Nâ in Rn :
kxk1 =

n
X
j=1

|xj |,

x = (x1 , x2 , . . . , xn )T â Rn .

(d) Ein weiteres Beispiel, das als UĚbungsaufgabe verbleibt, ist die Maximumsoder â-Norm in Rn gegeben durch
kxkâ = max{|x1 |, |x2 |, . . . , |xn |},

x = (x1 , x2 , . . . , xn )T â Rn .

Hat man in einem R-Vektorraum V eine Norm k Âˇ k, so kann man damit AbstaĚnde
messen. Die entsprechenden Begriffe liefert die folgende Definition.

67

3. Lineare Algebra
Definition 3.4.3. Es sei V ein R-Vektorraum mit einer Norm k Âˇ k und A und
B seien Teilmengen von V . Dann heiĂt
dist(A, B) := inf{ka â bk : a â A, b â B}
Abstand von A und B. Sind A = {a} und/oder B = {b} einelementig, so schreibt
man auch dist(a, B) oder dist(a, b) statt dist({a}, B), bzw. dist({a}, {b}).
Bemerkung 3.4.4. Der Abstand zwischen zwei Vektoren u und v aus V ist
damit gegeben durch ku â vk.
Nimmt man die euklidische Norm in R2 oder R3 , so stimmt dieser Abstandsbegriff
mit unserem alltaĚglich gemessenen Abstand uĚberein.
Man beachte auch, dass dank der HomogenitaĚt der Norm die fuĚr einen Abstand
recht sinnige Eigenschaft
dist(u, v) = ku â vk = k(â1)(v â u)k = |â1|kv â uk = kv â uk = dist(v, u)
gilt.
Definition 3.4.5. Es sei V ein R-Vektorraum. Eine Abbildung (Âˇ|Âˇ) : V Ă V â R
heiĂt Skalarprodukt, falls

(SP1) âx â V : (x|x) âĽ 0 und (x|x) = 0 ââ x = 0 . (Definitheit)
(SP2) âx, y â V : (x|y) = (y|x).

(Symmetrie)

(SP3) âx, y, z â V âÎą, Î˛ â R : (Îąx + Î˛y|z) = Îą(x|z) + Î˛(y|z).
(LinearitaĚt im ersten Argument)

Bemerkung 3.4.6. Aus (SP3) und (SP2) folgt wegen
(x|Îąy + Î˛z) = (Îąy + Î˛z|x) = Îą(y|x) + Î˛(z|x) = Îą(x|y) + Î˛(x|z)
fuĚr alle x, y, z â V und alle Îą, Î˛ â R auch die LinearitaĚt im zweiten Argument.

Beispiel 3.4.7. In Rn erhaĚlt man ein Skalarprodukt, das sogenannte Standardskalarprodukt, wenn man fuĚr x = (x1 , x2 , . . . , xn )T und y = (y1 , y2 , . . . , yn )T aus
Rn setzt
n
X
(x|y) :=
xj yj .
j=1

Das sieht man folgendermaĂen:

Pn
2
(SP1) FuĚr jedes x â Rn gilt (x|x) =
j=1 xj âĽ 0. Weiter ist offensichtlich
(0|0) = 0. Ist schlieĂlich x 6= 0 so gibt es einen Index j0 mit xj0 6= 0 und es
ist
n
X
(x|x) =
x2j âĽ x2j0 > 0,
j=1

also insbesondere (x|x) 6= 0 in diesem Fall. Damit folgt die Definitheit per
Kontraposition.

68

3.4. Normierte RaĚume
(SP2) Seien x, y â Rn . Dann gilt
(x|y) =

n
X

xj yj =

j=1

n
X

yj xj = (y|x).

j=1

(SP3) Seien x, y, z â Rn und Îą, Î˛ â R. Dann ist
(Îąx + Î˛y|z) =

n
X

(Îąxj + Î˛yj )zj =

j=1
n
X

=Îą

j=1

n
X

(Îąxj zj + Î˛yj zj )

j=1

xj zj + Î˛

n
X

yj zj = Îą(x|z) + Î˛(y|z).

j=1

UĚbungsaufgabe 3.4.8. Sei (Âˇ|Âˇ) ein Skalarprodukt auf einem R-Vektorraum V .
Dann gilt (x|0) = (0|x) = 0 fuĚr alle x â V .
Was hat nun ein Skalarprodukt mit AbstaĚnden und Normen zu tun? Eine ganze
Menge. Hat man ein Skalarprodukt (Âˇ|Âˇ) auf einem R-Vektorraum V , so werden
wir nun zeigen, dass dann durch
p
(3.1)
kxk := (x|x), x â V,

eine Norm definiert wird. Man beachte zunaĚchst, dass dank der Definitheit des
Skalarprodukts diese Setzung wohldefiniert ist, da das Argument der Wurzel nie
negativ werden kann.
Zum Nachweis, dass wir so wirklich eine Norm bekommen, benoĚtigen wir zunaĚchst
die folgende Ungleichung.

Satz 3.4.9 (Cauchy-Schwarz-Ungleichung). Es sei (Âˇ|Âˇ) ein Skalarprodukt auf
einem R-Vektorraum V und kÂˇk definiert wie in (3.1). Dann gilt fuĚr alle v, w â V
(v|w) â¤ kvk Âˇ kwk
und Gleichheit gilt genau dann, wenn v und w linear abhaĚngig sind.
Beweis. Wir betrachten zunaĚchst den Fall w = 0. Dann ist (w|w) = 0 und damit
auch kwk = 0. Aus UĚbungaufgabe 3.4.8 folgt dann
(v|w) = 0 = kvk Âˇ kwk
und wir koĚnnen diesen Fall zu den Akten legen.
Sei also nun w 6= 0. Dann gilt fuĚr alle Îą â R dank der Definitheit aus (SP1)
(SP3)

0 â¤ (v â Îąw|v â Îąw) = (v|v â Îąw) â Îą(w|v â Îąw)
3.4.6

(SP2)

= (v|v) â Îą(v|w) â Îą(w|v) + Îą2 (w|w) = (v|v) â 2Îą(v|w) + Îą2 (w|w).

69

3. Lineare Algebra
Da w 6= 0 ist, haben wir mit (SP1) (w|w) 6= 0 und koĚnnen nun Îą := (v|w)/(w|w)
setzen. Damit erhalten wir aus obiger Rechnung
(v|w)
(v|w)2
(v|w)2 (v|w)2
(v|w) +
(w|w)
=
(v|v)
â
2
+
(w|w)
(w|w)2
(w|w)
(w|w)
2
(v|w)
= (v|v) â
.
(w|w)

0 â¤ (v|v) â 2

Da (w|w) > 0 ist, koĚnnen wir die Ungleichung mit diesem Wert multiplizieren,
ohne dass sich das Relationszeichen umdreht. Das liefert
0 â¤ (v|v)(w|w) â (v|w)2.
Also ist

2

(v|w) = (v|w)2 â¤ (v|v)(w|w) = kvk2 Âˇ kwk2 ,
woraus die behauptete Ungleichung folgt.
AbschlieĂend muĚssen wir uns noch uĚberlegen, wann Gleichheit gilt. Nach unserer
Rechnung gilt Gleichheit genau dann, wenn (v â Îąw|v â Îąw) = 0 ist. Wegen der
Definitheit des Skalarprodukts gilt das aber genau dann, wenn v â Îąw = 0, d.h.
v = Îąw ist. Womit wir bei der linearen AbhaĚngigkeit von v und w sind.
Satz 3.4.10. Sei (Âˇ|Âˇ) ein Skalarprodukt auf einem R-Vektorraum V . Dann ist
k Âˇ k definiert wie in (3.1) eine Norm auf V .
Beweis. Wir rechnen die drei Norm-Axiome nach.
p
p
(N1) â
ZunaĚchst ist kvk = (v|v) âĽ 0 fuĚr jedes v âpV und es gilt k0k = (0|0) =
0 = 0. AuĂerdem folgt aus kvk = 0 auch (v|v) = 0, d.h. (v|v) = 0 und
das liefert uns mit (SP1) nun v = 0.
(N2) Seien Îą â R und v â V . Dann gilt dank der LinearitaĚt des Skalarprodukts
in beiden Variablen
â p
p
p
kÎąvk = (Îąv|Îąv) = Îą2 (v|v) = Îą2 (v|v) = |Îą|kvk.

(N3) Seien u, v â V . Dann gilt wieder mit der LinearitaĚt und dieses Mal zusaĚtzlich der Symmetrie des Skalarprodukts

ku+vk2 = (u+v|u+v) = (u|u)+(u|v)+(v|u)+(v|v) = kuk2 +2(u|v)+kvk2.
Mit der Cauchy-Schwarz-Ungleichung liefert das nun
2
ku + vk2 â¤ kuk2 + 2kukkvk + kvk2 = kuk + kvk ,

woraus ku + vk â¤ kuk + kvk folgt.

70

3.4. Normierte RaĚume
Korollar 3.4.11. Die Euklidische Norm k Âˇ k2 auf Rn , vgl. Beispiel 3.4.2 (b), ist
tatsaĚchlich eine Norm.
Beweis. FuĚr das Standardskalarprodukt auf Rn , vgl. Beispiel 3.4.7, gilt mit x â
Rn
v
uX
p
u n 2
(x|x) = t
xj = kxk2 .
j=1

Also folgt die Behauptung aus Satz 3.4.10.

Definition 3.4.12. Sei V ein R-Vektorraum mit einem Skalarprodukt (Âˇ|Âˇ).
(a) Zwei Vektoren v, w â V heiĂen senkrecht oder orthogonal, falls (v|w) = 0
ist. Man schreibt dann v âĽ w.
(b) Eine Basis B von V heiĂt Orthogonalbasis, falls fuĚr alle Wahlen von b1 , b2 â
B gilt b1 âĽ b2 .
(c) Eine Orthogonalbasis B von V heiĂt Orthonormalbasis, falls kbk = 1 fuĚr
alle b â B gilt.
Beispiel 3.4.13. (a) Die Standardbasis ist eine Orthonormalbasis von Rn mit
dem Standardskalarprodukt.
1
(b) Die Menge {( â1
) , ( 11 )} ist eine Orthogonalbasis von R2 mit dem Standardskalarprodukt, aber keine Orthonormalbasis.

Den folgenden BasisergaĚnzungssatz fuĚr Orhthonormalbasen wollen wir wieder
ohne Beweis stehen lassen.
Satz 3.4.14. Jeder R-Vektorraum mit Skalarprodukt hat eine Orthonormalbasis
und jede Menge von normierten und paarweise orthogonalen Vektoren laĚsst sich
zu einer Orthonormalbasis ergaĚnzen.
Bemerkung 3.4.15. Sei V ein n-dimensionaler R-Vektorraum mit Skalarprodukt (Âˇ|Âˇ) und {e1 , e2 , . . . , en } eine Orthonormalbasis von V . BezuĚglich dieser Basis gibt es dann zu einem gegebenen v â V den Koordinatenvektor
ďŁŤ ďŁś
Îą1
n
ďŁŹ Îą2 ďŁˇ
X
ďŁŹ ďŁˇ
Îąj ej
~v = ďŁŹ .. ďŁˇ , wobei v =
ďŁ­ . ďŁ¸
j=1
Îąn
ist, vgl. Definition 3.2.23. Das Problem ist, wie bestimmt man ganz konkret diesen
Koordinatenvektor?

71

3. Lineare Algebra
Bei einer Orthonormalbasis ist das zum GluĚck einfach. Wir multiplizieren fuĚr ein
k â {1, 2, . . . , n} obige Gleichung mit ek und erhalten dank der LinearitaĚt des
Skalarprodukts
(v|ek ) =

n
X
j=1



Îąj ej ek =

n
X
j=1

Îąj (ej |ek ) =

n
X

Îąj Î´jk = Îąk ,

j=1

wobei wir wieder das Kronecker-Delta, vgl. Bemerkung 3.1.2 (e), verwendet haben.
Zusammen gilt also
ďŁŤ
ďŁś
(v|e1 )
ďŁŹ (v|e2 ) ďŁˇ
ďŁŹ
ďŁˇ
~v = ďŁŹ .. ďŁˇ .
ďŁ­ . ďŁ¸
(v|en )

Man beachte, dass diese Formel nur fuĚr Orthonormalbasen gilt!

UĚbungsaufgabe 3.4.16. Es sei V ein n-dimensionaler R-Vektorraum mit Skalarprodukt und U sei ein Untervektorraum von V . Dann gibt es zu jedem v â V
genau ein u â U, so dass v â u auf allen Vektoren aus U senkrecht steht. Man
nennt u die Orthogonalprojektion von v auf U.

3.5. Geometrie im Rn
Der uns vertraute Raum und die euklidische Ebene lassen sich leicht mit R3
bzw. R2 identifizieren und tragen damit eine Vektorraum-Struktur. Wie diese
dazu dienen kann, elementargeometrische Betrachtungen anzustellen, wollen wir
in diesem Abschnitt ein wenig anreiĂen.
Dabei betrachten wir hier durchgaĚngig den reellen Standardvektorraum Rn mit
dem Standardskalarprodukt.
Definition 3.5.1.

(a) Es seien x, v â Rn mit v 6= 0. Dann heiĂt
g := {x + Îťv : Îť â R}

eine Gerade mit Aufpunkt x und Richtungsvektor v.
(b) Seien x, v, w â Rn und seien v und w linear unabhaĚngig. Dann heiĂt
E := {x + Îťv + Âľw : Îť, Âľ â R}
Ebene mit Aufpunkt x und Richtungsvektoren v und w.

72

3.5. Geometrie im Rn
Bemerkung 3.5.2. Man kann Geraden und Ebenen als um den Aufpunkt verschobene UntervektorraĚume auffassen. Mit den obigen Notationen also
g = x + hvi

und

E = x + hv, wi.

Ein solcher verschobener Untervektorraum wird auch affiner Raum genannt.
Oft werden Geraden durch die Angabe von zwei Punkten angegeben, durch die
die Gerade geht. Dass das ein sinnvolles Vorgehen ist, zeigt der folgende Satz.
Satz 3.5.3. Seien x, y â Rn mit x 6= y gegeben. Dann existiert genau eine Gerade
g mit x, y â g, naĚmlich g = {x + Îť(y â x) : Îť â R}.
Beweis. ZunaĚchst erhalten wir mit den speziellen Wahlen Îť = 0, bzw. Îť = 1,
dass x, y â g gilt.
Sei nun h = {u + Âľv : Âľ â R} eine andere Gerade mit x, y â h. Dann gibt es also
ein Âľx â R mit u + Âľx v = x und ein Âľy â R, so dass u + Âľy v = y gilt. AuĂerdem
ist Âľx 6= Âľy , denn es ist ja x 6= y.
Insbesondere haben wir damit x â Âľx v = u = y â Âľy v, d.h. x â y = (Âľx â Âľy )v.
Damit folgt
Âľx
u = x â Âľx v = x â
(x â y)
Âľx â Âľy

und das liefert schlieĂlich

o
n
Âľ
Âľx
(x â y) +
(x â y) : Âľ â R
h = {u + Âľv : Âľ â R} = x â
Âľx â Âľy
Âľx â Âľy
n
o
Âľ â Âľx
= x+
(x â y) : Âľ â R = {x + Îť(x â y) : Îť â R} = g.
Âľx â Âľy

Dabei haben wir im letzten Schritt Îť := (Âľ â Âľx )/(Âľx â Âľy ) gesetzt. Man beachte
dabei, dass Âľ 7â (Âľ â Âľx )/(Âľx â Âľy ) eine Bijektion von R nach R ist.
Bemerkung 3.5.4. In gleicher Weise liegt eine Ebene durch die Angabe von
drei Punkten x, y, z â Rn , die nicht auf einer Geraden liegen, fest. Die Ebene ist
dann gegeben durch
{x + Îť(y â x) + Âľ(z â x) : Îť, Âľ â R}.
Die Bedingung, dass die drei Punkte nicht auf einer Geraden liegen duĚrfen, sorgt
dann dafuĚr, dass die beiden Richtungsvektoren y â x und z â x linear unabhaĚngig
sind.
Beispiel 3.5.5. In R3 sei g die Gerade, die die Punkte (3, â4, 2)T und (3, 2, 4)T
enthaĚlt, sowie E die durch
ďŁąďŁŤ ďŁś
ďŁź
ďŁŤ ďŁś
ďŁŤ ďŁś
1
1
ďŁ˛ 1
ďŁ˝
E := ďŁ­0ďŁ¸ + Îť ďŁ­â1ďŁ¸ + Âľ ďŁ­0ďŁ¸ : Îť, Âľ â R
ďŁł
ďŁž
0
1
2
73

3. Lineare Algebra
gegebene Ebene. Wir bestimmen die Schnittmenge g âŠ E.
Nach Satz 3.5.3 ist
ďŁąďŁŤ ďŁś
ďŁź ďŁąďŁŤ ďŁś
ďŁź
ďŁŽďŁŤ ďŁś ďŁŤ ďŁśďŁš
ďŁŤ ďŁś
3
3
0
ďŁ˛ 3
ďŁ˝ ďŁ˛ 3
ďŁ˝
ďŁ­
ďŁ¸
ďŁ°
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
ďŁť
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
â4 + Îł
2 â â4
â4 + Îł 6 : Îł â R
g=
:ÎłâR =
ďŁł
ďŁž ďŁł
ďŁž
2
4
2
2
2
Damit ein Punkt x â g âŠ E existieren kann, muss es Îť, Âľ, Îł â R geben mit
ďŁŤ ďŁś
ďŁŤ ďŁś
ďŁŤ ďŁś ďŁŤ ďŁś
ďŁŤ ďŁś
1
1
1
3
0
ďŁ­0ďŁ¸ + Îť ďŁ­â1ďŁ¸ + Âľ ďŁ­0ďŁ¸ = ďŁ­â4ďŁ¸ + Îł ďŁ­6ďŁ¸ .
0
1
2
2
2
Das liefert uns das lineare Gleichungssystem
ďŁą
=
2
ďŁ˛ Îť + Âľ
âÎť
â 6Îł = â4
ďŁł
Îť + 2Âľ â 2Îł = 2.

Die erste Zeile liefert uns Âľ = 2 â Îť und die zweite verraĚt Îł = (â4 + Îť)/(â6) =
2/3 â 1/6 Âˇ Îť. Setzt man diese beiden Informationen in die dritte Zeile ein, so
erhaĚlt man
2 1 
2
8
2
2
Îť + 2(2 â Îť) â 2 â Îť = 2 ââ â Îť + = 2 ââ â Îť = â ââ Îť = 1.
3 6
3
3
3
3

Das bedeutet abschlieĂend Âľ = 1 und Îł = 1/2.
Da das Gleichungssystem eine eindeutige Loesung hat, gibt es genau einen Punkt
x â g âŠ E, und zwar
ďŁŤ ďŁś
ďŁŤ ďŁś ďŁŤ ďŁś
3
0
3
1
x = ďŁ­â4ďŁ¸ + ďŁ­6ďŁ¸ = ďŁ­â1ďŁ¸ .
2
2
2
3

Definition 3.5.6. Sei U ein (n â 1)-dimensionaler Untervektorraum des Rn und
x â Rn . Dann nennt man den affinen Raum x + U eine Hyperebene in Rn .

Beispiel 3.5.7. Geraden sind Hyperebenen in R2 und Ebenen sind Hyperebenen
in R3 . In R4 waĚre z.B.
ďŁź
ďŁąďŁŤ ďŁś
ďŁŤ ďŁś
ďŁŤ ďŁś
ďŁŤ ďŁś
0
0
1
1
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁ˝
ďŁ˛ďŁŹ ďŁˇ
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
0
1
0
1
ďŁŹ ďŁˇ + Îť ďŁŹ ďŁˇ + Âľ ďŁŹ ďŁˇ + Îł ďŁŹ ďŁˇ : Îť, Âľ, Îł â R
ďŁ­1ďŁ¸
ďŁ­0ďŁ¸
ďŁ­0ďŁ¸
ďŁ­1ďŁ¸
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁž
ďŁł
0
0
0
1
eine Hyperebene.

74

3.5. Geometrie im Rn
UĚbungsaufgabe 3.5.8. Ist H = x + U eine Hyperebene und y â H ein Punkt
in H, so gilt auch H = y + U. Anders formuliert, der Untervektorraum, der H
aufspannt, haĚngt nicht von der speziellen Wahl des Aufpunkts ab.
Satz 3.5.9. Es sei H = x + U eine Hyperebene in Rn . Dann existiert ein bis
auf sein Vorzeichen eindeutiger Vektor Î˝ â Rn mit kÎ˝k2 = 1 und Î˝ âĽ u fuĚr alle
u â U, ein sogenannter Normaleneinheitsvektor von H.
Beweis. Es sei Bâ˛ = {e1 , e2 , . . . , enâ1 } eine Orthonormalbasis von U und wir waĚhlen Î˝ â Rn so, dass dieser Vektor die Menge Bâ˛ zu einer Orthonormalbasis B =
{e1 , e2 , . . . , enâ1 , Î˝} von Rn ergaĚnzt, vgl. Satz 3.4.14. Dann gilt nach Konstruktion
kÎ˝k2 = 1 und Î˝ âĽ ej fuĚr jedes j â {1, 2, . . . , n â 1}. Ist u â U, so ist aber u eine
Linearkombination von e1 , e2 , . . . , enâ1 , also ist auch Î˝ âĽ u.
Es bleibt die Eindeutigkeit (bis auf ein Vorzeichen) zu zeigen. Sei dazu ein v â Rn
mit kvk2 = 1 und v âĽ u fuĚr jedes u â U gegeben. Nun ist B eine Orthonormalbasis, also gilt nach Bemerkung 3.4.15 und weil v âĽ ej fuĚr alle j = 1, 2, . . . , n â 1
gilt
nâ1
X
v=
(v|ej )ej + (v|Î˝)Î˝ = (v|Î˝)Î˝.
j=1

Der Vektor v ist also ein Vielfaches des Vektors Î˝. Da aber beide LaĚnge Eins
haben sollen, muss v = Î˝ oder v = âÎ˝ gelten und wir sind fertig.

Satz 3.5.10. Es sei H eine Hyperebene in Rn mit Normaleneinheitsvektor Î˝ und
es sei x0 â H. Dann gilt fuĚr d := (x0 |Î˝)
H = {x â Rn : (x|Î˝) = d}.
Beweis. Sei U der Untervektorraum von Rn , mit dem H = x0 + U gilt.
ââ Es sei x â H. Dann gibt es ein u â U mit x = x0 + u. Also ist dank der
â
LinearitaĚt des Skalarprodukts und mit Hilfe der Definition des Normaleneinheitsvektors
(x|Î˝) = (x0 + u|Î˝) = (x0 |Î˝) + (u|Î˝) = (x0 |Î˝) = d.
ââ Sei x â Rn so, dass (x|Î˝) = d = (x0 |Î˝) gilt. Dann ist (x â x0 |Î˝) = 0, d.h.
â
x â x0 âĽ Î˝. Ist wieder {e1 , e2 , . . . , enâ1 } eine Orthonormalbasis von U, so
ist {e1 , e2 , . . . , enâ1 , Î˝} eine Orthonormalbasis von Rn und wir haben mit
Hilfe von Bemerkung 3.4.15
x â x0 =

nâ1
X
j=1

(x â x0 |ej )ej + (x â x0 |Î˝)Î˝ =

nâ1
X
j=1

(x â x0 |ej )ej .

Das bedeutet aber, dass x â x0 â U liegt, d.h. x â x0 + U = H.

75

3. Lineare Algebra
Definition 3.5.11. Die Darstellung H = {x â Rn : (x|Î˝) = d} fuĚr eine Hyperebene H mit Normaleneinheitsvektor Î˝ heiĂt Hesse-Normalform von H.
Mit Hilfe der Hesse-Normalform ist die Bestimmung des Abstandes von der Hyperebene recht einfach, wie der folgende Satz zeigt. Zum Begriff des Abstandes
sei an Definition 3.4.3 erinnert.
Satz 3.5.12. Es sei H â Rn eine Hyperebene mit Hesse-Normalform H = {x â
Rn : (x|Î˝) = d} und x0 â Rn . Dann gilt
dist(x0 , H) = (x0 |Î˝) â d .
Insbesondere ist dist(0, H) = |d|.


Beweis. ZunaĚchst zeigen wir, dass y0 := x0 â (x0 |Î˝) â d Î˝ in H liegt. Dazu
rechnen wir


(y0 |Î˝) = x0 â ((x0 |Î˝) â d)Î˝ Î˝ = (x0 |Î˝) â (x0 |Î˝) â d (Î˝|Î˝)
= (x0 |Î˝) â (x0 |Î˝) + d = d.
Also ist y0 â H und wir bekommen
dist(x0 , H) = inf{kx0 â yk2 : y â H} â¤ kx0 â y0 k2 =
= (x0 |Î˝) â d kÎ˝k2 = (x0 |Î˝) â d .


(x0 |Î˝) â d Î˝

Es bleibt die umgekehrte Ungleichung zu zeigen. Sei dazu U der Untervektorraum
von V mit H = y0 + U. Ist nun z â H beliebig, so gilt y0 â z â U und wir haben


(x0 â y0 |y0 â z) = ((x0 |Î˝) â d)Î˝ y0 â z = (x0 |Î˝) â d (Î˝|y0 â z) = 0,
da Î˝ âĽ u fuĚr alle u â U gilt. Das liefert nun

kx0 â zk22 = (x0 â z|x0 â z) = (x0 â y0 + y0 â z|x0 â y0 + y0 â z)
= (x0 â y0 |x0 â y0 ) + (y0 â z|y0 â z) + 2(x0 â y0 |y0 â z).
Nun ist nach der VoruĚberlegung der letzte Summand Null und die ersten beiden
lassen sich als Normen schreiben, die beide positiv sind. Das ergibt
kx0 â zk22 = kx0 â y0 k22 + ky0 â zk22 âĽ kx0 â y0 k22 .
Also ist kx0 â y0 k2 â¤ kx0 â zk2 fuĚr jedes z â H, was uns zum Abschluss
kx0 â y0 k2 â¤ inf{kx0 â zk2 : z â H} = dist(x0 , H)
liefert.

76

3.5. Geometrie im Rn
Beispiel 3.5.13. Wir wollen das obige Verfahren zur Abstandsberechnung einmal beispielhaft anwenden. Wir betrachten dazu in R3 die Ebene
ďŁąďŁŤ ďŁś
ďŁź ďŁŤ ďŁś *ďŁŤ ďŁś ďŁŤ ďŁś+
ďŁŤ ďŁś
ďŁŤ ďŁś
1
1
1
1
1
ďŁ˛ 1
ďŁ˝
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
ďŁ­
0 + Îť 1 + Âľ 2 : Îť, Âľ â R = 0 +
1 , 2ďŁ¸ .
E :=
ďŁł
ďŁž
1
0
2
1
0
2

und bestimmen ihren Abstand vom Punkt (1, 1, 1)T . Dazu ermitteln wir zunaĚchst
die Hesse-Normalform von E, d.h. wir suchen einen Normaleneinheitsvektor Î˝ =
(Î˝1 , Î˝2 , Î˝3 )T â R3 mit Î˝ âĽ (1, 1, 0)T und Î˝ âĽ (1, 2, 2)T , sowie kÎ˝k2 = 1.
Aus der ersten Bedingung bekommen wir die Gleichung Î˝1 +Î˝2 = 0, d.h. Î˝1 = âÎ˝2 .
Die zweite Bedingung liefert Î˝1 + 2Î˝2 + 2Î˝3 = 0, d.h. wir erhalten durch Einsetzen
der ersten Gleichung Î˝1 â 2Î˝1 + 2Î˝3 = 0. Das liefert Î˝1 = 2Î˝3 . Wir setzen nun
Î˝1 = 2 und erhalten Î˝2 = â2 und Î˝3 = 1. Ein Vektor, der die ersten beiden
Bedingungen erfuĚllt, ist also Î˝Ě := (2, â2, 1)T .
Dieser hat nun noch nicht LaĚnge Eins. Wir beachten, dass mit Î˝Ě âĽ x auch ÎąÎ˝Ě âĽ x
fuĚr jedes Îą â R gilt. Auch die Vektoren
â ÎąÎ˝Ě erfuĚllen also weiter die ersten beiden
Bedingungen. Es reicht also kÎ˝Ěk2 = 4 + 4 + 1 = 3 zu bestimmen und Î˝Ě damit
zu normierenâ. Dann ist
ďŁŤ ďŁś
â
2
1
1ďŁ­ ďŁ¸
â2
Î˝ := Î˝Ě =
3
3
1
der gesuchte Normaleneinheitsvektor.
Weiter ist der Vektor x := (1, 0, 1)T ein Element von E, womit wir
ďŁŤ ďŁŤ ďŁś ďŁŤ ďŁśďŁś
1
2
1
1
d = (x|Î˝) = ďŁ­ ďŁ­0ďŁ¸ ďŁ­â2ďŁ¸ďŁ¸ = (2 + 1) = 1
3
3
1
1

bestimmen. Also ist die Hesse-Normalform gegeben durch
n
o
2
2
1
E = {y â Rn : (y|Î˝) = 1} = y = (y1 , y2 , y3 )T â R3 : y1 â y2 + y3 = 1 .
3
3
3
Damit bekommen wir nun sofort mit Satz 3.5.12
ďŁŤ ďŁŤ ďŁś ďŁŤ ďŁśďŁś
2
1

1
2
1 ďŁ­ ďŁ¸ďŁ¸
T
ďŁ­
ďŁ­
ďŁ¸
â2
1
â d = (2 â 2 + 1) â 1 = .
dist (1, 1, 1) , E =
3
3
3
1
1

Das muĚhsamste an obiger Berechnung war die Bestimmung von Î˝Ě, also eines Vektors, der senkrecht auf dem Untervektorraum steht, der die Ebene aufspannt. Im
fuĚr das reale Leben wichtigen Spezialfall des dreidimensionalen Raums R3 gibt es
zum GluĚck eine relativ einfache MoĚglichkeit einen solchen Vektor zu bestimmen.
Diese wollen wir zum Abschluss dieses Abschnitts noch schnell angeben.

77

3. Lineare Algebra
Definition 3.5.14. Es seien x = (x1 , x2 , x3 )T und y = (y1 , y2, y3 )T aus R3 . Dann
heiĂt der Vektor
ďŁŤ ďŁś ďŁŤ ďŁś
ďŁŤ
ďŁś
x1
y1
x2 y3 â y2 x3
ďŁ­x2 ďŁ¸ Ă ďŁ­y2 ďŁ¸ := ďŁ­x3 y1 â y3 x1 ďŁ¸
x3
y3
x1 y2 â y1 x2
das Kreuzprodukt von x und y.

Satz 3.5.15. Sind x, y â R3 , so gilt (x Ă y) âĽ x und (x Ă y) âĽ y.

3.6. Lineare Abbildungen
Definition 3.6.1. Es seien V und W zwei K-VektorraĚume bezuĚglich desselben
KoĚrpers K.
(a) Eine Abbildung ÎŚ : V â W heiĂt linear oder (Vektorraum-)Homomorphismus, falls fuĚr alle a, b â V und alle Îą â K
ÎŚ(a + b) = ÎŚ(a) + ÎŚ(b)

und

ÎŚ(Îąa) = ÎąÎŚ(a)

gilt.
(b) Ist ÎŚ zusaĚtzlich bijektiv, so heiĂt ÎŚ (Vektorraum-)Isomorphismus, und V
und W werden dann als isomorph bezeichnet, in Zeichen: V âź
= W.
Die beiden Bedingungen in der Definition einer lineare Abbildung koĚnnen zu einer
verschmolzen werden:
Satz 3.6.2. Seien V und W zwei K-VektorraĚume. Dann ist ÎŚ : V â W genau
dann linear, wenn fuĚr alle a, b â V und alle Îą, Î˛ â K gilt
ÎŚ(Îąa + Î˛b) = ÎąÎŚ(a) + Î˛ÎŚ(b).
Beweis. ââ Mit den beiden Eigenschaften aus Defintion 3.6.1 (a) folgt nachâ
einander
ÎŚ(Îąa + Î˛b) = ÎŚ(Îąa) + ÎŚ(Î˛b) = ÎąÎŚ(a) + Î˛ÎŚ(b).
ââ Seien a, b â V . Dann gilt nach Voraussetzung mit Îą = Î˛ = 1
â
ÎŚ(a + b) = ÎŚ(1 Âˇ a + 1 Âˇ b) = 1 Âˇ ÎŚ(a) + 1 Âˇ ÎŚ(b) = ÎŚ(a) + ÎŚ(b).
Sind a â V und Îą â K, so ist auf die gleiche Weise mit b = 0 und Î˛ = 0
ÎŚ(Îąa) = ÎŚ(Îąa + 0 Âˇ 0) = ÎąÎŚ(a) + 0 Âˇ ÎŚ(0) = ÎąÎŚ(a).
UĚbungsaufgabe 3.6.3. (a) FuĚr jeden Vektorraumhomomorphismus ÎŚ : V â
W gilt ÎŚ(0V ) = 0W .

78

3.6. Lineare Abbildungen
(b) Sind ÎŚ : V â W und Î¨ : W â X lineare Abbildungen zwischen KVektorraĚumen V , W und X, so ist auch Î¨ âŚ ÎŚ : V â X linear.
(c) Ist ÎŚ : V â W ein Isomorphismus, so ist auch ÎŚâ1 : W â V eine lineare
Abbildung, also wieder ein Isomorphismus.
Beispiel 3.6.4. (a) FuĚr zwei beliebige K-VektorraĚume V und W ist die Nullabbildung âŚ : V â W mit âŚ(a) = 0W fuĚr jedes a â V linear, denn fuĚr alle
a, b â V und alle Îą, Î˛ â K gilt
âŚ(Îąa + Î˛b) = 0W = Îą Âˇ 0W + Î˛ Âˇ 0W = ÎąâŚ(a) + Î˛âŚ(b).
(b) Sei V ein K-Vektorraum und Îť â K fest. Dann ist ÎŚÎť : V â V mit
ÎŚÎť (a) = Îťa, a â V , ein Homomorphismus. FuĚr Îť =
6 0 ist das sogar ein
â1
Isomorphismus mit ÎŚÎť = ÎŚ1/Îť .
(c) Zu vorgegebenem v 6= 0 aus einem K-Vektorraum V betrachten wir die
Abbildung Î¨v : V â V mit Î¨v (a) = a + v fuĚr jedes a â V . Dieses ist keine
lineare Abbildung, denn es gilt Î¨v (0) = 0 + v = v 6= 0.
Wir wollen uns noch ein paar sehr wichtige lineare Abbildungen im Anschauungsraum zu GemuĚte fuĚhren.
Beispiel 3.6.5. Die folgenden Abbildungen von Rn nach Rn sind linear:
(a) Die in UĚbungsaufgabe 3.4.16 definierte Orthogonalprojektion.
(b) Jede Spiegelung an einem (n â 1)-dimensionalen Untervektorraum.
(c) Die Streckung um einen Faktor Îť â R, vgl. Beispiel 3.6.4 (b)
(d) FuĚr n = 2 die Drehung der Ebene um einen Winkel Îą und fuĚr n = 3 die
Drehung des Raums um eine Ursprungsgerade.
(e) AuĂerdem alle Verkettungen der obigen, vgl. UĚbungsaufgabe 3.6.3 (b), also
alle Drehstreckspiegelungsprojektionen,. . .
Um den Isomorphie-Begriff zu beleuchten, zeigen wir beispielhaft den folgenden
Satz.
Satz 3.6.6. Es sei K ein KoĚrper und M eine Menge mit |M| = n â Nâ . Dann
gilt Abb(M, K) âź
= K n.
Beweis. Wir benennen M = {m1 , m2 , . . . , mn } und betrachten die Abbildung
(
Abb(M, K) â K n
T
ÎŚ:
f
7â f (m1 ), f (m2 ), . . . , f (mn ) ,
79

3. Lineare Algebra
von der wir nun zeigen wollen, dass sie ein Isomorphismus ist.
Zum Nachweis der LinearitaĚt seien f, g â Abb(M, K) und Îą, Î˛ â K. Dann gilt
ďŁŤ
ďŁś
ďŁś ďŁŤ
Îąf (m1 ) + Î˛g(m1 )
(Îąf + Î˛g)(m1)
ďŁŹ (Îąf + Î˛g)(m2) ďŁˇ ďŁŹ Îąf (m2 ) + Î˛g(m2 ) ďŁˇ
ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
ÎŚ(Îąf + Î˛g) = ďŁŹ
ďŁˇ
ďŁˇ=ďŁŹ
..
..
ďŁ­
ďŁ­
ďŁ¸
ďŁ¸
.
.
Îąf (mn ) + Î˛g(mn )
(Îąf + Î˛g)(mn )
ďŁŤ
ďŁś
ďŁŤ
ďŁś
f (m1 )
g(m1 )
ďŁŹ f (m2 ) ďŁˇ
ďŁŹ g(m2 ) ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
= Îą ďŁŹ .. ďŁˇ + Î˛ ďŁŹ .. ďŁˇ = ÎąÎŚ(f ) + Î˛ÎŚ(g)
ďŁ­ . ďŁ¸
ďŁ­ . ďŁ¸
f (mn )
g(mn )

und die LinearitaĚt von ÎŚ folgt aus Satz 3.6.2.
Es bleibt also noch die BijektivitaĚt von ÎŚ zu zeigen. Seien dazu f, g â Abb(M, K)
mit ÎŚ(f ) = ÎŚ(g) gegeben. Dann ist
T
T
f (m1 ), f (m2 ), . . . , f (mn ) = g(m1 ), g(m2 ), . . . , g(mn )

und damit f (mj ) = g(mj ) fuĚr jedes j â {1, 2, . . . , n}. Also ist f = g und wir
wissen, dass ÎŚ injektiv ist.
Sei weiter x = (x1 , x2 , . . . , xn )T â K n gegeben. FuĚr die Funktion f : M â K mit
T
f (mj ) = xj , j â {1, 2, . . . , n}, gilt dann ÎŚ(f ) = f (m1 ), f (m2 ), . . . , f (mn ) =
(x1 , x2 , . . . , xn )T = x. Also ist ÎŚ auch surjektiv und wir sind fertig.
UĚbungsaufgabe 3.6.7. (a) Zeigen Sie, dass die Abbildung
(
R3
â R4
ÎŚ:
(x1 , x2 , x3 )T 7â (x2 , x1 + x2 , x2 + x3 , x3 )T
linear und injektiv, aber nicht surjektiv ist.
(b) Finden Sie eine surjektive lineare Abbildung, die nicht injektiv ist oder
zeigen Sie, dass es eine solche nicht geben kann.
Satz 3.6.8. Es seien V und W zwei K-VektorraĚume, n â Nâ und ÎŚ : V â W
sei linear.
(a) Sind v1 , v2 , . . . , vn â V linear abhaĚngig, so sind auch ÎŚ(v1 ), ÎŚ(v2 ), . . . , ÎŚ(vn )
linear abhaĚngig.
(b) Ist ÎŚ injektiv und sind v1 , v2 , . . . , vn â V linear unabhaĚngig, so sind auch
ÎŚ(v1 ), ÎŚ(v2 ), . . . , ÎŚ(vn ) linear unabhaĚngig.
(c) Ist ÎŚ ein Isomorphismus und B eine Basis von V , so ist auch ÎŚ(B) eine
Basis von W . Insbesondere gilt dim(V ) = dim(W ).

80

3.6. Lineare Abbildungen
Pn
Beweis. (a) Es sei
j=1 Îąj vj = 0V eine nichttriviale Linearkombination des
Nullvektors. Dann gilt wegen UĚbungsaufgabe 3.6.3 (a) und der LinearitaĚt
von ÎŚ
n
n
X
 X
0W = ÎŚ(0V ) = ÎŚ
Îąj vj =
Îąj ÎŚ(vj )
j=1

j=1

und dieses ist eine nichttriviale Linearkombination des Nullvektors in W .
Also sind ÎŚ(v1 ), ÎŚ(v2 ), . . . , ÎŚ(vn ) linear abhaĚngig.
(b) Wir nehmen an, dass ÎŚ(v1 ), ÎŚ(v2 ), . . . , ÎŚ(vP
n ) linear abhaĚngig sind. Dann
gibt es eine nichttriviale Linearkombination nj=1 Îąj ÎŚ(vj ) = 0W . Mit dieser
und der LinearitaĚt von ÎŚ gilt nun
ÎŚ(0V ) = 0W =

n
X
j=1

n
X

Îąj ÎŚ(vj ) = ÎŚ
Îąj vj .
j=1

P
Nun ist ÎŚ nach Voraussetzung injektiv, also haben wir 0V = nj=1 Îąj vj , was
eine nichttriviale Linearkombination des Nullvektors aus v1 , v2 , . . . , vn waĚre
und damit ein Widerspruch zur linearen UnabhaĚngigkeit dieser Vektoren.
(c) Sei ÎŚ bijektiv und B eine Basis von V . Dann ist dank (b) auch ÎŚ(B) eine
linear unabhaĚngige Teilmenge von W . Wir muĚssen noch zeigen, dass ÎŚ(B)
ganz W erzeugt. Sei dazu w â W beliebig vorgegeben. Da ÎŚ surjektiv
ist, gibt es ein v â V mit ÎŚ(v) = w. Weiter ist B eine Basis von V , also
gibt P
es ein m â N und b1 , b2 , . . . , bm â B, sowie Îą1 , Îą2 , . . . , Îąm â K mit
v= m
j=1 Îąj bj . Dann ist aber
m
m
X
 X
w = ÎŚ(v) = ÎŚ
Îąj bj =
Îąj ÎŚ(bj ),
j=1

j=1

was bedeutet, dass w â hÎŚ(B)i ist.
Bemerkung 3.6.9. Damit haben wir im Zusammenspiel mit Satz 3.6.6 insbesondere gezeigt, dass dim(Abb(M, K)) = |M| gilt und das entsprechende cchen
aus Beipsiel 3.2.20 (c) eingeloĚst.
UĚbungsaufgabe 3.6.10. (a) Zeigen Sie, dass jeder n-dimensionale K-Vektorraum isomorph zu K n ist, vgl. Bemerkung 3.2.26.
(b) Seien V und W zwei endlichdimensionale VektorraĚume. Zeigen Sie, dass
V âź
= W genau dann gilt, wenn dim(V ) = dim(W ) ist.
Wir wollen nun den fundamental wichtigen Satz beweisen, dass eine lineare Abbildung durch die Angabe der Bilder der Basisvektoren festgelegt werden kann.

81

3. Lineare Algebra
Satz 3.6.11. Es seien V, W zwei K-VektorraĚume und V sei n-dimensional mit
einer Basis B = {b1 , b2 , . . . , bn }. FuĚr jede Wahl von w1 , w2 , . . . , wn â W gibt es
dann genau eine lineare Abbildung ÎŚ : V â W , fuĚr die ÎŚ(bj ) = wj fuĚr alle
j â {1, 2, . . . , n} gilt.

Beweis. Wir zeigen zunaĚchst, dass eine solche Abbildung ÎŚ, wenn sie existiert,
durch die Angabe der ÎŚ(bj ), j = 1, 2, . . . , n, eindeutig bestimmt ist. Sei dazu
v â V beliebig. Da B eine Basis von V P
ist, gibt es dann eindeutig bestimmte
Koeffizienten Îą1 , Îą2 , . . . , Îąn â K mit v = nj=1 Îąj bj . Wegen der LinearitaĚt von ÎŚ
muss dann gelten
n
n
n
X
 X
X
ÎŚ(v) = ÎŚ
Îąj ÎŚ(bj ) =
Îąj w j .
Îąj bj =
j=1

j=1

j=1

Das Bild eines jeden v â V liegt also durch die Angabe der Werte w1 , w2 , . . . , wn
bereits fest.
Obiger Eindeutigkeitsbeweis liefert nun auch gleich die Blaupause fuĚr die Konstruktion der gesuchten Abbildung. Wir definieren einfach fuĚr jedes v â V die
Abbildung ÎŚ durch
ÎŚ(v) :=

n
X

Îąj w j ,

falls v =

j=1

n
X

Îąj bj .

j=1

Wir muĚssen nun zeigen, dass das so definierte ÎŚ eine lineare Abbildung ist und
dass ÎŚ(bj ) = wj fuĚr jedes j â {1, 2, . . . , n} gilt.
Zum Nachweis der LinearitaĚt seien a, b â V und Îť, Âľ â KPgegeben. Mit den
Koeffizienten Îą1 , Îą2 , . . . , Îąn , Î˛1 , Î˛2 , . . . , Î˛n â K, fuĚr die a = nj=1 Îąj bj und b =
P
n
j=1 Î˛j bj gilt, haben wir dann nach der Definition von ÎŚ
n
n
n
 X

X

X
ÎŚ(Îťa + Âľb) = ÎŚ Îť
Îąj bj + Âľ
Î˛j bj = ÎŚ
(ÎťÎąj + ÂľÎ˛j )bj
j=1

=

n
X

j=1

(ÎťÎąj + ÂľÎ˛j )wj = Îť

j=1

j=1

n
X
j=1

Îąj w j + Âľ

n
X

Î˛j wj = ÎťÎŚ(a) + ÂľÎŚ(b).

j=1

SchlieĂlich ist fuĚr jedes j â {1, 2, . . . , n} die Basisdarstellung von bj gegeben
durch bj selbst, also gilt
ÎŚ(bj ) =

n
X
k=1
k6=j

0 Âˇ wk + 1 Âˇ wj = wj .

Definition 3.6.12. Es seien V, W zwei K-VektorraĚume und ÎŚ : V â W eine
lineare Abbildung. Dann heiĂt

ker(ÎŚ) := v â V : ÎŚ(v) = 0W
der Kern von ÎŚ.

82

3.6. Lineare Abbildungen
Satz 3.6.13. Es seien V, W zwei K-VektorraĚume und ÎŚ : V â W ein Homomorphismus. Dann gilt
(a) ker(ÎŚ) ist ein Untervektorraum von V .
(b) ÎŚ ist genau dann injektiv, wenn ker(ÎŚ) = {0V }.
(c) ÎŚ(V ) ist ein Untervektorraum von W , der sogenannte Bildraum von ÎŚ.
Beweis. (a) Zum Einen ist immer 0V â ker(ÎŚ), also ist der Kern nicht leer, zum
Anderen gilt fuĚr alle a, b â ker(ÎŚ) und alle Îą, Î˛ â K dank der LinearitaĚt
von ÎŚ
ÎŚ(Îąa + Î˛b) = ÎąÎŚ(a) + Î˛ÎŚ(b) = Îą Âˇ 0W + Î˛ Âˇ 0W = 0W
und damit auch Îąa + Î˛b â ker(ÎŚ). Die Behauptung folgt damit aus dem
Untervektorraumkriterium, vgl. Satz 3.2.3.
(b) Ist ÎŚ injektiv, so kann 0W auĂer 0V kein weiteres Urbild haben und es ist
ker(ÎŚ) = {0V }. Ist umgekehrt diese Mengengleichheit gegeben und haben
wir a, b â V mit ÎŚ(a) = ÎŚ(b), so gilt mit Hilfe der LinearitaĚt von ÎŚ
ÎŚ(a â b) = ÎŚ(a) â ÎŚ(b) = 0W ,

also a â b = 0V ,

womit a = b gezeigt ist. Das bedeutet aber gerade, dass ÎŚ injektiv ist.
(c) Zum Einen ist ÎŚ(V ) nicht leer und zum anderen gibt es fuĚr jede Wahl von
w, x â ÎŚ(V ) Vektoren u, v â V mit ÎŚ(u) = w und ÎŚ(v) = x. Damit ist fuĚr
alle Îą, Î˛ â K auch
Îąw + Î˛x = ÎąÎŚ(u) + Î˛ÎŚ(v) = ÎŚ(Îąu + Î˛v) â ÎŚ(V ),
die Behauptung folgt also wieder aus dem Untervektorraumkriterium.
Definition 3.6.14. Ist in der Situation von Satz 3.6.13 der Raum ÎŚ(V ) endlichdimensional, so heiĂt Rang(ÎŚ) := dim(ÎŚ(V )) der Rang von ÎŚ.
Wir haben in obigem Satz gezeigt, dass ker(ÎŚ) ein Untervektorraum von V ist. In
Erinnerung an Abschnitt 3.3 koĚnnen wir also den Faktorraum V /ker(ÎŚ) betrachten. Wie sieht eine Restklasse in diesem Raum aus? Erstens gilt 0f
V = ker(ÎŚ) und
allgemein ist nach der Definition des Faktorraums
a â bĚ ââ a â b â ker(ÎŚ) ââ ÎŚ(a â b) = 0W
ââ ÎŚ(a) â ÎŚ(b) = 0W ââ ÎŚ(a) = ÎŚ(b).

Das bedeutet, dass die Elemente einer AĚquivalenzklasse aĚ genau die Elemente
von V sind, die unter ÎŚ das selbe Bild wie a haben. Damit ist die Abbildung
(
V /ker(ÎŚ) â W
(3.2)
ÎŚĚ :
vĚ
7â ÎŚ(v)
wohldefiniert. Diese hat eine wesentliche Bedeutung durch den folgenden Satz.

83

3. Lineare Algebra
Satz 3.6.15 (Homomorphiesatz fuĚr VektorraĚume). Seien V und W zwei KVektorraĚume, ÎŚ : V â W linear und ÎŚĚ wie in (3.2) definiert. Dann ist ÎŚĚ :
V /ker(ÎŚ) â ÎŚ(V ) ein Isomorphismus und es gilt ÎŚ = ÎŚĚ âŚ Î˝, wobei Î˝ : V â
V /ker(ÎŚ) die kanonische Abbildung ist.
Beweis. Wir haben schon festgestellt, dass ÎŚĚ eine wohldefinierte Abbildung ist.
Wir zeigen also LinearitaĚt von ÎŚĚ. Seien dazu aĚ, bĚ â V /ker(ÎŚ) und Îą, Î˛ â K. Dann
gilt
^
ÎŚĚ(ÎąaĚ + Î˛ bĚ) = ÎŚĚ(Îąa
+ Î˛b) = ÎŚ(Îąa + Î˛b) = ÎąÎŚ(a) + Î˛ÎŚ(b) = ÎąÎŚĚ(aĚ) + Î˛ ÎŚĚ(bĚ).
Zum Nachweis der InjektivitaĚt von ÎŚĚ sei aĚ â ker(ÎŚĚ). Dann gilt ÎŚĚ(aĚ) = 0W . Also
f
ist ÎŚ(a) = 0W , was wiederum a â ker(ÎŚ) = 0f
V impliziert und schlieĂlich aĚ = 0V
liefert. Diese UĚberlegungen bedeuten ker(ÎŚĚ) = {0f
V } und mit Satz 3.6.13 (b) folgt
die InjektivitaĚt von ÎŚ.
Da wir den Zielbereich von ÎŚĚ auf ÎŚ(V ) eingeschraĚnkt haben, ist ÎŚĚ auch surjektiv
und wir haben bewiesen, dass ÎŚĚ : V â ÎŚ(V ) ein Isomorphismus ist.
AbschlieĂend bleibt noch zu bemerken, dass fuĚr jedes a â V gilt
(ÎŚĚ âŚ Î˝)(a) = ÎŚĚ(Î˝(a)) = ÎŚĚ(aĚ) = ÎŚ(a).
Korollar 3.6.16. Seien V und W zwei K-VektorraĚume, wobei V endliche Dimension habe. Ist dann ÎŚ : V â W eine lineare Abbildung, so gilt die Dimensionsformel
Rang(ÎŚ) + dim(ker(ÎŚ)) = dim(V ).
Beweis. Es gilt nach dem Homomorphiesatz V /ker(ÎŚ) âź
= ÎŚ(V ). Also erhalten wir
mit UĚbungsaufgabe 3.6.10 (b)
dim(V /ker(ÎŚ)) = dim(ÎŚ(V )) = Rang(ÎŚ).
Andererseits liefert Satz 3.3.5
dim(V /ker(ÎŚ)) = dim(V ) â dim(ker(ÎŚ))
und die Kombination der beiden Gleichungen liefert die Behauptung.
Satz 3.6.17. Es sei V ein endlichdimensionaler K-Vektorraum und ÎŚ : V â V
eine lineare Abbildung. Dann sind die folgenden Aussagen aĚquivalent:
(a) ÎŚ ist bijektiv.
(b) ÎŚ ist injektiv.
(c) ker(ÎŚ) = {0}.
(d) Rang(ÎŚ) = dim(V ).

84

3.6. Lineare Abbildungen
(e) ÎŚ ist surjektiv.
Beweis. Wir zeigen (a) â (b) â (c) â (d) â (e) â (a). Dann sind alle
AĚquivalenzen gezeigt, denn man kommt dann von jedem Buchstaben zu jedem
Buchstaben.
Die Implikation (a) â (b) ist klar und (b) â (c) findet sich in Satz 3.6.13 (b).
Wir zeigen also (c) â (d). Ist ker(ÎŚ) = {0}, so ist die Dimension dieses Raumes
Null und wir erhalten mit der Dimensionsformel aus Korollar 3.6.16 Rang(ÎŚ) =
dim(V ) und damit (d).
Zum Nachweis von (d) â (e) erinnern wir uns, dass Rang(ÎŚ) = dim(ÎŚ(V )) ist.
Gilt also (d), so haben wir dim(ÎŚ(V )) = dim(V ). Dann ist ÎŚ(V ) ein Unterraum
von V mit gleicher Dimension wie V , es muss also ÎŚ(V ) = V sein und ÎŚ ist
surjektiv.
Es bleibt noch (a) aus (e) zu folgern. Ist ÎŚ surjektiv, so gilt ÎŚ(V ) = V , also
insbesondere
Rang(ÎŚ) = dim(ÎŚ(V )) = dim(V ).
Mit der Dimensionsformel sehen wir, dass dann dim(ker(ÎŚ)) = 0 sein muss, also
ist ker(ÎŚ) = {0}. Das liefert aber nach Satz 3.6.13 (b) die InjektivitaĚt von ÎŚ und
damit (a).
Beispiel 3.6.18. Wir betrachten die lineare Abbildung ÎŚ : R3 â R3 mit
ďŁŤ
ďŁś
ďŁŤ ďŁś
x2 + x3
x1
ďŁ­
ďŁ¸
ďŁ­
ÎŚ(x) =
x1 + x3 ,
x = x2 ďŁ¸ â R3 .
âx1 + x2
x3

und wollen ihren Kern und ihr Bild bestimmen. Zur Bestimmung des Kerns suchen wir alle x = (x1 , x2 , x3 )T â R3 mit ÎŚ(x) = 0, also haben wir das Gleichungssystem
ďŁą
x2 + x3 = 0
ďŁ˛
x1 +
x3 = 0
ďŁł
âx1 + x2
= 0

zu loĚsen. Aus der letzten Gleichung bekommen wir x1 = x2 und aus der ersten
x2 = âx3 . Also ist auch x1 = âx3 und die zweite Gleichung ist automatisch
erfuĚllt. Die LoĚsungsmenge des obigen Gleichungssystems ist also
ďŁź *ďŁŤ ďŁś+
ďŁąďŁŤ ďŁś
1
ďŁ˝
ďŁ˛ Îą
3
ďŁ­
ďŁ­
ďŁ¸
1ďŁ¸ .
Îą
âR :ÎąâR =
ker(ÎŚ) =
ďŁž
ďŁł
â1
âÎą

Da damit dim(ker(ÎŚ)) = 1 ist, wissen wir nach der Dimensionsformel schon, dass
dim(ÎŚ(V )) = Rang(ÎŚ) = 2 sein muss. Damit reicht es zur Bestimmung des Bildes
von ÎŚ aus, wenn wir zwei linear unabhaĚngige Vektoren in ÎŚ(V ) angeben koĚnnen.
Dazu betrachten wir aufs Geratewohl die Bilder
ÎŚ((1, 0, 0)T ) = (0, 1, â1)T

und

ÎŚ((0, 1, 0)T ) = (1, 0, 1)T .

85

3. Lineare Algebra
Da diese beiden linear unabhaĚngig sind, gilt
*ďŁŤ 0 ďŁś ďŁŤ1ďŁś+
ÎŚ(V ) = ďŁ­ 1 ďŁ¸ , ďŁ­0ďŁ¸ .
â1
1

Bemerkung 3.6.19. Seien V und W endlichdimensionale K-VektorraĚume und
B := {b1 , b2 , . . . , bn } eine Basis von V , sowie C := {c1 , c2 , . . . , cp } eine Basis von
W . Weiter sei ÎŚ : V â W eine lineare Abbildung.
Ein gegebenes x â V koĚnnen wir nun bezuĚglich der Basis B darstellen und erhalten
n
X
x=
Îžk bk ,
k=1

T

n

wobei ~x = (Îž1 , Îž2 , . . . , Îžn ) â K der Koordinatenvektor von x bezuĚglich B ist.
Damit gilt nun dank der LinearitaĚt von ÎŚ
n
n
X
 X
ÎŚ(x) = ÎŚ
Îžk bk =
Îžk ÎŚ(bk ).
k=1

k=1

Stellen wir nun wiederum jedes ÎŚ(bk ) durch seine Koordinaten bzgl. C dar, also
bestimmen wir fuĚr jedes k â {1, 2, . . . , n} Koeffizienten Îą1,k , Îą2,k , . . . , Îąp,k â K
mit
p
X
Îąj,k cj ,
ÎŚ(bk ) =
j=1

so erhalten wir zusammen
p  n
p
n
n

X
X
X
X
X
Îąj,k Îžk cj .
Îąj,k cj =
Îžk
Îžk ÎŚ(bk ) =
ÎŚ(x) =
k=1

k=1

j=1

j=1

k=1

Was sagt uns dieser Zeichenwust nun? Wir haben eine Darstellung
Pnvon ÎŚ(x) in
der Basis C in W angegeben. Dabei koĚnnen wir die Koeffizienten k=1 Îąj,k Îžk berechnen, wenn wir zum Einen die Îžk , fuĚr jedes k kennen und zum anderen die Koeffizienten Îąj,k . Die Îžk lassen sich direkt aus x bestimmen, sobald wir die Basis B
haben, diese haben also nichts mit der speziellen Abbildung ÎŚ zu tun. Umgekehrt
bestimmen sich die Îąj,k ausschlieĂlich aus den Vektoren ÎŚ(b1 ), ÎŚ(b2 ), . . . , ÎŚ(bn )
und der Basis C. Diese sind also fuĚr jedes x â V die selben. Diese Beobachtungen
sind aus verschiedenen GruĚnden bemerkenswert:
1. Wir haben gesehen, dass es zur Berechnung von ÎŚ(x) fuĚr jedes x â V
ausreicht, wenn wir die n Vektoren ÎŚ(b1 ), ÎŚ(b2 ), . . . , ÎŚ(bn ) kennen.
2. Das bedeutet umgekehrt: Gibt uns jemand die gesamte Kollektion der Koeffizienten Îąj,k fuĚr j â {1, 2, . . . , p} und k â {1, 2, . . . , n}, so kennen wir die
lineare Abbildung ÎŚ, die dahinter steht komplett, denn wir koĚnnen dann
jedes ÎŚ(x) ausrechnen.

86

3.7. Matrizen und lineare Abbildungen

3.7. Matrizen und lineare Abbildungen
3.7.1. Matrixrechnung
Wir erinnern noch mal an den Vektorraum der pĂn-Matrizen uĚber einem KoĚrper
K, vgl. Beispiel 3.1.2 (b). Eine solche Matrix ist ein Schema von Elementen aus
K mit p Zeilen und n Spalten:
ďŁŤ
ďŁś
Îą11 Îą12 . . . Îą1n
ďŁŹÎą21 Îą22 . . . Îą2n ďŁˇ
ďŁŹ
ďŁˇ
A = ďŁŹ ..
..
.. ďŁˇ = (Îąjk )j=1,...,p,k=1,...,n ,
ďŁ­ .
. ÂˇÂˇÂˇ
. ďŁ¸
Îąp1 Îąp2 . . . Îąpn
wobei die Addition und Skalar-Multiplikation komponentenweise erklaĚrt sind.
Wichtig ist auĂerdem der Spezialfall n = 1. In diesem Fall hat die Matrix nur
eine Spalte, es hat also ein A â K pĂ1 die Form
ďŁŤ ďŁś
Îą11
ďŁŹÎą21 ďŁˇ
ďŁŹ ďŁˇ
A = ďŁŹ .. ďŁˇ .
ďŁ­ . ďŁ¸
Îąp1
und wir haben K pĂ1 âź
= K p.
Wir wollen nun eine weitere Rechenoperation einfuĚhren, die Matrixmultiplikation.

Definition 3.7.1. Es seien K ein KoĚrper und n, p, q â Nâ . Weiter seien zwei
Matrizen A = (Îąjâ )j=1,...,q,â=1,...,p â K qĂp , sowie B = (Î˛âk )â=1,...,p,k=1,...,n â K pĂn
gegeben. Dann definieren wir das Matrixprodukt A Âˇ B = AB â K qĂn als
AB :=

p
X
â=1

Îąjâ Î˛âk



j=1,...,q,k=1,...,n

.

Bemerkung 3.7.2. (a) Man beachte, dass das Produkt zweier Matrizen nur
dann definiert ist, wenn die Anzahl der Spalten der ersten Matrix gleich der
Anzahl der Zeilen der zweiten ist.
(b) Sieht man die Matrix A als Spaltenvektor ihrer Zeilen und B als Zeilenvektor der Spalten, d.h.
ďŁŤ ďŁś
aT1
ďŁŹaT ďŁˇ
ďŁŹ 2ďŁˇ
A = ďŁŹ .. ďŁˇ und B = (b1 , b2 , . . . , bn )
ďŁ­. ďŁ¸
aTq
87

3. Lineare Algebra
wobei a1 , a2 , . . . , aq â K p die Zeilen von A und b1 , b2 , . . . , bn â K p die
Spalten von B sind, so bekommt man den Eintrag Îłjk von AB fuĚr j â
{1, 2, . . . , q} und k â {1, 2, . . . , n}, indem man das Skalarprodukt der j-ten
Zeile von A mit der k-ten Spalte von B berechnet, also
Îłjk = (aj |bk ) =

p
X

Îąjâ Î˛âk .

â=1

Beispiel 3.7.3.
ďŁŤ
ďŁś


1 2
2 â1 0
â R2Ă3 und B = ďŁ­ 1 0ďŁ¸ â R3Ă2 .
(a) Wir betrachten A =
3 5 1
â1 5
Dann ist A Âˇ B â R2Ă2 mit

ďŁŤ
ďŁś




1 2
2 â1 0 ďŁ­
1 4
ďŁ¸
Âˇ 1 0 =
AÂˇB =
3 5 1
7 11
â1 5

und B Âˇ A â R3Ă3 mit
ďŁŤ

ďŁś
ďŁŤ
ďŁś


1 2
8 9 2
2 â1 0
B Âˇ A = ďŁ­ 1 0ďŁ¸ Âˇ
= ďŁ­ 2 â1 0ďŁ¸ .
3 5 1
â1 5
13 26 5

Man beachte insbesondere, dass damit die Matrixmultiplikation nicht kommutativ ist.
ďŁŤ
ďŁś
ďŁŤ ďŁś
3 1
0
3
3Ă3
ďŁ­
ďŁ¸
ďŁ­
(b) Sei nun A = 2 â1 1
âR
und x = 0ďŁ¸ â R3 = R3Ă1 .
0 2 â2
2
Dann ist auch Ax â R3Ă1 = R3 mit
ďŁŤ
ďŁśďŁŤ ďŁś ďŁŤ ďŁś
3 1
0
3
9
ďŁ­
ďŁ¸
ďŁ­
ďŁ¸
ďŁ­
2
â1
1
0
8 ďŁ¸.
Ax =
=
0 2 â2
2
â4

FuĚr die Matrixmultiplikation gelten die folgenden Rechenregeln:
Satz 3.7.4. Seien A, D â K qĂp und B, C â K pĂn , sowie Îť â K. Dann gilt
(a) A Âˇ (ÎťB) = (ÎťA) Âˇ B = Îť(A Âˇ B).
(b) A Âˇ (B + C) = A Âˇ B + A Âˇ C und (A + D) Âˇ B = A Âˇ B + D Âˇ B.

88

3.7. Matrizen und lineare Abbildungen
Beweis. Wir beweisen die Aussage in (a), die zweite Aussage verbleibt als UĚbung.
Es sei A = (Îąjâ )j=1,...,q,â=1,...,p und B = (Î˛âk )â=1,...,p,k=1,...,n . Dann gilt ÎťB =
(ÎťÎ˛âk )â=1,...,p,k=1,...,n und nach der Definition der Matrixmultiplikation
A Âˇ (ÎťB) =

p
X

Îąjâ (ÎťÎ˛âk )

â=1



j=1,...,q,k=1,...,n

=

p
X

(ÎťÎąjâ )Î˛âk

â=1



j=1,...,q,k=1,...,n

.

Dieses ist nun zum Einen gleich der Matrix (ÎťA)ÂˇB und zum Anderen koĚnnen wir
nun das Îť ganz aus der Summe und dann aus der Matrix ziehen und bekommen
so
p

 X
Îąjâ Î˛âk
A Âˇ (ÎťB) = Îť
â=1

j=1,...,q,k=1,...,n

=Îť

p
X

Îąjâ Î˛âk

â=1



j=1,...,q,k=1,...,n

= Îť(A Âˇ B).

Bemerkung 3.7.5. Insbesondere gilt also fuĚr A, B â K pĂn und x, y â K n damit
A(x + y) = Ax + Ay

und

(A + B)x = Ax + Bx.

Besonders wichtig ist in der Matrixrechnung der Fall p = n. Man spricht dann
von einer quadratischen Matrix . Hat man zwei gleich groĂe quadratische Matrizen
A, B â RnĂn , so sind beide Produkte AB und BA definiert und wieder Elemente
von RnĂn . TatsaĚchlich gilt sogar der folgende Satz.
Satz 3.7.6. Sei n â Nâ und K ein KoĚrper. Dann ist (K nĂn , +, Âˇ) ein Ring mit
Eins, der fuĚr n âĽ 2 nicht kommutativ ist.
Beweis. Nach Beispiel 3.1.2 (b) ist K nĂn ein K-Vektorraum, also ist (K nĂn , +)
insbesondere eine abelsche Gruppe. Das Distributivgesetz ist genau die Aussage aus Satz 3.7.4 (b). Das Assoziativgesetz fuĚr die Multiplikation folgt aus der
entsprechenden Eigenschaft von K, denn fuĚr drei Matrizen A, B, C â K nĂn gilt


A(BC) = (Îąjâ )j,â=1,...,n Âˇ (Î˛âm )â,m=1,...,n Âˇ (Îłmk )m,k=1,...,n
n
n
n

X

X
X
Îąjâ
Î˛âm Îłmk
= (Îąjâ )j,â=1,...,n Âˇ
Î˛âm Îłmk )
=
â,k=1,...,n

m=1

=

n X
n
X

â=1 m=1

= (AB)C,

Îąjâ (Î˛âm Îłmk )



j,k=1,...,n

=

â=1

n X
n
X
m=1 â=1

j,k=1,...,n

m=1

(Îąjâ Î˛âm )Îłmk



j,k=1,...,n

wobei man fuĚr das letzte Gleichheitszeichen, die davor getaĚtigte Rechnung wieder
ruĚckwaĚrts machen muss.

89

3. Lineare Algebra
Das Einselement ist die Matrix
ďŁŤ
1 0
ďŁŹ0 1
ďŁŹ
I := ďŁŹ .. ..
ďŁ­. .
0 0

ďŁś
... 0
. . . 0ďŁˇ
ďŁˇ
= (Î´jk )j,k=1,...,n ,
. . .. ďŁˇ
. .ďŁ¸
... 1

denn fuĚr alle A = (Îąjâ )j,â=1,...,n â K nĂn gilt
AI = (Îąjâ )j,â=1,...,n Âˇ (Î´âk )â,k=1,...,n =
und umgekehrt genauso.
Die NichtkommutativitaĚt
ďŁŤ
0 0 ...
ďŁŹ .. .. . .
ďŁŹ. .
.
ďŁŹ
ďŁ­0 0 . . .
1 0 ...

n
X
â=1

Îąjâ Î´âk



j,k=1,...,n

= (Îąjk )j,k=1,...,n = A

sieht man schlieĂlich fuĚr jedes n âĽ 2 an
ďŁś ďŁŤ
ďŁśďŁŤ
ďŁś
0 ... 0 1
0
0 ... 0 0
.. ďŁˇ ďŁŹ0 . . . 0 0ďŁˇ ďŁŹ .. . . .. .. ďŁˇ
ďŁŹ
ďŁˇ ďŁŹ
. . .ďŁˇ
.ďŁˇ
ďŁˇ ďŁŹ .. . . .. .. ďŁˇ = ďŁŹ .
ďŁˇ
. . . ďŁ¸ ďŁ­ 0 . . . 0 0ďŁ¸
0ďŁ¸ ďŁ­ .
0 ... 0 0
0
0 ... 0 1

und
ďŁŤ
0
ďŁŹ0
ďŁŹ
ďŁŹ ..
ďŁ­.
0

ďŁśďŁŤ
0
... 0 1
ďŁŹ
ďŁˇ
. . . 0 0ďŁˇ ďŁŹ ...
ďŁŹ
. . .. .. ďŁˇ
. . . ďŁ¸ ďŁ­0
1
... 0 0

ďŁś ďŁŤ
1 0
0 ... 0
.. . . .. ďŁˇ ďŁŹ0 0
. .ďŁˇ = ďŁŹ
.
ďŁˇ ďŁŹ. .
0 . . . 0ďŁ¸ ďŁ­ .. ..
0 0
0 ... 0

ďŁś
... 0
. . . 0ďŁˇ
ďŁˇ
.
. . .. ďŁˇ
. .ďŁ¸
... 0

Definition 3.7.7. Die Matrix I = (Î´jk )j,k=1,...,n â K nĂn , d.h. das Einselement
von K nĂn , wird auch Einheitsmatrix genannt.
Definition 3.7.8. Sei A = (Îąjk )j=1,...,p,k=1,...,n â K pĂn eine Matrix. Dann heiĂt
AT := (Îąkj )k=1,...,n,j=1,...,p â K nĂp
die zu A transponierte Matrix.
Beispielsweise ist
ďŁŤ
ďŁś
T

3 1
3 2 5
= ďŁ­2 2ďŁ¸
1 2 3
5 3

ďŁŤ ďŁś
1
und (1 3 7)T = ďŁ­3ďŁ¸ ,
7

vgl. Beispiel 3.1.2 (a).
FuĚr das Transponieren gelten die folgenden Rechenregeln:

90

3.7. Matrizen und lineare Abbildungen
Satz 3.7.9.

(a) FuĚr alle A, B â K pĂn und alle Îť â K gilt

â˘ (A + B)T = AT + B T ,
â˘ (AT )T = A,

â˘ (ÎťA)T = ÎťAT .

(b) FuĚr alle A â K qĂp und B â K pĂn gilt (AB)T = B T AT .
Beweis. Wir beweisen nur beispielhaft den dritten Punkt von (a), der Rest verbleibt als UĚbung.
Sei also A = (Îąjk )j=1,...,p,k=1,...,n â K pĂn und Îť â K. Dann gilt nach der Definition
der Skalar-Multiplikation in K pĂn
ďŁŤ
ďŁśT ďŁŤ
ďŁś
ÎťÎą11 ÎťÎą12 . . . ÎťÎą1n
ÎťÎą11 ÎťÎą21 . . . ÎťÎąp1
ďŁŹÎťÎą21 ÎťÎą22 . . . ÎťÎą2n ďŁˇ
ďŁŹ ÎťÎą12 ÎťÎą22 . . . ÎťÎąp2 ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
T
(ÎťA)T = ďŁŹ ..
=
ďŁŹ ..
..
..
.. ďŁˇ
..
..
.. ďŁˇ = ÎťA .
ďŁ­ .
ďŁ­ .
.
.
. ďŁ¸
.
.
. ďŁ¸
ÎťÎąp1 ÎťÎąp2 . . . ÎťÎąpn
ÎťÎą1n ÎťÎą2n . . . ÎťÎąpn
UĚbungsaufgabe 3.7.10. Es sei A â K nĂn eine Matrix und (Âˇ|Âˇ) das Standardskalarprodukt auf K n . Zeigen Sie, dass dann fuĚr alle x, y â K n gilt
(Ax|y) = (x|AT y).
Im Lichte dieser UĚbungsaufgabe ist auch das Vertauschen der Reihenfolge in
Satz 3.7.9 (b) zu sehen:

(AB)T x|y = (x|ABy) = (AT x|By) = (B T AT x|y).

3.7.2. Die Abbildungsmatrix einer linearen Abbildung

Was haben nun Matrizen mit linearen Abbildungen zu tun? Dieser Frage wollen
wir jetzt nachgehen. Dazu sei ÎŚ : V â W eine linere Abbildung zwischen zwei
endlichdimensionalen K-VektorraĚumen, sowie B = {b1 , b2 , . . . , bn } eine Basis von
V und C = {c1 , c2 , . . . , cp } eine Basis von W .
In dieser Situation haben wir in Bemerkung 3.6.19 folgendesP
gesehen: Bestimmt
man Îąjk fuĚr j = 1, . . . , p und k = 1, . . . , n so, dass ÎŚ(bk ) = pj=1 Îąjk cj gilt und
stellt man x â V durch seinen Koordinatenvektor ~x = (Îž1 , Îž2, . . . , Îžn )T â K n
bezuĚglich B dar, so gilt
ÎŚ(x) =

p  n
X
X
j=1

k=1

p

X
(A~x)j cj ,
Îąjk Îžk cj =
j=1

d.h. der Vektor A~x enthaĚlt die Koordinaten des Vektors ÎŚ(x) bezuĚglich der Basis
C.

91

3. Lineare Algebra
Definition 3.7.11. Es seien V , W , B, C, n, p und ÎŚ : V â W wie oben.
Die Matrix A = (Îąjk )j=1,...,p,k=1,...,n â K pĂn heiĂt dann Darstellungsmatrix oder
Abbildungsmatrix von ÎŚ bezuĚglich B und C. Wir bezeichnen diese mit A =:
MCB (ÎŚ).
Im Kopf haben sollten Sie die folgende
Merkregel:

In den Spalten der Abbildungsmatrix stehen die Koordinaten
der Bilder der Basisvektoren.

Beispiel 3.7.12. Es sei ÎŚ : R2 â R2 die Spiegelung an der x2 -Achse. Das ist
nach Beispiel 3.6.5 (b) eine lineare Abbildung. Statten wir R2 mit der Standardbasis B = {b1 , b2 } = {( 10 ) , ( 01 )} aus, so koĚnnen wir die Abbildungsmatrix von ÎŚ
bezuĚglich B und B angeben, indem wir die Bilder der Basisvektoren bestimmen:
   
â1
1
= â1 Âˇ b1 + 0 Âˇ b2
=
ÎŚ(b1 ) = ÎŚ
0
0
und

   
0
0
= 0 Âˇ b1 + 1 Âˇ b2 .
=
ÎŚ(b2 ) = ÎŚ
1
1


â1 0
B
Also ist MB (ÎŚ) =
.
0 1
TatsaĚchlich ist zum Beispiel fuĚr x = ( 21 ) der Koordinatenvektor ~x bezuĚglich B
gleich dem Vektor x und wir bekommen
   

âââ
â2
2
â1 0
B
= ÎŚ(x).
=
MB (ÎŚ)~x =
1
1
0 1
Beispiel 3.7.13. Wir betrachten die lineare Abbildung ÎŚ : R2 â R2 mit
  

x1
3x1 â 2x2
ÎŚ
=
.
x2
x1 + x2
(a) Es sei zunaĚchst B = C = {( 10 ) , ( 01 )}. Dann ist
   
3
1
= 3 Âˇ c1 + 1 Âˇ c2
=
ÎŚ(b1 ) = ÎŚ
1
0
und

Also ist

   
â2
0
= â2 Âˇ c1 + 1 Âˇ c2 .
=
ÎŚ(b2 ) = ÎŚ
1
1
MCB (ÎŚ)

92

=

MBB (ÎŚ)



3 â2
.
=
1 1

3.7. Matrizen und lineare Abbildungen
(b) AĚndert man die Basen, bekommt man auch eine andere Abbildungsmatrix fuĚr die selbe Abbildung. Wir behalten B = {( 10 ) , ( 01 )} wie oben, aber
ersetzen C durch D = {d1 , d2} = {( 31 ) , ( â2
1 )}. Dann ist
 
 
â2
3
= d2 .
= d1 und ÎŚ(b2 ) =
ÎŚ(b1 ) =
1
1
Also ist damit
MDB (ÎŚ)



1 0
.
=
0 1

Damit ist MDB (ÎŚ) ( 11 ) = ( 11 ), aber es ist doch ÎŚ((1, 1)T ) = (1, 2)T ? Was ist
hier schief gegangen?
Nichts! Wir muĚssen nur richtig interpretieren. Nach unserer Definition der
Abbildungsmatrix ist das Ergebnis von MDB (ÎŚ) ( 11 ) der Koordinatenvektor
von ÎŚ(x) bezuĚglich D und nicht ÎŚ(x) selbst! Da nun D nicht die Standardbasis ist, sind Vektor und Koordinatenvektor nicht mehr identisch. Aber
tatsaĚchlich gilt
     

3
â2
1
1 Âˇ d1 + 1 Âˇ d2 =
+
=
= ÎŚ (1, 1)T .
1
1
2
Beispiel 3.7.14. Wir waĚhlen V = W und betrachten die IdentitaĚt id : V â V
auf V . Diese ist linear, also hat sie zu einer gegebenen Basis B = {b1 , b2 , . . . , bn }
von V eine Abbildungsmatrix bezuĚglich B und B, die wir nun bestimmen wollen.
Es ist id(bj ) = bj , also ist der Koordinatenvektor von id(bj ) der j-te Einheitsvektor. In der Abbildungsmatrix MBB (id) der IdentitaĚt steht also in der j-ten Spalte
der j-te Einheitsvektor, also
ďŁś
ďŁŤ
1 0 ... 0
. . .. ďŁˇ
ďŁŹ
. .ďŁˇ
ďŁŹ0 1
B
MB (id) = ďŁŹ . .
ďŁˇ = I,
.
ďŁ­ .. . . . . 0ďŁ¸
0 ... 0 1
d.h. die Abbildungsmatrix ist die Einheitsmatrix.

Bemerkung 3.7.15. Es seien V und W endlichdimensionale K-VektorraĚume
und B = {b1 , b2 , . . . , bn } eine Basis von V und C = {c1 , c2 , . . . , cp } eine Basis von
W . Dann haben wir nun gesehen, dass es zu jeder linearen Abbildung ÎŚ : V â W
eine Abbildungsmatrix MCB (ÎŚ) gibt. Aber auch umgekehrt wird ein Schuh daraus:
Geben wir eine beliebige Matrix A = (Îąjk )j=1,...,p,k=1,...,n â K pĂn vor, so gibt es
nach Satz 3.6.11 genau eine lineare Abbildung ÎŚA : V â W mit
ÎŚA (bk ) =

p
X

Îąjk cj .

j=1

93

3. Lineare Algebra
FuĚr diese Abbildung gilt dann MCB (ÎŚA ) = A (warum?).
Damit haben wir zusammengefasst folgendes gesehen: WaĚhlt man die Basen B
und C fest, so gibt es eine eins-zu-eins-Beziehung zwischen den linearen Abbildungen von V nach W und den Matrizen aus K pĂn . Das bedeutet, dass wir jede
lineare Abbildung zwischen endlichdimensionalen K-VektorraĚumen, also ein u.U.
durchaus unuĚbersichtliches Objekt, durch eine solche Matrix, also etwas recht
uĚberschaubares, beschreiben koĚnnen und dass uns jede Erkenntnis uĚber Matrizen
eine Erkenntnis uĚber lineare Abbildungen beschert und umgekehrt.
Definition 3.7.16. Es sei A â K pĂn eine Matrix und a1 , a2 , . . . , an â K p seien
die Spalten von A. Dann heiĂt
(a) Rang(A) := dim(ha1 , a2 , . . . , an i) (Spalten-)rang von A.
(b) der Untervektorraum ker(A) := {x â K n : Ax = 0} von K n Kern von A.
Der folgende Satz zeigt ein paar Beziehungen zwischen einer linearen Abbildung
und ihrer Abbildungsmatrix auf. Er bleibt hier ohne Beweis stehen.
Satz 3.7.17. Es seien V und W endlichdimensionale K-VektorraĚume mit Basen
B, bzw. C, ÎŚ : V â W eine lineare Abbildung und A = MCB (ÎŚ). Dann gilt
(a) Rang(ÎŚ) = Rang(A).
(b) Rang(A) ist die Maximalanzahl linear unabhaĚngiger Spalten von A.
(c) dim(ker(ÎŚ)) = dim(ker(A)).
(d) dim(V ) = Rang(A) + dim(ker(A)).
Im Abschnitt uĚber lineare Abbildungen haben wir in UĚbungsaufgabe 3.6.3 gesehen, dass die Verkettung von linearen Abbildungen und die Umkehrfunktion von
Isomorphismen wieder lineare Abbildungen sind. Wie bestimmt man nun deren
Abbildungsmatrizen, d.h. wie bekommt man MDB (Î¨âŚÎŚ) und MBC (ÎŚâ1 ) aus MCB (ÎŚ)
und MDC (Î¨)?
Zuerst zeigen wir, dass die Abbildungsmatrix der Verkettung genau das Matrixprodukt der beiden Abbildungsmatrizen von Î¨ und ÎŚ ist.
Satz 3.7.18. Es seien V , W und X endlichdimensionale K-VektorraĚume mit
Basen B, C, bzw. D. Weiter seien ÎŚ : V â W und Î¨ : W â X lineare Abbildungen. Dann gilt
MDB (Î¨ âŚ ÎŚ) = MDC (Î¨) Âˇ MCB (ÎŚ).
Beweis. Es sei B = {b1 , b2 , . . . , bn }, C = {c1 , c2 , . . . , cp } und D = {d1 , d2 , . . . , dq }.
Weiterhin seien MDC (Î¨) = (Îąjâ )j=1,...,q,â=1,...,p und MCB (ÎŚ) = (Î˛âk )â=1,...,p,k=1,...,n ,
sowie MDB (Î¨ âŚ ÎŚ) = (Îłjk )j=1,...,q,k=1,...,n .

94

3.7. Matrizen und lineare Abbildungen
Um die Abbildungsmatrix von Î¨ âŚ ÎŚ zu bestimmen, muĚssen wir die Koordinaten
von (Î¨ âŚ ÎŚ)(bk ) bezuĚglich der Basis D fuĚr jedes k = 1, . . . , n bestimmen. Es gilt
nach der Definition der Abbildungsmatrix
(Î¨ âŚ ÎŚ)(bk ) = Î¨(ÎŚ(bk )) = Î¨

p
X
â=1


Î˛âk câ .

Verwenden wir nun die LinearitaĚt von Î¨ und dann die Abbildungsmatrix von Î¨,
so erhalten wir
q  p
q
p
p

X
X
X
X
X
Îąjâ Î˛âk dj .
Îąjâ dj =
Î˛âk
Î˛âk Î¨(câ ) =
(Î¨ âŚ ÎŚ)(bk ) =
â=1

â=1

j=1

j=1

â=1

Also ist die Abbildungsmatrix von Î¨ âŚ ÎŚ gegeben durch
Îłjk =

p
X

Îąjâ Î˛âk

â=1

und das ist genau die Definition des Matrixprodukts.
Beispiel 3.7.19. Die lineare Abbildung in R2 , die man bekommt, wenn man
zunaĚchst die Abbildung ÎŚ in Beispiel 3.7.13 ausfuĚhrt und dann die Spiegelung an
der x2 -Achse, vgl. Beispiel 3.7.12, hat also bezuĚglich der Standardbasis in R2 die
Abbildungsmatrix

 
 

â1 0
3 â2
â3 2
Âˇ
=
.
0 1
1 1
1 1
Wir wenden uns der Abbildungsmatrix der Umkehrfunktion zu. Es seien also V
und W endlichdimensionale K-VektorraĚume mit Basen B, bzw. C und ÎŚ : V â W
ein Isomorphismus.
Wir bemerken zunaĚchst, dass ÎŚ nur bijektiv sein kann, wenn die Dimensionen
von V und W uĚbereinstimmen, denn nach der Dimensionsformel gilt
ÎŚ surj.

ÎŚ inj.

dim(W ) = Rang(ÎŚ) = dim(V ) â dim(ker(ÎŚ)) = dim(V ).
Also muss die Abbildungsmatrix einer solchen bijektiven Abbildung quadratisch
sein.
Es gilt nun ÎŚ âŚ ÎŚâ1 = idW und ÎŚâ1 âŚ ÎŚ = idV , also ist nach Beispiel 3.7.14
MCB (ÎŚ) Âˇ MBC (ÎŚâ1 ) = MCC (idW ) = I

und MBC (ÎŚâ1 ) Âˇ MCB (ÎŚ) = MBB (idV ) = I.

Nach Satz 3.7.6 ist I das neutrale Element der Multiplikation im Ring K nĂn . Wir
haben also gerade gezeigt, dass man einen Vektorraum-Isomorphismus daran erkennt, dass seine Abbildungsmatrix in diesem Ring ein multiplikatives Inverses
besitzt. Und dieses mulitplikative Inverse ist dann die Abbildungsmatrix der Umkehrfunktion. Wir gieĂen das in eine Definition.

95

3. Lineare Algebra
Definition 3.7.20. Es sei n â Nâ und K ein KoĚrper. Eine Matrix A â K nĂn
heiĂt invertierbar oder regulaĚr, falls ein Aâ1 â K nĂn existiert mit A Âˇ Aâ1 = I
und Aâ1 Âˇ A = I. Die Matrix Aâ1 heiĂt dann die Inverse von A.
Ist A nicht regulaĚr, so nennt man A singulaĚr.
Bemerkung 3.7.21. (a) Nicht jede von der Nullmatrix verschiedene quadratische Matrix ist invertierbar, d.h. K nĂn ist kein KoĚrper. Z.B. ist

 
 

1 0
0 0
0 0
Âˇ
=
,
1 0
3 7
0 0
d.h. ( 11 00 ) ist ein linker Nullteiler.
(b) Die Inverse ist eindeutig, denn sind Aâ˛ und Aâ˛â˛ zwei Inverse von A, so ist mit
dem schon mehrfach bemuĚhten Argument Aâ˛ = Aâ˛ (AAâ˛â˛ ) = (Aâ˛ A)Aâ˛â˛ = Aâ˛â˛ .
Satz 3.7.22. Es sei n â Nâ , K ein KoĚrper und A â K nĂn . Dann sind die
folgenden Aussagen aĚquivalent:
(a) A ist invertierbar.
(b) Rang(A) = n.
(c) ker(A) = {0}.
Beweis. Nach unseren obigen UĚberlegungen ist A genau dann invertierbar, wenn
die Abbildung ÎŚA , vgl. Bemerkung 3.7.15, bijektiv ist. Dies wiederum ist wegen
Satz 3.6.17 und Satz 3.7.17 (a) aĚquivalent dazu, dass Rang(A) = Rang(ÎŚA ) = n
ist. Damit haben wir (a) ââ (b) gezeigt.
Mit Hilfe der Dimensionsformel n = Rang(A) + dim(ker(A)) aus Satz 3.7.17 (d)
sieht man, dass (b) genau dann gilt, wenn dim(ker(A)) = 0 ist, was wiederum zu
(c) aĚquivalent ist.
Beispiel 3.7.23. Die Matrix


2 3
A=
â R2Ă2
â1 2
denn
â1

AA
und

=






1 7 0
1 2 â3
2 3
=
=I
Âˇ
â1 2
7 1 2
7 0 7

1
A A=
7

Im Kontrast dazu ist

96

ist invertierbar mit A



1 2 â3
,
=
7 1 2



â1

B :=

â1






 


1 7 0
2 3
2 â3
=
Âˇ
= I.
â1 2
1 2
7 0 7

2Ě 3Ě
f
â1 2Ě



â Z2Ă2
7

nicht invertierbar,

3.8. Lineare Gleichungssysteme
denn es ist



2Ě
5Ě f
â1



=



e
10
f
â5



 
3Ě
.
=
2Ě

Das bedeutet, dass die zweite Spalte von B ein Vielfaches der ersten Spalte ist,
die beiden sind also linear abhaĚngig. Die maximale Anzahl linear unabhaĚngiger
Spaltenvektoren von B ist folglich 1 und Satz 3.7.17(b) liefert, dass Rang(B) = 1
ist. Das bedeutet wiederum nach Satz 3.7.22 das Aus fuĚr die Invertierbarkeit.
Bemerkung 3.7.24. (a) Sind A, B â K nĂn invertierbare Matrizen, so ist auch
ihr Produkt AB invertierbar mit (AB)â1 = B â1 Aâ1 , denn
(AB)(B â1 Aâ1 ) = A(BB â1 )Aâ1 = AIAâ1 = AAâ1 = I
und genauso gilt umgekehrt B â1 Aâ1 AB = I.
Man beachte, dass sich die Reihenfolge der beiden Matrizen umkehrt!
(b) AuĂerdem gilt fuĚr jedes Îť â K \ {0} und jedes invertierbare A â K nĂn
(ÎťA)â1 = Îťâ1 Aâ1 .
UĚbungsaufgabe 3.7.25. Die Menge
GL(n, K) := {A â K nĂn : A invertierbar}
ist mit der Matrixmultiplikation als VerknuĚpfung eine Gruppe. Diese heiĂt allgemeine lineare Gruppe. Die AbkuĚrzung kommt von der englischen Bezeichnung
âgeneral linear groupâ.

3.8. Lineare Gleichungssysteme
In vielen ZusammenhaĚngen stoĚĂt man in der Linearen Algebra (und auch anderswo) auf die Problematik lineare Gleichungssysteme loĚsen zu muĚssen.
Die Problemstellung ist die folgende: Gegeben Koeffizienten Îąjk , j â {1, 2, . . . , p},
k â {1, 2, . . . , n}, aus einem KoĚrper K und weitere Elemente b1 , b2 , . . . , bp â K,
bestimme x1 , x2 , . . . , xn â K mit
Îą11 x1 + Îą12 x2 + Âˇ Âˇ Âˇ + Îą1n xn = b1
Îą21 x1 + Îą22 x2 + Âˇ Âˇ Âˇ + Îą2n xn = b2
..
..
..
..
..
.
.
.
.
.
Îąp1 x1 + Îąp2 x2 + Âˇ Âˇ Âˇ + Îąpn xn = bp .

97

3. Lineare Algebra
Setzt man b := (b1 , b2 , . . . , bp )T â K p und A := (Îąjk )j=1,...,p,k=1,...,n â K pĂn , so
kann man jedes lineare Gleichungssystem in der Matrixform
Ax = b
schreiben.
Definition 3.8.1. (a) Ein lineares Gleichungssystem (LGS) ist die Gleichung
Ax = b, wobei A â K pĂn und b â K p gegeben und der Vektor x â K n
gesucht ist.
(b) Ein LGS heiĂt homogen, falls b = 0 ist. Sonst heiĂt es inhomogen.
(c) Das LGS Ax = b heiĂt loĚsbar, falls ein x â K n existiert mit Ax = b und
es heiĂt eindeutig loĚsbar, wenn genau ein solches x â K n existiert. Gibt es
kein x â K n mit dieser Eigenschaft, so nennt man das LGS unloĚsbar.

3.8.1. LoĚsbarkeitstheorie
Bemerkung 3.8.2. In folgenden SpezialfaĚllen ist es leicht, das LoĚsungsverhalten
von linearen Gleichungssystemen zu bestimmen:
(a) Ist p = n und A â K nĂn invertierbar, so ist Ax = b fuĚr jedes b â K n
eindeutig loĚsbar durch x = Aâ1 b, denn mit dieser Wahl gilt
Ax = A(Aâ1 b) = (AAâ1 )b = Ib = b
und ist y â K n eine weitere LoĚsung, gilt also Ay = b, so ist x = Aâ1 b =
Aâ1 Ay = y.
(b) Ist A â K pĂn die Nullmatrix, so ist Ax = b nur fuĚr b = 0 loĚsbar, dann aber
ist jedes x â K n eine LoĚsung.
(c) Das homogene System Ax = 0 ist fuĚr jede Matrix A loĚsbar, da der Nullvektor eine LoĚsung ist. AuĂerdem bilden alle LoĚsungen den Untervektorraum
ker(A), vgl. Definition 3.7.16.
Kennt man den Kern von A, so ist zur LoĚsung des Systems Ax = b schon ein
GroĂteil der Arbeit getan, denn es gilt der folgende Satz uĚber die Struktur der
LoĚsungsmenge eines linearen Gleichungssystems.
Satz 3.8.3. Es seien A â K pĂn und b â K p . Hat das LGS Ax = b eine LoĚsung
xs â K n , so sind alle LoĚsungen des LGS gegeben durch
{x â K n : Ax = b} = {xs + y : y â ker(A)}.
Kennt man also eine LoĚsung, so erhaĚlt man alle LoĚsungen als die Elemente der
Restklasse dieser einen LoĚsung in V /ker(A).

98

3.8. Lineare Gleichungssysteme
Beweis. Wir beweisen zunaĚchst ââ. Sei dazu x â K n eine LoĚsung von Ax = b.
â
Dann gilt
A(x â xs ) = Ax â Axs = b â b = 0.
Also ist x â xs â ker(A) und es gibt ein y â ker(A) mit x = xs + y.
FuĚr die umgekehrte Inklusion sei y â ker(A). Dann gilt
A(xs + y) = Axs + Ay = b + 0 = b.
Somit ist xs + y eine LoĚsung von Ax = b und wir sind fertig.
Man nennt die LoĚsung xs , deren Existenz man irgendwoher bekommen muss,
um obigen Satz anwenden zu koĚnnen, eine spezielle LoĚsung oder auch PartikulaĚrloĚsung des LGS.
Der folgende Satz bietet ein Kriterium fuĚr die grundsaĚtzliche LoĚsbarkeit eines
LGS.
Satz 3.8.4. Es seien A â K pĂn und b â K p . Bezeichnen wir mit A|b die Matrix
in K pĂ(n+1) , die durch anfuĚgen von b als (n + 1)-te Spalte an A entsteht (man
nennt diese auch erweiterte Koeffizientenmatrix), so gilt
(a) Das LGS Ax = b ist genau dann loĚsbar, wenn Rang(A) = Rang(A|b) gilt.
(b) Die folgenden Aussagen sind aĚquivalent:
i) Das LGS Ax = b ist eindeutig loĚsbar.
ii) Ax = b ist loĚsbar und ker(A) = {0}.

iii) Rang(A) = Rang(A|b) = n.

Beweis. (a) Wir bezeichnen mit a1 , a2 , . . . , an â K p die Spalten von A. Zum
Nachweis von ââ bemerken wir zunaĚchst, dass auf jeden Fall Rang(A) â¤
â
Rang(A|b) gilt. Sei nun x â K n eine LoĚsung von Ax = b, d.h. es gilt
(a1 a2 . . . an )Âˇx = b. Dann ist nach der Definition der Matrixmultiplikation
n
X

xj aj = b,

j=1

was uns sagt, dass b eine Linearkombination der Vektoren a1 , a2 , . . . , an ,
also der Spalten von A, ist. Damit ist der Rang von A|b sicher nicht groĚĂer
als der von A und wir haben die behauptete Gleichheit gezeigt.
Wir beweisen ââ. Rang(A|b) = Rang(A) bedeutet, dass die beiden Unâ
tervektorraĚume ha1 , a2 , . . . , an , bi und ha1 , a2 , . . . , an i von K p die selbe Dimension haben. Es muss also schon b â ha1 , P
a2 , . . . , an i gelten. Das ben
deutet, dass es x1 , x2 , . . . , xn â K gibt mit
j=1 xj aj = b. Damit ist
T
n
x = (x1 , x2 , . . . , xn ) â K eine LoĚsung des LGS Ax = b und wir sind
fertig.

99

3. Lineare Algebra
(b) Wir zeigen (b)i)â (b)ii)â(b)iii) â(b)i).
Ist Ax = b eindeutig loĚsbar, so ist das LGS natuĚrlich loĚsbar und enthielte
ker(A) mehr als die Null, so gaĚbe es nach Satz 3.8.3 mehr als eine LoĚsung,
somit haben wir (b)i)â(b)ii).
Gilt (b)ii), so haben wir mit Teil (a) sofort Rang(A|b) = Rang(A). Weiter
folgt dank der Dimensionsformel fuĚr Matrizen, vgl. Satz 3.7.17 (d), aus
dim(ker(A)) = 0 auch Rang(A) = n.
Zum Nachweis von (b)iii)â(b)i) sehen wir zunaĚchst, dass die LoĚsbarkeit
des LGS aus (a) folgt. Es bleibt die Eindeutigkeit der LoĚsung zu zeigen.
Wegen Rang(A) = n und wiederum der Dimensionsformel bekommen wir
ker(A) = {0} und damit liefert Satz 3.8.3 die gewuĚnschte Eindeutigkeit.

3.8.2. Der GauĂ-Algorithmus
Nachdem wir nun im letzten Abschnitt einiges Wissen uĚber die Struktur der
LoĚsungsmenge von linearen Gleichungssystemen gesammelt haben, wollen wir
uns nun der Frage zuwenden, wie man konkrete Systeme algorithmisch loĚsen
kann. Das meist verwendete Verfahren heiĂt GauĂ-Algorithmus. Dieser soll hier
intuitiv anhand von Beispielen behandelt werden, ohne exakte mathematische
BegruĚndung.
Ist A = (Îąjk )j=1,...,p,k=1,...,n â K pĂn und b = (b1 , . . . , bp )T â K p so aĚndern folgende
Umformungen die Menge der LoĚsungen des linearen Gleichungssystems
Îą11 x1 + Îą12 x2 + Âˇ Âˇ Âˇ + Îą1n xn = b1
Îą21 x1 + Îą22 x2 + Âˇ Âˇ Âˇ + Îą2n xn = b2
..
..
..
..
..
.
.
.
.
.
Îąp1 x1 + Îąp2 x2 + Âˇ Âˇ Âˇ + Îąpn xn = bp .
nicht:
1. Vertauschen zweier Zeilen (Gleichungen),
2. Multiplizieren einer Zeile (Gleichung) mit einem Îť 6= 0 aus K,
3. Addition des Vielfachen einer Zeile (Gleichung) zu einer anderen Zeile (Gleichung).
Man nennt diese Umformungen Elementarumformungen. Der GauĂâsche Algorithmus zur LoĚsung von linearen Gleichungssystemen beruht nun auf der Anwendung dieser drei Umformungen.

100

3.8. Lineare Gleichungssysteme
Beispiel 3.8.5. Wir betrachten das LGS:
x1
2x1
âx1
2x1
bzw. in Matrixform

+ x2
+ x2
â x2
+ x2

+ x3
â 2x3
+ 2x3
â x3

+ x4 = 5
â 3x4 = 4
â x4 = 1
+ 2x4 = 1,

ďŁśďŁŤ ďŁś ďŁŤ ďŁś
5
x1
1
1
1
1
ďŁˇ
ďŁŹ
ďŁŹ
ďŁˇ
ďŁŹ2
1 â2 â3ďŁˇ ďŁŹx2 ďŁˇ ďŁŹ4ďŁˇ
ďŁˇ
ďŁŹ
ďŁ­â1 â1 2 â1ďŁ¸ ďŁ­x3 ďŁ¸ = ďŁ­1ďŁ¸ .
1
x4
2
1 â1 2
ďŁŤ

Wir specken die Notation noch weiter ab und schreiben das LGS als
ďŁś
ďŁŤ
1
1
1
1 5
ďŁŹ2
1 â2 â3 4ďŁˇ
ďŁˇ
ďŁŹ
ďŁ­â1 â1 2 â1 1ďŁ¸ .
2
1 â1 2 1

Als erstes muss man nun durch das Vertauschen von Zeilen dafuĚr sorgen, dass
links oben in der Ecke ein Eintrag steht, der nicht Null ist. Das ist hier schon
der Fall, so dass wir gleich damit beginnen koĚnnen die erste Spalte aufzuraĚumen.
Wir addieren nach Elementarumformung 3. die erste Zeile zur dritten dazu und
addieren ihr (â2)-faches zur 2. und 4. Zeile. Das notiert man folgendermaĂen:
ďŁś
ďŁŤ
ďŁś
ďŁŤ
1 1
1
1 5
1
1
1
1 5 Âˇ(â2) Âˇ1
ďŁŹ0 â1 â4 â5 â6ďŁˇ
ďŁŹ2
|
1 â2 â3 4ďŁˇ
ďŁˇ.
ďŁŹ
ďŁˇ â
ďŁŹ
ďŁ­0 0
ďŁ­â1 â1 2 â1 1ďŁ¸
3
0 6ďŁ¸
| â
0 â1 â3 0 â9
â
2
1 â1 2 1

Nun arbeiten wir mit der zweiten Zeile. Diese multiplizieren wir zunaĚchst mit
(â1), so dass wir wieder eine fuĚhrende 1 haben. Im Falle, dass dort Null steht, so
tauscht man sich wieder eine passende Zeile dorthin. Ist die zweite Spalte sogar
in allen Zeilen nach der ersten Null, so geht man gleich zur dritten Spalte uĚber.
Also
ďŁś
ďŁś
ďŁŤ
ďŁŤ
â
1 1
1 1 5
1 1
1
1 5
ďŁˇ Âˇ(â1) Âˇ1
ďŁŹ0 1
ďŁŹ0 â1 â4 â5 â6ďŁˇ Âˇ(â1)
6
4
5
ďŁˇ
ďŁˇ
ďŁŹ
ďŁŹ
ďŁ­0 0
ďŁ­0 0
|
3 0 6ďŁ¸
3
0 6ďŁ¸
â
0 â1 â3 0 â9
0 â1 â3 0 â9
ďŁś
ďŁŤ
1 0 â3 â4 â1
ďŁŹ0 1 4
5 6ďŁˇ
ďŁˇ
ďŁŹ
ďŁ­0 0 3
0 6ďŁ¸
0 0 1
5 â3
101

3. Lineare Algebra
Auf diese Weise haben wir dafuĚr gesorgt, dass auch in der zweiten Spalte nur
noch eine Eins und sonst nur Nullen stehen.
Nun ist die dritte Spalte dran, wir multiplizieren zunaĚchst die dritte Zeile mit
1/3 und raĚumen dann wieder auf:
ďŁś
ďŁŤ
ďŁś
ďŁŤ
1 0 â3 â4 â1 â
1 0 â3 â4 â1
ďŁŹ0 1 4
ďŁŹ0 1 4
5 6ďŁˇ
5 6ďŁˇ
ďŁˇ | â
ďŁŹ
ďŁˇ
ďŁŹ
ďŁ­
ďŁ¸
ďŁ­0 0 3
0 0 1
0 2 ďŁ¸ Âˇ3 Âˇ(â4) Âˇ(â1)
0 6 :3
â
0 0 1
5 â3
0 0 1
5 â3
ďŁś
ďŁŤ
1 0 0 â4 5
ďŁŹ0 1 0 5 â2ďŁˇ
ďŁˇ
ďŁŹ
ďŁ­0 0 1 0 2 ďŁ¸ .
0 0 0 5 â5

Nun noch mal
Zeile:
ďŁŤ
1
ďŁŹ0
ďŁŹ
ďŁ­0
0
ďŁŤ
1
ďŁŹ0
ďŁŹ
ďŁ­0
0

das selbe Spielchen in der vierten Spalte mit der 5 in der vierten
0
1
0
0
0
1
0
0

ďŁś
0 â4 5
0 5 â2ďŁˇ
ďŁˇ
1 0 2ďŁ¸
0 5 â5 : 5
ďŁś
0 0 1
0 0 3ďŁˇ
ďŁˇ.
1 0 2ďŁ¸
0 1 â1

ďŁŤ
1
ďŁŹ0
ďŁŹ
ďŁ­0
0

0
1
0
0

ďŁś
â
0 â4 5
â
|
0 5 â2ďŁˇ
ďŁˇ
ďŁ¸
|
|
1 0 2
0 1 â1 Âˇ(â5) Âˇ4

AbschlieĂend haben wir damit das urspruĚngliche LGS durch Elemtarumformungen auf die Form
x1
=1
x2
=3
x3
=2
x4 = â1
gebracht und haben damit die LoĚsung dastehen. Das LGS ist eindeutig loĚsbar
mit x = (1 3 2 â1)T .

Bemerkung 3.8.6. Formt man ein LGS Ax = b mit Hilfe von Elementarumformungen um, so aĚndert sich nichts an der LoĚsungsmenge und insbesondere auch
nichts an allen Eigenschaften der Koeffizientenmatrix und der erweitereten Koeffizientenmatrix, die mit dem LoĚsungsverhalten des LGS zu tun haben. So bleiben
z.B. Rang(A) und Rang(A|b) erhalten.
Beispiel 3.8.7. Der GauĂ-Algorithmus funktioniert auch fuĚr nicht-quadratische
lineare Gleichungssysteme. FuĚr a â R betrachten wir z.B.
â2x1
x1

102

x2
â x2
+ x2

+ 3x3
+ x3
+ x3

+ 5x4 = a
+ x4 = 0
+ 2x4 = 1.

3.8. Lineare Gleichungssysteme
Also
ďŁŤ

0
1
ďŁ­â2 â1
1
1
ďŁŤ
1 1 1
ďŁ­0 1 3
0 1 3

ďŁś
3 5 a â
1 1 0ďŁ¸ |
1 2 1 â
ďŁś
â
2 1
ďŁ¸
5 2 Âˇ(â1)
â
5 a

ďŁś
1
1 1 2 1 Âˇ2
ďŁ­â2 â1 1 1 0ďŁ¸ â
0
1 3 5 a
ďŁś
ďŁŤ
1 0 â2 â3 â1
ďŁ­0 1 3
2 ďŁ¸.
5
0 0 0
0 aâ2
ďŁŤ

Nun muĚssen wir zwei FaĚlle unterscheiden. Ist a 6= 2, so gilt Rang(A) = 2 6= 3 =
Rang(A|b), also ist in diesem Fall das LGS unloĚsbar.
Im Fall a = 2 ist die letzte Zeile eine komplette Nullzeile und damit sind der Rang
der Matrix und der erweiterten Koeffizientenmatrix beide 2. Das LGS ist also nach
Satz 3.8.4 (a) loĚsbar. Nach Satz 3.8.3 ist weiterhin die LoĚsungsmenge von der
Form xs + ker(A), wobei xs eine spezielle LoĚsung und A die Koeffizientenmatrix
des LGS ist.
Da der Rang von A nach obiger Rechnung 2 ist, vgl. Bemerkung 3.8.6, bekommen
wir mit dem (b)-Teil von Satz 3.8.4, dass dim(ker(A)) = 2 ist. Die LoĚsung des
verbleibenden LGS
ďŁś
ďŁŤ
1 0 â2 â3 â1
ďŁ­0 1 3
5 2ďŁ¸
0 0 0
0 0

erhalten wir nun so: wir setzen die noch unbearbeitetenâ Variablen x3 = Îť und
â
x4 = Âľ. Dann liefern uns die beiden verbliebenen Gleichungen x1 = â1 + 2Îť + 3Âľ
und x2 = 2 â 3Îť â 5Âľ. Also sind alle x â R4 mit
ďŁŤ ďŁś
ďŁŤ ďŁś
ďŁś ďŁŤ ďŁś
ďŁŤ ďŁś ďŁŤ
3
2
â1
â1 + 2Îť + 3Âľ
x1
ďŁŹâ5ďŁˇ
ďŁŹâ3ďŁˇ
ďŁŹx2 ďŁˇ ďŁŹ 2 â 3Îť â 5Âľ ďŁˇ ďŁŹ 2 ďŁˇ
ďŁˇ = ďŁŹ ďŁˇ+ÎťďŁŹ ďŁˇ +ÂľďŁŹ ďŁˇ
ďŁˇ ďŁŹ
x=ďŁŹ
ďŁ­0ďŁ¸
ďŁ­1ďŁ¸
ďŁ¸ ďŁ­0ďŁ¸
ďŁ­x3 ďŁ¸ = ďŁ­
Îť
1
0
0
Âľ
x4
die LoĚsungen des LGS. Wir erhalten also als LoĚsungsmenge
ďŁź
ďŁąďŁŤ ďŁś
ďŁŤ ďŁś
ďŁŤ ďŁś
3
2
â1
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁ˝
ďŁ˛ďŁŹ ďŁˇ
ďŁˇ
ďŁˇ
ďŁŹ
ďŁŹ
â5
â3
2
ďŁˇ + Îť ďŁŹ ďŁˇ + Âľ ďŁŹ ďŁˇ : Îť, Âľ â R .
L= ďŁŹ
ďŁ­0ďŁ¸
ďŁ­1ďŁ¸
ďŁ­0ďŁ¸
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁž
ďŁł
1
0
0
Beispiel 3.8.8. Ein weiteres Beispiel uĚber dem KoĚrper Z5 :
ďŁś
ďŁŤ
3Ě 2Ě 1Ě 0Ě Âˇ2Ě
ďŁ­1Ě 1Ě 4Ě 1ĚďŁ¸
1Ě 3Ě 1Ě 2Ě

ďŁś
ďŁŤ
f
1Ě 4Ě 2Ě 0Ě Âˇ(â1)
ďŁ­1Ě 1Ě 4Ě 1ĚďŁ¸ â
1Ě 3Ě 1Ě 2Ě
â

ďŁŤ

1Ě
ďŁ­0Ě
0Ě

ďŁś
4Ě
2Ě 0Ě
f 2Ě 1ĚďŁ¸
â3
f â1
f 2Ě
â1
103

3. Lineare Algebra
ďŁŤ
1Ě
ďŁ­0Ě
0Ě
ďŁŤ
1Ě
ďŁ­0Ě
0Ě

ďŁś
4Ě 2Ě 0Ě
2Ě 2Ě 1ĚďŁ¸ Âˇ3Ě
4Ě 4Ě 2Ě
ďŁś
0Ě 3Ě 3Ě
1Ě 1Ě 3ĚďŁ¸ .
0Ě 0Ě 0Ě

ďŁś
ďŁŤ
â
1Ě 4Ě 2Ě 0Ě
ďŁ­0Ě 1Ě 1Ě 3ĚďŁ¸ Âˇ(â4)
f
0Ě 4Ě 4Ě 2Ě
â

ďŁś
g
f â12
1Ě 0Ě â2
ďŁ­0Ě 1Ě 1Ě
3Ě ďŁ¸
g
0Ě 0Ě 0Ě â10
ďŁŤ

Der LoĚsungsraum ist damit eindimensional. Wir setzen x3 = Îť und bekommen
aus den beiden ersten verbliebenen Gleichungen x1 = 3Ě â 3ĚÎť und x2 = 3Ě â Îť. Also
ist
ďŁź
ďŁź ďŁąďŁŤ ďŁś
ďŁąďŁŤ
ďŁŤ ďŁś
ďŁś
2Ě
ďŁ˝
ďŁ˝ ďŁ˛ 3Ě
ďŁ˛ 3Ě â 3ĚÎť
L = ďŁ­ 3Ě â Îť ďŁ¸ : Îť â Z5 = ďŁ­3ĚďŁ¸ + Îť ďŁ­4ĚďŁ¸ : Îť â Z5 .
ďŁž
ďŁž ďŁł
ďŁł
Îť
1Ě
0Ě

Bemerkung 3.8.9. Der GauĂ-Algorithmus ist auch ein Mittel zur Berechnung
von Inversen invertierbarer Matrizen, denn dieses laĚsst sich folgendermaĂen als
das LoĚsen mehrerer linearer Gleichungssysteme auffassen. Ist A â K nĂn eine
invertierbare Matrix und ist xj â K n fuĚr jedes j â {1, 2, . . . , n} die j-te Spalte von
Aâ1 , so muss AAâ1 = I gelten, d.h. es ist Axj die j-te Spalte der Einheitsmatrix
I, was gerade der Vektor ej := (Î´jk )nk=1 ist.
Die Inversion der Matrix A entspricht also dem LoĚsen der n linearen Gleichungssysteme Axj = ej fuĚr j = 1, 2, . . . , n. Diese kann man mit Hilfe des GauĂâschen
Algorithmus simultan loĚsen.
ďŁŤ
ďŁś
1 â1 0
Beispiel 3.8.10. Es sei A = ďŁ­0 1 2ďŁ¸ â R3Ă3 .
2 â1 3
Wir loĚsen die drei LGSe simultan:
ďŁś
ďŁś
ďŁŤ
ďŁŤ
1 â1 0 1 0 0 â
1 â1 0 1 0 0 Âˇ(â2)
ďŁ­0 1 2 0 1 0ďŁ¸ Âˇ1 Âˇ(â1)
ďŁ­0 1 2 0 1 0ďŁ¸
|
â
â
0 1 3 â2 0 1
2 â1 3 0 0 1
ďŁś
ďŁś
ďŁŤ
ďŁŤ
3 â2
1 0 0 5
1 0
â
1 0 2 1
ďŁ­
ďŁ¸
ďŁ­0 1 2 0
3 â2ďŁ¸ .
â
0 1 0 4
1 0
0 0 1 â2 â1 1
0 0 1 â2 â1 1 Âˇ(â2)
ďŁś
ďŁŤ
5
3 â2
3 â2ďŁ¸.
Also ist Aâ1 = ďŁ­ 4
â2 â1 1

3.9. Basiswechsel
Es seien V und W zwei endlichdimensionale K-VektorraĚume, B eine Basis von V
und C eine Basis von W . Ist ÎŚ : V â W eine lineare Abbildung, so haben wir zu

104

3.9. Basiswechsel
dieser in Abschnitt 3.7.2 die Abbildungsmatrix MCB (ÎŚ) gefunden. Nun seien Bâ˛
eine weitere Basis von V und C â˛ eine weitere Basis von W und wir wollen uns der
â˛
Frage zuwenden, wie man die Abbildungsmatrix MCBâ˛ (ÎŚ) aus MCB (ÎŚ) bestimmen
kann.
Die Idee dazu ist die Abbildung ÎŚ kompliziert als idW âŚ ÎŚ âŚ idV zu schreiben und
dann nach Satz 3.7.18
â˛

â˛

â˛

MCBâ˛ (ÎŚ) = MCBâ˛ (idW âŚ ÎŚ âŚ idV ) = MCCâ˛ (idW ) Âˇ MCB (ÎŚ) Âˇ MBB (idV )
zu berechnen.
Satz 3.9.1. Es seien V und W zwei endlichdimensionale K-VektorraĚume mit
Basen B und Bâ˛ , bzw. C und C â˛ wie oben. Ist dann ÎŚ : V â W linear, so existieren
invertierbare Matrizen S und T mit
â˛

MCBâ˛ (ÎŚ) = T MCB (ÎŚ)S.
Beweis. Nach obigen VoruĚberlegungen ist nur noch zu zeigen, dass die Matrizen
â˛

T := MCCâ˛ (idW ) und S := MBB (idV )
invertierbar sind. Dazu uĚberlegen wir uns mit Hilfe von Satz 3.7.18 und Beispiel 3.7.14
â˛

S Âˇ MBBâ˛ (idV ) = MBB (idV ) Âˇ MBBâ˛ (idV ) = MBB (idV âŚ idV ) = MBB (idV ) = I
und genauso
â˛

â˛

MBBâ˛ (idV ) Âˇ S = MBBâ˛ (idV ) Âˇ MBB (idV ) = MBBâ˛ (idV ) = I.
Die Argumentation fuĚr T verlaĚuft analog.
Besonders wichtig ist der Spezialfall V = W , also fuĚr lineare Abbildungen ÎŚ :
V â V.

Satz 3.9.2. Es seien V ein endlichdimensionaler K-Vektorraum, B und Bâ˛ Baâ˛
sen von V und ÎŚ : V â V linear. Sind A := MBB (ÎŚ) und Aâ˛ := MBBâ˛ (ÎŚ) die
Abbildungsmatrizen von A bezuĚglich B, bzw. Bâ˛ , so existiert eine invertierbare
Matrix S mit
Aâ˛ = S â1 AS.
Die Matrix S in obigem Satz, die die Abbildungsmatrizen bezuĚglich der verschiedenen Basen ineinander uĚbersetzt, heiĂt Basiswechselmatrix .
â˛

Beweis. Nach Satz 3.9.1 gilt fuĚr S := MBB (id) und T := MBBâ˛ (id) die Beziehung
Aâ˛ = T AS. Da auĂerdem
â˛

â˛

T S = MBBâ˛ (id)MBB (id) = MBBâ˛ (id) = I
ST =

â˛
MBB (id)MBBâ˛ (id)

=

MBB (id)

und

=I

gilt, ist T = S â1 und wir sind fertig.

105

3. Lineare Algebra
Bemerkung 3.9.3. Es bleibt natuĚrlich die Frage, wie man denn die Basiswechselmatrix S im konkreten Fall berechnet. Dazu bezeichnen wir B = {b1 , b2 , . . . , bn }
â˛
und Bâ˛ = {bâ˛1 , bâ˛2 , . . . , bâ˛n } und erinnern uns, dass S = MBB (id) gilt.
In den Spalten von S stehen also die Koordinaten von id(bâ˛1 ), id(bâ˛2 ), . . . , id(bâ˛n ),
d.h. von bâ˛1 , bâ˛2 , . . . , bâ˛n , bezuĚglich der Basis B. Um die Matrix S zu erhalten, muss
man also die Basisvektoren bâ˛1 , bâ˛2 , . . . , bâ˛n in der Basis B ausdruĚcken, das bedeutet
man hat im schlimmsten Fall n lineare Gleichungssysteme zu loĚsen.
Ein wichtiger und sehr einfacher Spezialfall ist der, wenn V = K n und B die
Standardbasis ist, denn dann sind die Koordinaten von bâ˛1 , bâ˛2 , . . . , bâ˛n bezuĚglich B
einfach die Vektoren bâ˛1 , bâ˛2 , . . . , bâ˛n selbst, d.h. diese bilden dann die Spalten der
Basiswechselmatrix S.
Beispiel 3.9.4. Wir betrachten die lineare Abbildung ÎŚ : R3 â R3 mit
ďŁŤ
ďŁś ďŁŤ
ďŁśďŁŤ ďŁś
x1 â 4x2 â 4x3
1 â4 â4
x1
ďŁ¸=ďŁ­ 0
3x2 + 2x3
3
2 ďŁ¸ ďŁ­x2 ďŁ¸ .
ÎŚ(x) = ďŁ­
â2x1 â 7x2 â 4x3
â2 â7 â4
x3

Ist B die Standardbasis von R3 , so ist also
ďŁś
ďŁŤ
1 â4 â4
3
2 ďŁ¸.
A = MBB (ÎŚ) = ďŁ­ 0
â2 â7 â4

Wir wollen nun die Abbildungsmatrix von ÎŚ bezuĚglich der Basis
ďŁąďŁŤ ďŁś ďŁŤ ďŁś ďŁŤ ďŁśďŁź
2 ďŁ˝
4
ďŁ˛ â1
Bâ˛ := ďŁ­ 1 ďŁ¸ , ďŁ­â2ďŁ¸ , ďŁ­â1ďŁ¸
ďŁž
ďŁł
3
1
â1

berechnen. Da B die Standardbasis ist, gilt fuĚr die Basiswechselmatrix
ďŁŤ
ďŁś
â1 4
2
S = ďŁ­ 1 â2 â1ďŁ¸ .
â1 1
3

Deren Inverse bestimmt man mit dem GauĂ-Verfahren, vgl. Beispiel 3.8.10, zu
ďŁŤ
ďŁś
5 10 0
1
S â1 = ďŁ­2 1 â1ďŁ¸ .
5
1 3 2
Damit haben wir nach Satz 3.9.2
ďŁŤ
ďŁśďŁŤ
ďŁśďŁŤ
ďŁś
5 10 0
1 â4 â4
â1 4
2
1
â˛
3
2 ďŁ¸ ďŁ­ 1 â2 â1ďŁ¸
MBBâ˛ (ÎŚ) = S â1 AS = ďŁ­2 1 â1ďŁ¸ ďŁ­ 0
5
1 3 2
â2 â7 â4
â1 1
3

106

3.9. Basiswechsel
ďŁŤ
5
1ďŁ­
2
=
5
1
ďŁŤ
1 0
= ďŁ­0 2
0 0

ďŁśďŁŤ
ďŁś
ďŁŤ
ďŁś
10 0
â1 8 â6
5 0
0
1
1 â1ďŁ¸ ďŁ­ 1 â4 3 ďŁ¸ = ďŁ­0 10 0 ďŁ¸
5
3 2
â1 2 â9
0 0 â15
ďŁś
0
0 ďŁ¸.
â3

Dieses Beispiel zeigt auch, dass man durch eine angepasste Wahl der Basis die
Abbildungsmatrix sehr stark vereinfachen kann. Mit der Frage, wie man eine
solche Basis findet, werden wir uns im Abschnitt 3.11 beschaĚftigen.
Beispiel 3.9.5. Die Technik des Basiswechsels kann auch dazu genutzt werden, die Abbildungsmatrix einer durch geometrische Angaben definierten linearen Abbildung zu ermitteln. Beispielhaft betrachten
 â4 
  wirals lineare Abbildung
1
0
3
3
ÎŚ : R â R die Projektion auf den Unterraum h 1 , 2 i in Richtung h â1 i.
1

1

2

Gesucht ist die Abbildungsmatrix dieser Abbildung bezuĚglich der Standardbasis
B. Da es sehr schwierig ist, die Abbildung direkt in dieser Basis zu beschreiben,
betrachten wir zunaĚchst die dem Problem angepasste Basis
ďŁąďŁŤ ďŁś ďŁŤ ďŁś ďŁŤ ďŁśďŁź
1
â4 ďŁ˝
ďŁ˛ 0
â˛
ďŁ¸
ďŁ­
ďŁ¸
ďŁ­
ďŁ­
1 , 2 , â1ďŁ¸ = {bâ˛1 , bâ˛2 , bâ˛3 }.
B =
ďŁł
ďŁž
1
1
2

Nach der Definition von ÎŚ gilt dann ÎŚ(bâ˛1 ) = bâ˛1 , ÎŚ(bâ˛2 ) = bâ˛2 und ÎŚ(bâ˛3 ) = 0.
Also ist
ďŁŤ
ďŁś
1 0 0
â˛
MBBâ˛ (ÎŚ) = ďŁ­0 1 0ďŁ¸ .
0 0 0
Mit der Basiswechselmatrix

ďŁŤ
ďŁś
0 1 â4
S = ďŁ­1 2 â1ďŁ¸ ,
1 1 2
â˛

die den Basiswechsel von B nach Bâ˛ vermittelt, gilt nun MBBâ˛ (ÎŚ) = S â1 MBB (ÎŚ)S.
â˛
â˛
Also ist SMBBâ˛ (ÎŚ) = MBB (ÎŚ)S und schlieĂlich MBB (ÎŚ) = SMBBâ˛ (ÎŚ)S â1 .
Mit dem GauĂ-Verfahren kann man wieder
S â1

ďŁŤ

ďŁś
5 â6 7
= ďŁ­â3 4 â4ďŁ¸
â1 1 â1
107

3. Lineare Algebra
bestimmen. Also ist die gesuchte Abbildungsmatrix
ďŁŤ
ďŁśďŁŤ
ďŁśďŁŤ
ďŁś
0 1 â4
1 0 0
5 â6 7
â˛
MBB (ÎŚ) = SMBBâ˛ (ÎŚ)S â1 = ďŁ­1 2 â1ďŁ¸ ďŁ­0 1 0ďŁ¸ ďŁ­â3 4 â4ďŁ¸
1 1 2
0 0 0
â1 1 â1
ďŁŤ
ďŁśďŁŤ
ďŁś ďŁŤ
ďŁś
0 1 â4
5 â6 7
â3 4 â4
= ďŁ­1 2 â1ďŁ¸ ďŁ­â3 4 â4ďŁ¸ = ďŁ­â1 2 â1ďŁ¸ .
1 1 2
0
0
0
2 â2 3

Definition 3.9.6. Zwei Matrizen A, B â K nĂn heiĂen aĚhnlich, wenn es eine
invertierbare Matrix S â K nĂn gibt mit B = S â1 AS.

Bemerkung 3.9.7. (a) Darstellungsmatrizen einer linearen Abbildung bezuĚglich verschiedener Basen sind immer zueinander aĚhnlich.
(b) Die AĚhnlichkeit von Matrizen ist eine AĚquivalenzrelation. Machen Sie sich
das als UĚbung klar!
(c) Sind alle Matrizen zueinander aĚhnlich? Nein!
ďŁŤ
ďŁś
Îą11 Îą12 . . . Îą1n
n
ďŁŹ Îą21 Îą22 . . . Îą2n ďŁˇ
X
ďŁŹ
ďŁˇ
nĂn
, so heiĂt Spur(A) :=
Îąjj die
Ist A = ďŁŹ ..
..
.. ďŁˇ â K
..
ďŁ­ .
.
.
. ďŁ¸
j=1
Îąn1 Îąn2 . . . Îąnn
Spur von A.

Man kann zeigen: Sind A und B aĚhnliche Matrizen, dann gilt Spur(A) =
Spur(B).
Eine weitere charakteristische GroĚĂe einer Matrix, die sich beim Basiswechsel nicht aĚndert, werden wir im Abschnitt 3.10 kennenlernen.
Zum Abschluss dieses Abschnitts betrachten wir noch den Spezialfall, dass die
beiden Basen B und Bâ˛ , zwischen denen gewechselt wird, jeweils Orthonormalbasen sind.
Lemma 3.9.8. Sind B und Bâ˛ Orthonormalbasen eines n-dimensionalen R-Vektorraums V mit Skalarprodukt (Âˇ|Âˇ)V , so gilt fuĚr die Basiswechselmatrix S =
â˛
MBB (id) â RnĂn , dass ihre Spalten eine Orthonormalbasis des Rn bezuĚglich des
Standardskalarproduktes (Âˇ|Âˇ)Rn bilden.
Beweis. Es sei B = {b1 , b2 , . . . , bn } und Bâ˛ = {bâ˛1 , bâ˛2 , . . . , bâ˛n }.
Die j-te Spalte sj von S ist nach Bemerkung 3.9.3 fuĚr jedes j = 1, 2, . . . , n gegeben
durch den Koordinatenvektor von bâ˛j bezuĚglich B. Wie wir in Bemerkung 3.4.15
gesehen haben, ist dieser Koordinatenvektor gegeben durch
ďŁŤ â˛
ďŁś
(bj |b1 )V
ďŁŹ (bâ˛ |b2 )V ďŁˇ
ďŁŹ j
ďŁˇ
sj = ďŁŹ
ďŁˇ.
..
ďŁ­
ďŁ¸
.
(bâ˛j |bn )V

108

3.9. Basiswechsel
Also ist fuĚr alle j, k â {1, 2, . . . , n}
(sk |sj )Rn

n
n

 X
X
â˛
â˛
â˛
(bâ˛k |bâ )V bâ .
(bk |bâ )V Âˇ (bj |bâ )V = bj
=
â=1

â=1

V

Die Summe im zweiten Argument ist nun nach Bemerkung 3.4.15 gerade der
Vektor bâ˛k . Also haben wir
(sk |sj )Rn = (bâ˛j |bâ˛k )V = Î´jk
und sind fertig.
Definition 3.9.9. Eine Matrix A â RnĂn heiĂt orthogonal, falls die Spalten von
A eine Orthonormalbasis bezuĚglich des Standardskalarproduktes bilden.
Man beachte, dass eine orthogonale Matrix immer invertierbar ist, da ihre Spalten
eine Basis bilden und der Rang somit gleich der Spaltenanzahl ist.
UĚbungsaufgabe 3.9.10. Es sei A â RnĂn . Beweisen Sie, dass die folgenden
Aussagen aĚquivalent sind:
(a) A ist orthogonal.
(b) A ist invertierbar und es gilt Aâ1 = AT .
(c) Die Zeilen von A bilden eine Orthonormalbasis bezuĚglich des Standardskalarproduktes.
(d) A ist invertierbar und Aâ1 ist orthogonal.
(e) AT ist orthogonal.
Bemerkung 3.9.11. Beim Basiswechsel zwischen Orthonormalbasen ist vor allem die Beziehung Aâ1 = AT nuĚtzlich, denn das Transponieren einer Matrix ist
vom Rechenaufwand her viel einfacher als das Invertieren.
UĚbungsaufgabe 3.9.12. Die Menge
O(n, R) := {A â RnĂn : A orthogonal}
ist eine Untergruppe von GL(n, R), genannt orthogonale Gruppe.

109

3. Lineare Algebra

3.10. Determinanten
Wie sieht man einer Matrix A = ( ac db ) â K 2Ă2 an, ob sie invertierbar ist? Berechnet man allgemein die Inverse, so findet man


1
d âb
â1
,
falls ad â bc 6= 0.
A =
ad â bc âc a
Anscheinend ist also der Wert ad â bc von besonderer Bedeutung. Man nennt ihn
die Determinante. Allgemein ist diese folgendermaĂen definiert.
Definition 3.10.1. (a) Es seien A â K nĂn und j, k â {1, 2, . . . , n}. Dann
bezeichne Ajk â K (nâ1)Ă(nâ1) die Matrix, die aus A durch Streichen der
j-ten Zeile und der k-ten Spalte entsteht.
(b) FuĚr A = (Îą) â K 1Ă1 definieren wir die Determinante durch det(A) := a.
(c) FuĚr ein A = (Îąjk )nj,k=1 â K nĂn mit n > 1 erklaĚren wir die Determinante
als
det(A) =

n
X

(â1)1+k Îą1k det(A1k ).

(Entwicklung nach der ersten Zeile)

k=1

(d) FuĚr die Determinante einer Matrix A =
auch
Îą11 Îą12
Îą21 Îą22
det(A) = ..
..
.
.
Îąn1 Îąn2
Beispiel 3.10.2.

(Îąjk )nj,k=1 â K nĂn schreibt man
. . . Îą1n
. . . Îą2n
.. .
..
.
.
. . . Îąnn

(a) Im Fall n = 2 gilt nach obiger Definition tatsaĚchlich
a b
= (â1)2 a det((d)) + (â1)3 b det((c)) = ad â bc.
c d

(b) Schon fuĚr n = 3 wird die Berechnung allerdings ein bisschen muĚhsamer:
2 1 3
4 0
0 5
4 5
4 0 5 = (â1)2 Âˇ 2 Âˇ
+ (â1)3 Âˇ 1 Âˇ
+ (â1)4 Âˇ 3 Âˇ
7 8
7 6
6 8
7 6 8
= 2 Âˇ (0 Âˇ 8 â 5 Âˇ 6) â 1 Âˇ (4 Âˇ 8 â 5 Âˇ 7) + 3 Âˇ (4 Âˇ 6 â 0 Âˇ 7)
= â60 â 32 + 35 + 72 = 15.
Von groĂer praktischer Bedeutung ist die folgende Beobachtung.

110

3.10. Determinanten
Beispiel 3.10.3. Es sei A â K nĂn eine sogenannte untere Dreiecksmatrix , d.h.
ďŁŤ

ďŁŹ
ďŁŹ
A=ďŁŹ
ďŁ­

Îą11

0

â
..
.

Îą22
..
.

â

...

...
..
.
..
.
â

ďŁś
0
.. ďŁˇ
. ďŁˇ
ďŁˇ,
0 ďŁ¸
Îąnn

wobei anstelle der Sterne ââ irgendwelche Elemente aus K stehen. Dann gilt
â
nach der Definition der Determinante

det(A) = Îą11 Âˇ

Îą22

0

â
..
.

Îą33
..
.

â

...

...
..
.
..
.
â

0
..
.
0
Îąnn

Îą33
= Îą11 Îą22 Âˇ

= Âˇ Âˇ Âˇ = Îą11 Îą22 Âˇ . . . Âˇ Îąnn .

â
..
.
â

0

...
0
..
..
.
.
Îą44
..
..
.
. 0
. . . â Îąnn

Insbesondere gilt damit immer
det(I) = 1.
Im folgenden Satz fassen wir einige grundlegende Rechenregeln fuĚr die Determinante ohne Beweis zusammen.
Satz 3.10.4. Es sei

ďŁŤ ďŁś
a1
ďŁŹ a2 ďŁˇ
ďŁŹ ďŁˇ
A = ďŁŹ .. ďŁˇ â K nĂn ,
ďŁ­.ďŁ¸
an

wobei aT1 , aT2 , . . . , aTn â K n die Zeilen von A seien. Dann gilt
(a) Vertauscht man zwei beliebige Zeilen der Matrix, so aĚndert sich das Vorzeichen der Determinante, z.B. ist
a2
a1
a3
a4 = â det(A).
..
.
an

111

3. Lineare Algebra
(b) Die Determinante det(A) ist linear in jeder Zeile, d.h. fuĚr jeden Vektor
b â K n , fuĚr alle Îť, Âľ â K, sowie jedes j â {1, 2, . . . , n} gilt
a1
..
.

a1
..
.

ajâ1
ajâ1
Îťaj + Âľb = Îť det(A) + Âľ b .
aj+1
aj+1
..
..
.
.
an
an
(c) Sei Îť â K. Addiert man zu einer Zeile von A das Îť-fache einer anderen
Zeile von A hinzu, so aĚndert sich die Determinante nicht.
(d) Man kann statt nach der ersten Zeile zu entwickeln, vgl. die Definition
der Determinante, auch nach der j-ten Zeile fuĚr jedes j â {1, 2, . . . , n}
entwickeln. Genauer gesagt gilt fuĚr jedes solche j
det(A) =

n
X

(â1)j+k Îąjk det(Ajk ).

k=1

(e) Es ist det(A) = det(AT ).
(f ) Ist B â K nĂn eine weitere Matrix, so ist det(AB) = det(A) det(B).
Korollar 3.10.5. (a) Die Aussagen in Satz 3.10.4 (a)-(d) gelten auch fuĚr Spalten statt Zeilen. Als Formel fuĚr das Entwickeln nach der k-ten Spalte bekommt man
n
X
det(A) =
(â1)j+k Îąjk det(Ajk ).
j=1

(b) Ist Îť â K und A â K nĂn , so gilt det(ÎťA) = Îťn det(A).
Beweis.

(a) Das folgt aus Satz 3.10.4 (e).
 a1 
 Îťa1 
.
.. , also ist durch n-malige
(b) Mit A = .. wie in Satz 3.10.4 gilt ÎťA =
.
an

Anwendung von (b) aus Satz 3.10.4

Îťan

a1
Îťa1
Îťa2
Îťa2
det(ÎťA) = .. = Îť .. = Âˇ Âˇ Âˇ = Îťn
.
.
Îťan
Îťan

112

a1
a2
n
.. = Îť det(A).
.
an

3.10. Determinanten
Die praktische Relevanz obiger Rechenregeln liegt unter Anderem darin, dass (a)
bis (c) aus Satz 3.10.4 uns sagen, was mit der Determinante unter den Elementarumformungen aus dem GauĂ-Verfahren passiert und dass wir dieses Werkzeug damit zur Berechnung von Determinanten nutzen koĚnnen. Dank Korollar 3.10.5 (a)
koĚnnen wir es sogar sowohl auf die Zeilen als auch auf die Spalten der Matrix
anwenden. Wir betrachten die Berechnung von Determinanten anhand zweier
Beispiele.
Beispiel 3.10.6.

(a) Wir berechnen
1 â2 3 5 8
0 â1 â1 2 3
D := 2 4 â1 3 1 .
0 0
5 0 0
1 3
0 4 â1

Zuerst ist es sinnvoll nach der vierten Zeile zu entwickeln, denn dabei sind
vier der fuĚnf Summanden Null:
1 â2 5 8
0 â1 2 3
.
D = â5 Âˇ
2 4 3 1
1 3 4 â1
Nun koĚnnen wir los gauĂenâ. Wir machen zunaĚchst ein paar der schon geâ
wohnten Zeilenumformungen. Dabei bleibt die Determinante unveraĚndert.
â
1 â2 5 8
|
0 â1 2 3
= â5 Âˇ
D = â5 Âˇ
â
|
2 4 3 1
1 3 4 â1 Âˇ(â2) Âˇ(â1)

0 â5 1
9
0 â1 2
3
.
0 â2 â5 3
1 3
4 â1

Nun haben wir die Sache so weit vereinfacht, dass wir mit Gewinn nach der
ersten Spalte entwickeln koĚnnen, vgl. Korollar 3.10.5:
â5 1 9
D = â5 Âˇ (â1) Âˇ 1 Âˇ â1 2 3 .
â2 â5 3
Im naĚchsten Schritt verwenden wir (b) aus Satz 3.10.4 um die dritte Spalte
durch drei zu teilen und machen dann wieder ein paar GauĂâsche Zeilenumformungen, um eine neue Spalte mit nur einem von Null verschiedenen
Eintrag zu bekommen:
1 16 0
â
â5 1 3
7 0
| = 15 Âˇ 1
D = 5 Âˇ 3 Âˇ â1 2 1 â
â2 â9 1
â2 â5 1 Âˇ(â1) Âˇ(â3)
113

3. Lineare Algebra
Nun entwickeln wir nach der extra praĚparierten dritten Spalte und rechnen
fertig:
1 16
= 15 Âˇ (7 â 16) = â15 Âˇ 9 = â135.
D = 15 Âˇ 1 Âˇ
1 7
(b) FuĚr jede Wahl von a, b â K berechnen wir die folgende (nĂn)-Determinante
a
b
D := .
..
b

a
b
b
. . . b Âˇ(â1)
bâa aâb
0
. . ..
. . â
a
0
aâb
.. = b â a
.. ..
.
. b
..
..
.
.
bâa
.
... b a â
bâa
0
...
b

...
b
...
0
..
..
.
. .
..
.
0
0 aâb

Nun addieren wir die zweite bis n-te Spalte zur ersten Spalte und erhalten:

D=

a + (n â 1)b
b
0
aâb
0
..
.

0
..
.

0

0

b
0

...
b
...
0
..
.
a â b ..
. .
..
..
.
.
0
...
0 aâb

Der Sinn der ganzen Aktion erschlieĂt sich nun: Es ist eine obere Dreiecksmatrix uĚbriggeblieben. Wenn wir diese transponieren, aĚndert sich nach
Satz 3.10.4 (e) die Determinante nicht und wir erhalten eine untere Dreiecksmatrix, deren Determinante wir nach Beispiel 3.10.3 einfach bestimmen
koĚnnen:

D=

a + (n â 1)b
0
b
aâb
b
..
.

0
..
.

b

0

0
0

...
0
...
0

..
..
.
aâb
.
= a + (n â 1)b (a â b)nâ1 .
..
..
.
.
0
...
0 aâb

UĚbungsaufgabe 3.10.7. Rechnen Sie die Formel von Sarrus nach:
a b c
d e f = aei + bf g + cdh â ceg â af h â bdi.
g h i
Satz 3.10.8. Ist A â K nĂn mit Rang(A) < n, so ist det(A) = 0.
Beweis. 1. Schritt: Ist die erste Spalte von A der Nullvektor, so gilt det(A) = 0.

114

3.10. Determinanten
Es seien a2 , a3 , . . . , an â K n die weiteren Spalten von A. Dann ist dank der LinearitaĚt der Determinante in jeder Spalte, vgl. Satz 3.10.4 (b) und Korollar 3.10.5 (a)
det(A) = 0 a2 a3 . . . an = 0 + 0 a2 a3 . . . an
= 0 a2 a3 . . . an + 0 a2 a3 . . . an = det(A) + det(A).
Also ist det(A) = 0.
2. Schritt: Beweis des Satzes.
Die Voraussetzung Rang(A) < n bedeutet, dass die Spaltenvektoren a1 , . . . , an â
K n von A linear
sind. Also existieren Îť1 , Îť2 , . . . , Îťn â K, die nicht alle
PabhaĚngig
n
Null sind, mit j=1 Îťj aj = 0. Sei j0 â {1, . . . , n} ein Index mit Îťj0 6= 0. Dann
gilt nach den verschiedenen Rechenregeln fuĚr Determinanten und abschlieĂend
Schritt 1
det(A) = |a1 a2 . . . aj0 . . . an | =
=

1
|a1 a2 . . . Îťj0 aj0 . . . an |
Îťj 0

n
X
1
1
a1 a2 . . .
Îťj aj . . . an =
|a1 a2 . . . 0 . . . an |
Îťj 0
Îť
j
0
j=1

=â

1
|0 a2 . . . a1 . . . an | = 0.
Îťj 0

Satz 3.10.9. Eine Matrix A â K nĂn ist genau dann invertierbar, wenn det(A) 6=
0 gilt. In diesem Fall ist det(Aâ1 ) = det(A)â1 .
Beweis. Ist A invertierbar, so gilt A Âˇ Aâ1 = I. Mit Hilfe von Beispiel 3.10.3 und
Satz 3.10.4 (f) ist dann
1 = det(I) = det(A Âˇ Aâ1 ) = det(A) Âˇ det(Aâ1 ).
Damit haben wir sowohl det(A) 6= 0 als auch det(Aâ1 ) = det(A)â1 gezeigt.
Gilt umgekehrt det(A) 6= 0, so muss nach Satz 3.10.8 der Rang von A voll, d.h.
gleich n sein. Also ist mit Hilfe von Satz 3.7.22 die Matrix A invertierbar.
Korollar 3.10.10. Es seien A, B â K nĂn zwei aĚhnliche Matrizen. Dann gilt
det(A) = det(B).
Beweis. Nach Definition der AĚhnlichkeit gibt es eine invertierbare Matrix S â
K nĂn mit B = S â1 AS. Also gilt mit vorstehendem Satz
det(B) = det(S â1 AS) = det(S â1 ) det(A) det(S) =

1
det(A) det(S)
det(S)

= det(A).

115

3. Lineare Algebra
Bemerkung 3.10.11. Ist V ein endlichdimensionaler Vektorraum und ÎŚ : V â
V eine lineare Abbildung, so besagt Korollar 3.10.10, dass der Wert det(MBB (ÎŚ))
fuĚr jede Wahl der Basis B derselbe ist. Die Determinante ist also eine charakteristische GroĚĂe der linearen Abbildung und haĚngt nicht von der speziellen Wahl
der Basis und damit der Abbildungsmatrix ab. Man schreibt deshalb auch det(ÎŚ)
fuĚr diesen Wert und spricht von der Determinante der linearen Abbildung.
Anschaulich ist det(ÎŚ) der Faktor, um den die Abbildung ÎŚ bei ihrer Anwendung
Volumina veraĚndert.
UĚbungsaufgabe 3.10.12. Beweisen oder widerlegen Sie die folgenden Aussagen:
(a) FuĚr jede orthogonale Matrix A â RnĂn gilt | det(A)| = 1.
(b) FuĚr alle A, B â K nĂn gilt det(A + B) = det(A) + det(B).

3.11. Eigenwerttheorie
In diesem abschlieĂenden Kapitel zur linearen Algebra wollen wir der schon in
Beispiel 3.9.4 angekuĚndigten Frage nachgehen, wie man zu einer gegebenen linearen Abbildung ÎŚ : V â V eine Basis finden kann, in der die Abbildungsmatrix
MBB (ÎŚ) moĚglichst einfach wird.
Definition 3.11.1. Es sei V ein K-Vektorraum und ÎŚ : V â V eine lineare
Abbildung. Ein Îť â K heiĂt Eigenwert von ÎŚ, falls es einen Vektor v â V gibt
mit v 6= 0 und ÎŚ(v) = Îťv. Ein solches v heiĂt dann Eigenvektor von ÎŚ zum
Eigenwert Îť.
Beispiel 3.11.2. Ist ÎŚ : R2 â R2 die Spiegelung an der x2 -Achse, vgl. Beispiel 3.7.12, so gilt fuĚr den ersten Standardbasisvektor e1 = ( 10 ) die Beziehung
ÎŚ(e1 ) = âe1 , dieser ist also ein Eigenvektor von ÎŚ zum Eigenwert â1. Weiter ist
der zweite Standardbasisvektor e2 = ( 01 ) ein Eigenvektor zum Eigenwert 1, denn
ÎŚ(e2 ) = e2 , vgl. Abbildung 3.2.
Definition 3.11.3. Es sei A â K nĂn eine Matrix. Ein Îť â K heiĂt Eigenwert
von A, falls es ein x â K n mit x 6= 0 und Ax = Îťx gibt. Ein solcher Vektor x
heiĂt dann Eigenvektor von A zum Eigenwert Îť.
Wir wollen nun zeigen, dass die beiden Definitionen fuĚr Eigenwerte von linearen
Abbildungen und Matrizen zusammenpassen. Dazu machen wir uns zunaĚchst
klar, dass Eigenwerte bei Basiswechseln erhalten bleiben.
Satz 3.11.4. Ist Îť â K ein Eigenwert von A â K nĂn und ist S â K nĂn invertierbar, so ist Îť auch ein Eigenwert von B = S â1 AS.

116

3.11. Eigenwerttheorie
x2

e = ÎŚ( e )
2

ÎŚ( e 1 )

2

e1

x1

Abbildung 3.2.: Die Spiegelung an der x2 -Achse ÎŚ mit Eigenvektoren
Beweis. Es sei x â K n \ {0} ein Eigenvektor von A zum Eigenwert Îť. Da S
invertierbar ist und x =
6 0 gilt, muss auch y := S â1 x 6= 0 gelten. FuĚr diesen
Vektor gilt
By = (S â1 AS)(S â1 x) = S â1 A(SS â1 )x = S â1 (Ax) = S â1 (Îťx) = ÎťS â1 x = Îťy,
er ist also ein Eigenvektor von B zum Eigenwert Îť. Insbesondere hat B den
Eigenwert Îť.
Satz 3.11.5. Es sei V ein endlichdimensionaler K-Vektorraum und ÎŚ : V â V
eine lineare Abbildung. Dann ist Îť â K genau dann ein Eigenwert von ÎŚ, wenn
Îť fuĚr jede Basis B von V ein Eigenwert von MBB (ÎŚ) ist.
Beweis. ââ Es sei B eine beliebige Basis von V und v â V ein Eigenvektor
â
von ÎŚ zum Eigenwert Îť. Weiter bezeichnen wir wie uĚblich mit ~v den Koordinatenvektor von v bezuĚglich B. Dann ist auch ~v 6= 0 und es gilt bezuĚglich
dieser Basis auch
âââ â
â
ÎŚ(v) = Îťv = Îť~v .
Also ist nach der Definition der Abbildungsmatrix
âââ
MBB (ÎŚ)~v = ÎŚ(v) = Îť~v
und wir haben gezeigt, dass Îť auch ein Eigenwert von MBB (ÎŚ) ist.
ââ Es sei B = {b1 , b2 , . . . , bn } eine Basis von V und Îť ein Eigenwert von MBB (ÎŚ)
â
mit zugehoĚrigem Eigenvektor
x = (x1 , x2 , . . . , xn )T â K n . Dann ist x 6= 0
Pn
und damit auch v := j=1 xj bj 6= 0V . AuĂerdem ist
âââ
â
â
ÎŚ(v) = MBB (ÎŚ)~v = MBB (ÎŚ)x = Îťx = Îť~v = Îťv.

Also ist ÎŚ(v) = Îťv.

117

3. Lineare Algebra
Bemerkung 3.11.6. Gibt es genuĚgend linear unabhaĚngige Eigenvektoren einer
linearen Abbildung, so kann man tatsaĚchlich eine einfache Darstellungsmatrix finden. Wir betrachten auf einem endlichdimensionalen K-Vektorraum V eine lineare Abbildung ÎŚ : V â V und setzen voraus, dass es eine Basis B = {b1 , b2 , . . . , bn }
von V gibt, die aus Eigenvektoren von ÎŚ besteht. Wir nennen Îť1 , Îť2 , . . . , Îťn die
jeweils zugehoĚrigen Eigenwerte, d.h. es gelte ÎŚ(bj ) = Îťj bj fuĚr jedes j = 1, 2, . . . , n.
ââââ
BezuĚglich der Basis B ist dann ÎŚ(bj ) = (0, . . . , 0, Îťj , 0 . . . , 0)T mit dem Eintrag
Îťj an der j-ten Stelle. Also ist dann die Abbildungsmatrix
ďŁŤ
ďŁś
Îť1 0 . . . 0
.
..
ďŁŹ
. .. ďŁˇ
ďŁŹ0 Îť
ďŁˇ
MBB (ÎŚ) = ďŁŹ . . 2 .
ďŁˇ.
.. .. 0 ďŁ¸
ďŁ­ ..
0 . . . 0 Îťn
Solch eine Matrix nennt man eine Diagonalmatrix .

Definition 3.11.7. (a) Es sei V ein endlichdimensionaler Vektorraum. Eine
lineare Abbildung ÎŚ : V â V heiĂt diagonalisierbar, wenn es eine Basis B
von V gibt, so dass MBB (ÎŚ) eine Diagonalmatrix ist.
(b) Eine Matrix A â K nĂn heiĂt diagonalisierbar, wenn es eine invertierbare
Matrix S â K nĂn gibt, fuĚr die S â1 AS eine Diagonalmatrix ist.

Die naĚchste Frage ist nun wie man die Eigenwerte und Eigenvektoren einer konkret gegebenen linearen Abbildung, bzw. Matrix, bestimmt. Dazu uĚberlegen wir
uns fuĚr eine Matrix A â K nĂn :
Îť â K ist Eigenwert von A.
ââ Es gibt ein x â K n \ {0} mit Ax = Îťx.
ââ Es gibt ein x â K n \ {0} mit Ax â Îťx = 0.
ââ Es gibt ein x â K n \ {0} mit (A â ÎťI)x = 0.
ââ ker(A â ÎťI) 6= {0}.
ââ A â ÎťI ist nicht invertierbar.
ââ det(A â ÎťI) = 0.
Wir haben also gezeigt:

Satz 3.11.8. Ein Îť â K ist genau dann ein Eigenwert von A â K nĂn , wenn
det(A â ÎťI) = 0 ist.
Ist A â K nĂn , so ist der Ausdruck det(A â ÎťI) ein Polynom vom Grad n mit
Koeffizienten in K. Dieses heiĂt charakteristisches Polynom von A.
Die folgende UĚbungsaufgabe zeigt, dass nicht nur die Eigenwerte sondern das
gesamte charakteristische Polynom bei einem Basiswechsel unveraĚndert bleibt.
Man kann also auch vom charakteristischen Polynom einer linearen Abbildung
sprechen.

118

3.11. Eigenwerttheorie
UĚbungsaufgabe 3.11.9. Sind A, B â K nĂn aĚhnliche Matrizen, so gilt det(A â
ÎťI) = det(B â ÎťI).
Beispiel 3.11.10. Wir betrachten die lineare Abbildung ÎŚ : R3 â R3 mit
ďŁś
ďŁŤ
x2 + x3
ÎŚ((x1 , x2 , x3 )T ) = ďŁ­ x1 + x3 ďŁ¸ .
âx1 + x2

AuĂerdem sei B die Standardbasis. Dann ist
ďŁŤ
ďŁś
0 1 1
A := MBB (ÎŚ) = ďŁ­ 1 0 1ďŁ¸
â1 1 0
und

det(A â ÎťI) =

âÎť 1
1
1 âÎť 1 â
â1 1 âÎť Âˇ1

=

âÎť
1
1
0 âÎť + 1 âÎť + 1
â1
1
âÎť

â
âÎť 1 1
= (1 â Îť) 0 1 1 Âˇ(â1)
â1 1 âÎť
= (1 â Îť)(âÎť)

âÎť 0 0
= (1 â Îť) 0 1 1
â1 1 âÎť

1 1
= Îť(Îť â 1)(âÎť â 1).
1 âÎť

Die Eigenwerte der Abbildung sind die Nullstellen dieses Polynoms, also Îť1 = 0,
Îť2 = 1 und Îť3 = â1.
Zur Bestimmung der zugehoĚrigen Eigenvektoren loĚst man nun die linearen Gleichungssysteme (A â Îťj I)x = 0 fuĚr j = 1, 2 und 3.
Hier ist die Rechnung beispielhaft fuĚr Îť2 = 1. Es ist
ďŁŤ
ďŁś
â1 1
1
A â 1 Âˇ I = ďŁ­ 1 â1 1 ďŁ¸ .
â1 1 â1
Wir loĚsen also
ďŁś
ďŁŤ
â1 1
1 0 â
ďŁ­ 1 â1 1 0ďŁ¸ Âˇ1
â1 1 â1 0 â

ďŁś
ďŁŤ
0 0 2 0 : 2 Âˇ(â1)
ďŁ­1 â1 1 0ďŁ¸
â
0 0 0 0

ďŁś
ďŁŤ
0 0 1 0
ďŁ­1 â1 0 0ďŁ¸ .
0 0 0 0

Wir haben also x3 = 0 und x1 = x2 . Alle Eigenvektoren zu Îť2 = 1 bilden also die
Menge
ďŁąďŁŤ ďŁś
ďŁź *ďŁŤ ďŁś+
1
ďŁ˛ Îą
ďŁ˝
ker(A â I) \ {0} = ďŁ­ÎąďŁ¸ : Îą â R \ {0} = ďŁ­1ďŁ¸ \ {0}.
ďŁł
ďŁž
0
0
119

3. Lineare Algebra
Satz 3.11.11. Es sei V ein endlichdimensionaler K-Vektorraum und ÎŚ : V â V
linear. Weiter sei A â K nĂn . Dann gilt
(a) FuĚr jedes Îť â K ist der Eigenraum von ÎŚ zum Eigenwert Îť
E(ÎŚ, Îť) := {v â V : ÎŚ(v) = Îťv}

ein Untervektorraum von V und der Eigenraum von A zum Eigenwert Îť
E(A, Îť) := {x â K n : (A â ÎťI)x = 0} = ker(A â ÎťI)

ein Untervektorraum von K n .

(b) Eigenvektoren zu verschiedenen Eigenwerten von ÎŚ (bzw. A) sind linear
unabhaĚngig.
(c) Sind Îť1 , Îť2 , . . . , Îťr verschiedene Eigenwerte von ÎŚ (bzw. A) und B1 , B2 ,
. . . , Br Basen von E(ÎŚ, Îť1 ), E(ÎŚ, Îť2 ), . . . , E(ÎŚ, Îťr ) (bzw. von E(A, Îť1 ),
E(A, Îť2 ), . . . , E(A, Îťr )), so ist die Menge B := B1 âŞ B2 âŞ . . . âŞ Br linear
unabhaĚngig.
Der Beweis von Teil (a) ist eine direkte Anwendung des Untervektorraumkriteriums, die Aussagen in (b) und (c) wollen wir ohne Beweis hinnehmen.
UĚbungsaufgabe 3.11.12. Es sei V ein endlichdimensionaler K-Vektorraum mit
Basis B, ÎŚ : V â V eine lineare Abbildung und A := MBB (ÎŚ). Zeigen Sie,
dass der Eigenraum von A zu einem Eigenwert Îť genau die Koordinatenvektoren
(bezuĚglich B) der Vektoren aus dem Eigenraum von ÎŚ zum selben Îť enthaĚlt.

Wir beweisen nun die folgende wichtige Konsequenz aus obigem Satz.

Satz 3.11.13. Es sei V ein n-dimensionaler K-Vektorraum und ÎŚ : V â V eine
lineare Abbildung. Dann ist ÎŚ genau dann diagonalisierbar, wenn die Summe der
Dimensionen aller EigenraĚume gleich n ist.
Beweis. ââ Ist ÎŚ diagonalisierbar, so gibt es
â
{b1 , b2 , . . . , bn } von V , fuĚr die
ďŁŤ
Îą1 0
ďŁŹ
ďŁŹ0 Îą
MBB (ÎŚ) = ďŁŹ . . 2
..
ďŁ­ ..
0 ...

nach Definition eine Basis B =
...
..
.
..
.
0

ďŁś
0
.. ďŁˇ
. ďŁˇ
ďŁˇ
0ďŁ¸
Îąn

gilt. Dann gilt fuĚr jeden Basisvektor nach der Definition der Abbildungsmatrix
ďŁŤ ďŁś ďŁŤ ďŁś
ďŁŤ
ďŁś 0
0
Îą1 0 . . . 0 ďŁŹ . ďŁˇ ďŁŹ . ďŁˇ
.
.. ďŁˇ ďŁŹ . ďŁˇ ďŁŹ .. ďŁˇ
..
ďŁŹ
ââââ
â
â
â
â
.
0
Îą
. ďŁˇďŁŹ ďŁˇ ďŁŹ ďŁˇ
ďŁŹ
ÎŚ(bj ) = MBB (ÎŚ) bj = ďŁŹ . . 2 .
ďŁˇ ďŁŹ1ďŁˇ = ďŁŹÎąj ďŁˇ = Îąj bj
.. .. 0 ďŁ¸ ďŁŹ.ďŁˇ ďŁŹ . ďŁˇ
ďŁ­ ..
ďŁ­ .. ďŁ¸ ďŁ­ .. ďŁ¸
0 . . . 0 Îąn
0
0

120

3.11. Eigenwerttheorie
Also ist ÎŚ(bj ) = Îąj bj , d.h. B ist eine Basis von Eigenvektoren, was bedeutet,
dass tatsaĚchlich die Summe der Dimensionen der EigenraĚume gleich n ist.
ââ Es sei nun die Summe der Dimensionen der EigenraĚume gleich der Raumdiâ
mension n. Wir waĚhlen dann in jedem Eigenraum eine Basis und erhalten so
Basen B1 , B2 , . . . , Br zu jeweils verschiedenen Eigenwerten von ÎŚ. Dann ist
nach Satz 3.11.11 (c) die Menge B := B1 âŞ B2 âŞ Âˇ Âˇ Âˇ âŞ Br linear unabhaĚngig
und enthaĚlt nach Voraussetzung n Vektoren, ist also eine Basis von V .
BezuĚglich dieser Basis aus Eigenvektoren ist dann MBB (ÎŚ) diagonalisierbar,
vgl. Bemerkung 3.11.6.
Beispiel 3.11.14. Leider gibt es Matrizen und damit auch lineare Abbildungen,
die nicht diagonalisierbar sind. Als Beispiel diene A = ( 00 10 ) â R2Ă2 . Dann ist
det(A â ÎťI) =

âÎť 1
= Îť2 .
0 âÎť

Also ist nur Îť1 = 0 ein Eigenwert von A. Dessen Eigenraum berechnet sich als
LoĚsungsmenge des LGS


0 1 0
0 0 0
zu

 
Îą
:ÎąâR .
0
Der Eigenraum hat Dimension 1, es gibt also nur einen linear unabhaĚngigen
Eigenvektor.

Bemerkung 3.11.15. Wir haben gesehen, dass die Eigenwerte genau die Nullstellen des charakteristischen Polynoms sind. Das Problem Eigenwerte zu finden,
ist also ein Nullstellenproblem fuĚr Polynome. Zumindest in theoretischer Hinsicht
ist dieses mit dem Fundamentalsatz der Algebra 2.5.14 befriedigend geloĚst: Jedes
komplexe Polynom zerfaĚllt uĚber C in Linearfaktoren.
Das ist der Grund, warum Eigenwerttheorie eigentlich immer uĚber C betrieben
wird, selbst wenn alle beteiligten Matrizen rein reell sind.
Auch hier sind alle folgenden Betrachtungen in diesem Sinne zu verstehen.
Satz 3.11.16. Es sei n â Nâ und A â QnĂn , RnĂn oder CnĂn . Dann hat A
mindestens einen komplexen Eigenwert.
Beweis. Das charakteristische Polynom von A ist ein Polynom vom Grad n mit
Koeffizienten in Q, R oder C. Die Existenz einer komplexen Nullstelle, und damit
eines Eigenwertes, folgt nun aus dem Fundamentalsatz der Algebra 2.5.14.
Definition 3.11.17. Eine Matrix A â K nĂn heiĂt symmetrisch, falls A = AT
gilt.

121

3. Lineare Algebra
Es gilt der folgende Hauptsatz fuĚr symmetrische Matrizen, den wir wieder nicht
beweisen wollen.
Satz 3.11.18. Es sei A â RnĂn symmetrisch. Dann sind alle Eigenwerte reell
und es gibt eine Orthonormalbasis von Rn aus Eigenvektoren von A. Insbesondere
ist jede symmetrische Matrix also diagonalisierbar.


1 4
.
Beispiel 3.11.19. Es sei A =
4 â5
Dann ist
1âÎť
4
= Îť2 + 4Îť â 21
det(A â ÎťI) =
4
â5 â Îť
und wir erhalten fuĚr die Eigenwerte:
â
Îť1/2 = â2 Âą 4 + 21 = â2 Âą 5,

also Îť1 = â7,

Îť2 = 3.

Die Eigenvektoren zu Îť1 = â7 ergeben sich als LoĚsungen des linearen Gleichungssystems






2 1 0
2 1 0 Âˇ(â1)
8 4 0 :4
.
â
0 0 0
2 1 0
4 2 0 :2
 
â1
Also ist x1 =
ein Eigenvektor zum Eigenwert Îť1 = â7.
2
 
2
Genauso findet man x2 =
als einen Eigenvektor zu Îť2 = 3.
1
Damit ist

 
 
1 â1
1 2
â
,â
5 2
5 1
eine Orthonormalbasis von R2 aus Eigenvektoren von A.
Definition 3.11.20. Es sei (Âˇ|Âˇ) das Standardskalarprodukt auf Rn . Eine symmetrische Matrix A â RnĂn heiĂt
(a) positiv definit, falls (x|Ax) > 0 fuĚr alle x â Rn \ {0} gilt.

(b) positiv semidefinit, falls (x|Ax) âĽ 0 fuĚr alle x â Rn \ {0} gilt.
(c) negativ definit, falls (x|Ax) < 0 fuĚr alle x â Rn \ {0} gilt.
(d) negativ semidefinit, falls (x|Ax) â¤ 0 fuĚr alle x â Rn \ {0} gilt.
(e) indefinit, falls es Vektoren x, y â Rn gibt mit (x|Ax) > 0 und (y|Ay) < 0.
Bemerkung 3.11.21. Man beachte, dass alle Definitheitsbegriffe von vornherein
nur fuĚr symmetrische Matrizen A definiert sind. Spricht man von einer positiv
oder negativ definiten Matrix, so ist damit immer, auch wenn es nicht dasteht,
auch gemeint, dass die Matrix symmetrisch ist.

122

3.11. Eigenwerttheorie
Im folgenden Satz sind zum Abschluss dieses Abschnitts noch einige Kriterien
zum Nachweis von positiver und negativer Defintheit gesammelt. Wir werden
diesen Begriffen in einem ganz anderen Zusammenhang in der Matehmatik II
noch einmal begegnen.
Satz 3.11.22. Es sei A = (Îąjk )nj,k=1 â RnĂn symmetrisch.
(a) A ist genau dann positiv definit, wenn âA negativ definit ist.
(b) Die folgenden Aussagen sind aĚquivalent:
i) A ist positiv definit.
ii) Alle Eigenwerte von A sind groĚĂer als Null.
iii) Es gilt fuĚr jedes m â {1, 2, . . . , n}, dass det(Îąjk )m
j,k=1 > 0.
(c) Die folgenden Aussagen sind aĚquivalent:
i) A ist negativ definit.
ii) Alle Eigenwerte von A sind kleiner als Null.
iii) Es gilt fuĚr jedes m â {1, 2, . . . , n}, dass (â1)m+1 det(Îąjk )m
j,k=1 < 0.
Bemerkung 3.11.23. Die Teildeterminanten det(Îąjk )m
j,k=1 in (b)iii) und (c)iii)
heiĂen Unterminoren der Determinante. Man kann sich merken: Eine symmetrische Matrix ist genau dann positiv definit, wenn alle Unterminoren positiv sind
und genau dann negativ definit, wenn die Unterminoren alternierende Vorzeichen
haben, wobei es mit einer negativen Zahl losgehen muss.
Dass es genau so sein muss, macht man sich am besten an Diagonalmatrizen klar.
Wir betrachten noch beispielhaft zwei Matrizen
Beispiel 3.11.24. Es
ďŁŤ
1
A = ďŁ­â2
2

seien
ďŁś
â2 2
5 0ďŁ¸
0 30

und

ďŁś
ďŁŤ
â2 3
0
B = ďŁ­ 3 â5 0 ďŁ¸ .
0
0 â1

Dann sind die Unterminoren von A gegeben durch

det (1) = 1 > 0


1 â2
det
=5â4=1>0
â2 5

det(A) = 150 â 20 â 120 = 10 > 0,

also ist A positiv definit.

123

3. Lineare Algebra
Die Unterminoren von B sind

det (â2) = â2 < 0


â2 3
= 10 â 9 = 1 > 0
det
3 â5
det(B) = â1 Âˇ

â2 3
= â1 Âˇ 1 = â1 < 0,
3 â5

und wir erhalten, dass B negativ definit ist.
UĚbungsaufgabe 3.11.25. Es sei (Âˇ|Âˇ) das Standardskalarprodukt auf R und
A â RnĂn positiv definit. Zeigen Sie, dass durch
(x|y)A := (x|Ay),
ein Skalarprodukt auf Rn definiert wird.

124

x, y â Rn ,

4. Analysis â Teil I: Konvergenz
und Stetigkeit
4.1. Die reellen Zahlen
Wir erinnern uns an den Begriff eines angeordneten KoĚrpers:
Dies ist ein KoĚrper K mit einer Totalordnung â¤, fuĚr die gilt:
â˘ âa, b, c â K : a â¤ b =â a + c â¤ b + c und
â˘ âa, b, c â K : (a â¤ b und 0 â¤ c) =â ac â¤ bc.
Definition 4.1.1. Die Menge der reellen Zahlen R ist der kleinste angeordnete
KoĚrper, der Z enthaĚlt, und das VollstaĚndigkeitsaxiom
Jede nichtleere Teilmenge, die eine obere Schranke besitzt, hat ein Supremum.
erfuĚllt.
Bemerkung 4.1.2. Auch die rationalen Zahlen Q sind ein angeordneter KoĚrper,
der Z enthaĚlt, aber dieser erfuĚllt nicht das VollstaĚndigkeitsaxiom, denn {x â Q :
x2 < 2} hat obere Schranken aber kein Supremum in Q, vgl. Beispiel 1.3.9 (b).
Definition 4.1.3. Eine Teilmenge M â R heiĂt
(a) nach oben (unten) beschraĚnkt, wenn sie eine obere (untere) Schranke besitzt.
(b) beschraĚnkt, wenn sie nach oben und unten beschraĚnkt ist.
Satz 4.1.4. Jede nach unten beschraĚnkte, nichtleere Teilmenge von R besitzt ein
Infimum.
Beweis. Es sei M â R eine nach unten beschraĚnkte und nichtleere Menge. Dann
c :=
gibt es eine untere Schranke C von M. Wir betrachten nun die Teilmenge M
{âx : x â M} von R, die ebenfalls nichtleer ist. Da C eine untere Schranke von
M ist, gilt C â¤ x fuĚr alle x â M. Das bedeutet, dass âC âĽ âx fuĚr alle x â M
c
ist. Anders formuliert, erhalten wir âC âĽ y fuĚr jedes y â M.
c. Nach dem VollstaĚndigkeitsaxiom existiert
Also ist âC eine obere Schranke von M
c
nun s := sup(M) und wir wollen zeigen, dass âs das Infimum von M ist.
125

4. Analysis â Teil I: Konvergenz und Stetigkeit
Dazu zeigen wir zunaĚchst, dass âs eine untere Schranke ist: Es sei x â M beliebig.
c und es gilt nach der Konstruktion von s auf jeden Fall âx â¤ s.
Dann gilt âx â M
Also ist x âĽ âs fuĚr jedes x â M, was zeigt, dass âs eine untere Schranke von M
ist.
Sei nun t â R eine weitere untere Schranke von M. Dann ist mit der selben
c Nun ist s die kleinste
Argumentation wie oben ât eine obere Schranke von M.
c
obere Schranke von M , also muss s â¤ ât gelten. Damit ist aber auch âs âĽ t.
Also ist âs die groĚĂte untere Schranke von M, d.h. das Infimum.
Eine wichtige Rolle in der Analysis spielt die Betragsfunktion, an die wir ebenfalls
noch schnell erinnern wollen.
Definition 4.1.5. Die Funktion | Âˇ | : R â R mit
(
x,
falls x âĽ 0,
|x| :=
âx,
falls x < 0,
heiĂt Betragsfunktion und |x| heiĂt Betrag von x.
Es gelten die folgenden Rechenregeln, vgl. Satz 2.5.12 und Beispiel 3.4.2 (a):
Satz 4.1.6. FuĚr alle x, y â R gilt
(a) |x| âĽ 0,
(b) |x| = |âx|,
(c) Âąx â¤ |x|,
(d) |xy| = |x| Âˇ |y|,
(e) |x| = 0 genau dann, wenn x = 0,
(f ) |x + y| â¤ |x| + |y|

(Dreiecksungleichung),

UĚbungsaufgabe 4.1.7. Zeigen Sie die umgekehrte Dreiecksungleichung
|x| â |y| â¤ |x â y| fuĚr alle x, y â R.
Wir beschlieĂen diesen Abschnitt mit der Definition von Intervallen.
Definition 4.1.8. Es seien zwei Zahlen a, b â R mit a < b gegeben. Dann heiĂen
â˘ (a, b) := {x â R : a < x < b} offenes Intervall,
â˘ [a, b] := {x â R : a â¤ x â¤ b} abgeschlossenes Intervall,
â˘ (a, b] := {x â R : a < x â¤ b} und

126

4.2. Wurzeln, FakultaĚten und Binomialkoeffizienten
â˘ [a, b) := {x â R : a â¤ x < b} halboffene Intervalle.
Um auch die FaĚlle von Halbstrahlen abzudecken, definieren wir weiter:
â˘ [a, â) := {x â R : a â¤ x},

â˘ (ââ, a] := {x â R : x â¤ a},

â˘ (a, â) := {x â R : a < x},

â˘ (ââ, a) := {x â R : x < a},

â˘ (ââ, â) := R.
SchlieĂlich schreiben wir
â˘ R+ := [0, â),

â˘ Râ := (ââ, 0].

4.2. Wurzeln, FakultaĚten und Binomialkoeffizienten
Eine wichtige Schlussfolgerung aus dem VollstaĚndigkteitsaxiom ist die Existenz
der n-ten Wurzeln in R+ .
Bevor wir in diese Betrachtungen einsteigen, definieren wir der VollstaĚndigkeit
halber noch die ganzzahligen Potenzen.
Definition 4.2.1. FuĚr jedes x â R und jedes n â Nâ ist
(a) xn := |x Âˇ x {z
Âˇ . . . Âˇ x},
n Faktoren

(b) xân :=

1
, falls x 6= 0, sowie
xn

(c) x0 := 1.
Satz 4.2.2. FuĚr jedes a â R+ und alle n â Nâ gibt es genau ein x â R+ mit
xn = a.
Beweisidee. Man zeigt zunaĚchst, dass xn â¤ y n ââ x â¤ y fuĚr jede Wahl von
x, y â R+ und alle n â Nâ gilt.
FuĚr den Nachweis der Eindeutigkeit seien x, y â R mit xn = a = y n gegeben.
Dann gilt xn â¤ y n und y n â¤ xn . Nach der VoruĚberlegung ist dann x â¤ y und
y â¤ x, also x = y.
FuĚr die Existenz betrachtet man zunaĚchst den Fall a = 0. Dann ist offensichtlich
x = 0 eine LoĚsung. Sei also ab jetzt a > 0. Wir betrachten die Menge M :=
{x â R+ : xn â¤ a}. Dann ist 0 â M, also ist M 6= â. Weiterhin ist M nach oben
beschraĚnkt, z.B. ist 1 + a eine obere Schranke (Achtung, a ist im Allgemeinen
keine!).
Also hat die Menge M nach dem VollstaĚndigkeitsaxiom ein Supremum x und fuĚr
dieses gilt xn = a.

127

4. Analysis â Teil I: Konvergenz und Stetigkeit
Definition 4.2.3. Es seien a â R+ und n â Nâ . Die eindeutige
Zahl x â R+ mit
â
n
n
x = a heiĂt n-te Wurzel von x und
schreibt x = a. FuĚr den wichtigsten
â manâ
2
Fall n = 2 gibt es die Konvention a := a.
Satz 4.2.4. Es seien q â Q und m, p â Z, sowie n, r â Nâ so, dass q = m/n =
p/r ist. Dann gilt fuĚr jedes x â R+
â
â
( n x)m = ( r x)p .
Beweis. FuĚr jedes x â R+ gilt
r
â
â
â p
â
( n x)m = ( n x)mr = ( n x)np = ( n x)n = xp
â r
â p
â
( r x)p = ( r x)pr = ( r x)r = xp .

und

Also folgt aus der Eindeutigkeit der Wurzel die Behauptung.

Definition 4.2.5. FuĚr jedes x â R+ und jedes q = n/m â Q mit n â Z und
m â Nâ ist die rationale Potenz definiert durch
â
xq = xn/m := ( m x)n .
Bemerkung 4.2.6. Auch fuĚr rationale Exponenten gelten die bekannten Rechenregeln fuĚr Potenzen: FuĚr alle x, y â R+ \ {0} und alle p, q â Q gilt
â˘ xp xq = xp+q

â˘

xp
xq

â˘ xp y p = (xy)p

â˘

xp
yp

â˘ (xp )q = xpq
Definition 4.2.7.

= xpâq
p
= xy

(a) Es sei n â Nâ . Dann wird die Zahl
n! := 1 Âˇ 2 Âˇ . . . Âˇ n

als n FakultaĚt bezeichnet.
Weiterhin definieren wir 0! := 1.
(b) Es seien n, k â N mit k â¤ n. Dann heiĂt
 
n!
n
.
:=
k!(n â k)!
k
Binomialkoeffizient n uĚber kâ.
â

Bemerkung 4.2.8. Die beiden GroĚĂen n! und nk haben auch eine anschauliche
Bedeutung:
n! ist die Anzahl der moĚglichen Reihenfolgen von n unterscheidbaren Dingen.
n
ist die Anzahl der MoĚglichkeiten aus n unterscheidbaren Dingen genau k
k
auszuwaĚhlen.

128

4.2. Wurzeln, FakultaĚten und Binomialkoeffizienten
Satz 4.2.9. Es seien n, k â N mit k â¤ n und a, b â R. Dann gilt

 
  
   
n+1
n
n
n
n
.
=
+
=1
und
=
(a)
k
kâ1
k
n
0
n+1

(b) a

n+1

âb

= (a â b)

n
X

anâk bk .

k=0

n  
X
n nâk k
a b .
(c) (a + b) =
k
k=0
n

Beweis.

(Binomialformel)

(a) Es ist
 
 
n!
n
n!
n!
n!
n
=
=
=
= 1 und
=
= 1.
0
0!(n â 0)!
n!
n
n!(n â n)!
n!

AuĂerdem ist

  
n!
n
n!
n
=
+
+
kâ1
k
k!(n â k)! (k â 1)!(n â k + 1)!
n!(n â k + 1)
n!k
n!(n â k + 1 + k)
=
+
=
k!(n â k + 1)! k!(n â k + 1)!
k!(n â k + 1)!


n!(n + 1)
(n + 1)!
n+1
=
.
=
=
k
k!(n â k + 1)!
k!(n + 1 â k)!
(b) Es gilt
(a â b)

n
X

anâk bk = a

n
X
k=0

k=0

=

n
X
k=0

anâk bk â b

anâk+1 bk â

= an+1 +
= an+1 +
n+1

=a

n
X

k=1
nâ1
X

anâk bk

k=0

n
X

anâk bk+1

k=0

anâk+1 bk â
anâk bk+1 â

k=0
n+1

âb

n
X

.

nâ1
X

k=0
nâ1
X
k=0

anâk bk+1 â bn+1
anâk bk+1 â bn+1

(c) Dies ist eine gute AuffrischungsuĚbung in vollstaĚndiger Induktion.
Eine Summe, wie sie im Beweis von Teil (b) auftritt, bei der sich bis auf zwei alle
Summanden gegenseitig wegheben, nennt man auch anschaulich Teleskopsumme.

129

4. Analysis â Teil I: Konvergenz und Stetigkeit
Bemerkung 4.2.10. Mit der zweiten Formel aus Satz 4.2.9 (a) kann man sich die
Binomialkoeffizienten gut merken. Schreibt man diese in ein dreieckiges Schema:

0

 0
1
1
1
0



2
2
2
0

..
.

1

..
.

2

..
.

..
.

so sagt diese Formel gerade, dass man einen Eintrag bekommt, indem man die
beiden diagonal links und rechts daruĚber zusammenzaĚhlt. Das ergibt das sogenannte Pascalâsche Dreieck
1
1
1
1
1
..
.

1
2

3
4
..
.

1
3

6
..
.

1
4
..
.

1
..
.

Aus diesem kann man nun nach Teil (c) des obigen Satzes z.B. direkt ablesen,
dass
(a + b)4 = a4 + 4a3 b + 6a2 b2 + 4ab3 + b4
gilt.

4.3. Konvergenz von Folgen
Wir wollen uns nun dem zentralen Thema der Analysis zuwenden, der mathematisch exakten Behandlung des unendlich Kleinen und unendlich GroĂen. Beispielsweise kann es darum gehen, unendlich viele Zahlen aufzuaddieren, wie in
der unendlichen Summe
1+

1 1 1
1
+ + +
+ ...,
2 4 8 16

der wir im Folgenden einen exakten Sinn geben werden.
Hierbei ist einige Vorsicht geboten, denn beim Umgang mit dem Unendlichen
koĚnnen sehr unintuitive Dinge passieren, so dass anschauliche Argumentationen
schnell in die Irre fuĚhren koĚnnen. Unser Ziel wird also zunaĚchst sein, eine exakte
mathematische Definition fuĚr solche Grenzwertfragen zu geben. Diese Aufgabe
wollen wir in diesem fuĚr alles weitere zentralen Kapitel angehen.
Wir erinnern noch einmal an den Begriff einer Folge aus Beispiel 3.1.2 (d). Eine
Folge war dort eine Abbildung von N in einen KoĚrper K. In Erweiterung der
dortigen Definition lassen wir nun statt einem KoĚrper allerdings eine beliebige

130

4.3. Konvergenz von Folgen
nichtleere Menge X zu und sagen, dass eine Folge eine Abbildung a : N â X
ist. Um klar zu machen, was die Zielmenge dieser Abbildung ist, nennen wir a
genauer eine Folge in X. Statt Folge in R, bzw. C sagt man auch oft reelle Folge,
bzw. komplexe Folge.
Wir schreiben wieder an statt a(n) und bezeichnen die Folge a mit (an )nâN oder
(an )nâĽ0 oder (a0 , a1 , a2 , . . . ). Manchmal werden wir auch etwas verkuĚrzt einfach
(an ) schreiben.
Zuweilen ist es praktisch mit der ZaĚhlung nicht bei Null, sondern einer anderen
natuĚrlichen Zahl zu beginnen. Wir schreiben dann beispielsweise (an )nâĽ4 oder
(a4 , a5 , a6 , . . . ).
Die meisten Betrachtungen in diesem Abschnitt gelten fuĚr die KoĚrper Q, R und C
gleichermaĂen. In diesem Abschnitt steht deshalb K fuĚr einen dieser drei KoĚrper.

4.3.1. Der Konvergenzbegriff und wichtige Beispiele
Definition 4.3.1. (a) Es sei (an ) eine Folge in K und a â K. Die Folge (an )
heiĂt konvergent gegen a, falls fuĚr jedes Îľ > 0 ein n0 â N existiert mit
|an â a| < Îľ fuĚr alle n âĽ n0 .
In diesem Fall heiĂt a der Grenzwert oder Limes von (an ) und wir schreiben
lim an = a

nââ

oder an â a (n â â).

(b) Ist (an ) eine Folge in K, die gegen kein a â K konvergiert, so heiĂt diese
divergent.
Man kann zeigen, dass eine Folge in K hoĚchstens einen Grenzwert haben kann.
Wenn eine Folge konvergiert, ist der Limes also eindeutig.
Beispiel 4.3.2.
(a) Wir betrachten die Folge (an ) = (1/n)nâĽ1 = (1, 1/2, 1/3, 1/4, . . . ).
Behauptung: (an ) ist konvergent und es gilt limnââ an = 0.
Beweis. Sei Îľ > 0. Dann gibt es ein n0 â N mit n0 > 1/Îľ. Also ist 1/n0 < Îľ
und wir haben fuĚr alle n âĽ n0
|an â a| = |an â 0| = |an | =

1
1
< Îľ.
â¤
n
n0

Eine solche Folge, die gegen Null konvergiert, nennt man auch eine Nullfolge.

131

4. Analysis â Teil I: Konvergenz und Stetigkeit
(b) Es sei (an ) = ((â1)n )nâN = (1, â1, 1, â1, 1, â1, . . . ).
Behauptung: Die Folge (an ) divergiert.

Beweis. Wir nehmen an, es gaĚbe ein a â K mit an â a (n â â). Dann gibt
es zu Îľ = 1 ein n0 â N, so dass fuĚr jedes n âĽ n0 die Ungleichung |an âa| < 1
gilt. FuĚr n âĽ n0 gilt dann aber mit Hilfe der Dreiecksungleichung
2 = |an â an+1 | = |an â a + a â an+1 | â¤ |an â a| + |a â an+1 | < 1 + 1 = 2.
Also folgt 2 < 2, ein Widerspruch.
n2 + 2n â 1
, n â N.
n2 + 2
Behauptung: (an ) konvergiert und limnââ an = 1.

(c) Sei an =

Beweis. Es gilt
|an â 1| =

|2n â 3|
|2n â 3|
2n + 3
n2 + 2n â 1 â n2 â 2
= 2
â¤
â¤
,
2
2
n +2
n +2
n
n2

wobei wir bei der letzten AbschaĚtzung die Dreiecksungleichung angewendet
haben. Nun verwenden wir noch, dass fuĚr alle n âĽ 1 gilt 2n + 3 â¤ 2n + 3n =
5n und erhalten damit
5n
5
|an â 1| â¤ 2 = .
n
n
Sei nun Îľ > 0. Dann gibt es wieder ein n0 â N mit n0 > 5/Îľ. Also haben
wir nach obiger AbschaĚtzung fuĚr alle n âĽ n0
|an â 1| â¤

5
5
â¤
< Îľ.
n
n0

UĚbungsaufgabe 4.3.3. Es sei (an ) eine Folge in K und a â K. Zeigen Sie:
(a) Gibt es eine reelle Nullfolge (Îąn ) mit
|an â a| â¤ Îąn

fuĚr alle n â N,

so konvergiert (an ) gegen a.
(b) Die Folge (an ) konvergiert genau dann gegen a, wenn die Folge (|an â a|)
gegen Null konvergiert.
Definition 4.3.4. (a) Eine Folge (an ) in K heiĂt beschraĚnkt, wenn die Menge
{an : n â N} = {a0 , a1 , a2 , . . . } beschraĚnkt in K ist.

132

4.3. Konvergenz von Folgen
(b) Ist K = R oder K = Q, so setzen wir weiter
â

sup an := sup an := sup{an : n â N},
nâN

n=0
â

inf an := inf an := inf{an : n â N}.

nâN

n=0

Satz 4.3.5. Jede konvergente Folge in K ist beschraĚnkt.
Beweis. Sei (an ) eine konvergente Folge in K mit Grenzwert a. Nach der Definition der Konvergenz existiert zu Îľ = 1 ein n0 â N mit |an â a| < 1 fuĚr alle n âĽ n0 .
Wir setzen C := max{|a0 |, |a1|, |a2 |, . . . , |an0 â1 |, 1 + |a|}. Dann gilt fuĚr alle n < n0
sofort |an | â¤ C und auch fuĚr alle n âĽ n0 gilt diese Ungleichung, denn
|an | = |an â a + a| â¤ |an â a| + |a| < 1 + |a| â¤ C.
Zusammengenommen gilt also |an | â¤ C fuĚr alle n â N und somit die Behauptung.
Warnung 4.3.6. Die Umkehrung von Satz 4.3.5 ist falsch! Es gibt durchaus
beschraĚnkte Folgen, die nicht konvergieren, vgl. Beispiel 4.3.2 (b).
Die Berechnung von Grenzwerten ist allein uĚber die Definition sehr muĚhsam und
wird bei groĚĂeren AusdruĚcken mehr oder weniger unmoĚglich. Eine groĂe Hilfe ist
der folgende Satz, der es erlaubt, die Berechnung komplizierter Grenzwerte auf
die Betrachtung einfacherer Einzelteile zu reduzieren.
Satz 4.3.7 (GrenzwertsaĚtze). Es seien (an ), (bn ) und (cn ) Folgen in K. Dann
gilt:
(a) Ist limnââ an = a, so gilt limnââ |an | = |a|.
(b) Gilt limnââ an = a und limnââ bn = b, so folgt
i) limnââ (an + bn ) = a + b.
ii) limnââ (Îąan ) = Îąa fuĚr alle Îą â K.

iii) limnââ (an Âˇ bn ) = a Âˇ b.

iv) Ist zusaĚtzlich bn 6= 0 fuĚr alle n â N und b 6= 0, so ist limnââ an /bn =
a/b.

Ist K entweder R oder Q, so gilt auĂerdem
(c) Ist an â¤ bn fuĚr alle n â N und limnââ an = a sowie limnââ bn = b, so folgt
a â¤ b.
(d) Ist an â¤ cn â¤ bn fuĚr alle n â N und sind (an ) und (bn ) konvergent mit
limnââ an = limnââ bn = a, so ist auch die Folge (cn ) konvergent und es
gilt limnââ cn = a (Sandwich-Theorem).

133

4. Analysis â Teil I: Konvergenz und Stetigkeit
Beweis. (a) Sei Îľ > 0. Da (an ) gegen a konvergiert, gibt es dann ein n0 â N mit
|an âa| < Îľ fuĚr alle n âĽ n0 . FuĚr alle diese n gilt dann nach der umgekehrten
Dreiecksungleichung, s. UĚbungsaufgabe 4.1.7
|an | â |a| â¤ |an â a| < Îľ.
Also konvergiert die Folge (|an |) gegen |a|.
(b) Die Teile (b)i)-(b)iii) verbleiben als UĚbungsaufgabe. Wir beweisen hier
(b)iv).
Da (bn ) gegen b konvergiert, konvergiert nach (a) auch (|bn |) gegen |b|. Da
weiter b 6= 0 ist, gilt |b| > 0. Zu Îľ := |b|/2 > 0 gibt es also ein n1 â N
mit ||bn | â |b|| â¤ |b|/2 fuĚr alle n âĽ n1 . FuĚr all diese n gilt dann mit der
Dreiecksungleichung
|b| = |b| = |b| â |bn | + |bn | â¤ |b| â |bn | + |bn | â¤

|b|
+ |bn |.
2

Zieht man |b|/2 auf beiden Seiten ab, haben wir also fuĚr alle n âĽ n1
|bn | âĽ

|b|
,
2

bzw.

1
2
â¤ .
|bn |
|b|

Das liefert wiederum fuĚr alle diese n
b â bn
|bn â b|
1
2|bn â b|
2
1
=
=
â
â¤
= 2 |bn â b|.
2
bn
b
bbn
|b||bn |
|b|
|b|
Sei nun Îľ > 0 und n2 â N so, dass |bn â b| < Îľ|b|2 /2 fuĚr alle n âĽ n2 . Dann
gilt fuĚr alle n â N mit n âĽ max{n1 , n2 }
1
2 Îľ|b|2
1
< 2
â
= Îľ.
bn
b
|b| 2
Also konvergiert (1/bn ) gegen (1/b).
Die Konvergenz von (an /bn ) gegen a/b folgt nun aus (b)iii).
(c) Wir nehmen an, es waĚre a > b. Dann ist Îľ := (a â b)/2 > 0 und dank
der Konvergenz von (an ) und (bn ) gibt es nun ein n1 â N, so dass bn â
(b â Îľ, b + Îľ) und an â (a â Îľ, a + Îľ) fuĚr alle n âĽ n1 gilt. Da
b+Îľ=b+

a b
aâb
aâb
= + =aâ
=aâÎľ
2
2 2
2

gilt, haben wir also fuĚr diese n auch bn < an im Widerspruch zur Voraussetzung.

134

4.3. Konvergenz von Folgen
(d) Sei Îľ > 0. Dann gibt es ein n0 â N, so dass fuĚr alle n âĽ n0 sowohl |an âa| < Îľ
als auch |bn â a| < Îľ gilt. Hieraus und aus der Voraussetzung folgern wir
fuĚr alle diese n
a â Îľ < an â¤ cn â¤ bn < a + Îľ.
Also ist a â Îľ < cn < a + Îľ oder, anders ausgedruĚckt, âÎľ < cn â a < Îľ,
d.h. |cn â a| < Îľ fuĚr alle n âĽ n0 und damit konvergiert die Folge (cn ) gegen
a.
Warnung 4.3.8. Die Aussage in (c) gilt nicht mit <â statt â¤â! Als Beispiel
â
â
koĚnnen die Folgen (an ) = (0)nâĽ1 und (bn ) = (1/n)nâĽ1 dienen. Dann gilt naĚmlich
an < bn fuĚr alle n âĽ 1, aber die beiden Grenzwerte sind gleich.
Wir wollen nun an zwei Beispielen zeigen, wie mit Hilfe dieses Satzes komplizierte
Grenzwerte angegangen werden koĚnnen.
Beispiel 4.3.9. (a) Sei p â Nâ fest gewaĚhlt und an = 1/np fuĚr n â Nâ . Dann
gilt fuĚr alle n â Nâ die Ungleichung n â¤ np und damit
0 â¤ an =

1
1
â¤ .
p
n
n

Da sowohl die Folge, die konstant Null ist, als auch die Folge (1/n) gegen
Null konvergiert, ist damit nach Satz 4.3.7 (d) auch die Folge (an ) konvergent und ebenfalls eine Nullfolge.
(b) Wir untersuchen
n2 + 2n + 3
, n â N.
n2 + 3
Dazu kuĚrzen wir den Bruch durch die hoĚchste auftretende Potenz:
an =

1 + n2 + n32
n2 + 2n + 3
1+0+0
an =
ââ
=
= 1 (n â â).
3
2
n +3
1+0
1 + n2
Dabei stuĚtzen wir uns auf fuĚr die Berechnung des Grenzwertes von (1/n2 )
auf das Beispiel in (a) und zum Zusammenbau des Gesamtausdruckes auf
(b)i), (b)ii) und (b)iv) aus Satz 4.3.7.
Dieses Vorgehen (KuĚrzen durch die hoĚchste auftretende Potenz) ist bei allen
Grenzwerten der Form Polynom in n geteilt durch Polynom in nâ Erfolg
â
versprechend.
Bemerkung 4.3.10. Hier finden Sie weitere wichtige Beispiele von konvergenten
Folgen.
(a) Ist (an ) eine konvergente Folge in R mit Grenzwert a und
â gilt an âĽ 0 fuĚr
â
alle n â N, so ist fuĚr jedes p â Nâ auch limnââ p an = p a.

135

4. Analysis â Teil I: Konvergenz und Stetigkeit
(b) Die Folge (q n )nâN mit q â R konvergiert genau dann, wenn q â (â1, 1] ist
und es gilt
(
1, falls q = 1,
lim q n =
nââ
0, falls â 1 < q < 1.
Ist q â C mit |q| < 1, so gilt ebenfalls limnââ q n = 0.
â
(c) limnââ n c = 1 fuĚr jedes c â R+ .
â
(d) limnââ n n = 1.
(e) Die Folge


1 n
an := 1 +
,
n âĽ 1,
n
ist konvergent. Ihr Grenzwert

1 n
e := lim 1 +
nââ
n

heiĂt Eulersche Zahl . Diese ist eine irrationale Zahl mit
e â 2, 718281828459.

Warnung 4.3.11. Die Folge (an ) aus Teil (e) in obiger Bemerkung bietet eine gute Gelegenheit vor einem verbreiteten Fehler bei der Bestimmung von Grenzwerten zu warnen, der Unterteilung in eiligereâ und traĚgereâ n. Falsch ist naĚmlich
â
â
folgende UĚberlegung: Die Folge (1 + 1/n) geht offensichtlich gegen 1, also geht
(an ) gegen 1n und das ist immer 1, was zu dem Ergebnis fuĚhre (an ) wuĚrde gegen
1 streben. Das ist, vgl. oben, grob falsch. Der Grund ist folgender: Bei dieser
UĚberlegung werden nicht alle n in der Formel gleich behandelt. Das n innerhalb
der Klammer wird (quasi als Vorhut) zuerst nach â geschickt, waĚhrend das n
im Exponenten noch warten muss, also zum traĚgenâ n ernannt wird. Das geht
â
nicht. Merke: Alle n sind gleich!
Mit der gleichen Berechtigung koĚnnte man auch argumentieren, dass 1 + 1/n
immer echt groĚĂer als 1 ist und da q n fuĚr alle q > 1 divergiert, divergiert der
Ausdruck in der Klammer, also auch die ganze Folge. Nun ist das andere n zum
Warten gezwungen worden, und das Ergebnis ist genauso falsch wie das erste.
Diese EroĚrterung zeigt aber, was hier passiert. Das 1/n in der Klammer bringt
den Ausdruck immer naĚher an 1, waĚhrend es groĂ wird, und macht es dem n im
Exponenten damit immer schwerer, die Werte von an zu vergroĚĂern. Die beiden
beeinflussen den Wert also in verschiedene Richtungen und die Frage ist, wer
dabei erfolgreicher ist: Schafft es das n in der Klammer, die Sache nach 1 zu
druĚcken, oder ist das n im Exponent staĚrker und die Folge divergiert? Daran,
dass das Ergebnis irgendwo zwischen 2 und 3 liegt, sieht man, dass die beiden
sich in magischer Weise im Gleichgewicht halten.

136

4.3. Konvergenz von Folgen
Beispiel 4.3.12. Es bleiben noch zwei wichtige Beispiele nachzutragen.
(a) Den folgenden Trick muss man mal gesehen haben. Es ist fuĚr alle n â N
0â¤

â

n+1â

â

â
â â
â
( n + 1 â n)( n + 1 + n)
(n + 1) â n
â
n=
=â
â
â
n+1+ n
n+1+ n
r
1
1
1 1
=â
.
â â¤ â =
2 n
2 n
n+1+ n

p
Da wegen (a) aus Bemerkung 4.3.10 und Beispiel 4.3.2 (a) auch ( 1/n)
gegen Null geht, folgt damit aus dem Sandwich-Theorem
lim

nââ

â

n+1â

â 
n = 0.

(b) Wir betrachten fuĚr jedes q â C die Folge
an :=

n
X
k=0

qk = 1 + q + q2 + q3 + Âˇ Âˇ Âˇ + qn,

n â N.

Dann gilt
(1 â q)

n
X
k=0

k

q =

n
X
k=0

k

q â

n
X

q

k+1

0

=q +

k=0

= 1 â q n+1 +

n
X
k=1

n
X
k=1

qk â

n
X
k=1

k

q â

nâ1
X
k=0

q k+1 â q n+1

q k = 1 â q n+1 .

Wir erhalten so fuĚr q 6= 1 die geometrische Summenformel
n
X
k=0

1 â q n+1
q =
,
1âq
k

q 6= 1.

Mit Bemerkung 4.3.10 (b) gilt also fuĚr |q| < 1
1
1 â q n+1
=
,
lim an = lim
nââ
nââ 1 â q
1âq

|q| < 1.

Definition 4.3.13. Eine Folge (an ) in R oder Q divergiert bestimmt nach â
(ââ) und wir schreiben limnââ an = â (ââ), wenn es fuĚr jedes C âĽ 0 ein
n0 â N gibt, so dass an âĽ C (an â¤ âC) fuĚr alle n âĽ n0 gilt.

137

4. Analysis â Teil I: Konvergenz und Stetigkeit

4.3.2. Konvergenzkriterien
Wir wollen uns als naĚchstes mit dem Monotonie-Verhalten von Folgen auseinandersetzen. Das geht naturgemaĚĂ nicht in C, so dass wir uns auf R einschraĚnken
muĚssen.
Definition 4.3.14. Eine relle Folge (an ) heiĂt
(a) monoton wachsend, wenn an+1 âĽ an fuĚr alle n â N gilt.
(b) monoton fallend, wenn an+1 â¤ an fuĚr alle n â N gilt.
(c) monoton, wenn sie monoton wachsend oder monoton fallend ist.
Damit koĚnnen wir folgendes Konvergenzkriterium beweisen.
Satz 4.3.15 (Monotonie-Kriterium). Ist die reelle Folge (an ) nach oben (bzw.
unten) beschraĚnkt und monoton wachsend (bzw. fallend), so ist (an ) konvergent
und es gilt
lim an = sup an (bzw. lim an = inf an ).
nââ

nââ

nâN

nâN

Beweis. Es sei (an ) nach oben beschraĚnkt und monoton wachsend, sowie a :=
supnâN an . WaĚhlen wir nun ein Îľ > 0, so ist sicherlich a â Îľ keine obere Schranke
von {an : n â N}. Damit muss aber ein n0 â N existieren, so dass an0 > a â Îľ
ist und somit haben wir unsere Folge umzingelt, denn es gilt nun wegen der
Monotonie und der BeschraĚnktheit von (an ) fuĚr alle n âĽ n0 :
a â Îľ < an0 â¤ an â¤ a < a + Îľ
und hiermit |an â a| < Îľ fuĚr alle n âĽ n0 . Da Îľ > 0 beliebig war, sind wir mit
der ungeklammerten Aussage fertig. Die Aussage fuĚr monoton fallende Folgen
beweist man analog.
Wir betrachten ein Beispiel fuĚr die Anwendung dieses Satzes.
Beispiel 4.3.16. Wir betrachten eine rekursiv definierte Folge, die gegeben ist
durch
â
â
3
a0 := 6 und an+1 := 3 6 + an , n â N.

Also ist

q
â
3
3
a1 = 6 + 6,

a2 =

r
3

6+

q
3

6+

â
3

s
3

6,

a3 =

6+

r
3

q
â
3
3
6 + 6 + 6, . . . .

So abstrus dieses Beispiel auch aussieht, in dieser Weise gegebene Folgen treten
sehr haĚufig auf, so liefert z.B. jedes iterative NaĚherungsverfahren eine solche Folge.
Wie untersuchen wir aber ein solches Monstrum auf Konvergenz? Wir wenden
unser Monotoniekriterium an, zeigen also, dass (an ) nach oben beschraĚnkt und
monoton wachsend ist. Genauer gesagt beweisen wir

138

4.3. Konvergenz von Folgen
1. an < 2 und an+1 > an fuĚr alle n â N,
2. (an ) konvergiert und limnââ an = 2.
Das erste ist eine InduktionsuĚbung:
â
â
â
â
Induktionsanfang: Es gilt a0 = 3 6 < 3 8 = 2 und a1 = 3 6 + a0 > 3 6 = a0 , da
a0 âĽ 0 ist. Also ist die Aussage fuĚr n = 0 richtig.
Induktionsvoraussetzung: FuĚr ein n â N gelte an < 2 und an+1 > an .
Induktionsschritt: Es ist mit Hilfe der Induktionsvoraussetzung
â
â
â
3
an+1 = 3 6 + an < 3 6 + 2 = 8 = 2
und
an+2 =

p
3

6 + an+1 >

â
3

6 + an = an+1 .

Wir wenden uns also der Konvergenz in 2. zu.
Nach Satz 4.3.15 wissen wir nun, dass (an ) konvergiert, und dass limnââ an =
supnâN an â¤ 2 ist, denn 2 ist eine obere Schranke der Folge. AuĂerdem wissen wir,
dass a3n+1 = 6 + an fuĚr alle n â N gilt. Nach den Rechenregeln fuĚr Grenzwertbildung aus Satz 4.3.7 konvergieren bei dieser Gleichung die Folgen auf beiden Seiten
des Gleichheitszeichens. Gehen wir also in dieser Gleichung zum Limes uĚber, so
erhalten wir fuĚr a := limnââ an die Beziehung a3 = 6 + a, bzw. a3 â a â 6 = 0.
Eine LoĚsung dieser Gleichung ist a = 2. Dividieren wir diese ab, so erhalten wir
(a â 2)(a2 + 2a + 3) = 0 und a2 + 2a + 3 = (a + 1)2 + 2 = 0 hat keine weiteren
reellen LoĚsungen. Also muss a = 2 sein und wir haben limnââ an = 2.
Noch ein Kommentar zum Verfahren. Obwohl es nicht immer zum Ziel fuĚhrt, ist
dieses doch ein starkes Hilfsmittel zur Behandlung rekursiver Folgen, das man
immer wieder mit Gewinn verwenden kann.
UĚbungsaufgabe 4.3.17. (Babylonisches Wurzelziehen). Zeigen Sie, dass fuĚr jedes x â R und jeden beliebigen Startwert a0 > 0 die Folge (an ) mit an+1 =
â
an +x/an
, n â N, konvergiert mit Grenzwert x.
2

Definition 4.3.18. Eine Folge (an ) in K heiĂt Cauchy-Folge, wenn fuĚr jedes
Îľ > 0 ein Index n0 â N existiert, so dass
|an â am | < Îľ

fuĚr alle n, m âĽ n0

gilt.
Satz 4.3.19. Jede konvergente Folge in K ist eine Cauchy-Folge.
Beweis. Sei (an ) eine konvergente Folge in K mit Grenzwert a und sei Îľ > 0.
Dann gibt es ein n0 â N, so dass |an â a| < Îľ/2 fuĚr alle n âĽ n0 . Also ist fuĚr alle
n, m âĽ n0 mit der Dreiecksungleichung
Îľ Îľ
|an â am | = |an â a + a â am | â¤ |an â a| + |am â a| < + = Îľ.
2 2

139

4. Analysis â Teil I: Konvergenz und Stetigkeit
TatsaĚchlich gilt fuĚr reelle und komplexe Folgen auch die Umkehrung, die wir hier
nicht beweisen wollen.
Satz 4.3.20 (Cauchy-Kriterium). Eine reelle oder komplexe Folge konvergiert
genau dann, wenn sie eine Cauchy-Folge ist. (FuĚr Folgen in Q ist das falsch!)
Bemerkung 4.3.21. (a) Bemerkenswert an den beiden behandelten Konvergenzkriterien ist, dass sie beide eine Konvergenzaussage liefern, ohne dass
man eine a-priori Vermutung uĚber den Grenzwert braucht.
(b) WaĚhrend die Aussage, dass jede konvergente Folge eine Cauchy-Folge ist,
allgemeinguĚltig ist, geht in den Beweis der umgekehrten Implikation massiv das VollstaĚndigkeitsaxiom ein. Dass die Implikation ohne dieses Axiom falsch wird, kann man sich klarmachen, wenn man sich die Folge aus
UĚbungsaufgabe 4.3.17 mit a = 2 als Folge in Q anschaut.
Da sie in R konvergiert, ist sie dort (und dann auch in Q) eine CauchyFolge, aber in Q ist sie nicht konvergent, denn ein etwaiger Grenzwert in
Q waĚre dann auch
â ein Grenzwert in R und wegen der Eindeutigkeit des
Grenzwertes also 2, was nun mal nicht in Q liegt.

4.3.3. Teilfolgen und HaĚufungswerte
Definition 4.3.22. Es sei (an ) eine Folge in K. Ein a â K heiĂt HaĚufungswert
der Folge, falls fuĚr jedes Îľ > 0 die Menge {n â N : |an â a| < Îľ} unendlich viele
Elemente hat.
Offensichtlich ist der Grenzwert einer konvergenten Folge ein HaĚufungswert derselben, denn fuĚr jedes Îľ > 0 liegen ja dann sogar alle bis auf die ersten paar
Folgeglieder naĚher als Îľ am Grenzwert. Aber es gibt auch divergente Folgen die
HaĚufungswerte haben, z.B. hat ((â1)n )nâN die HaĚufungswerte 1 und â1 und die
komplexe Folge (in )nâN hat deren vier, naĚmlich i, â1, âi und 1. Es gibt sogar reelle (bzw. komplexe) Folgen, die jede reelle (bzw. komplexe) Zahl als HaĚufungswert
haben.
Definition 4.3.23. Es sei (an ) eine Folge in K. Ist {n1 , n2 , n3 , . . . } â N eine
unendliche Menge von Indizes mit n1 < n2 < n3 < . . . , so heiĂt die Folge (ank )kâN
eine Teilfolge von (an ).
Beispielsweise ist (a0 , a2 , a4 , a6 , . . . ) die Teilfolge der Folgeglieder mit geradem
Index. Eine andere Teilfolge ist (a0 , a1 , a4 , a9 , a16 , . . . ). Keine Teilfolgen waĚren
(a0 , a0 , a2 , a2 , a4 , a4 , . . . ) oder (a2 , a1 , a4 , a3 , a6 , a5 , . . . ).
Den engen Zusammenhang zwischen Teilfolgen, HaĚufungswerten und Konvergenz
beschreibt der folgende Satz.
Satz 4.3.24. Es sei (an ) eine Folge in K. Dann gilt

140

4.4. Asymptotik
(a) Ein Îą â K ist genau dann ein HaĚufungswert von (an ), wenn eine Teilfolge
(ank ) von (an ) existiert, die gegen Îą konvergiert.
(b) Ist (an ) konvergent mit Grenzwert a, so konvergiert auch jede Teilfolge von
(an ) gegen a.
(c) Ist (an ) konvergent, so hat (an ) genau einen HaĚufungswert, naĚmlich den
Grenzwert limnââ an .
Beweis. Wir behandeln hier nur den Beweis von (a), die Teile (b) und (c) verbleiben als UĚbungsaufgabe.
ââ Es sei Îą â K ein HaĚufungswert von (an ). Dann existiert insbesondere fuĚr
â
Îľ = 1 ein n1 â N mit |an1 â Îą| < 1. Da es auch fuĚr Îľ = 1/2 unendlich viele
Folgenglieder von (an ) gibt, die weniger als 1/2 von Îą entfernt sind, muss
es auch ein n2 â N mit n2 > n1 geben, so dass |an2 â Îą| < 1/2 gilt. Genauso
finden wir ein n3 â N mit n3 > n2 , so dass |an3 â Îą| < 1/3 gilt.
Verfahren wir immer weiter so, erhalten wir schlieĂlich eine Folge von Indizes n1 , n2 , n3 , . . . mit n1 < n2 < n3 < . . . , so dass
|ank â Îą| â¤

1
k

fuĚr alle k â N

(4.1)

gilt. Nun ist (ank ) eine Teilfolge von (an ), und diese konvergiert nach UĚbungsaufgabe 4.3.3 gegen Îą.
ââ Sei nun (ank ) eine Teilfolge von (an ), die gegen ein Îą â K konvergiert. Zu
â
gegebenem Îľ > 0 existiert dann ein k0 â N, so dass |ank â Îą| < Îľ fuĚr alle
k âĽ k0 gilt. Insbesondere gilt damit |an â Îą| â¤ Îľ fuĚr die unendlich vielen
Indizes nk0 , nk0 +1 , nk0 +2 , . . . , d.h. Îą ist ein HaĚufungswert von (an ).
UĚbungsaufgabe 4.3.25. ErklaĚren Sie jemandem aus ihrem Semester, warum
die Umkehrung der Aussage in Teil (c) von Satz 4.3.24 im Allgemeinen falsch ist.

4.4. Asymptotik
Ein Thema, bei dem Folgen in der Informatik prominent auftauchen, ist die
Laufzeit- bzw. AufwandsabschaĚtzung von Algorithmen. Dabei ist an die Laufzeit
(der Aufwand) des Algorithmusâ, wenn der verarbeitete Datensatz den Umfang
n â N hat.
Bei genauerer Betrachtung kommt es nicht auf den genauen Wert von an an,
sondern nur auf das qualitative Verhalten, d.h. wie schnell an mit wachsendem
n groĂ wird. Ist also z.B. die Laufzeit bei der Bearbeitung von n Eingabedaten
an = 3n4 + 2 Âľs , so ist die +2â reichlich egal, und auch das 3Âˇâ interessiert nur
â
â

141

4. Analysis â Teil I: Konvergenz und Stetigkeit
am Rande, die wichtige Information ist, dass der Aufwand in der vierten Potenz
waĚchst.
Wir wollen dieses ungefaĚhrâ rechnen nun exaktâ formalisieren. Das ist kein
â
â
Widerspruch, sondern diese Idee fuĚhrt zum sehr leistungsfaĚhigen O-KalkuĚlâ,
â
dem Sie in Ihrem Studium noch haĚufig begegnen werden.
Definition 4.4.1.

(a) Wir bezeichnen mit

F+ := (an ) Folge in R : an > 0 fuĚr alle n â N .

(b) Es sei (bn ) â F+ . Dann definieren wir die Landau-Symbole durch
n
a 
o
n
O(bn ) := (an ) â F+ :
beschraĚnkt ,
(GroĂ-O von bn )
bn nâN
n
o
an
o(bn ) := (an ) â F+ : lim
=0 ,
(Klein-O von bn ).
nââ bn
Andere uĚbliche Schreibweisen fuĚr (an ) â O(bn ), gesprochen (an ) ist ein
â
GroĂ-/Klein-O von bnâ, sind an â O(bn ) und an = O(bn ), bzw. an â o(bn )
und an = o(bn ).
Bemerkung 4.4.2. (a) Zur oben angefuĚhrten, sehr oft verwendeten, Schreibweise an = O(bn ), bzw. an = o(bn ) ist eine deutliche Warnung angebracht,
denn das =â-Zeichen wird hier nicht im mathematisch uĚblichen Sinne geâ
braucht. Z.B. ist n = O(n) und 3n + 2 = O(n), aber n 6= 3n + 2.
Im Folgenden wird die Kompromiss-Notation an â O(bn ) verwendet werden.

(b) Es gilt immer o(bn ) â O(bn ), denn jede Nullfolge ist nach Satz 4.3.5 auch
beschraĚnkt.
(c) Aus dem gleichen Grund gilt das folgende wichtige Kriterium:
a 
n
konvergent â an â O(bn ).
bn nâN
(d) Anschaulich bedeutet an â O(bn ), dass die Folge (an ) hoĚchstens so schnell
waĚchst wie ein Vielfaches von (bn ), vgl. die folgende UĚbungsaufgabe.
UĚbungsaufgabe 4.4.3. Zeigen Sie fuĚr (an ), (bn ) â F+ die folgenden Aussagen:
(a) an â O(bn ) genau dann, wenn es ein C > 0 und ein n0 â N gibt mit
an â¤ Cbn fuĚr alle n âĽ n0 .
(b) an â o(bn ) genau dann, wenn es fuĚr jedes C > 0 ein n0 â N gibt mit
an â¤ Cbn fuĚr alle n âĽ n0 .

142

4.4. Asymptotik
Beispiel 4.4.4. Es seien (an ) = (2n2 + 3n + 1)nâN , (bn ) = (n2 )nâN und (cn ) =
(n7 )nâN . Dann gilt an â O(bn ) und an â o(cn ), denn
2 + n3 +
an
2n2 + 3n + 1
= lim
=
lim
nââ bn
nââ
nââ
n2
1
lim

1
n2

=2

und
2n2 + 3n + 1
an
=
=
lim
nââ cn
n7

2
n5

+

3
n6

+

1
n7

1

= 0.

Allgemein gilt fuĚr jedes Polynom p(n) = a0 + a1 n + Âˇ Âˇ Âˇ + ak nk vom Grad k, dass
p(n) â O(nk ) ist.
Satz 4.4.5. Es seien (an ), (bn ), (cn ), (dn ) â F+ und Îą, Î˛ â R+ . Dann gilt
(a) Sind an , bn â O(cn ), so ist auch Îąan + Î˛bn â O(cn ).
(b) Gilt an â O(bn ) und cn â O(dn ), so ist an cn â O(bn dn ).
(c) Aus an â O(bn ) und bn â O(cn ) folgt an â O(cn ).
(d) an â O(bn ) genau dann, wenn

(TransitivitaĚt)

1
1
âO
.
bn
an

Weiterhin gelten alle diese Aussagen auch mit Klein-O anstelle von GroĂ-O.
Beweis. Wir fuĚhren den Beweis fuĚr groĂe Os, die kleinen verbleiben als UĚbung.
(a) Nach Voraussetzung sind die Folgen (an /cn ) und (bn /cn ) beschraĚnkt, also
gibt es Konstanten C1 , C2 > 0 mit an /cn â¤ C1 und bn /cn â¤ C2 fuĚr alle
n â N. Dann ist auch
Îąan + Î˛bn
an
bn
= Îą + Î˛ â¤ ÎąC1 + Î˛C2 ,
cn
cn
cn
d.h. die Folge ((Îąan + Î˛bn )/cn ) ist beschraĚnkt, woraus Îąan + Î˛bn â O(cn )
folgt.
(b) Nach Voraussetzung existieren wieder C1 , C2 > 0 mit an /bn â¤ C1 und
cn /dn â¤ C2 fuĚr alle n â N. Also ist
an cn
an cn
=
Âˇ
â¤ C1 C2
bn dn
bn dn
fuĚr alle n â N und damit an cn â O(bn dn ).

143

4. Analysis â Teil I: Konvergenz und Stetigkeit
(c) Gilt an /bn â¤ C1 und bn /cn â¤ C2 fuĚr alle n â N, so haben wir fuĚr all diese
n auch
an bn
an
=
Âˇ
â¤ C1 C2 ,
cn
bn cn
woraus die Behauptung folgt.
(d) FuĚr die Richtung ââ sei an â O(bn ). Dann gibt es ein C > 0, so dass
â
an /bn â¤ C fuĚr alle n â N gilt. Damit ist dann ebenfalls fuĚr alle n â N
1/bn
an
=
â¤ C,
1/an
bn

also haben wir 1/bn â O(1/an ).

FuĚr die umgekehrte Beweisrichtung wendet man obiges Argument nochmals
auf 1/bn an.
Bemerkung 4.4.6. Am haĚufigsten findet man die folgenden Landau-Symbole.
Die entsprechende KomplexitaĚt eines Algorithmus wird auch mit passenden Namen versehen:
Landau-Symbol Bezeichnung Bemerkung
O(1)
beschraĚnkt
O(loga (n))
logarithmisch
a>1
O(n)
linear
O(n loga (n))
n log nâ
a>1
â
2
O(n )
quadratisch
3
O(n )
kubisch
O(nk )
polynomial
k â Nâ
O(an )
exponentiell
a>1
Die Darstellung in obiger Tabelle ist nach GroĚĂe der Menge sortiert. Es gilt
also O(1) â O(loga (n)) â O(n) â O(n loga (n)) â O(n2 ) â O(n3 ) â O(nk ) â
O(nâ ) â O(an ) fuĚr 3 â¤ k â¤ â.

Beispiel 4.4.7.

(a) Was ist die kleinste Klasse aus Bemerkung 4.4.6 in der

an = 5n â 3 ln(n) + 9n2 + 3n ln(n) + n3 + 0.1 Âˇ 2n
liegt. Nach Satz 4.4.5 (a) ist die Summe von mehreren Termen immer in
der groĚĂten der beteiligten O-Mengen enthalten. Das groĚĂte Wachstum hat
nach der Aufstellung in dieser Bemerkung hier der Term 0.1 Âˇ 2n , also ist
an â O(2n ).
(b) Exponentielle Algorithmen sind viel schlechter als polynomiale. FuĚr die
Laufzeit in Mikrosekunden gilt
an â
O(n2 )
O(2n )

144

n = 10
n = 50
n = 100
100
2 500
10 000
16
1 024 36 Jahre 4 Âˇ 10 Jahre

4.5. Reihen

4.5. Reihen
In diesem Abschnitt steht der Buchstabe K fuĚr R oder C. Den Fall K = Q
werden wir hier nicht behandeln, einfach weil zu vieles des hier gesagten auf das
Cauchy-Kriterium und damit auf das VollstaĚndigkeitsaxiom aufbaut.
Definition 4.5.1. Es sei (an ) eine Folge in K. Dann heiĂt
â
X

an = a0 + a1 + a2 + a3 + . . .

n=0

P
die Reihe uĚber (an ). FuĚr jedes k â N heiĂt dann sk := kn=0 an die k-te Teilsumme oder Partialsumme der Reihe.
Ist die Folge (sk )kâN konvergent, so nennen wir die Reihe konvergent mit dem
Reihenwert
â
k
X
X
an := lim sk = lim
an ,
n=0

kââ

kââ

n=0

ist (sk ) divergent, so nennen wir auch die Reihe divergent.

Beispiel 4.5.2. (a) In Beispiel 4.3.12 (b) haben wir schon eine P
Reihe betrachn
tet ohne sie so zu nennen, naĚmlich die geometrische Reihe â
n=0 q . Diese
ist nach dem dortigen Resultat konvergent, wenn q â C mit |q| < 1 ist und
dann gilt
â
X
1
qn =
,
|q| < 1.
1
â
q
n=0
(b) Wir betrachten die Reihe
â
X
n=1

1 1
1
1
1
1
= + +
+
+
+ ....
n(n + 1)
2 6 12 20 30

Es gilt fuĚr jedes k â Nâ
k
X
n=1

X n + 1 â n X 1
1  X1 X 1
1
=
=
=
â
â
n(n + 1) n=1 n(n + 1)
n n+1
n n=1 n + 1
n=1
n=1
k

k

k

k

k
k
X
1 X1
1
1
=1+
â
â
=1â
.
n n=2 n k + 1
k+1
n=2

Also ist
â
X
n=1

k

X
1 
1
1
= 1.
= lim
= lim 1 â
n(n + 1) kââ n=1 n(n + 1) kââ
k+1

145

4. Analysis â Teil I: Konvergenz und Stetigkeit
(c) Die Reihe
â
X
1
n
n=1

heiĂt harmonische Reihe. FuĚr die 2k-te Partialsumme s2k =
s2k

P2k

n=1 1/n

gilt

k
X
1
1
1
1
1
1
=
+
+ ... +
+
âĽ sk + k Âˇ
= sk + .
n |k {z
+ 1} |k {z
+ 2} |{z} |{z}
2k
2k
2
n=1
âĽ1/(2k)

âĽ1/(2k)

...

âĽ1/(2k)

Nehmen wir nun an, dass die Reihe konvergent ist, so ist nach Definition der
Reihenkonvergenz die Folge (sk )kâĽ1 konvergent. Den Grenzwert nennen wir
s. Nach Satz 4.3.24 (b) konvergiert dann auch die Teilfolge (s2k )kâĽ1 gegen
s und wir bekommen nach Satz 4.3.7 (c) die Ungleichung
s = lim s2k
kââ

1
1
=s+
âĽ lim sk +
kââ
2
2

und damit einen sauberen Widerspruch. Die harmonische Reihe ist also
divergent.
Der folgende Satz ergibt sich direkt aus den GrenzwertsaĚtzen fuĚr Folgen.
P
Pâ
Satz 4.5.3. Seien P â
n=0 an und
n=0 bn zwei konvergente Reihen in K und Îą, Î˛ â
â
K. Dann ist auch n=0 (Îąan + Î˛bn ) kovergent und es gilt
â
X

(Îąan + Î˛bn ) = Îą

n=0

â
X
n=0

an + Î˛

â
X

bn .

n=0

Im Allgemeinen ist das konkrete Ausrechnen eines Reihenwertes ein sehr schwieriges Unterfangen. Deshalb sind die wenigen Reihen, bei denen man den Wert exakt
angeben kann, wertvolle SchaĚtze. Einen besonders wichtigen solchen wollen wir
nun noch ohne Beweis heben:
â
X
1
Satz 4.5.4. Es gilt
= e.
n!
n=0

Da es fuĚr kompliziertere Reihen schon schwer genug ist, uĚberhaupt herauszubekommen, ob sie konvergieren oder nicht (vom Berechnen des Reihenwertes reden
wir schon gar nicht), sind Konvergenzkriterien essentiell wichtig. Wir beginnen
mit einem notwendigen Kriterium
P
Satz 4.5.5. Ist â
n=0 an eine konvergente Reihe in K, so ist (an ) eine Nullfolge
in K.
146

4.5. Reihen
Beweis. Da die Reihe konvergiert, gilt limkââ
P
â
â
n=0 an =: s. Da auĂerdem fuĚr jedes k â N
ak =

k
X
n=0

gilt, ist limkââ ak = s â s = 0.

an â

kâ1
X

Pk

n=0

an = limkââ

Pkâ1

n=0 an

=

an

n=0

Hier sind noch zwei Kriterien, die sich direkt aus den entsprechenden Aussagen
fuĚr Folgen ergeben:
P
Satz 4.5.6. Es sei (an ) eine Folge in K und sk := kn=0 an , k â N. Dann gilt
P
(a) Ist an âĽ 0 fuĚr alle n â N und (sk )kâN nach oben beschraĚnkt, so ist â
n=0 an
konvergent. (Monotonie-Kriterium)
P
(b) Die Reihe â
n=0 an ist genau dann konvergent, wenn fuĚr jedes Îľ > 0 ein
n0 â N existiert mit
k
X

n=â+1

an < Îľ fuĚr alle k, â â N mit k > â âĽ n0 .

(Cauchy-Kriterium)

Beweis. (a) Die Konvergenz der Reihe ist nach Definition gleichbedeutend mit
der Konvergenz der Folge (sk )kâN . Da alle an âĽ 0 sind, gilt fuĚr diese
sk+1 =

k+1
X

an =

n=0

k
X
n=0

an + ak+1 âĽ

k
X

an = sk

n=0

fuĚr alle k â N. Also ist (sk )kâN monoton wachsend und nach Voraussetzung
nach oben beschraĚnkt. Die Konvergenz folgt damit aus Satz 4.3.15.
(b) Wir zeigen, dass (sk )kâN eine Cauchyfolge ist, dann folgt die Konvergenz
aus dem Cauchy-Kriterium, Satz 4.3.20. Sei dazu Îľ > 0. Dann gibt es nach
Voraussetzung ein n0 â N, so dass fuĚr alle k, â âĽ n0 mit k > â gilt
|sk â sâ | =

k
X
n=0

an â

â
X
n=0

an =

k
X

an < Îľ.

n=â+1

Das folgende Konvergenzkriterium behandelt reelle Folgen, deren Summanden
wechselnde Vorzeichen haben.
Satz 4.5.7 (Leibniz-Kriterium). Es seiP
(an ) eine monoton fallende Folge in R
n
mit limnââ an = 0. Dann ist die Reihe â
n=0 (â1) an konvergent.
Beispiel 4.5.8. Das Leibniz-Kriterium liefert z.B. sehr schnell die Konvergenz
der alternierenden harmonischen Reihe
â
X
1
(â1)n
.
n+1
n=0

Der Reihenwert (ln(2)) ist dagegen deutlich schwerer zu bestimmen.

147

4. Analysis â Teil I: Konvergenz und Stetigkeit

4.5.1. Absolute Konvergenz
In diesem Abschnitt geht es vor allem darum, weitere, alltagstauglichere Konvergenzkriterien fuĚr Reihen anzugeben. Dazu benoĚtigen wir zunaĚchst einen weiteren
Begriff, der eine VerschaĚrfung der Konvergenz bedeutet.
Pâ
Definition
Pâ 4.5.9. Eine Reihe n=0 an in K heiĂt absolut konvergent, wenn die
Reihe n=0 |an | in K konvergiert.
P
Satz 4.5.10. Jede absolut konvergente Reihe â
n=0 an in K ist auch konvergent
in K und es gilt die verallgemeinerte Dreiecksungleichung
â
X
n=0

an â¤

â
X
n=0

|an |.

Beweis. Wir verwenden das Cauchy-Kriterium. Sei dazu Îľ > 0 gegeben. Dann
gibt es dank der absoluten Konvergenz ein n0 â N, so dass
k
X

n=â+1

|an | =

k
X

n=â+1

|an | < Îľ

fuĚr alle k > â âĽ n0 gilt. Mit der Dreiecksungleichung gilt dann sofort auch
k
X

n=â+1

an = |aâ+1 + aâ+2 + Âˇ Âˇ Âˇ + ak | â¤ |aâ+1 | + |aâ+2 | + Âˇ Âˇ Âˇ + |ak | =

k
X

n=â+1

|an | < Îľ.

Also ist die Reihe nach dem Cauchy-Kriterium konvergent.
Mit demselben Dreicksungleichungs-Argument erhalten wir nun
â
X
n=0

an = lim

kââ

k
X
n=0

an = lim

kââ

k
X
n=0

an â¤ lim

kââ

k
X
n=0

|an | =

â
X
n=0

|an |.

Bemerkung 4.5.11. (a) Ein Beispiel dafuĚr, dass die Umkehrung dieses Satzes
nicht gilt, ist die alternierende harmonische Reihe aus Beispiel 4.5.8. Diese
ist nach dem dortigen Ergebnis konvergent, aber nicht absolut konvergent,
denn die zugehoĚrige Reihe mit BetraĚgen ist die harmonische Reihe und diese
divergiert, vgl. Beispiel 4.5.2 (c).
(b) Absolut konvergente Reihen haben gegenuĚber konvergenten einen groĂen
Vorteil: Ist eine Reihe nur konvergent, aber nicht absolut konvergent, so
kann man durch bloĂes Umsortieren der Summanden den Reihenwert veraĚndern. Dieser Effekt tritt bei absolut konvergenten Reihen nicht auf. Deren
Reihenwert ist von der Summationsreihenfolge unabhaĚngig.

148

4.5. Reihen
Eine haĚufige Methode zur Konvergenzuntersuchung ist der Vergleich der zu untersuchenden Reihe mit einer bekannten Reihe. Der folgende Satz ist mit Hilfe
des Cauchy-Kriteriums schnell zu beweisen, er ist aber so intuitiv, dass wir auf
den Beweis hier auch verzichten koĚnnen.
Satz 4.5.12. Es seien (an ) und (bn ) reelle Folgen.
(a) Ist
Pâ|an | â¤ bn fuĚr alle n â N und konvergiert die Reihe
(Majorantenkriterium)
n=0 an absolut konvergent.

Pâ

(b) Ist an âĽ bn âĽ 0 fuĚr allePn â N und divergiert die Reihe
(Minorantenkriterium)
divergiert auch die Reihe â
n=0 an .

n=0 bn ,

Pâ

so ist

n=0 bn ,

so

Bemerkung 4.5.13. (a) Die Vergleichsfolge (bn ) im obigen Satz heiĂt im Fall
von Teil (a) konvergente Majorante und im Teil (b) divergente Minorante.
(b) Mit Hilfe der O-Notation kann man den Satz auch folgendermaĂen formulieren:
Es sei (an ) eine Folge in R und (bn ) â F+ .
Pâ
P
â˘ Ist â
n=0 an absolut, falls
n=0 bn konvergent, so konvergiert die Reihe
|an | â O(bn ) gilt.
Pâ
Pâ
â˘ Ist
n=0 bn divergent, so divergiert auch die Reihe
n=0 an , falls
(an ) â F+ gilt und bn â O(an ) gilt.
Beispiel 4.5.14. (a) Da die harmonische Reihe divergiert, divergieren nach
dem Minorantenkriterium die Reihen uĚber alle langsamer fallenden Folgen.
Insbesondere ist
â
X
1
nÎą
n=1
fuĚr alle Îą < 1 divergent, denn dann gilt

1
1
âO Îą .
n
n
(b) Die Reihe
â
X
1
n2
n=1

ist dagegen absolut konvergent. Dazu beobachten wir zunaĚchst, dass
â
â
X
X
1
1
=
2
n
(n + 1)2
n=1
n=0

149

4. Analysis â Teil I: Konvergenz und Stetigkeit
ist. Weiter gilt fuĚr alle n â Nâ die AbschaĚtzung (n + 1)2 âĽ n(n + 1) und
damit
1
1
â¤
=: bn .
(n + 1)2
n(n + 1)
Die Reihe uĚber bn ist nach Beispiel 4.5.2 (b) konvergent, kann uns also als
konvergente Majorante dienen und wir erhalten die behauptete Konvergenz
aus dem Majorantenkriterium.
P
1
Bemerkung 4.5.15. TatsaĚchlich ist die Reihe â
n=1 nÎą genau dann konvergent,
wenn Îą > 1 ist. Die harmonische Reihe ist also genau der Grenzfall.
Wir kommen nun zu zwei weiteren haĚufig verwendeten Konvergenzkriterien.
P
Satz 4.5.16. Es sei â
n=0 an eine Reihe in K.
p
n
|an | oder ist die Folge
(a) (Wurzelkriterium)
Existiert
der
Grenzwert
lim
nââ
p
( n |an |) unbeschraĚnkt, so ist die Reihe
p
â˘ absolut konvergent, wenn limnââ n |an | < 1 ist und
p
â˘ divergent, wenn limnââ n |an | > 1 ist oder die Folge unbeschraĚnkt ist.
(b) (Quotientenkriterium) Ist an 6= 0 fuĚr alle n â N und existiert der Grenzwert limnââ |an+1 /an | oder ist die Folge (|an+1 /an |) unbeschraĚnkt, so ist
die Reihe
|
< 1 ist und
â˘ absolut konvergent, wenn limnââ |a|an+1
n|

â˘ divergent, wenn limnââ

|an+1 |
|an |

> 1 ist oder die Folge unbeschraĚnkt ist.

Beweis. Wir beweisen nur exemplarisch
das Wurzelkriterium. Es sei also (an )
p
n
eine Folge in K, fuĚr die Îą := limnââ |an | existiert.
p
Ist Îą < 1, so waĚhlen wir ein q â (Îą, 1). p
Dank der Konvergenz von ( n |an |) gegen
Îą, muss es ein n0 â N geben, so dass | n |an | â Îą| fuĚr alle n âĽ n0 kleiner ist als
der Abstand von Îą zu q. Insbesondere ist also
p
n
|an | â¤ q < 1
fuĚr alle n âĽ n0 .

Daraus folgt |an | â¤ q n fuĚr alle n âĽ n0 . Das impliziert wiederum, dass die Folge (|an |/q nP
) beschraĚnkt ist und wir haben |an | â O(q n ). Da q â (0, 1) ist, ist
n
die Reihe â
n=0 q nach Beispiel 4.5.2 (a) konvergent. Also liefert uns das Majorantenkriterium, Satz 4.5.12
Pâ(a), vgl. Bemerkung 4.5.13 (b), auch die absolute
Konvergenz unserer Reihe n=0 an .
p
Es sei nun Îą > 1. Dann gibt es ein n0 â N, so dass n |an | > 1 fuĚr alle n âĽ n0
gilt. Damit ist auch |an | > 1 fuĚr alle n P
âĽ n0 und (an ) ist definitiv keine Nullfolge.
Nach Satz 4.5.5 kann dann die Reihe â
n=0 an nicht konvergent sein.
150

4.5. Reihen
Bemerkung 4.5.17. Liefert der Grenzwert in einem der beiden Kriterien genau
Eins, so kann man daraus keine Aussage uĚber Konvergenz oder Divergenz der
Reihe ableiten, vgl. das folgende Beispiel.
P
1
Beispiel 4.5.18. (a) Wie wir gesehen haben, ist die Reihe â
n=1 nÎą je nach
der GroĚĂe von Îą konvergent oder divergent, vgl. Beispiel 4.5.14 und Bemerkung 4.5.15. FuĚr jedes Îą â Q gilt allerdings
lim

nââ

r
n

âÎą
â
1
1
n
â
=
lim
=
lim
n
= 1âÎą = 1.
n
Îą
Îą
nââ
nââ
n
n

Dies zeigt, dass im Falle, dass das Wurzelkriterium als Grenzwert 1 liefert,
keine Aussage uĚber die Konvergenz der Reihe moĚglich ist.
Gleiches gilt fuĚr das Quotientenkriterium.
(b) FuĚr jedes z â C betrachten wir die Reihe
â
X
zn
n=0

n!

.

Im Fall z = 0 ist die Konvergenz der Reihe offensichtlich, da sie dann nur
einen Summanden hat. Sei also nun z 6= 0. Dann gilt mit an := z n /n!
|an+1 |
=
|an |

z n+1
(n+1)!
zn
n!

=

|z|n+1 n!
|z|
=
.
n
|z| (n + 1)!
n+1

Also ist fuĚr jedes z 6= 0
|an+1 |
|z|
= lim
= 0 < 1.
nââ |an |
nââ n + 1
lim

Nach dem Quotientenkriterium ist also die Reihe fuĚr jedes z â C absolut
konvergent.
Man nennt diese uĚberaus wichtige Reihe die Exponentialreihe. Diese definiert uns eine Funktion
(
C âC
E:
P
zn
z 7â E(z) := â
n=0 n! ,

die Exponentialfunktion. Wie es zu diesem Namen kommt, wird in KuĚrze
klar werden.

151

4. Analysis â Teil I: Konvergenz und Stetigkeit

4.5.2. Das Cauchy-Produkt
In Satz 4.5.3 haben wir gesehen, dass die Summe konvergenter Reihen wieder
konvergent ist. Wir wollen uns nun dem Produkt zuwenden. Naiv koĚnnte man
folgendermaĂen rechnen:
â
X
n=0

an

â
X
n=0



bn = (a0 + a1 + a2 + a3 + . . . )(b0 + b1 + b2 + b3 + . . . )
= a0 b0
+ a0 b1 + a1 b0
+ a0 b2 + a1 b1 + a2 b0
+ a0 b3 + a1 b2 + a2 b1 + a3 b0
+ ...
n
â X
X
ak bnâk .
=
n=0 k=0

Diese Rechnung ist allerdings im Allgemeinen falsch! Wir haben hier naĚmlich
die Summanden in einer willkuĚrlichen Reihenfolge aufsummiert und nach Bemerkung 4.5.11 (b) ist der Reihenwert nicht immer von der Summationsreihenfolge
unabhaĚngig. Bei absolut konvergenten Reihen allerdings schon und tatsaĚchlich
gilt der folgende Satz.
P
Pâ
Satz 4.5.19. Es seien â
a
und
zwei absolut konvergente Reihen in
n
n=0
n=0 bnP
P
n
K. Dann konvergiert auch die Reihe â
k=0 ak bnâk absolut und es gilt fuĚr
n=0
die Reihenwerte
â X
n
â
â
X
X

X
ak bnâk =
an
bn .
n=0 k=0

n=0

n=0

Pâ
Pâ Pn
Die Reihe
k=0 ak bnâk heiĂt Cauchy-Produkt der beiden Reihen
n=0 an
n=0
Pâ
und n=0 bn .
Wir wollen diesen Satz hier nicht beweisen sondern einmal anwenden, indem wir
die Funktionalgleichung der Exponentialfunktion zeigen.

Satz 4.5.20. FuĚr alle z, w â C gilt E(z + w) = E(z)E(w).
Beweis. Es gilt mit Hilfe des Cauchy-Produkts, da alle beteiligten Reihen absolut
konvergent sind,
E(z)E(w) =
=

â
â
X
z n X w n 

n=0
â
X
n=0

152

n!

n=0

n!

â X
n
X
z k w nâk
=
k! (n â k)!
n=0 k=0

n
n  
â
X
1 X
n!
1 X n k nâk
k nâk
z w .
z w
=
k
n! k=0 k!(n â k)!
n!
n=0
k=0

4.5. Reihen
Nun gilt nach der Binomialformel 4.2.9 (c)
n  
X
n k nâk
z w
= (z + w)n ,
k
k=0

also ist zusammengefasst

E(z)E(w) =

â
X
(z + w)n

n!

n=0

= E(z + w).

Das ist aber nur der erste Streich. In Satz 4.5.4 haben wir gesehen, dass
â
X
1
=e
E(1) =
n!
n=0
gilt. Mit obigem Resultat ist damit fuĚr jedes k â Nâ

E(k) = E(1| + 1 +{zÂˇ Âˇ Âˇ + 1}) = E(1)k = ek .
k Mal

Weiter sieht man sofort, dass
â
X
0n
= 1+0+0+0+ÂˇÂˇÂˇ = 1
E(0) =
n!
n=0

gilt. Also muss fuĚr jedes k â Nâ gelten

1 = E(0) = E(k + (âk)) = E(k)E(âk).
Das liefert uns, dass E(k) 6= 0 ist und E(âk) = E(k)â1 = (ek )â1 = eâk fuĚr jedes
k â N gilt. Zusammen haben wir nun schon E(k) = ek fuĚr jedes k â Z.
Doch hier hoĚren wir nicht auf. Es sei nun q = k/â â Q mit k â Z und â â Nâ .
ZunaĚchst beobachten wir, dass mit dem gleichen Trick wie oben
 1
1 1
 1 â
1
e = E(1) = E â Âˇ
=E
+ +ÂˇÂˇÂˇ+
=E
â
â}
â
|â â {z
â Mal

ist und damit

1/â

e

1
.
=E
â

Das liefert schlieĂlich
 1
 1 k
E(q) = E k Âˇ
=E
= (e1/â )k = ek/l = eq
fuĚr alle q â Q.
â
â
Es liegt also nahe ex fuĚr alle reellen Zahlen x ebenfalls durch die Exponentialreihe
zu definieren. Wir sind sogar gleich noch mutiger:
Definition 4.5.21. FuĚr alle z â C ist
ez := E(z) =

â
X
zn
n=0

n!

.

153

4. Analysis â Teil I: Konvergenz und Stetigkeit

4.6. Konvergenz in normierten RaĚumen
Folgen und Reihen kann man nicht nur in R oder C, sondern auch in Rd , Cd ,
d â Nâ , oder noch anderen VektorraĚumen betrachten. Allerdings muss man, um
den Begriff der Konvergenz einfuĚhren zu koĚnnen, in irgendeiner Form AbstaĚnde
messen koĚnnen. Das fuĚhrt uns wieder auf den Begriff des normierten Raums
aus Abschnitt 3.4. Wie schon dort betrachten wir hier nur den Fall reeller VektorraĚume. Im gesamten Abschnitt sei also V ein normierter R-Vektorraum mit
Norm k Âˇ kV .
Die Konvergenzdefinition ist genau identisch, wir ersetzen nur BetraĚge durch
Normen:
Definition 4.6.1. (a) Eine Folge (an )nâN in V heiĂt konvergent gegen ein a â
V , wenn fuĚr jedes Îľ > 0 ein n0 â N existiert, so dass
kan â akV < Îľ fuĚr alle n âĽ n0
gilt.
Die Folge heiĂt divergent, wenn sie nicht konvergent ist.
(b) Eine Folge (an )nâN in V heiĂt Cauchy-Folge, wenn es fuĚr jedes Îľ > 0 ein
n0 â N gibt mit
kan â am kV < Îľ

fuĚr alle n, m âĽ n0 .

P
(c) Eine Reihe â
mit Reihenwert s â V , wenn die
n=0 an in V heiĂt konvergent
Pk
Folge der Partialsummen sk := n=0 an , k â N, in V gegen s konvergiert.
P
Pâ
Konvergiert die Reihe â
ka
k
in
R,
so
heiĂt
die
Reihe
n
V
n=0
n=0 an absolut
konvergent.
Ist die Reihe nicht konvergent, so nennt man sie divergent.

Definition 4.6.2. Eine Menge M â V heiĂt beschraĚnkt, falls es ein C âĽ 0 gibt,
so dass kxkV â¤ C fuĚr alle x â M gilt.
Beispiel 4.6.3.

(a) In V = R3 mit der 1-Norm betrachten wir die Folge
ďŁŤ
ďŁś
1
an := ďŁ­ n1 ďŁ¸ ,
n â Nâ .
nâ1
n

Dann gilt limnââ an = (1, 0, 1)T . Das sieht man so: FuĚr jedes n â Nâ gilt
kan â (1, 0, 1)T k1 = |1 â 1| +

154

1
nâ1
2
1
nâ1ân
= .
â0 +
â1 = +
n
n
n
n
n

4.6. Konvergenz in normierten RaĚumen
Ist nun Îľ > 0 vorgegeben, so gibt es ein n0 â N mit n0 > 2/Îľ und fuĚr alle
n âĽ n0 gilt dann
2
2
2
<
= Îľ.
kan â (1, 0, 1)T k1 = â¤
n
n0
2/Îľ
(b) Die Folge an = (n, 1/n), n â Nâ , in R2 mit der 2-Norm ist wegen
â
p
kan k2 = n2 + 1/n2 âĽ n2 = n
fuĚr alle n â Nâ

unbeschraĚnkt. Da auch im normierten Raum jede konvergente Folge beschraĚnkt ist, vgl. UĚbungsaufgabe 4.6.4 ist (an ) damit divergent.

UĚbungsaufgabe 4.6.4. UĚbertragen Sie die Aussagen in UĚbungsaufgabe 4.3.3,
Satz 4.3.5, Satz 4.3.7 (a) bis (b)ii), Satz 4.3.19, Satz 4.5.3 und Satz 4.5.5 in den
Kontext von metrischen RaĚumen und beweisen Sie sie.

Satz 4.6.5. Es sei (an )nâN = (an,1 , an,2 , . . . , an,d)T nâN eine Folge in Rd mit der
p
2-Norm kxk2 = x21 + x22 + Âˇ Âˇ Âˇ + x2d . Dann ist (an ) in Rd genau dann konvergent,
wenn fuĚr jedes j â {1, 2, . . . , d} die Koordinatenfolge (an,j )nâN in R konvergent
ist. In diesem Fall ist
ďŁŤ
ďŁś ďŁŤ
ďŁś
an,1
limnââ an,1
ďŁŹan,2 ďŁˇ ďŁŹlimnââ an,2 ďŁˇ
ďŁŹ
ďŁˇ ďŁŹ
ďŁˇ
lim ďŁŹ .. ďŁˇ = ďŁŹ
ďŁˇ.
..
nââ ďŁ­ . ďŁ¸
ďŁ­
ďŁ¸
.
an,d
limnââ an,d
Beweis. ââ Es sei (an ) konvergent in Rd mit Grenzwert a = (a1 , a2 , . . . , an )T â
â
Rd . Dann gilt fuĚr jedes j â {1, 2, . . . , d} und alle n â N
v
u d
q
uX
|an,j â aj | = (an,j â aj )2 â¤ t (an,k â ak )2 = kan â ak2 .
k=1

Letzteres ist dank der Konvergenz von (an ) eine Nullfolge in R. Also konvergiert (an,j ) in R gegen aj .

ââ Es seien nun fuĚr jedes j â {1, 2, . . . , d} die Koordinatenfolgen (an,j )jâN
â
konvergent in R mit jeweiligem Grenzwert aj â R. Dann gilt fuĚr jedes
solche j auch limnââ |an,j â aj | = 0 nach UĚbungsaufgabe 4.3.3 (b). Nach
P
den GrenzwertsaĚtzen ist dann dj=1(an,j â aj )2 ebenfalls konvergent mit
P
Grenzwert dj=1 02 = 0. SchlieĂlich folgern wir aus Bemerkung 4.3.10 (a)
v
u d
uX
lim kan â ak2 = lim t (an,j â aj )2 = 0.
nââ

nââ

j=1

Nach UĚbungsaufgabe 4.6.4 folgt daraus die Konvergenz von (an ) in Rd gegen
a = (a1 , a2 , . . . , an )T .

155

4. Analysis â Teil I: Konvergenz und Stetigkeit
Beispiel 4.6.6. Die Folge in R3 mit
ďŁŤ

2n2 ân
4n2 â3n+5

ďŁś

an = ďŁ­(1 + 1/n)n ďŁ¸
â
n
n

ist mit Hilfe dieses Satzes sehr schnell auf Konvergenz untersucht. Wir muĚssen
uns nur die Koordinatenfolgen anschauen. Es ist
1
2n2 â n
= ,
2
nââ 4n â 3n + 5
2
lim


1 n
lim 1 +
= e und
nââ
n

lim

nââ

â
n

n = 1,

also ist (an ) konvergent in R3 (mit der 2-Norm) und der Grenzwert ist (1/2, e, 1)T .
Bemerkung 4.6.7. Ein identischer Satz gilt fuĚr jede moĚgliche Norm auf Rd . Wir
werden spaĚter sehen, dass in endlichdimensionalen R-VektorraĚumen die Wahl der
Norm fuĚr die Frage der Konvergenz keine Rolle spielt: Wenn eine Folge bezuĚglich
einer Norm konvergiert, dann auch bezuĚglich jeder anderen und die Grenzwerte
sind gleich.
In R haben wir offene und abgeschlossene Intervalle betrachtet. Mengen, zu denen
ihr Rand gar nicht oder komplett dazugehoĚrt, spielen auch in normierten RaĚumen
eine wichtige Rolle. Wir wollen nun die entsprechenden Begriffe definieren.
Definition 4.6.8.

(a) Es seien x0 â V und r â (0, â). Dann heiĂt die Menge

Br (x0 ) := x â V : kx â x0 kV < r

(offene) Kugel um x0 mit Radius r.

(b) Eine Menge M â V heiĂt offen, falls es fuĚr jeden Punkt x0 â M einen
Radius r > 0 gibt, so dass Br (x0 ) â M gilt.
(c) Eine Menge M â V heiĂt abgeschlossen, wenn die Menge M c = V \ M
offen ist.
Bemerkung 4.6.9. (a) Ist eine Menge abgeschlossen, so bedeutet das anschaulich, dass ihr Rand zur Menge dazugehoĚrt. Umgekehrt ist eine offene Menge
eine, die keinen ihrer Randpunkte enthaĚlt. Die Begriffe Randâ und Randâ
â
punktâ definieren wir hier nicht. Trotzdem sollte dies ein richtiges intuitives
GefuĚhl fuĚr die Begriffe offen und abgeschlossen geben.
(b) Achtung: Mengen sind keine TuĚren! Die meisten Mengen sind weder offen
noch abgeschlossen, betrachten Sie z.B. ein halboffenes Intervall in R. HuĚten
Sie sich also vor dem Fehlschluss: Ich habe festgestellt, dass meine Menge
nicht offen ist, also ist sie abgeschlossen. . .

156

4.6. Konvergenz in normierten RaĚumen
Beispiel 4.6.10. (a) FuĚr jeden Mittelpunkt x0 â V und jeden Radius r > 0
ist die eben definierte Kugel Br (x0 ) eine offene Menge.
Um das einzusehen waĚhlen wir ein x â Br (x0 ). Wir muĚssen nun zeigen, dass
es einen Radius Ěş > 0 gibt, so dass BĚş (x) â Br (x0 ) gilt. Da x â Br (x0 ) ist,
gilt kx â x0 kV < r. Die Zahl
Ěş :=

r â kx â x0 kV
2

ist also strikt groĚĂer als Null.
Sei nun y â BĚş (x). Dann gilt nach der Dreiecksungleichung
ky â x0 kV = ky â x + x â x0 kV â¤ ky â xkV + kx â x0 kV .
Der erste Summand ist kleiner als Ěş = (r â kx â x0 kV )/2, denn y ist ja in
der Kugel um x mit Radius Ěş. Also erhalten wir
ky â x0 kV <

r kx â x0 kV
r kx â x0 kV
â
+ kx â x0 kV = +
.
2
2
2
2

Weiter war kx â x0 kV < r, also finden wir
ky â x0 kV <

r r
+ = r.
2 2

Damit haben wir BĚş (x) â Br (x0 ) gezeigt und sind fertig.
(b) Die Kugel mit Rand {x â V : kx â x0 kV â¤ r} ist dagegen fuĚr jedes x0 â V
und alle r > 0 eine abgeschlossene Menge. Das sieht man am einfachsten
mit Hilfe des folgenden Satzes ein.
Satz 4.6.11. Eine Teilmenge M von V ist genau dann abgeschlossen, wenn fuĚr
jede Folge in M, die in V konvergiert, der Grenzwert ein Element aus M ist.
Beweis. ââ Es sei M â V abgeschlossen und (an ) eine Folge in M, die in
â
V konvergiert. Wir nehmen nun an, es waĚre a := limnââ an 6â M, d.h.
a â M c.
Da M abgeschlossen ist, ist die Menge M c nach Definition offen. Es gibt
also ein r > 0 mit Br (a) â M c . Weiter ist a der Limes der Folge (an ). Also
gibt es ein n0 â N, so dass
kan â akV < r

fuĚr alle n âĽ n0

gilt. Das bedeutet an â Br (a) â M c fuĚr alle n âĽ n0 und wir haben einen
Widerspruch zu der Voraussetzung, dass an â M fuĚr alle n â N gilt.

157

4. Analysis â Teil I: Konvergenz und Stetigkeit
ââ Wir nehmen an, M waĚre nicht abgeschlossen, d.h. M c ist nicht offen. Dann
â
gibt es ein x0 â M c , so dass die Kugeln Br (x0 ) fuĚr kein r > 0 ganz in M c
liegen. Anders gesagt, fuĚr jedes r > 0 gilt Br (x0 ) âŠ M 6= â. Betrachten wir
speziell r = 1/n fuĚr jedes n â Nâ , so erhalten wir fuĚr jedes n â Nâ ein
an â B1/n (x0 ) âŠ M.
Die so konstruierte Folge (an ) ist nun eine Folge in M, fuĚr die
kan â x0 kV â¤

1
n

fuĚr alle n â Nâ

gilt. Damit ist die Folge (an ) konvergent in V mit limnââ an = x0 . Nach
Voraussetzung muss also x0 â M liegen, was ein Widerspruch zu x0 â M c
ist.
Damit koĚnnen wir nun die Behautpung aus Beispiel 4.6.10 (b) fertig begruĚnden.
Es sei also (an ) eine Folge in M := {x â V : kx â x0 kV â¤ r}, die in V konvergiert
und wir setzen a := limnââ an . Da fuĚr jede konvergente Folge (bn ) in V
k lim bn kV = lim kbn kV
nââ

nââ

gilt (vgl. Satz 4.3.7 (a) und UĚbungsafgabe 4.6.4), haben wir
ka â x0 kV = k lim an â x0 kV = k lim (an â x0 )kV = lim kan â x0 kV .
nââ

nââ

nââ

Weiter liegt jedes an in M, also folgt nun aus der Monotonie des Grenzwertes
(Satz 4.3.7 (c))
ka â x0 kV â¤ lim r = r.
nââ

Damit ist auch a â M und M somit nach Satz 4.6.11 abgeschlossen.
UĚbungsaufgabe 4.6.12. Diskutieren Sie, ob â und V offene und/oder abgeschlossene Mengen in V sind.
Definition 4.6.13. Ist V ein endlichdimensionaler normierter R-Vektorraum, so
heiĂt eine Teilmenge M â V kompakt, wenn sie abgeschlossen und beschraĚnkt
ist.
Warnung 4.6.14. Auch in unendlichdimensionalen RaĚumen gibt es den Begriff
einer kompakten Teilmenge (und eigentlich wird er sogar erst dort richtig wichtig).
Dann sieht allerdings die Definition voĚllig anders aus, und es gibt dann Mengen,
die abgeschlossen und beschraĚnkt aber nicht kompakt sind!
Im Moment kann Ihnen das egal sein. Diese Warnung soll nur vorbeugen, dass
Sie gewarnt sind, wenn Sie in Ihrem Leben einmal uĚber unendlichdimensionale
normierte RaĚume stolpern und den Kompaktheitsbegriff brauchen.
Die Definitionen von Teilfolge und HaĚufungswert koĚnnen wir wieder aus der eindimensionalen Situation abschreiben, indem wir BetraĚge durch Normen ersetzen.

158

4.6. Konvergenz in normierten RaĚumen
Definition 4.6.15. Es sei (an ) eine Folge in (V, k Âˇ kV ).
(a) Ein a â V heiĂt HaĚufungswert von (an ), falls fuĚr jedes Îľ > 0 die Menge


n â N : kan â akV < Îľ = n â N : an â BÎľ (a)
unendlich viele Elemente hat.

(b) Ist {n1 , n2 , n3 , . . . } eine unendliche Teilmenge von N mit n1 < n2 < n3 <
. . . , so heiĂt (ank )kâN eine Teilfolge von (an ).
UĚbungsaufgabe 4.6.16. UĚbertragen Sie Satz 4.3.24 mitsamt Beweis in die Situation von normierten RaĚumen.
Satz 4.6.17 (Satz von Bolzano-WeierstraĂ). Ist (V, k Âˇ kV ) ein endlichdimensionaler normierter Raum und M â V kompakt. Dann besitzt jede Folge in M eine
konvergente Teilfolge mit Grenzwert in M.
Bemerkung 4.6.18. HaĚufig findet man die Aussage dieses Satzes auch in der folgenden Formulierung: Ist (V, k Âˇ kV ) ein endlichdimensionaler normierter Raum,
â
so besitzt jede beschraĚnkte Folge in V mindestens einen HaĚufungswert.â
Anschaulich bedeutet das: In einer beschraĚnkten Teilmenge des Rn ist nicht genug Platz, als dass man mit einer Folge so wild herumspringen kann, dass sie
sich nirgends haĚuft. Anders gesagt: Wenn man unendlich viele Punkte in einer
beschraĚnkten Menge unterbringen will, so muĚssen die irgendwo klumpen.
Definition 4.6.19. Ein normierter R-Vektorraum (V, k Âˇ kV ) heiĂt vollstaĚndig,
wenn jede Cauchy-Folge in V konvergiert. Ein vollstaĚndiger normierter R-Vektorraum wird auch Banachraum genannt.
Wird die Norm kÂˇkV auĂerdem durch ein Skalarprodukt auf V induziert, so nennt
man V Hilbertraum.
Beispiel 4.6.20. (a) Der Standardvektorraum Rd ist fuĚr jedes d â Nâ mit jeder
darauf definierten Norm ein Banachraum. WaĚhlt man speziell die durch das
Standardskalarprodukt induzierte 2-Norm, so ist (Rd , k Âˇ k2 ) sogar ein Hilbertraum.
(b) Die Menge
â
X

â := (an ) : (an ) reelle Folge, so dass
a2n konvergent
2

n=0

ist ein R-Vektorraum. Weiter ist die Abbildung (Âˇ|Âˇ) : â2 Ă â2 â R, die fuĚr
zwei Folgen (an ), (bn ) â â2 durch


(an ) (bn ) :=

â
X

an bn

n=0

159

4. Analysis â Teil I: Konvergenz und Stetigkeit
definiert ist, ein Skalarprodukt. Mit der davon induzierten Norm
â
X
1
p
2 2
k(an )k2 = ((an )|(an )) =
an
n=0

wird â2 zu einem Hilbertraum.
(c) Die Menge
â
X

â1 := (an ) : (an ) reelle Folge, so dass
|an | konvergent
n=0

ist mit der Norm k(an )k1 :=
raum ist.

Pâ

n=0

|an | ein Banachraum, der kein Hilbert-

UĚbungsaufgabe 4.6.21. Die folgenden Resultate aus den Abschnitten 4.3 und
4.5 gelten in beliebigen BanachraĚumen: Satz 4.3.19, Satz 4.5.10, Satz 4.5.12 (a)
(Majorantenkriterium) und Satz 4.5.16 (Wurzel- und Quotientenkriterium). UĚbertragen Sie die Aussagen und Beweise.
Zum Abschluss dieses Abschnittes wollen wir noch einen wichtigen Satz beweisen, den Banachâschen Fixpunktsatz. Dieser gibt unter anderem ein einfaches
Kriterium an eine Iterationsvorschrift an, das deren Konvergenz garantiert.
Satz 4.6.22 (Banachâscher Fixpunktsatz). Es sei (V, k Âˇ kV ) ein Banachraum,
M â V abgeschlossen und f : M â M eine Funktion. Weiter existiere ein
q â (0, 1), so dass
kf (x) â f (y)kV â¤ qkx â ykV

fuĚr alle x, y â M

gilt. Dann gelten die folgenden Aussagen:
(a) Es gibt genau ein v â M mit f (v) = v. (D.h. f hat genau einen Fixpunkt
in M.)
(b) FuĚr jedes x0 â M konvergiert die Folge (xn ) mit xn+1 = f (xn ), n â N,
gegen v und es gelten die folgenden FehlerabschaĚtzungen fuĚr jedes n â Nâ :
qn
kxn â vkV â¤
kx1 â x0 kV
1âq
q
kxn â vkV â¤
kxn â xnâ1 kV
1âq

(A-priori-AbschaĚtzung),
(A-posteriori-AbschaĚtzung).

Beweis. Wir waĚhlen ein beliebiges x0 â M und betrachten die in der Formulierung des Satzes schon erwaĚhnte Folge mit xn+1 = f (xn ) fuĚr jedes n â N. Wir
zeigen zunaĚchst
kxn+1 â xn kV â¤ q n kx1 â x0 kV

160

fuĚr alle n â N

4.6. Konvergenz in normierten RaĚumen
per Induktion. FuĚr n = 0 lautet diese Ungleichung kx1 â x0 kV â¤ kx1 â x0 kV , ist
also wahr. Wir wenden uns dem Induktionsschritt von n nach n+1 zu. TatsaĚchlich
gilt mit der Voraussetzung an f
kxn+2 â xn+1 kV = kf (xn+1 ) â f (xn )kV â¤ qkxn+1 â xn kV
und dann der Induktionsvoraussetzung
â¤ qq n kx1 â x0 kV = q n+1 kx1 â x0 kV .
Nun wollen wir als naĚchstes zeigen, dass (xn ) eine Cauchyfolge in V ist. Seien
dazu zwei Indizes n, m â N mit m > n gegeben. Dann gilt mit dem Ergebnis von
eben
kxm â xn kV = kxm â xmâ1 + xmâ1 â xmâ2 + Âˇ Âˇ Âˇ â xn+1 + xn+1 â xn kV
=
=q

mâ1
X

(xk+1 â xk )

k=n
mâ1
X
kân
n

â¤ qn

q

k=n
â
X
k=0

V

â¤

mâ1
X
k=n

kxk+1 â xk kV â¤

kx1 â x0 kV = q n

q k kx1 â x0 k =

n

mâ1ân
X
k=0

mâ1
X
k=n

q k kx1 â x0 kV

q k kx1 â x0 kV

q
kx1 â x0 kV .
1âq

(4.2)

Wir beobachten nun zunaĚchst, dass im Fall x1 = x0 aus dieser Ungleichung xm =
xn fuĚr alle m > n âĽ 0 folgt. Wir haben dann also eine konstante Folge (xn ), die
offensichtlich konvergent gegen x0 ist. Es sei also in allen weiteren UĚberlegungen
x1 6= x0 .
Sei nun Îľ > 0. Da q â (0, 1) gilt, konvergiert die Folge (q n ) gegen Null, also gibt
es ein n0 â N mit q n < Îľ(1 â q)/kx1 â x0 kV fuĚr alle n âĽ n0 . FuĚr alle m > n âĽ n0
gilt dann
kxm â xn kV â¤

qn
Îľ(1 â q)
kx1 â x0 kV <
kx1 â x0 kV = Îľ.
1âq
(1 â q)kx1 â x0 kV

Damit haben wir gezeigt, dass (xn ) eine Cauchyfolge in V ist. Da V nach Voraussetzung vollstaĚndig ist, ist dies also eine konvergente Folge in V , deren Grenzwert
wir v := limnââ xn nennen.
Von diesem Grenzwert wollen wir nun natuĚrlich zeigen, dass er ein Fixpunkt von
f ist. Dazu beobachten wir, dass fuĚr jedes n â N gilt
kf (v) â vkV = kf (v) â xn + xn â vkV â¤ kf (v) â xn kV + kxn â vkV
= kf (v) â f (xnâ1 )kV + kxn â vkV â¤ qkv â xnâ1 kV + kxn â vkV .

161

4. Analysis â Teil I: Konvergenz und Stetigkeit
Geht man nun in dieser Ungleichung zum Grenzwert n â â uĚber, so bekommt
man

kf (v) â vkV = lim kf (v) â vkV â¤ lim qkv â xnâ1 kV + kxn â vkV = q Âˇ 0 + 0 = 0.
nââ

nââ

Die Definitheit der Norm liefert also f (v) = v.
Es bleiben noch die Eindeutigkeit des Fixpunktes und die beiden FehlerabschaĚtzungen zu zeigen. Zum Nachweis der Eindeutigkeit sei w ein weiteres Element von
M mit f (w) = w. Dann gilt nach der Voraussetzung an f
kv â wkV = kf (v) â f (w)kV â¤ qkv â wkV .

Nehmen wir nun an, es waĚre v 6= w, so folgt kv â wkV > 0 und wir koĚnnen durch
diesen Ausdruck teilen. Das liefert den Widerspruch 1 â¤ q. Der Fixpunkt ist also
eindeutig.
Die A-Priori-AbschaĚtzung haben wir schon weiter oben fast gezeigt. Geht man
naĚmlich in der Ungleichung (4.2) zum Grenzwert fuĚr m â â uĚber, so erhaĚlt man

 qn
qn
kx1 â x0 kV =
kx1 â x0 kV .
kv â xn kV = lim kxm â xn k â¤ lim
mââ
mââ 1 â q
1âq

Zum Beweis der A-posteriori-AbschaĚtzung uĚberlegen wir uns, dass fuĚr jedes n â
Nâ gilt

kv â xn kV = kf (v) â f (xnâ1)kV â¤ qkv â xnâ1 kV â¤ q kv â xn kV + kxn â xnâ1 kV .
Daraus folgt

(1 â q)kv â xn kV â¤ qkxn â xnâ1 kV ,
was nach Division durch 1 â q > 0 die behauptete AbschaĚtzung impliziert.

162

Teil II.
Mathematik II

163

4.7. Stetigkeit reeller Funktionen

4.7. Stetigkeit reeller Funktionen
Nach der Behandlung von Folgen und Reihen wollen wir uns nun dem Hauptuntersuchungsgegenstand der Analysis zuwenden: Funktionen von (Teilmengen
von) R nach R, bzw. spaĚter auch von (Teilmengen von) Rn nach Rm . An dieser
Stelle lohnt es sich also, die elementaren Definitionen zum Thema Funktionen
aus Abschnitt 1.4 wieder praĚsent zu haben.

4.7.1. Der Grenzwertbegriff fuĚr Funktionen
Definition 4.7.1. Es sei D â R eine Menge, f : D â R eine Funktion und
x0 â R.
(a) Wir nennen x0 einen HaĚufungspunkt von D, falls es eine Folge (an ) in D
mit an 6= x0 fuĚr alle n â N gibt, die gegen x0 konvergiert.
(b) Ist x0 ein HaĚufungspunkt von D, so sagen wir, dass f fuĚr x gegen x0 den
Grenzwert y hat, wenn fuĚr jede Folge (an ) in D, die gegen x0 konvergiert
und fuĚr die an 6= x0 fuĚr alle n â N gilt, die Folge (f (an )) gegen y konvergiert.
Wir schreiben dafuĚr
lim f (x) = y.
xâx0

(c) Ist x0 ein HaĚufungspunkt von D+ := {x â D : x > x0 }, so hat f fuĚr
x gegen x0 den rechtsseitigen Grenzwert y, wenn fuĚr jede Folge (an ) in
D+ , die gegen x0 konvergiert, die Folge (f (an )) gegen y konvergiert. Wir
schreiben
lim f (x) = y.
xâx0 +

(d) Ist x0 ein HaĚufungspunkt von Dâ := {x â D : x < x0 }, so hat f fuĚr x gegen
x0 den linksseitigen Grenzwert y, wenn fuĚr jede Folge (an ) in Dâ , die gegen
x0 konvergiert, die Folge (f (an )) gegen y konvergiert. Wir schreiben
lim f (x) = y.

xâx0 â

Bemerkung 4.7.2. (a) Der Begriff des HaĚufungspunkt dient hier nur der technisch noĚtigen Voraussetzung, dass es in (b), (c) und (d) uĚberhaupt eine Folge wie dort gefordert gibt. Dass x0 ein HaĚufungspunkt von D ist, bedeutet
anschaulich, dass man x0 aus D \ {x0 } heraus annaĚhern kann.
Beispielsweise hat (0, 1] alle Punkte in [0, 1] als HaĚufungspunkte und die
Menge {1/n : n â Nâ } hat nur einen HaĚufungspunkt, naĚmlich Null.

(b) Eine besondere Betonung sollte beim Lesen der Grenzwertdefinitionen auf
den Worten jede Folgeâ liegen.
â

165

4. Analysis â Teil I: Konvergenz und Stetigkeit
(c) Die folgenden SaĚtze und Definitionen sind jeweils nur fuĚr den Grenzwert
limxâx0 formuliert. Sie gelten, wann immer diese Sinn machen, aber auch
fuĚr die rechts- und linksseitigen Grenzwerte limxâx0 + und limxâx0 â .
Beispiel 4.7.3. Wir setzen D = (0, 1] und
ďŁą
ďŁ´
ďŁ´
x2 ,
ďŁ´
ďŁ´
ďŁ˛
f (x) = 1, falls
ďŁ´
ďŁ´
ďŁ´
ďŁ´
ďŁł 2,

betrachten die Funktion
1
0<x< ,
2
1
â¤ x < 1,
2
x = 1.

Wie wir schon oben beobachtet haben, ist jedes x â [0, 1] ein HaĚufungspunkt von
D. Wir koĚnnen also Grenzwertbetrachtungen in all diesen Punkten anstellen. An
den interessanten Stellen 0, 1/2 und 1 gilt
lim f (x) = lim f (x) = 0

xâ0+

und

xâ0

1
lim f (x) = 1
lim f (x) = ,
xâ1/2+
xâ1/2â
4
lim f (x) = lim f (x) = 1
xâ1â

xâ1

und
und

lim f (x) nicht definiert,

xâ0â

lim f (x) existiert nicht,

xâ1/2

lim f (x) nicht definiert.

xâ1+

Der folgende Satz ergibt sich aus den GrenzwertsaĚtzen fuĚr Folgen, vgl. Satz 4.3.7.
Er ermoĚglicht genau wie dort, die Berechnung von Grenzwerten durch Zerlegen
des Problems in einfachere Bausteine.
Satz 4.7.4. Es sei D â R und x0 ein HaĚufungspunkt von D. Desweiteren seien
drei Funktionen f, g, h : D â R gegeben, so dass die Grenzwerte limxâx0 f (x)
und limxâx0 g(x) existieren. Dann gilt:
(a) Die Grenzwerte fuĚr x gegen x0 von f + g, f g und |f | existieren und es gilt

lim f (x) + g(x) = lim f (x) + lim g(x),
xâx0
xâx0
xâx0

lim f (x)g(x) = lim f (x) Âˇ lim g(x) und
xâx0

xâx0

xâx0

lim |f (x)| = lim f (x) .

xâx0

xâx0

(b) Gilt f (x) â¤ g(x) fuĚr alle x â D \ {x0 }, so ist limxâx0 f (x) â¤ limxâx0 g(x).
(c) Ist limxâx0 f (x) = limxâx0 g(x) und gilt
f (x) â¤ h(x) â¤ g(x)

fuĚr alle x â D \ {x0 },

so gilt auch limxâx0 h(x) = limxâx0 f (x) = limxâx0 g(x).

166

4.7. Stetigkeit reeller Funktionen
(d) Ist y := limxâx0 g(x) 6= 0, so existiert ein Î´ > 0, so dass |g(x)| âĽ |y|/2 fuĚr
alle x â (D âŠ (x0 â Î´, x0 + Î´)) \ {x0 } ist. Wir koĚnnen also die Funktion

f
: D âŠ (x0 â Î´, x0 + Î´) \ {x0 } â R
g

mit

f
f (x)
(x) :=
g
g(x)

definieren. FuĚr diese existiert dann der Limes fuĚr x gegen x0 mit
lim

xâx0

limxâx0 f (x)
f (x)
=
.
g(x)
limxâx0 g(x)

Mit Hilfe der bestimmten Divergenz aus Definition 4.3.13 koĚnnen wir nun zum
Einen auch entsprechende Grenzwerte von Funktionen definieren, und zum Anderen den Grenzwerten limxâÂąâ f (x) einen Sinn geben.
Definition 4.7.5. (a) Es seien D â R, f : D â R eine Funktion und x0 ein
HaĚufungspunkt von D. Wir schreiben

lim f (x) = â bzw. lim f (x) = ââ ,
xâx0

xâx0

wenn fuĚr jede Folge (an ) in D, die gegen x0 konvergiert und fuĚr die an 6=
x0 fuĚr alle n â N gilt, die Folge (f (an )) bestimmt gegen â (bzw. ââ)
divergiert.

(b) Es sei D â R nicht nach oben (bzw. unten) beschraĚnkt, f : D â R eine
Funktion und y â R âŞ {â, ââ}. Wir sagen

lim f (x) = y bzw. lim f (x) = y ,
xââ

xâââ

wenn fuĚr jede Folge (an ) in D, die bestimmt gegen â (bzw. ââ) divergiert,
limnââ f (an ) = y gilt.

Beispiel 4.7.6.

(a) Es gilt

1
= 0,
xââ x

1
= â,
xâ0+ x

lim

1
= ââ,
xâ0â x

lim

lim

lim x = â.

xââ

(b) Wir betrachten
Pâ xn wieder die Exponentialfunktion E : R â R mit E(x) =
x
e = n=0 n! und bestimmen ihre Grenzwerte fuĚr x gegen Âąâ.
FuĚr alle x âĽ 0 gilt

ex =

â
X
xn
n=0

also ist auch

âĽ

n!

lim ex âĽ lim x = â,

xââ

xââ

x1
= x,
1!

d.h. lim ex = â.
xââ

Damit bekommen wir auch
lim ex = lim

xâââ

1

xâââ eâx

1
= 0.
tââ et

= lim

167

4. Analysis â Teil I: Konvergenz und Stetigkeit

4.7.2. Stetigkeit
Stetigkeit ist ein in der Analysis fundamentaler Begriff. Flapsig gesprochen, bedeutet diese, dass ein kleines Wackeln an der Variablen den Funktionswert einer
Funktion auch nur wenig veraĚndert, d.h. kleine StoĚrungen haben auch nur kleine
Wirkungen. Damit ist Stetigkeit, zumeist unbemerkt, eine haĚufige Grundannahme unseres Lebens.
Definition 4.7.7. Es sei D â R und x0 â D. Eine Funktion f : D â R heiĂt
stetig in x0 , falls fuĚr jede Folge (an ) in D, die gegen x0 konvergiert, auch die
Folge (f (an )) konvergiert und limnââ f (an ) = f (x0 ) gilt.
Weiter heiĂt f stetig auf D, wenn f in jedem Punkt x0 â D stetig ist.
SchlieĂlich setzen wir noch
C(D) := {f : D â R : f stetig auf D}.
Bemerkung 4.7.8. (a) Wieder sollte die Betonung auf jede Folgeâ liegen,
â
vgl. Bemerkung 4.7.2 (b).
(b) Anschaulich bedeutet das: Wie auch immer man sich auf der x-Achse an die
Stelle x0 â D herantastet, die entsprechenden Funktionswerte naĚhern sich
immer dem Wert f (x0 ) an. Das heiĂt, um den Bogen zu obiger Beschreibung zu schlagen: Wenn man sich schon nahe herangepirscht hat, muss der
Funktionswert auch nahe am Wert f (x0 ) liegen.
Beispiel 4.7.9. (a) Die Funktion f (x) = |x|, x â R, ist stetig auf R. Das sieht
man folgendermaĂen: Sei x0 â R und (an ) eine Folge in R, die gegen x0
konvergiert. Dann gilt
lim f (an ) = lim |an | = | lim an | = |x0 | = f (x0 )

nââ

nââ

nââ

nach Satz 4.3.7 (a).
(b) Die Funktion f (x) = cxk , x â R, ist fuĚr jedes c â R und alle k â N stetig
auf R, denn fuĚr jede konvergente Folge (an ) in R mit limnââ an = x0 â R
gilt nach den GrenzwertsaĚtzen
lim f (an ) = lim cakn = c( lim an )k = cxk0 = f (x0 ).

nââ

nââ

nââ

(c) Dagegen ist die Signum-Funktion sign : R â R, gegeben durch
ďŁą
ďŁ´
x < 0,
ďŁ˛â1,
sign(x) := 0, falls x = 0,
ďŁ´
ďŁł
1,
x > 0,
168

4.7. Stetigkeit reeller Funktionen
in Null nicht stetig, denn die Folge an = 1/n, n â Nâ , ist eine Nullfolge,
aber
lim sign(an ) = lim sign(1/n) = lim 1 = 1 6= 0 = sign(0).

nââ

nââ

nââ

Satz 4.7.10. Es sei D â R und f : D â R eine Funktion. Ist x0 â D ein
HaĚufungspunkt von D, so ist f in x0 genau dann stetig, wenn
lim f (x) = f (x0 )

xâx0

gilt.
Bemerkung 4.7.11. In dieser Form ist die Stetigkeit am einpraĚgsamsten: Damit
f in x0 stetig ist, muss
lim f (x) = f (x0 ) = f ( lim x)

xâx0

xâx0

gelten. Stetigkeit bedeutet also, dass man den GrenzuĚbergang mit der Funktionsauswertung vertauschen kann.
UĚbungsaufgabe 4.7.12. Diese Aufgabe ist eher eine Diskussionsanregung. Auf
der Menge D = [0, 1] âŞ {2} sei die Funktion
(
x2 , falls x â [0, 1],
f (x) =
43, falls x = 2,
gegeben. Skizzieren Sie den Graphen und diskutieren Sie, ob diese auf D stetig
ist.
Der folgende Satz erlaubt wieder, wie schon Satz 4.3.7 fuĚr Folgen, Stetigkeitsuntersuchungen komplizierter Funktionen auf die Untersuchung einfacherer Bausteine zu reduzieren. Sein Beweis ergibt sich aus der Kombination der Stetigkeitsdefinition mit den entsprechenden Aussagen aus Satz 4.3.7.
Satz 4.7.13. Es sei D â R und f, g : D â R seien stetig in x0 â D. Dann sind
die Funktionen f + g, f g und |f | stetig in x0 .
Ist x0 â D â := {x â D : g(x) 6= 0}, so ist die Funktion f /g : D â â R stetig in
x0 .
Genauso wichtig ist die folgende MoĚglichkeit durch Verkettung komplizierte stetige Funktionen zu basteln:
Satz 4.7.14. Es seien D, E â R und f : D â E, sowie g : E â R Funktionen.
Ist f stetig in x0 â D und g stetig in f (x0 ), so ist g âŚ f : D â R stetig in x0 .

169

4. Analysis â Teil I: Konvergenz und Stetigkeit
Beweis. Es sei (an ) eine Folge in D, die gegen x0 konvergiert. Da f in x0 stetig
ist, gilt dann limnââ f (an ) = f (x0 ). Also ist dank der Stetigkeitsvoraussetzung
an g
lim (g âŚ f )(an ) = lim g(f (an )) = g(f (x0 )) = (g âŚ f )(x0 )
nââ

nââ

und wir sind fertig.
Beispiel 4.7.15. (a) Mit Satz 4.7.13 bekommt man in Kombination mit Beispiel 4.7.9 (b) sofort, dass jede Polynomfunktion auf ganz R stetig ist.
(b) In Zusammenarbeit liefern Satz 4.7.13 und Satz 4.7.14 z.B. die Stetigkeit
auf R der Funktion
f (x) =

|x|7 (x â 5)3 â 15
,
x2 + |x|3 + 4

x â R.

Neben der Addition, Multiplikation und VerknuĚpfung von Funktionen, kann man
(zumindest bijektive) Funktionen noch umkehren. Es erhebt sich also die Frage, ob die Umkehrfunktion einer bijektiven, stetigen Funktion wieder stetig ist.
die Antwort ist im Allgemeinen nein, doch unter geeigneten Zusatzvoraussetzungen geht das gut. Um diese zu formulieren, benoĚtigen wir jedoch noch ein paar
Begriffe:
Definition 4.7.16. Es sei D â R. Eine Funktion f : D â R heiĂt
(a) monoton wachsend, falls fuĚr alle x, y â D gilt x â¤ y â f (x) â¤ f (y),
(b) monoton fallend, falls fuĚr alle x, y â D gilt x â¤ y â f (x) âĽ f (y),
(c) streng monoton wachsend, falls fuĚr alle x, y â D gilt x < y â f (x) < f (y),
(d) streng monoton fallend, falls fuĚr alle x, y â D gilt x < y â f (x) > f (y)
und
(e) (streng) monoton, wenn sie (streng) monoton wachsend oder (streng) monoton fallend ist.
P
n
Beispiel 4.7.17. Die Exponentialfunktion ex = E(x) = â
n=0 x /n!, x â R, ist
streng monoton wachsend.
Um das einzusehen, bemerken wir zunaĚchst, dass fuĚr alle x > 0 gilt
x

e =

â
X
xn
n=0

n!

>1+

â
X
0n
n=1

n!

= 1.

Da eâx = 1/ex fuĚr alle x â R ist, muss damit fuĚr negative x der Wert ex â (0, 1)
liegen. Insbesondere ist ex > 0 fuĚr alle x â R.

170

4.7. Stetigkeit reeller Funktionen
Seien nun x, y â R mit x < y gegeben. Dann ist y â x > 0, d.h. wir haben

ey
.
ex
Nun ist ex strikt positiv, wir duĚrfen also die Ungleichung damit multiplizieren
ohne das Relationszeichen zu drehen und erhalten damit ex < ey .
1 < eyâx =

Nun kommen wir zur Stetigkeit der Umkehrfunktion. Das entsprechende Resultat
wollen wir nicht beweisen.
Satz 4.7.18. Es sei I â R ein Intervall und f â C(I) streng monoton. Dann ist
f : I â f (I) bijektiv, f (I) ein Intervall und f â1 : f (I) â I ist stetig auf f (I)
und streng monoton.
Ebenfalls ohne Beweis erwaĚhnen wir noch das Îľ-Î´-Kriterium, eine aĚquivalente
Bedingung fuĚr Stetigkeit, die vor allem von theoretischem Interesse ist.
Satz 4.7.19. Es sei D â R und x0 â D. Eine Funktion f : D â R ist in x0
genau dann stetig, wenn es fuĚr jedes Îľ > 0 ein Î´ > 0 gibt, so dass
|f (x) â f (x0 )| < Îľ fuĚr alle x â D mit |x â x0 | < Î´
gilt.
Definition 4.7.20. Es sei D â R. Eine Funktion f : D â R heiĂt Lipschitzstetig, falls es ein L > 0 gibt mit
|f (x) â f (y)| â¤ L|x â y| fuĚr alle x, y â D.
Lipschitz-Stetigkeit ist ein strengerer Begriff als Stetigkeit:
Satz 4.7.21. Ist D â R und f : D â R Lipschitz-stetig, so ist f stetig auf D.
Die Umkehrung dieser Aussage ist falsch.
Beweis. Es sei f : D â R Lipschitz-stetig und x0 â D beliebig. Ist Îľ > 0 und
Î´ := Îľ/L, so gilt fuĚr alle x â D mit |x â x0 | < Î´ dank der Lipschitz-Stetigkeit
Îľ
|f (x) â f (x0 )| â¤ L|x â x0 | < LÎ´ = L = Îľ.
L
Also ist f mit Hilfe von Satz 4.7.19 stetig in x0 .
Zum Nachweis, dass die Umkehrung falsch ist, betrachten wir ein Gegenbeispiel
mit D = R und f (x) = x2 , x â R. Dann ist f nach Beispiel 4.7.9 (b) stetig auf
R. Wir nehmen nun an, f waĚre Lipschitz-stetig, d.h. es gibt ein L > 0 mit
|f (x) â f (y)| â¤ L|x â y| fuĚr alle x, y â R.
Dann bekommen wir mit x := 2L und y := L
L2 = L|2L â L| = L|x â y| âĽ |f (x) â f (y)| = |(2L)2 â L2 | = 3L2 ,
was, dank L > 0, den Widerspruch 1 âĽ 3 liefert.

171

4. Analysis â Teil I: Konvergenz und Stetigkeit
Bemerkung 4.7.22. (a) Die strikten Kontraktionen aus Satz 4.6.22 sind Lipschitz-stetig mit Konstante L = q â (0, 1).
(b) Anschaulich bedeutet Lipschitz-Stetigkeit, dass die Steigung des Graphen,
vgl. Abschnitt 5.1, beschraĚnkt bleibt. Wir werden spaĚter in Beispiel 5.2.5
darauf zuruĚckkommen.

4.7.3. Eigenschaften stetiger Funktionen
In diesem Abschnitt sammeln wir einige wichtige SaĚtze, die fuĚr stetige Funktionen
auf R gelten und die wir immer wieder verwenden werden. Das erste ist der
Zwischenwertsatz, auf dessen Beweis wir verzichten wollen.
Satz 4.7.23 (Zwischenwertsatz). Es seien a, b â R mit a < b gegeben und f â
C([a, b]). Ist y0 eine reelle Zahl zwischen f (a) und f (b), so gibt es ein x0 â [a, b]
mit f (x0 ) = y0 .
f(x)

f(b)
y
0

a

x0

b

x

f(a)

Abbildung 4.1.: Der Zwischenwertsatz
Eine wichtige Folgerung aus diesem Satz ist die folgende.
Satz 4.7.24 (Nullstellensatz von Bolzano). Es seien a, b â R mit a < b und
f â C([a, b]) erfuĚlle f (a)f (b) < 0. Dann gibt es ein x0 â (a, b) mit f (x0 ) = 0.
Beweis. Wir muĚssen uns nur klarmachen, dass die Voraussetzung f (a)f (b) < 0
bedeutet, dass entweder f (a) > 0 und f (b) < 0 oder f (a) < 0 und f (b) > 0 ist.
Dann folgt die Behauptung sofort aus Satz 4.7.23.
Definition 4.7.25. Es sei D â R. Eine Funktion f : D â R heiĂt beschraĚnkt,
falls die Menge f (D) beschraĚnkt ist, d.h. falls ein C âĽ 0 existiert, so dass |f (x)| â¤
C fuĚr alle x â D gilt.
Wir koĚnnen nun den fundamentalen Satz formulieren, der das Verhalten stetiger
Funktionen auf kompakten Mengen beschreibt.

172

4.7. Stetigkeit reeller Funktionen
Satz 4.7.26. Es sei K â R kompakt und nicht-leer, sowie f â C(K). Dann gibt
es xâ , xâ â K, so dass
f (xâ ) â¤ f (x) â¤ f (xâ ) fuĚr alle x â K
gilt. Insbesondere ist f beschraĚnkt.
Man beachte, dass damit insbesondere max f (K) = f (xâ ) und min f (K) = f (xâ )
existieren. In Worte gefasst lautet dieser Satz damit:
Eine stetige Funktion auf einem Kompaktum nimmt ihr Maximum und
ihr Minimum auf dem Kompaktum an.
Dass dabei jede der Voraussetzungen zwingend noĚtig ist, veranschaulichen die
folgenden Beispiele.
Beispiel 4.7.27.

(a) Ist K = [0, 1] und
ďŁą
ďŁ˛ x, falls x â (0, 1),
f (x) = 1
ďŁł , falls x = 0 oder x = 1,
2

so ist zwar f (K) = (0, 1), aber es gibt keine xâ , xâ â [0, 1] mit f (xâ ) = 0
und f (xâ ) = 1. Wir brauchen also die Stetigkeit von f .
(b) Ist K = (0, 1] und f (x) = 1/x fuĚr x â K, so ist f auf K stetig, aber die
Menge f (K) ist nicht beschraĚnkt, da limxâ0+ f (x) = â ist. Wir brauchen
also die Abgeschlossenheit von K.
(c) Ist schlieĂlich K = R und f (x) = E(x), so ist diese Funktion wieder auf
ganz K stetig und K ist abgeschlossen, aber f nicht beschraĚnkt. Wir brauchen also die BeschraĚnktheit von K.
Beweis von Satz 4.7.26: Wir zeigen zunaĚchst, dass unter den Voraussetzungen
des Satzes die Funktion f beschraĚnkt ist. WaĚre dem nicht so, gaĚbe es fuĚr jedes
n â N ein an â K mit |f (an )| > n. Wegen der Kompaktheit von K koĚnnen
wir nun nach dem Satz von Bolzano-WeierstraĂ 4.6.17 aus der Folge (an ) eine
konvergente Teilfolge (ank )kâN mit x0 := limkââ ank â K auswaĚhlen. Da weiter
f stetig auf K ist, muss dann die Folge (f (ank ))kâN gegen f (x0 ) konvergieren.
Insbesondere ist damit die Folge (f (ank )) beschraĚnkt, was im Widerspruch zur
Konstruktion der Folge (an ) steht, nach der |f (ank )| > nk fuĚr alle k â N gilt.
Da nun f (K) beschraĚnkt ist, existieren zumindest sup f (K) und inf f (K) nach
dem VollstaĚndigkeitsaxiom. Wir betrachten hier nur S := sup f (K), die Untersuchung fuĚr das Infimum verlaĚuft analog. Zu zeigen ist, dass es ein xâ â K gibt mit
f (xâ ) = S. Dazu stellen wir zunaĚchst fest, dass fuĚr jedes n â Nâ die Zahl S â 1/n

173

4. Analysis â Teil I: Konvergenz und Stetigkeit
keine obere Schranke von f (K) sein kann. Also gibt es jeweils ein yn â f (K),
und damit ein xn â K mit f (xn ) = yn , so dass
Sâ

1
< f (xn ) â¤ S
n

fuĚr alle n â Nâ

gilt. FuĚr die so gewonnene Folge (xn )nâNâ gilt nach dem Sandwich-Theorem
lim f (xn ) = S.

nââ

AuĂerdem enthaĚlt sie wie jede Folge in K nach dem Satz von Bolzano-WeierstraĂ
eine konvergente Teilfolge (xnk )kâN . Wir setzen
xâ := lim xnk .
kââ

Dann gilt wegen der Abgeschlossenheit von K sofort xâ â K und dank der Stetigkeit von f haben wir limkââ f (xnk ) = f (xâ ). Wie wir oben gesehen haben,
konvergiert aber die gesamte Folge (f (xn ))nâNâ gegen S. Nach Satz 4.3.24 (b) hat
dann auch jede Teilfolge diesen Grenzwert. Das liefert schlieĂlich
f (xâ ) = lim f (xnk ) = lim f (xn ) = S.
nââ

kââ

4.8. Stetigkeit von Funktionen mehrerer Variablen
Auch der Stetigkeitsbegriff uĚbertraĚgt sich direkt auf den Fall von normierten RVektorraĚumen. ZunaĚchst muĚssen wir den Grenzwertbegriff fuĚr Funktionen zwischen normierten RaĚumen verallgemeinern.
Definition 4.8.1. Es seien V und W normierte R-VektorraĚume, D â V und
f : D â W eine Funktion.
(a) Wir nennen x0 â D HaĚufungspunkt von D, falls es eine Folge (an ) in D
mit an 6= x0 fuĚr alle n â N gibt, die gegen x0 konvergiert.
(b) Sei x0 ein HaĚufungspunkt von D. Dann ist
lim f (x) = y,

xâx0

falls fuĚr jede Folge (an ) in D, die gegen x0 konvergiert und an 6= x0 fuĚr alle
n â N erfuĚllt, die Folge (f (an )) gegen y konvergiert.
Bemerkung 4.8.2. Die Begriffe links- und rechtsseitiger Grenzwert koĚnnen wir
nicht auf normierte RaĚume uĚbertragen, da es dort nicht so eindimensional zugeht
und es insofern Unmengen Richtungen gibt, aus denen sich eine Folge auf den
Punkt x0 zubewegen kann, zumal sie ja noch nicht mal aus einer definierten
Richtung kommen muss, sondern auch auf den Grenzwert zuspiralen kann.

174

4.8. Stetigkeit von Funktionen mehrerer Variablen
Definition 4.8.3. Es seien V, W zwei normierte R-VektorraĚume, D â V und
x0 â D. Eine Funktion f : D â W heiĂt stetig in x0 , wenn fuĚr jede Folge (an ) in D, die gegen x0 konvergiert, auch die Folge (f (an )) konvergiert und
limnââ f (an ) = f (x0 ) gilt.
Weiter heiĂt f stetig auf D, wenn f in jedem Punkt x0 â D stetig ist.
AuĂerdem setzen wir wieder

C(D; W ) := f : D â W : f stetig auf D .

Im Falle, dass W = R ist, schreibt man oft auch einfach C(D) statt C(D; R),
vgl. Definition 4.7.7.
Von besonderem Interesse ist natuĚrlich der Fall, dass V und W endlichdimensional
sind, also, bis auf Isomorphie, V = Rd und W = Rp fuĚr irgendwelche d, p â Nâ
gilt und etwas anderes wollen wir vorerst auch gar nicht betrachten.
Ist also D â Rd und f : D â Rp eine Funktion, so ist f (x) = f ((x1 , x2 , . . . , xd )T ),
eine Funktion, die jedem Vektor in Rd einen Vektor in Rp zuordnet. Die Funktion
hat also d reelle Eingabewerte, woher die Sprechweise Funktion mehrerer Variaâ
blenâ in der KapiteluĚberschrift kommt. TatsaĚchlich nimmt man in der Analysis
meist diesen Standpunkt ein, dass die Funktion von d reellen Eingangsparametern x1 , x2 , . . . , xd abhaĚngt und schreibt, nicht ganz korrekt, f (x1 , x2 , . . . , xd ) statt
f ((x1 , x2 , . . . , xd )T ).
Der Wert f (x) ist fuĚr jedes x â D ein Vektor in Rp . Man hat also
ďŁŤ
ďŁś ďŁŤ
ďŁś
f (x)1
f1 (x)
ďŁŹf (x)2 ďŁˇ ďŁŹf2 (x)ďŁˇ
ďŁŹ
ďŁˇ ďŁŹ
ďŁˇ
f (x) = ďŁŹ .. ďŁˇ = ďŁŹ .. ďŁˇ ,
ďŁ­ . ďŁ¸ ďŁ­ . ďŁ¸
f (x)p
fp (x)
mit den p sogenannten Koordinatenfunktionen f1 , f2 , . . . , fp : D â R.
Der naĚchste Satz zeigt, dass es in Bezug auf Stetigkeit sogar ausreicht den Fall
p = 1 zu betrachten.
Satz 4.8.4. Es sei D â Rd und x0 â D. Dann ist f : D â Rp genau dann in x0
stetig, wenn alle Koordinatenfunktionen f1 , f2 , . . . , fp : D â R in x0 stetig sind.
Beweis. Nach Defintion haben wir genau dann Stetigkeit in x0 , falls fuĚr jede
Folge (an ) in D, die gegen x0 konvergiert, die Folge (f (an )) in Rp gegen f (x0 )
konvergiert. Nach Satz 4.6.5 ist diese Konvergenz aĚquivalent zur Konvergenz der
Koordinatenfolgen (f (an )j ) = (fj (an )) gegen fj (x0 ) fuĚr alle j = 1, 2, . . . , p. Diese
Bedingung ist wiederum fuĚr alle solchen Folgen (an ) genau dann erfuĚllt, wenn die
Koordinatenfunktionen in x0 stetig sind.
Da wir damit Stetigkeit aus der Stetigkeit der einzelnen Koordinaten bekommen,
koĚnnen wir uns im folgenden Satz auf den Fall p = 1 beschraĚnken. Er enthaĚlt
die Verallgemeinerung von Satz 4.7.13 und Satz 4.7.14 auf Funktionen mehrerer
Variablen.

175

4. Analysis â Teil I: Konvergenz und Stetigkeit
Satz 4.8.5. Es seien D â Rd , x0 â D und f, g : D â R stetig in x0 , sowie
h : f (D) â R stetig in f (x0 ). Dann sind auch f + g, f g, |f | und h âŚ f als
Funktionen von D nach R stetig in x0 .
Ist auĂerdem x0 â D â := {x â D : g(x) 6= 0}, so ist auch f /g : D â â R stetig in
x0 .
So einfach, zumindest auf einem formalen Level, die UĚbertragung des Stetigkeitsbegriffs auf Funktionen mehrerer Variablen war, so unuĚbersichtlich koĚnnen
leider Stetigkeitsuntersuchungen am konkreten Beispiel werden. Wir betrachten
ein paar Beispiele im fuĚr die Anschauung noch zugaĚnglichen Fall d = 2 und p = 1.

0.5
0.25
0.0
-0.25
-0.5
-1.0

-0.5

0.0
x

0.5

1.0

1.0
0.5
0.0 y
-0.5
-1.0

Abbildung 4.2.: Der Graph der Funktion (x, y) 7â

xy
x2 +y 2

Beispiel 4.8.6. Wir betrachten die beiden Funktionen f, g : R2 â R mit
(
xy
falls (x, y) 6= (0, 0),
2
2,
f (x, y) = x +y
0,
falls (x, y) = (0, 0),
( 2
x y
falls (x, y) 6= (0, 0),
2
2,
g(x, y) = x +y
0,
falls (x, y) = (0, 0),
vgl. Abbildungen 4.2 und 4.3. ZunaĚchst ist in beiden Funktionsvorschriften der
Nenner fuĚr alle (x, y) 6= (0, 0) immer ungleich Null, also sind beide Funktionen
f und g in allen Punkten (x0 , y0 ) 6= (0, 0) nach Satz 4.8.5 stetig. Es bleibt uns
nur die Stetigkeit in (0, 0) zu uĚberpruĚfen. Hier nutzt obiger Satz nichts und wir
muĚssen mit der Definition der Stetigkeit arbeiten.

176

4.8. Stetigkeit von Funktionen mehrerer Variablen

0.5
0.25
0.0
-0.25
-0.5
-1.0

-0.5

0.0
x

0.5

1.0

1.0
0.5
0.0 y
-0.5
-1.0

Abbildung 4.3.: Der Graph der Funktion (x, y) 7â

x2 y
x2 +y 2

Zu f : Wir betrachten die Folge
(an )nâNâ = (1/n, 1/n)T



nâNâ

,

in R2 . Diese konvergiert gegen (0, 0)T , wobei an 6= (0, 0)T fuĚr alle n â Nâ gilt.
Daher ist
1
1
2
f (an ) = f (1/n, 1/n) = 1 n 1 =
2
+ n2
n2
fuĚr alle n â Nâ . Das liefert uns aber limnââ f (an ) = 1/2 6= 0 = f (0, 0) =
f (limnââ an ). Also ist f in (0, 0)T nicht stetig.
Zu g: Es sei (an ) eine Folge in R2 mit limnââ an = (0, 0)T . Dann ist an =
(xn , yn )T fuĚr n â N mit den beiden reellen Koordinatenfolgen (xn ) und (yn ).
AuĂerdem gilt fuĚr alle n â N fuĚr die an 6= (0, 0)T ist
|g(an )| =

x2n |yn |
x2n |yn |
x2n yn
â¤
= |yn |.
=
x2n + yn2
x2n + yn2
x2n

Ist an = (0, 0)T , so gilt auf jeden Fall |g(an )| = 0 â¤ |yn |, also haben wir diese
Ungleichung fuĚr alle n â N.
Da (yn ) und damit auch (|yn |) eine Nullfolge ist, gilt damit nach dem SandwichTheorem limnââ g(an ) = 0 = g(0, 0) = g(limnââ an ), was gerade bedeutet, dass
g in (0, 0)T stetig ist.

177

4. Analysis â Teil I: Konvergenz und Stetigkeit
UĚbungsaufgabe 4.8.7. Zeigen Sie, dass jede lineare Abbildung ÎŚ : Rd â Rp
stetig ist.
Hinweis: Dazu reicht es zu zeigen, dass fuĚr jedes A â RpĂd die Abbildung ÎŚA :
Rd â Rp mit ÎŚA x = Ax, x â Rd , stetig ist.
Wie fuĚr Funktionen in einer Variablen gilt der folgende wichtige Satz uĚber stetige
Funktionen auf kompakten Mengen, vgl. Satz 4.7.26.
Satz 4.8.8. Es sei K â Rd kompakt und nicht-leer, sowie f â C(K). Dann gibt
es xâ , xâ â K, so dass
f (xâ ) â¤ f (x) â¤ f (xâ )

fuĚr alle x â K

gilt. Insbesondere ist f beschraĚnkt.
UĚbungsaufgabe 4.8.9. Es sei K â Rd kompakt und nicht-leer. Zeigen Sie, dass
k Âˇ kâ : C(K) â R mit
kf kâ := max |f (x)|,

f â C(K),

xâK

eine Norm ist. Diese heiĂt Supremums-Norm
Mit Hilfe dieses Satzes kann nun auĂerdem ein Versprechen aus Bemerkung 4.6.7
eingeloĚst werden. Wir beweisen dazu zunaĚchst das folgende Resultat.
Satz 4.8.10. Es sei k Âˇ k irgendeine Norm auf Rd und k Âˇ k2 die 2-Norm auf Rd .
Dann gibt es zwei Konstanten c und C mit 0 < c â¤ C, so dass
fuĚr alle x â Rd

ckxk2 â¤ kxk â¤ Ckxk2
gilt.

d
d
Beweis.
Pd Es sei x â R und {e1 , e2 , . . . , ed } die Standardbasis von R . Dann ist
x = j=1 xj ej und wir bekommen mit der Dreiecksungleichung

kxk =

d
X
j=1

xj ej â¤

d
X
j=1

kxj ej k =

d
X
j=1

|xj |kej k.

Auf diese Summe wenden wir nun die Cauchy-Schwarz-Ungleichung aus Satz 3.4.9
an und erhalten
kxk â¤

d
X
j=1

Setzen wir nun C :=
hauptung.

178

|xj |

P
d

2

d
1/2 X
j=1

2
j=1 kej k

kej k

1/2

2

1/2

= kxk2

d
X
j=1

kej k

2

1/2

.

, so erhalten wir den rechten Teil der Be-

4.8. Stetigkeit von Funktionen mehrerer Variablen
Nun betrachten wir auf D := {x â Rd : kxk2 = 1} â (Rd , k Âˇ k2 ) die Funktion f :
D â R mit f (x) = kxk. Wir zeigen zunaĚchst, dass dies eine stetige Funktion auf
D ist. Sei dazu (an ) eine Folge in D, die gegen x0 konvergiert. Damit ist gemeint,
dass (an ) bezuĚglich der Norm k Âˇ k2 konvergiert, d.h. limnââ kan â x0 k2 = 0. Wir
muĚssen nun zeigen, dass f (an ) gegen f (x0 ) konvergiert. Dazu beobachten wir mit
der umgekehrten Dreiecksungleichung und dem ersten Teil des Beweises
f (an ) â f (x0 ) = kan k â kx0 k â¤ kan â x0 k â¤ Ckan â x0 k2 .
Mit dem Sandwich-Theorem folgt nun tatsaĚchlich, dass limnââ |f (an ) â f (x0 )| =
0 und damit limnââ f (an ) = f (x0 ) gilt.
Weiter ist D abgeschlossen. Das sieht man folgendermaĂen: FuĚr jede Folge (an )
in D, die gegen ein x0 â Rd konvergiert, haben wir
kx0 k2 = k lim an k2 = lim kan k2 = lim 1 = 1.
nââ

nââ

nââ

Also ist auch x0 â D und wir erhalten die Abgeschlossenheit aus Satzâ4.6.11. Da
D auĂerdem beschraĚnkt ist (Die Norm aller Elemente uĚbersteigt z.B. 17 nicht),
ist D kompakt.
Also ist f eine stetige Funktion auf einem Kompaktum und wir erhalten aus
Satz 4.8.8, dass es ein yâ â D gibt mit kyk = f (y) âĽ f (yâ) = kyâ k fuĚr alle y â D.
Ist nun x â Rd beliebig, so ist x/kxk2 â D und es gilt also
kxk = kxk2

x
âĽ kxk2 kyâ k.
kxk2

Setzt man also c := kyâk, so ist zum Einen c > 0, denn yâ â D und damit kann
nicht yâ = 0 sein. Zum Anderen gilt damit nach dem oben gezeigten ckxk2 â¤ kxk
fuĚr alle x â Rd und wir sind fertig.
Aus diesem Satz koĚnnen wir nun einiges folgern.
Satz 4.8.11. (a) Sind kÂˇk und |kÂˇk| zwei Normen auf Rd , so gibt es Konstanten
0 < c â¤ C, so dass
ckxk â¤ |kxk| â¤ Ckxk

fuĚr alle x â Rd

(4.3)

gilt.
(b) Ist eine Folge (an ) in Rd bezuĚglich einer Norm konvergent, so konvergiert
sie auch bezuĚglich jeder anderen Norm und der Grenzwert ist derselbe.
Beweis.

(a) Nach Satz 4.8.10 gibt es Konstanten c1 , c2 , C1 , C2 mit
c1 kxk2 â¤ kxk â¤ C1 kxk2

und c2 kxk2 â¤ |kxk| â¤ C2 kxk2

179

4. Analysis â Teil I: Konvergenz und Stetigkeit
fuĚr alle x â Rd . Also ist fuĚr alle x â Rd auch
kxk â¤ C1 kxk2 â¤
Das liefert

C1
C1
C1 C2
|kxk| â¤
C2 kxk2 â¤
kxk.
c2
c2
c2 c1

c2
C2
kxk â¤ |kxk| â¤
kxk
C1
c1

fuĚr alle x â Rd .
(b) Es sei (an ) eine bezuĚglich k Âˇ k in Rd gegen x0 â Rd konvergente Folge und
|k Âˇ k| eine weitere Norm in Rd . Dann gilt limnââ kan â x0 k = 0 und wegen
der AĚquivalenz der Normen gilt
0 â¤ |kan â x0 k| â¤ Ckan â x0 k.
Daraus folgt dem Sandwich-Theorem limnââ |kan â x0 k| = 0, d.h. (an )
konvergiert bezuĚglich |k Âˇ k| ebenfalls gegen x0 .
Bemerkung 4.8.12. (a) Gilt fuĚr zwei Normen eine AbschaĚtzung in beide Richtungen wie in (4.3), so nennt man die beiden Normen auch aĚquivalent.
(b) Da die Konvergenz von Reihen und die Stetigkeit von Funktionen mit Hilfe
von Folgenkonvergenz definiert sind, sind damit auch diese Begriffe von der
konkreten Wahl der Norm unabhaĚngig.
(c) Ein entsprechender Satz gilt in unendlichdimensionalen RaĚumen nicht, z.B.
sind auf dem Raum der endlichen Folgen c00 , die 1- und die â-Norm nicht
aĚquivalent.
UĚbungsaufgabe 4.8.13. UĚbertragen Sie den Begriff der Lipschitz-Stetigkeit und
Satz 4.7.21 in den Kontext von Funktionen auf Rd .

4.9. Potenzreihen
In Abschnitt 4.5 haben wir festgestellt, dass fuĚr die Exponentialfunktion gilt
ez =

â
X
zn
n=0

1
1
= 1 + z + z2 + z3 + . . . ,
n!
2
6

z â C.

Sie ist damit durch eine Entwicklung in ein unendlich langes Polynomâ 1 gegeben.
â
Solche Reihendarstellungen gibt es fuĚr viele elementare Funktionen und wir wollen
uns nun der Theorie dieser sogenannten Potenzreihen zuwenden.
In diesem Kapitel steht der Buchstabe K wieder entweder fuĚr R oder C.
1

Das muss in ganz groĂen AnfuĚhrungszeichen bleiben, denn Polynome sind immer nur endliche
Summen.

180

4.9. Potenzreihen
Definition 4.9.1. Es sei (an ) eine Folge in K. Eine Reihe der Form
â
X

an xn = a0 + a1 x + a2 x2 + a3 x3 + . . .

n=0

heiĂt Potenzreihe.
Arbeitet man mit Potenzreihen in C, so schreibt man natuĚrlich eher z statt x,
aber das dient nur der groĚĂeren UĚbersichtlichkeit.
Beispiel 4.9.2. Neben der Exponentialreihe haben wir schon eine weitere Potenzreihe kennengelernt, naĚmlich die geometrische Reihe
â
X

xn

n=0

mit an = 1 fuĚr jedes n â N. In Beispiel 4.5.2 (a) haben wir gesehen, dass diese
fuĚr |x| > 1 divergiert und fuĚr |x| < 1 konvergiert mit
â
X
n=0

xn =

1
.
1âx

Die beiden Beispiele der Exponential- und der geometrischen Reihe zeigen, dass
die Frage, fuĚr welche x eine Potenzreihe konvergiert, verschiedene Antworten haben kann. Das einzig offensichtliche ist, dass jede Potenzreihe fuĚr x = 0 konvergiert. Zur weiteren Untersuchung der Konvergenz einer Potenzreihe verwenden
wir das Wurzelkriterium.
P
n
Es sei also â
n=0 an x eine Potenzreihe. Wir betrachten also
p
p
p
p
n
|an xn | = n |an | n |x|n = n |an ||x|.

Existiert nun der Grenzwert fuĚr n gegen unendlich dieses Ausdrucks und ist dieser
groĚĂer als Eins, so haben wir Divergenz und fuĚr einen Grenzwert kleiner als Eins
absolute Konvergenz nach dem Wurzelkriterium. Wir formulieren diese wichtige
Erkenntnis als Satz.
Satz 4.9.3 (Satz von Hadamard).
Es sei (an ) eine Folge
in K, so dass der
p
p
n
n
Grenzwert Ěş := limnââ |an | existiert oder die Folge ( |an |) unbeschraĚnkt
ist.
P
n
Dann gelten die folgenden Konvergenzaussagen fuĚr die Potenzreihe â
a
x
:
n=0 n
p
(a) Ist die Folge ( n |an |) unbeschraĚnkt, so konvergiert die Potenzreihe nur fuĚr
x = 0.
(b) Ist Ěş = 0, so konvergiert die Potenzreihe fuĚr alle x â K absolut.

181

4. Analysis â Teil I: Konvergenz und Stetigkeit
(c) Ist Ěş â (0, â), so ist die Potenzreihe fuĚr alle x â K mit |x| < 1/Ěş absolut
konvergent und fuĚr alle x â K mit |x| > 1/Ěş divergent.
Wir erben vom Wurzelkriterium auch die Unsicherheit, die dort fuĚr den Grenzfall
auftrat, dass der betrachtete Limes den Wert 1 annimmt. Diese macht sich hier
insofern bemerkbar, als wir im dritten Punkt dieses Satzes fuĚr |x| = 1/Ěş, also auf
dem Rand des Konvergenzgebietes, keine Aussage treffen koĚnnen.
Beachten Sie, dass der Konvergenzbereich K einer Potenzreihe damit immer entweder K = {0} oder K = K ist, oder wir haben
n
n
1o
1o
x â K : |x| <
â K â x â K : |x| â¤
.
Ěş
Ěş

Der Konvergenzbereich hat also die Form eines Intervalles in R, bzw. eines Kreises
in C. Das erklaĚrt die folgende Begriffsbildung fuĚr die Zahl 1/Ěş.
P
n
Definition 4.9.4. Es sei â
n=0 an x eine Potenzreihe die die Voraussetzungen
von Satz 4.9.3 erfuĚllt und Ěş wie in diesem Satz definiert. Dann heiĂt die Zahl
ďŁą
0, falls in obigem Satz (a) gilt,
ďŁ´
ďŁ´
ďŁ˛
â, falls in obigem Satz (b) gilt,
r :=
ďŁ´
ďŁ´
ďŁł 1 , falls in obigem Satz (c) gilt,
Ěş
der Konvergenzradius der Potenzreihe.

Wir betrachten einige Beispiele in R, die verdeutlichen, dass am Rand des Konvergenzintervalls alles passieren kann.
Beispiel 4.9.5.

(a) Wir beginnen mit an := 1, n â N, d.h. der Potenzreihe
â
X

xn .

n=0

Ja genau, das ist unsere altbekannte geometrische
p Reihe und insofern ist
es auch nicht verwunderlich, dass hier limnââ n |an | = limnââ 1 = 1 gilt
und damit der Konvergenzradius 1 ist, d.h. diese Potenzreihe konvergiert
absolut fuĚr |x| < 1 und divergiert fuĚr |x| > 1. UĚber das Konvergenzverhalten
am Rand des Konvergenzintervalls gibt uns der Konvergenzradius keine
Auskunft. FuĚr diese Reihe haben wir offensichtlich sowohl fuĚr x = â1 als
auch fuĚr x = 1 eine divergente Reihe.
(b) Nun sei an = 1/n, n â Nâ , wir betrachten also
â
X
xn
n=1

182

n

.

4.9. Potenzreihen
Auch hier ist der Konvergenzradius 1, denn
r
1
1
n 1
â = 1,
lim
= lim â
=
n
nââ
n nââ n
limnââ n n
vgl. Beispiel 4.3.10 (d). Damit konvergiert diese Reihe fuĚr x â (â1, 1) absolut und divergiert fuĚr |x| > 1. An den RaĚndern des Intervalls (â1, 1)
erhalten wir fuĚr x = 1 genau die harmonische Reihe (divergent) und fuĚr
x = â1 die alternierende harmonische Reihe (konvergent), so dass wir also Konvergenz fuĚr alle x â [â1, 1) und Divergenz fuĚr alle anderen x â R
haben.
(c) SchlieĂlich nehmen wir an = 1/n2 , d.h. die Potenzreihe
â
X
xn
n=1

n2

.

FuĚr diese ist ebenso der Konvergenzradius 1, aber diese Reihe konvergiert an
beiden Randpunkten des Konvergenzintervalls, denn fuĚr x = 1 erhalten wir
2
die nach Beispiel 4.5.14 (b) konvergente Reihe uĚber 1/n
x = â1
Pâ und fuĚr
n
2
die nach dem Leibniz-Kriterium konvergente Reihe
n=0 (â1) /n . Also
ist in diesem Fall das Konvergenzintervall der Potenzreihe [â1, 1] und wir
haben Divergenz fuĚr alle x â R mit |x| > 1.
Eigentlich haben wir bisher nur einen Spezialfall von Potenzreihen angeschaut,
der allerdings repraĚsentativ ist, d.h. alle wesentlichen Eigenschaften lassen sich
daran untersuchen und die Ergebnisse dann leicht auf den allgemeinen Fall erweitern.
Wir wollen nun also den Potenzreihen-Begriff noch einmal erweitern.
Definition 4.9.6. Es sei (an ) eine Folge in K, n0 â N und x0 â K. Dann nennt
man eine Reihe der Form
â
X
an (x â x0 )n
n=n0

Potenzreihe. Der Punkt x0 wird Entwicklungspunkt der Potenzreihe genannt.

Bemerkung 4.9.7. Den Konvergenzradius einer solchen Potenzreihe berechnet
man genauso wie oben zu 0 oder â oder
â1

p
r = lim n |an |
.
nââ

Allerdings ist zu beachten, dass dann das Konvergenzgebiet der Reihe der Kreis
mit Radius r um x0 statt um 0 ist. Es ist eine gute UĚbung, wenn Sie sich das
analog zu den UĚberlegungen zu Satz 4.9.3 klarmachen.

183

4. Analysis â Teil I: Konvergenz und Stetigkeit
Beispiel 4.9.8. Wir wollen herausfinden fuĚr welche x â R die Potenzreihe
â
X
(â4)n
n=1

n

(x â 1)n
n

, n â Nâ , und x0 = 1 ist.
konvergiert. Dazu bemerken wir, dass hier an := (â4)
n
Dann berechnen wir zunaĚchst den Konvergenzradius r mittels
r
r
n
p
(â4)n
4
n
n 4
n
= lim
= lim â
Ěş := lim
|an | = lim
= 4.
n
nââ
nââ
nââ
nââ
n
n
n
zu
1
1
r= = .
Ěş
4
Also haben wir auf jeden Fall absolute Konvergenz fuĚr alle x â R mit |xâ1| < 1/4,
also fuĚr x â (3/4, 5/4) und Divergenz fuĚr alle x â R mit |x â 1| > 1/4, d.h. fuĚr
x â (ââ, 3/4) âŞ (5/4, â).
Es bleiben die beiden Randpunkte x = 3/4 und x = 5/4 zu klaĚren. In x = 5/4
ist unsere Potenzreihe
â
â
X
(â4)n  1 n X (â1)n
=
.
n
4
n
n=1
n=1
Dies ist die alternierende harmonische Reihe, die nach Beispiel 4.5.8 konvergiert.
FuĚr x = 3/4 bekommt man mit einer analogen Rechnung, dass die betrachtete
Potenzreihe die Form
â
â
n X
X
(â4)n  3
1
â1 =
n
4
n
n=1
n=1

hat. Dies ist nun die harmonische Reihe, die nach Beispiel 4.5.2 (c) divergiert.
Also konvergiert unsere Potenzreihe genau dann, wenn x â (3/4, 5/4] ist und
divergiert in allen anderen FaĚllen.
Bemerkung 4.9.9. Die SaĚtze und Definitionen im Rest dieses Abschnitts sind
wieder fuĚr den Spezialfall x0 = 0 und n0 = 0 formuliert. Sie gelten aber, geeignet
abgeaĚndert, auch im allgemeinen Fall.
NatuĚrlich kann man auch das Quotientenkriterium zur Untersuchung der Konvergenz von Potenzreihen verwenden. Das liefert das folgende Ergebnis.
Satz 4.9.10. Es sei (an ) eine Folge in K mit an 6= 0 fuĚr alle n â N, so dass
Ď := limnââ | an+1
| existiert oderPdie Folge (| an+1
|) unbeschraĚnkt ist. Dann gilt
an
an
â
n
fuĚr den Konvergenzradius r von n=0 an x
ďŁą
ďŁ´
falls (|an+1 /an |) unbeschraĚnkt ist,
ďŁ˛0,
1
r = Ď , falls Ď â (0, â),
ďŁ´
ďŁł
â, falls Ď = 0.
184

4.9. Potenzreihen
Der Beweis verbleibt als UĚbung. Wir betrachten noch zwei Beispiele
Beispiel 4.9.11.
also

(a) ZunaĚchst betrachten wir die Potenzreihe mit an = nn /n!,
â
X
nn

n!

n=0

xn .

FuĚr das Quotientenkriterium bestimmen wir zunaĚchst
an+1
(n + 1)n+1 n!
(n + 1)(n + 1)n
= lim
=
lim
nââ an
nââ (n + 1)! nn
nââ
(n + 1)nn


 n + 1 n
1 n
= lim 1 +
= e.
= lim
nââ
nââ
n
n

Ď : = lim

Also ist der Konvergenzradius gegeben durch
r=

1
1
= .
Ď
e

(b) Eine Falle ist bei der folgenden Reihe eingebaut:
â
X
x3n
n=0

2n

.

Durch den Exponenten 3nâ sind der Satz von Hadamard 4.9.3 und der
â
Satz 4.9.10 nicht anwendbar. Es gibt nun zwei MoĚglichkeiten, die Sache
anzugehen. Die erste ist, das Wurzel- bzw. Quotientenkriterium fuĚr Reihen,
d.h. Satz 4.5.16, direkt anzuwenden.
Wir verwenden die zweite MoĚglichkeit,
naĚmlich die Substitution y := x3 .
Pâ yn
Dann haben wir die Reihe n=0 2n , fuĚr die wir nach dem Satz von Hadamard wegen
r
1
1
1
n
lim
= lim =
n
nââ
nââ 2
2
2
den Konvergenzradius 2 bekommen.
Nun muĚssen wir aber aufpassen: Das bedeutet Konvergenz fuĚr alle y = x3 â
(â2, 2) undâDivergenz
auĂerhalb von [â2, 2]. Also haben wir Konvergenz fuĚr
â
3
3
alle x â (â 2, 2) und Divergenz auĂerhalb des abgeschlossenen Intervalls
mit denselben
Grenzen. Der Konvergenzradius der urspruĚnglichen Reihe ist
â
3
also 2.
UĚbungsaufgabe 4.9.12. Bestimmen Sie die Konvergenzradien der Potenzreihen
â
X
(â1)n
n=0

(2n)!

z

2n

und

â
X
(â1)n 2n+1
z
(2n + 1)!
n=0

in C. Diese werden uns spaĚter noch einmal begegnen.

185

4. Analysis â Teil I: Konvergenz und Stetigkeit
Im Abschnitt 4.5.2 haben wir das Produkt zweier absolut konvergenter Reihen
mit Hilfe des Cauchyprodukts bestimmt. Die entsprechende Rechnung fuĚr Potenzreihen ergibt das folgende Resultat, dessen Beweis als UĚbungsaufgabe verbleibt.
P
Pâ
n
n
Satz 4.9.13. Es seien â
n=0 an x und
n=0 bn x Potenzreihen in K mit Konvergenzradien r1 , r2 > 0. Dann hat die Potenzreihe
n
â X
X

ak bnâk xn

n=0 k=0

mindestens den Konvergenzradius r := min{r1 , r2 } und es gilt fuĚr alle x â K mit
|x| < r
â
â
â X
n
X
X

X
n
n
ak bnâk x =
an x
bn xn .
n=0 k=0

n=0

n=0

Von groĂer Bedeutung fuĚr die Analysis ist der folgende Satz, den wir hier nicht
beweisen wollen.
Pâ
n
Satz 4.9.14. Es sei
n=0 an x eine Potenzreihe in K mit Konvergenzradius
r > 0. Dann
dadurch gegebene Funktion f : {x â K : |x| < r} â K mit
Pâ ist die
n
f (x) = n=0 an x stetig auf {x â K : |x| < r}.
Bemerkung 4.9.15. Damit ist insbesondere die Exponentialfunktion eine stetige Funktion auf R, bzw. C. Wir koĚnnen das nun nutzen um fuĚr das Bild
E(R) = {ex : x â R} = (0, â)
zu zeigen.
Bereits in Beispiel 4.7.17 haben wir gesehen, dass ex > 0 fuĚr alle x â R gilt. Also
ist E(R) â (0, â) und es bleibt die umgekehrte Inklusion zu zeigen.
Sei dazu y0 â (0, â). Wir wissen aus Beispiel 4.7.6 (b), dass
lim E(x) = 0 und

xâââ

lim E(x) = â

xââ

gilt. Also gibt es ein a â R, so dass E(a) < y0 gilt und ein b â R mit E(b) >
y0 . Da damit zwangslaĚufig E(a) < E(b) gilt und die Exponentialfunktion nach
Beispiel 4.7.17 streng monoton wachsend ist, muss a < b gelten. Mit der soeben
festgestellten Stetigkeit der Exponentialfunktion sind alle Voraussetzungen des
Zwischenwertsatzes 4.7.23 erfuĚllt. Es gibt also ein x0 â R mit E(x0 ) = y0 . Damit
ist y0 â E(R) und wir sind fertig.
Beispiel 4.9.16. Die Stetigkeit von Funktionen, die durch Potenzreihen gegeben
sind, kann man auch ausnutzen, um Grenzwerte zu bestimmen. Wir betrachten
als Beispiel
ex â 1
lim
.
xâ0
x

186

4.10. Wichtige Funktionen
FuĚr alle x â R gilt
 1 X xn X xnâ1 X xn
1 X xn
ex â 1
=
â1 =
=
=
.
x
x n=0 n!
x n=1 n!
n!
(n
+
1)!
n=1
n=0
â

â

â

â

Mit dem Quotientenkriterium stellt man leicht fest, dass die Potenzreihe, die
wir so am Ende erhalten haben, ebenfalls Konvergenzradius Unendlich hat. Die
dadurch gegeben Funktion ist also auf R und insbesondere in Null stetig. Damit
gilt
â
â
X
X
ex â 1
xn
0n
lim
= lim
=
= 1.
xâ0
xâ0
x
(n + 1)! n=0 (n + 1)!
n=0

4.10. Wichtige Funktionen
4.10.1. Exponentialfunktion und Logarithmus
Satz 4.10.1. Die Exponentialfunktion E : R â (0, â) ist bijektiv.
Beweis. Nach Beispiel 4.7.17 ist E streng monoton wachsend. Da auĂerdem nach
der UĚberlegung in Bemerkung 4.9.15 E(R) = (0, â) ist, liefert Satz 4.7.18 die
BijektivitaĚt von E.
Definition 4.10.2. Die Umkehrfunktion von E : R â (0, â) wird mit ln :=
E â1 : (0, â) â R bezeichnet und heiĂt natuĚrlicher Logarithmus.
Der Logarithmus hat die folgenden Eigenschaften.
Satz 4.10.3.
noton.

(a) Die Funktion ln ist auf (0, â) stetig und waĚchst streng mo-

(b) Es gilt ln(1) = 0 und ln(e) = 1.
(c) lim ln(x) = â und lim ln(x) = ââ.
xââ

xâ0+

(d) FuĚr alle x, y â (0, â) und q â Q gilt
x
ln(xy) = ln(x) + ln(y),
ln
= ln(x) â ln(y),
y
Beweis.

ln(xq ) = q ln(x).

(a) Dies liefert Satz 4.7.18.

(b) Ergibt sich aus e0 = 1 und e1 = e, vgl. die UĚberlegungen nach Satz 4.5.20.
(c) Ergibt sich aus limxââ ex = â und limxâââ ex = 0, vgl. Beispiel 4.7.6 (b).

187

4. Analysis â Teil I: Konvergenz und Stetigkeit
(d) Wir setzen Îž := ln(x) und Îˇ := ln(y). Dann gilt nach der Funktionalgleichung der Exponentialfunktion in Satz 4.5.20
eÎž+Îˇ = eÎž eÎˇ = eln(x) eln(y) = xy.
Also ist
ln(xy) = ln(eÎž+Îˇ ) = Îž + Îˇ = ln(x) + ln(y).
Die zwei anderen Rechenregeln verbleiben als UĚbungsaufgabe.

4

2

K

0

1

1

2

3

4

x

K

2

K

4

E(x)

ln(x)

Abbildung 4.4.: Die Graphen der Exponentialfunktion und des Logarithmus
Sehen wir uns die Aussage in (d) noch einmal an, so folgt daraus insbesondere
q
fuĚr alle a â (0, â) und alle q â Q die Beziehung aq = eln(a ) = eq ln(a) . Diese
verwenden wir nun um die allgemeine Potenzfunktion zu definieren.
Definition 4.10.4. FuĚr alle a â (0, â) und alle x â R definieren wir die allgemeine Potenz durch
ax := exÂˇln(a) .
Satz 4.10.5. Es sei a â (0, â). Dann ist die Funktion x 7â ax stetig auf R und
es gelten die bekannten Rechenregeln fuĚr Potenzen wie beispielsweise
1
aâ1 = , (ax )y = axy .
a
Beweis. Wir beobachten zunaĚchst, dass die beiden Funktionen x 7â x Âˇ ln(a)
und y 7â ey jeweils auf R stetig sind, also ist auch die Potenzfunktion als deren
Verkettung nach Satz 4.7.14 stetig.
Die Rechenregeln lassen sich alle direkt aus jenen fuĚr die Exponentialfunktion
ableiten. Wir behandeln deshalb hier nur beispielhaft
ax+y = ax ay ,

ax+y = e(x+y) ln(a) = ex ln(a)+y ln(a) = ex ln(a) ey ln(a) = ax ay .

188

4.10. Wichtige Funktionen

4.10.2. Trigonometrische Funktionen
In UĚbungsaufgabe 4.9.12 haben Sie herausgefunden, dass der Konvergenzradius
der beiden dort betrachteten Potenzreihen jeweils unendlich ist. Beide definieren
also je eine auf ganz C, bzw. R, stetige Funktion. Diese beiden bekommen jetzt
(wahrscheinlich vertraute) Namen.
Definition 4.10.6.
â
X
(â1)n 2n+1
sin(z) :=
z
,
(2n + 1)!
n=0

cos(z) :=

â
X
(â1)n
n=0

(2n)!

z 2n ,

z â C,

(Sinus)

z â C,

(Cosinus)

Bemerkung 4.10.7. Die so definierten Funktionen stimmen natuĚrlich mit den
Ihnen bekannten Kreisfunktionen uĚberein, die uĚber die VerhaĚltnisse von Dreiecksseiten definiert sind. Zumindest solange man alle Winkel im BogenmaĂ ausdruĚckt.
Dieses ist in der gesamten Mathematik uĚblich.
Ein Winkel wird im BogenmaĂ durch die LaĚnge des Bogens des Einheitskreises
beschrieben, der durch diesen Winkel gebildet wird, vgl. Abbildung 4.5
1
1
Îą
1

Abbildung 4.5.: Das BogenmaĂ eines Winkels
In der folgenden Tabelle finden Sie die wichtigsten Werte von Sinus und Cosinus,
zusammen mit den Winkeln in Grad und im BogenmaĂ.
0âŚ
0
sin

0

cos

1

30âŚ

45âŚ

60âŚ

90âŚ

Ď
6
1
â2
3
2

Ď
4
â1
2
â1
2

Ď
â3
3
2
1
2

Ď
2

1
0

Die Graphen von Sinus und Cosinus finden Sie in Abbildung 4.7.

189

4. Analysis â Teil I: Konvergenz und Stetigkeit
Aus der geometrischen Anschauung ergibt sich sofort der folgende Zusammenhang, vgl. Abbildung 4.6.
Satz 4.10.8 (Trigonometrischer Pythagoras).
sin2 (x) + cos2 (x) = 1 fuĚr alle x â R.
Einen analytischen Beweis dieser Behauptung werden wir in Bemerkung 5.2.3
sehen.
1
1
sin(Îą)

Îą

.

cos(Îą)

1

Abbildung 4.6.: Trigonometrischer Pythagoras
Definition 4.10.9. Eine Funktion f : R â R oder f : C â C heiĂt
(a) ungerade, falls f (âx) = âf (x) fuĚr alle x â R, bzw. C, gilt.
(b) gerade, falls f (âx) = f (x) fuĚr alle x â R, bzw. C, gilt.
(c) periodisch mit Periode L â R, bzw. C, wenn f (x + L) = f (x) fuĚr alle
x â R, bzw. C gilt.
Betrachtet man die Definitionen von Sinus und Cosinus, so findet man fuĚr alle
zâC
sin(âz) =

â
â
â
X
X
X
(â1)n
(â1)3n+1 2n+1
(â1)n 2n+1
(âz)2n+1 =
z
z
=â
(2n
+
1)!
(2n
+
1)!
(2n
+
1)!
n=0
n=0
n=0

= â sin(z)
und
cos(âz) =

â
X
(â1)n
n=0

(2n)!

(âz)2n =

â
X
(â1)n
n=0

(2n)!

z 2n = cos(z).

Also haben wir
Satz 4.10.10. Der Cosinus ist gerade und der Sinus ist ungerade.

190

4.10. Wichtige Funktionen
Der Beweis des folgenden wichtigen Zusammenhangs zwischen den trigonometrischen Funktionen und der Exponentialfunktion ergibt sich durch geradliniges
Umformen der Potenzreihen. Er verbleibt als UĚbungsaufgabe.
Satz 4.10.11 (Eulersche Formel). FuĚr alle z â C gilt
eiz = cos(z) + sin(z)i.
Insbesondere gilt fuĚr alle x â R damit
Re(eix ) = cos(x)

und

Im(eix ) = sin(x).

Daraus ergeben sich nun einige wichtige Formeln fuĚr die trigonometrischen Funktionen.
Satz 4.10.12. FuĚr alle x, y â R gilt
(a) | sin(x)| â¤ 1 und | cos(x)| â¤ 1.
(b) Additionstheoreme:
sin(x + y) = sin(x) cos(y) + sin(y) cos(x),
cos(x + y) = cos(x) cos(y) â sin(x) sin(y).
(c) Rechenregeln fuĚr verschobene Funktionen:
cos(x + Ď/2) = â sin(x),
cos(x + Ď) = â cos(x),
cos(x + 2Ď) = cos(x).

sin(x + Ď/2) = cos(x),
sin(x + Ď) = â sin(x),
sin(x + 2Ď) = sin(x),

Insbesondere sind Sinus und Cosinus periodisch mit Periode 2Ď.
Beweis.

(a) Mit der Eulerschen Formel gilt fuĚr alle x â R
q
ix
|e | = cos(x) + sin(x)i = cos2 (x) + sin2 (x) = 1

nach dem trigonometrischen Pythagoras, vgl. Satz 4.10.8. Insbesondere sind
| cos(x)| = |Re(eix )| â¤ |eix | = 1
ix

und

ix

| sin(x)| = |Im(e )| â¤ |e | = 1.

(b) Es gilt mit der Funktionalgleichung der Exponentialfunktion und der Eulerschen Formel


ei(x+y) = eix eiy = cos(x) + sin(x)i cos(y) + sin(y)i

= cos(x) cos(y) â sin(x) sin(y) + cos(x) sin(y) + sin(x) cos(y) i.
191

4. Analysis â Teil I: Konvergenz und Stetigkeit
Betrachtet man nun den Real- und den ImaginaĚrteil dieser Gleichung, so
findet man
cos(x + y) = Re(ei(x+y) ) = cos(x) cos(y) â sin(x) sin(y)
und
sin(x + y) = Im(ei(x+y) ) = cos(x) sin(y) + sin(x) cos(y).
(c) Die Formeln ergeben sich direkt aus den Additionstheoremen und den bekannten Werten cos(Ď/2) = 0 und sin(Ď/2) = 1. So erhaĚlt man z.B.
sin(x + Ď/2) = sin(x) cos(Ď/2) + sin(Ď/2) cos(x) = sin(x) Âˇ 0 + 1 Âˇ cos(x)
= cos(x).
und die Aussage fuĚr den Cosinus in einer analogen Rechnung. Damit kann
man dann beispielsweise weiter machen:
sin(x + Ď) = sin(x + Ď/2 + Ď/2) = cos(x + Ď/2) = â sin(x).
Die uĚbrigen Formeln ergeben sich auf die selbe Weise.
Im folgenden Satz werden alle Nullstellen von Sinus und Cosinus beschrieben.
Dieser bleibt hier ohne Beweis.
Satz 4.10.13. Es ist
sin(z) = 0 ââ z = kĎ fuĚr ein k â Z,
Ď
cos(z) = 0 ââ z = + kĎ fuĚr ein k â Z.
2
Neben Sinus und Cosinus taucht eine weitere trigonometrische Funktion haĚufiger
auf, der Tangens:
Definition 4.10.14. Die Funktion tan : C \ {Ď/2 + kĎ : k â Z} â C mit
tan(z) =

sin(z)
cos(z)

heiĂt Tangens.
Den Graphen der Tangens-Funktion finden Sie ebenfalls in Abbildung 4.7.
In dieser Abbildung erkennt man auch, dass die folgenden Setzungen fuĚr Umkehrfunktionen von Sinus, Cosinus und Tangens sinnvoll sind.

192

4.10. Wichtige Funktionen
2

y

K2 p

1

Kp

0

p
x

2p

K1

Sinus

Cosinus

K2

Tangens

Arcustangens

Abbildung 4.7.: Die Graphen von Sinus, Cosinus, Tangens und Arcustangens
Definition 4.10.15.
arcsin : [â1, 1] â [âĎ/2, Ď/2]
arccos : [â1, 1] â [0, Ď]
arctan : R â (âĎ/2, Ď/2)

(Arcussinus),
(Arcuscosinus),
(Arcustangens).

Vor allem der Arcustangens taucht immer mal wieder auf. Sein Graph ist auch
in Abbildung 4.7 dargestellt.
Beispiel 4.10.16. Wir wollen im R2 den Winkel Îą bestimmen, den ein Vektor
(x, y)T 6= (0, 0)T p
zur positiven x-Achse hat. Dazu macht man sich klar, dass mit
r := k(x, y)k2 = x2 + y 2 gerade
x
y
und
cos(Îą) = ,
sin(Îą) =
r
r
also
y
sin(Îą)
=
tan(Îą) =
cos(Îą)
x
gilt. Mit der einfachen Schlussfolgerung, dann sei Îą = arctan(y/x) sollten wir
nun allerdings vorsichtig sein, denn das gilt nur fuĚr Îą zwischen âĎ/2 und Ď/2,
vgl. den obigen Wertebereich des Arcustangens. Die volle Wahrheit ist:
ďŁą

ďŁ´
arctan xy ,
x > 0,
ďŁ´
ďŁ´

ďŁ´
y
ďŁ´
ďŁ´
x < 0 und y > 0,
ďŁ˛arctan x  + Ď,
y
Îą = arctan x â Ď, falls x < 0 und y < 0,
ďŁ´
ďŁ´
Ď
ďŁ´
,
x = 0 und y > 0,
ďŁ´
2
ďŁ´
ďŁ´
ďŁłâ Ď ,
x = 0 und y < 0.
2
193

4. Analysis â Teil I: Konvergenz und Stetigkeit

4.10.3. Die Polardarstellung komplexer Zahlen
Unsere neuen Erkenntnisse geben uns ein sehr wertvolles Werkzeug zur Beschreibung von komplexen Zahlen in die Hand, das wir nun kennenlernen.
Wir haben komplexe Zahlen als Elemente der Ebene R2 bisher durch ihren Realund ImaginaĚrteil, sozusagen in kartesischen Koordinaten, beschrieben. Einen Vektor der Ebene kann man aber auch anders beschreiben, indem man seine LaĚnge
und den in Beispiel 4.10.16 betrachteten Winkel, den er zur positiven x-Achse
bildet, angibt. Das fuĚhrt auf die sogenannten Polarkoordinaten.
Definition
4.10.17. Es sei z = x + yi â C \ {0} mit x, y â R. Dann heiĂt
p
2
r := x + y 2 der Betrag von z und der Winkel Ď, der zwischen z und der
positiven reellen Achse eingeschlossen wird das Argument von z.
Beide Werte (r, Ď) zusammen sind die Polarkoordinaten von z.
Bemerkung 4.10.18. (a) Man beachte, dass das Argument so eigentlich gar
nicht ordentlich definiert ist, da es mehrdeutig ist. Die Polarkoordinaten
(1, Ď) und (1, 3Ď) beschreiben dieselbe komplexe Zahl, naĚmlich â1. Um
diese Mehrdeutigkeit zu eliminieren, legt man sich uĚblicherweise auf ein
Intervall der LaĚnge 2Ď fest, aus dem das Argument gewaĚhlt wird. Meist
wird mit (âĎ, Ď] oder mit [0, 2Ď) gearbeitet.
(b) Ist das Argument auf den Bereich (âĎ, Ď] festgelegt, so gelten die folgenden
Umrechnungsformeln zwischen den Darstellungen fuĚr eine komplexe Zahl
z = x + yi mit Polarkoordinaten (r, Ď), vgl. Beispiel 4.10.16:
x = r cos(Ď),
y = r sin(Ď),
p
r = x2 + y 2 ,
ďŁą

y
ďŁ´
arctan
,
x > 0,
ďŁ´
x
ďŁ´
ďŁ´
y
ďŁ´
ďŁ´
ďŁ˛arctan x  + Ď, falls x < 0 und
Ď = arctan xy â Ď,
x < 0 und
ďŁ´
ďŁ´
Ď
ďŁ´
,
x = 0 und
ďŁ´
2
ďŁ´
ďŁ´
ďŁłâ Ď ,
x = 0 und
2

y
y
y
y

> 0,
<0
> 0,
< 0.

(c) FuĚr jede komplexe Zahl z auf dem Einheitskreis, d.h. |z| = 1, gilt z =
cos(Ď) + sin(Ď)i mit einem geeigneten Ď â (âĎ, Ď], vgl. Abbildung 4.6.
TatsaĚchlich gilt fuĚr jedes z â C \ {0} mit Polarkoordinaten (r, Ď)

z
= r cos(Ď) + sin(Ď)i = reiĎ .
z = |z|
|z|
Das ist die gebraĚuchlichste und praktischste Art mit Polarkoordinaten zu
arbeiten. Die Bedeutung sieht man z.B. im folgenden Satz.

194

4.10. Wichtige Funktionen
Satz 4.10.19. Es seien z = reiĎ , w = seiĎ â C \ {0} mit Polarkoordinaten (r, Ď),
bzw. (s, Ď) gegeben. Dann hat z Âˇ w die Polarkoordinaten (rs, Ď + Ď) und z/w die
Polarkoordinaten (r/s, Ď â Ď)
Beweis. In der Exponentialschreibweise sieht man sofort
zw = reiĎ seiĎ = rsei(Ď+Ď)
und
reiĎ
r
z
= iĎ = ei(ĎâĎ) ,
w
se
s
woraus die Behauptung folgt.
Beispiel 4.10.20. (a) Wir berechnen (1+i)2011 . Die Polarkoordinaten von 1+i
ergeben
sich nach Bemerkung 4.10.18 (b) (oder durch Anschauung) zu r =
â
2 und Ď = Ď/4. Also ist
â
â 2011 2008Ď 3Ď
Ď
i 3Ď
4
(1 + i)2011 = r 2011 eiÂˇ2011 4 = 2 ei( 4 + 4 ) = 21005 2 |eiÂˇ502Ď
{z } e
=1
â i 3Ď
1005
1005
4
2e = 2 (â1 + i).
=2
(b) Wir bestimmen alle komplexen LoĚsungen von z 5 = 1 in Polardarstellung.
Ist z = reiĎ , so muss also gelten
5
1 = z 5 = reiĎ = r 5 ei5Ď .

Also muss r 5 = 1 und damit r = 1 sein, sowie ei5Ď = 1. Das ist dann der
Fall, wenn 5Ď = 2kĎ fuĚr ein k â Z gilt, also falls Ď = 2kĎ/5 fuĚr ein k â Z
ist. Das liefert fuĚnf moĚgliche Argumente zwischen âĎ und Ď:
4
Ď1 = â Ď,
5

2
Ď2 = â Ď,
5

Ď3 = 0,

2
Ď4 = Ď
5

4
und Ď5 = Ď.
5

UĚbungsaufgabe 4.10.21. Beweisen Sie die Formel von De Moivre: Ist z =
reiĎ â C \ {0}, so gilt fuĚr jedes n â N

z n = r n cos(nĎ) + sin(nĎ)i .

4.10.4. Hyperbolische Funktionen

Definition 4.10.22.
ez â eâz
sinh(z) :=
, zâC
(Sinus hyperbolicus)
2
ez + eâz
cosh(z) :=
, zâC
(Cosinus hyperbolicus)
2
o
n
Ď
sinh(z)
i:kâZ
(Tangens hyperbolicus)
, z â C \ kĎ +
tanh(z) :=
cosh(z)
2

195

4. Analysis â Teil I: Konvergenz und Stetigkeit

3

2

1

K

2

K

0

1

1

2

x

K

1

K

2

K

3

sinh(x)

cosh(x)

tanh(x)

Abbildung 4.8.: Die Graphen von Sinus, Cosinus und Tangens hyperbolicus
UĚbungsaufgabe 4.10.23. Zeigen Sie die folgenden Beziehungen
(a) FuĚr alle x â R gilt cosh2 (x) â sinh2 (x) = 1.
(b) FuĚr alle z = x + yi â C mit x, y â R gilt
sin(x + yi) = sin(x) cosh(y) + cos(x) sinh(y)i und
cos(x + yi) = cos(x) cosh(y) â sin(x) sinh(y)i.

196

5. Analysis â Teil II: Differentialund Integralrechnung
5.1. Differenzierbarkeit von Funktionen in einer
Variablen
5.1.1. Der Ableitungsbegriff
Schon aus der Schule werden Sie das Thema dieses Abschnitts kennen. Man
moĚchte das AĚnderungsverhalten einer Funktion in einem Punkt, d.h. anschaulich
gesprochen die Steigung des Funktionsgraphen an dieser Stelle quantitativ fassen.
Dazu naĚhert man die Tangentensteigung mit den bekannten Sekantensteigungen
an und kommt auf den Differenzenquotienten. Dessen Grenzwert, die Ableitung,
gibt dann die Steigung an. Auch die Differenzierbarkeit einer Funktion ist so im
Grunde nichts anderes als ein Grenzwertproblem, das wir mit unseren bisherigen
Erkenntnissen behandeln koĚnnen.
In der Differentialrechnung gibt es sehr groĂe Unterschiede in der Behandlung von
Funktionen auf R oder C. Wir werden in dieser Vorlesung nur den reellen Fall
behandeln. Die Untersuchung der Differenzierbarkeit von Funktionen komplexer
Variablen ist Gegenstand der sogenannten Funktionentheorie.
In diesem ganzen Kapitel sei I â R immer ein Intervall.
Definition 5.1.1. (a) Es sei x0 â I. Eine Funktion f : I â R heiĂt differenzierbar in x0 , wenn der Grenzwert
f (x) â f (x0 )
xâx0
x â x0
lim

in R existiert. In diesem Fall heiĂt dieser Grenzwert die Ableitung von f
in x0 und wird mit f â˛ (x0 ) bezeichnet.
(b) Eine Funktion f : I â R heiĂt differenzierbar auf I, falls sie in allen
Punkten x0 â I differenzierbar ist. In diesem Fall wird durch x 7â f â˛ (x)
fuĚr x â I eine Funktion f â˛ : I â R definiert. Diese Funktion heiĂt die
Ableitung oder auch Ableitungsfunktion von f auf I.

197

5. Analysis â Teil II: Differential- und Integralrechnung
Bemerkung 5.1.2. Der Grenzwert in obiger Definition existiert genau dann,
wenn der Grenzwert
f (x0 + h) â f (x0 )
lim
hâ0
h
existiert, und die Werte der beiden stimmen dann uĚberein. Man kann also je
nachdem, was in der jeweiligen Situation uĚbersichtlicher erscheint, den einen oder
den anderen Grenzwert untersuchen.
Beispiel 5.1.3.
Dann ist

(a) Es sei zunaĚchst f (x) = c â R konstant fuĚr alle x â I.
f (x) â f (x0 )
0
= lim
= 0,
xâx0
xâx0 x â x0
x â x0
lim

also ist f in I differenzierbar und es gilt f â˛ (x) = 0 fuĚr alle x â I.
(b) Wir betrachten I = R, x0 = 0 und
f (x) = |x|,
Dann gilt
f (x) â f (x0 )
|x|
=
=
x â x0
x

x â R.
(

1, fuĚr x > 0,
â1, fuĚr x < 0.

Also existiert der Grenzwert dieses Ausdrucks fuĚr x â x0 = 0 nicht, d.h. f
ist in 0 nicht differenzierbar. Man beachte, dass f aber in 0 stetig ist.
Wir haben soeben gesehen, dass es stetige Funktionen gibt, die nicht differenzierbar sind. Wir wollen nun zeigen, dass aber umgekehrt jede differenzierbare
Funktion notwendigerweise stetig ist.
Satz 5.1.4. Es sei f : I â R in x0 â I differenzierbar. Dann ist f stetig in x0 .
Beweis. Es gilt

f (x) â f (x0 )
lim f (x) â f (x0 ) = lim f (x) â f (x0 ) = lim
(x â x0 )
xâx0
xâx0
xâx0
x â x0
= f â˛ (x0 ) Âˇ 0 = 0.
Damit haben wir limxâx0 f (x) = f (x0 ), also ist f in x0 stetig.
Warnung 5.1.5. Es sei noch einmal darauf hingewiesen, dass die Umkehrung
dieses Satzes falsch ist, vgl. Beispiel 5.1.3 (b).
Wir berechnen beispielhaft noch weitere Ableitungen.

198

5.1. Differenzierbarkeit von Funktionen in einer Variablen
Beispiel 5.1.6.

(a) Wir betrachten
f (x) = x2 ,

x â R.

Dann gilt fuĚr jedes x0 â R
f (x) â f (x0 )
x2 â x20
(x â x0 )(x + x0 )
=
=
= x + x0 .
x â x0
x â x0
x â x0
Also ist
f (x) â f (x0 )
= lim (x + x0 ) = 2x0 .
xâx0
xâx0
x â x0
lim

Damit ist f auf R differenzierbar und es gilt f â˛ (x) = 2x, x â R.

Allgemein ist fuĚr jedes n â Nâ die Funktion g(x) := xn , x â R, auf ganz
R differenzierbar und es gilt g â˛(x) = nxnâ1 . Das kann man mit Hilfe von
Satz 4.2.9 (b) zeigen.

(b) Es sei wieder I = R und jetzt
f (x) = E(x) = ex ,

x â R.

Dann gilt
ex0 +h â ex0
ex0 eh â ex0
eh â 1
f (x0 + h) â f (x0 )
=
=
= ex0
.
h
h
h
h
Mit Hilfe von Beispiel 4.9.16 folgt daraus
f (x0 + h) â f (x0 )
= ex0 Âˇ 1 = ex0 .
hâ0
h
lim

Also ist die Exponentialfunktion auf R differenzierbar und es gilt E â˛ (x) =
ex = E(x).
Die folgende Umformulierung der Differenzierbarkeits-Definition wird uns spaĚter
retten, wenn wir Funktionen in mehreren Variablen differenzieren wollen.
Satz 5.1.7. Eine Funktion f : I â R ist in x0 â I genau dann differenzierbar
mit f â˛ (x0 ) = a, wenn
f (x) = f (x0 ) + a(x â x0 ) + r(x),

x â I,

ist und fuĚr die Funktion r : I â R gilt
|r(x)|
= 0.
xâx0 |x â x0 |
lim

199

5. Analysis â Teil II: Differential- und Integralrechnung
Beweis. Es gilt r(x) = f (x) â f (x0 ) â a(x â x0 ) und damit
r(x)
f (x) â f (x0 )
=
â a.
x â x0
x â x0

(5.1)

ââ Ist f in x0 differenzierbar mit f â˛ (x0 ) = a, so konvergiert fuĚr x gegen x0
â
nach der Definition der Differenzierbarkeit die rechte Seite der Gleichung
gegen Null. Also gilt dann auch
lim

xâx0

|r(x)|
= 0.
|x â x0 |

ââ Gilt umgekehrt diese Grenzwertbeziehung fuĚr r, so bekommen wir aus (5.1)
â
sofort
f (x) â f (x0 )
lim
âa =0
xâx0
x â x0
(x0 )
und das bedeutet gerade limxâx0 f (x)âf
= a, d.h. wir haben Differenzierxâx0
barkeit von f in x0 mit f â˛ (x0 ) = a.

Bemerkung 5.1.8. Diesen Satz kann man auch grafisch-anschaulich interpretieren. Die Funktion g(x) = f (x0 ) + a(x â x0 ) ist eine lineare Funktion, deren Graph
eine Gerade durch (x0 , f (x0 )) mit Steigung a ist. Der Satz sagt, dass die Ableitung von f in x0 gerade die Zahl a ist, fuĚr die der Fehler r bei Approximation der
Funktion f durch die Gerade mit Steigung a am kleinsten wird und diese Gerade
ist genau die Tangente an den Graphen von f , die durch f (x0 ) + f â˛ (x0 )(x â x0 ),
x â R, beschrieben wird.

5.1.2. Ableitungsregeln
Um kompliziertere Ableitungen berechnen zu koĚnnen, brauchen wir Rechenregeln.
Satz 5.1.9. Es seien f, g : I â R in x0 â I differenzierbar und Îą, Î˛ â R. Dann
gilt
(a) Îąf + Î˛g ist in x0 differenzierbar und
(Îąf + Î˛g)â˛(x0 ) = Îąf â˛ (x0 ) + Î˛g â˛(x0 ).

(LinearitaĚt)

(b) f g ist differenzierbar in x0 und
(f g)â˛(x0 ) = f â˛ (x0 )g(x0 ) + f (x0 )g â˛ (x0 ).

(Produktregel)

(c) Ist g(x0 ) 6= 0, so existiert ein Intervall J â I mit x0 â J und g(x) 6= 0 fuĚr
alle x â J. AuĂerdem ist die Funktion f /g : J â R differenzierbar in x0
und es gilt
 f â˛
f â˛ (x0 )g(x0 ) â f (x0 )g â˛ (x0 )
(x0 ) =
.
(Quotientenregel)
2
g
g(x0 )
200

5.1. Differenzierbarkeit von Funktionen in einer Variablen
Beweis. Die Aussagen (a) und (b) behandeln wir als UĚbungsaufgaben.
Zum Beweis von (c) muĚssen wir zuerst die Existenz von J begruĚnden. Da g in x0
stetig ist, gibt es nach der Îľ-Î´-Charakterisierung der Stetigkeit in Satz 4.7.19 ein
Î´ > 0, so dass |g(x) â g(x0 )| < |g(x0 )|/2 fuĚr alle x â (x0 â Î´, x0 + Î´) =: J gilt.
FuĚr diese x ist dann |g(x)| > |g(x0 )|/2 > 0, also insbesondere g(x) 6= 0.
Weiter gilt fuĚr alle x â J
f (x)
g(x)

â

f (x0 )
g(x0 )

x â x0

1
f (x)g(x0 ) â f (x0 )g(x)
Âˇ
g(x)g(x0 )
x â x0
f (x)g(x0 ) â f (x0 )g(x0 ) + f (x0 )g(x0 ) â f (x0 )g(x)
1
Âˇ
=
g(x)g(x0 )
x â x0


1
f (x) â f (x0 )
g(x) â g(x0 )
=
g(x0 ) â
f (x0 ) .
g(x)g(x0 )
x â x0
x â x0

=

Da g in x0 differenzierbar ist, ist diese Funktion insbesondere in x0 stetig (vgl.
Satz 5.1.4), also koĚnnen wir in obiger Gleichung zum Grenzwert fuĚr x gegen x0
uĚbergehen und erhalten die Behauptung.
Es folgt sogleich die Rechenregel fuĚr die Verkettung differenzierbarer Funktionen.
Satz 5.1.10 (Kettenregel). Es seien I, J â R Intervalle und g : I â J sei
differenzierbar in x0 â I. Weiter sei f : J â R differenzierbar in y0 = g(x0 ).
Dann ist auch die Funktion f âŚ g : I â R differenzierbar in x0 und es gilt
(f âŚ g)â˛ (x0 ) = f â˛ (g(x0 )) Âˇ g â˛(x0 ).
Beweis. Wir betrachten die Hilfsfunktion fË : J â R mit
ďŁą
ďŁ˛ f (y) â f (y0 ) , fuĚr y â J mit y 6= y ,
0
Ë
y â y0
f (y) =
ďŁł â˛
f (y0 ),
fuĚr y = y0 .

Dann gilt

fË(y)(y â y0 ) = f (y) â f (y0 )

(5.2)

fuĚr alle y â J (auch fuĚr y0 !). Da f in y0 differenzierbar ist, haben wir nun
lim fË(y) = fË(y0 ) = f â˛ (y0 ) = f â˛ (g(x0 )),

yây0

insbesondere ist fË stetig in y0 . Nach Satz 5.1.4 ist g stetig in x0 , und da die
Verkettung von stetigen Funktionen wieder stetig ist, sehen wir damit
lim fË(g(x)) = fË(g(x0 )) = f â˛ (g(x0 )).

xâx0

201

5. Analysis â Teil II: Differential- und Integralrechnung
Daher folgt schlieĂlich mit Hilfe von (5.2)

fË(g(x)) g(x) â g(x0 )
f (g(x)) â f (g(x0))
lim
= lim
xâx0
xâx0
x â x0
x â x0
g(x) â g(x0 )
= f â˛ (g(x0 )) Âˇ g â˛ (x0 ).
= lim fË(g(x))
xâx0
x â x0
Beispiel 5.1.11. Es sei a > 0 gegeben. Wir betrachten auf I = R die allgemeine
Potenzfunktion
Ď(x) := ax , x â R,

vgl. Definition 4.10.4. Dann gilt Ď(x) = ex ln(a) . Um die Kettenregel anzuwenden,
setzen wir f (y) := ey , y â R und g(x) := x ln(a), x â R, und haben so Ď = f âŚ g.
Da sowohl f als auch g auf ganz R differenzierbar sind, sind die Voraussetzungen
von Satz 5.1.10 erfuĚllt und es gilt
Ďâ˛ (x) = f â˛ (g(x))g â˛(x) = eg(x) ln(a) = ex ln(a) ln(a) = ax ln(a).
Wir koĚnnen sogar eine allgemeine Rechenregel fuĚr die Ableitung der Umkehrfunktion angeben.

Satz 5.1.12. Es sei f â C(I) streng monoton und in x0 â I differenzierbar
mit f â˛ (x0 ) 6= 0. Dann existiert die Umkehrfunktion f â1 : f (I) â R, diese ist
differenzierbar in y0 = f (x0 ) und es gilt
(f â1 )â˛ (y0 ) =

1
f â˛ (x0 )

.

Beweis. Die Existenz der Umkehrfunktion folgt sofort aus der strengen Monotonie von f , vgl. Satz 4.7.18.
Zu gegebenem h 6= 0 setzen wir
k := f â1 (y0 + h) â f â1 (y0 ) = f â1 (y0 + h) â x0 .

Da f â1 nach Satz 4.7.18 in y0 stetig ist, folgt aus h â 0 sofort k â 0. AuĂerdem
ist x0 + k = f â1 (y0 + h), d.h. f (x0 + k) = y0 + h und wir erhalten
h = f (x0 + k) â f (x0 ).
Nun ist damit fuĚr h 6= 0


f â1 f (x0 + k) â x0
f â1 (y0 + h) â f â1 (y0 )
x0 + k â x0
=
=
h
f (x0 + k) â f (x0 )
f (x0 + k) â f (x0 )
k
=
f (x0 + k) â f (x0 )

und wir erhalten
f â1 (y0 + h) â f â1 (y0 )
k
1
lim
= lim
= â˛
,
hâ0
kâ0 f (x0 + k) â f (x0 )
h
f (x0 )
da f â˛ (x0 ) 6= 0 gilt.

202

5.1. Differenzierbarkeit von Funktionen in einer Variablen
Bemerkung 5.1.13. Man beachte, dass die Voraussetzung f â˛ (x0 ) 6= 0 notwendig
ist. Als Beispiel diene hierzu I = [0, â) und f (x) = x2 . Dann â
ist f â˛ (x) = 2x und
somit f â˛ (0) = 0. TatsaĚchlich ist die Umkehrfunktion f â1 (x) = x in x0 = 0 nicht
differenzierbar, denn es gilt
lim

xâ0+

â

â
â
xâ 0
x
1
= lim
= lim â = â.
xâ0+ x
xâ0+
xâ0
x

Beispiel 5.1.14. (a) Wir bestimmen die Ableitung des Logarithmus als Umkehrfunktion der Exponentialfunktion. Sei dazu I = R und f (x) = ex auf I.
Dann ist f â1 (x) = ln(x), x â (0, â), und mit Satz 5.1.12 gilt fuĚr y = f (x)
die Beziehung
(ln)â˛ (y) = (f â1 )â˛ (y) =

1
f â˛ (x)

=

1
1
1
= ln(y) = ,
x
e
e
y

y â (0, â).

(b) FuĚr x > 0, Îą â R und f (x) := xÎą = eÎą ln(x) erhalten wir
f â˛ (x) = eÎą ln(x) (Îą ln(x))â˛ = xÎą

Îą
= ÎąxÎąâ1 .
x

Die Ableitungsregel fuĚr die ganzzahlige Potenz aus Beispiel 5.1.6 (a) verallgemeinert sich also auch auf die allgemeine Potenz, solange x > 0.
Insbesondere haben wir im Fall Îą = 1/2
â
1
( Âˇ)â˛ (x) = â ,
2 x

x > 0.

(c) Der Arcustangens ist die Umkehrfunktion des Tangens. Seine Ableitung
ergibt sich also uĚberall da wo die Ableitung des Tangens tanâ˛ nicht Null ist
zu
1
arctanâ˛ (x) =
â˛
tan (arctan(x))
Unsere Aufgabe bleibt es die Ableitung des Tangens zu bestimmen. Dazu
dient der folgende Satz.
P
n
Potenzreihe in R mit KonvergenzraSatz 5.1.15. Es sei f (x) = â
n=0 an x eineP
nâ1
dius r > 0. Dann hat auch die Potenzreihe â
den Konvergenzradius
n=1 nan x
r, die Funktion f ist in allen x â (âr, r) differenzierbar und es gilt
f â˛ (x) =

â
X
n=1

nan xnâ1 ,

x â (âr, r).

203

5. Analysis â Teil II: Differential- und Integralrechnung
In Worten formuliert bedeutet dieser Satz, dass man eine Potenzreihe im Inneren
ihres Konvergenzgebietes summandenweise, unter dem Summenzeichen differenzieren darf. Das ist nicht selbstverstaĚndlich, da dabei die Reihenfolge von zwei
Grenzwertprozessen vertauscht wird, einmal bildet man die Ableitung der Reihe,
im Anderen Fall die Reihe der Ableitungen.
Auf einen Beweis wollen wir hier verzichten, sondern direkt den Nutzen daraus ziehen, dass wir nun die Ableitungen unserer durch Potenzreihen gegebenen
Funktionen bestimmen koĚnnen.
Beispiel 5.1.16. (a) Die Potenzreihen von Sinus und Cosinus konvergieren
beide auf ganz R. Also sind beide nach Satz 5.1.15 auf ganz R differenzierbar und die Ableitungen berechnen sich zu
â
â
X
(â1)n 2n+1 â˛ X (â1)n
x
(2n + 1)x2n
=
sin (x) =
(2n
+
1)!
(2n
+
1)!
n=0
n=0
â˛

=

â
X
(â1)n
n=0

(2n)!

x2n = cos(x)

und analog
cosâ˛ (x) = â sin(x).
(b) Damit haben wir nach der Quotientenregel fuĚr alle x â R mit cos(x) 6= 0
sinâ˛ (x) cos(x) â cosâ˛ (x) sin(x)
cos2 (x) + sin2 (x)
tan (x) =
=
.
cos2 (x)
cos2 (x)
â˛

Das laĚsst sich nun auf zwei verschiedene Weisen vereinfachen, durch kuĚrzen
und mit Hilfe des trigonometrischen Pythagoras. Damit erhaĚlt man
tanâ˛ (x) = 1 + tan2 (x) =

1
.
cos2 (x)

Je nach Kontext sind beide Darstellungen von Nutzen.
Damit koĚnnen wir nun die Ableitung des Arcustangens aus Beispiel 5.1.14
fertig bestimmen. Dort hatten wir
arctanâ˛ (x) =

1
1
1
=
=
.
2
tan (arctan(x))
1 + tan (arctan(x))
1 + x2
â˛

Wir sammeln unseren Funktionenzoo noch einmal in einer Tabelle:

204

5.1. Differenzierbarkeit von Funktionen in einer Variablen
Name
E-funktion
(nat.) Logarithmus
Sinus
Cosinus
Tangens
Arcussinus
Arcuscosinus
Arcustangens
Sinus hyperbolicus
Cosinus hyp.
Tangens hyp.

Symbol
eÂˇ
ln
sin
cos
tan
arcsin
arccos
arctan
sinh
cosh
tanh

Definitionsbereich
R
(0, â)
R
R
R \ {(k + 1/2)Ď}
[â1, 1]
[â1, 1]
R
R
R
R

Bild
(0, â)
R
[â1, 1]
[â1, 1]
R
[âĎ/2, Ď/2]
[0, Ď]
(âĎ/2, Ď/2)
R
[1, â)
(â1, 1)

Ableitung
eÂˇ
1
x

cos
â sin
1
= 1 + tan2
cos2
â 1
1âx2
1
â â1âx
2
1
1+x2

cosh
sinh
1
cosh2

= 1 â tanh2

Den Satz uĚber die Differenzierbarkeit von Potenzreihen koĚnnen wir aber auch
andersherum gewinnbringend verwenden, um den Reihenwert von Potenzreihen
auszurechnen. Wir betrachten das an einem Beispiel.
P
n
Beispiel 5.1.17. Wir betrachten die Potenzreihe â
n=1 nx . Diese hat nach dem
Satz von Hadamard den Konvergenzradius 1. Also ist auf (â1, 1) durch diese
Reihe eine Funktion gegeben. Aber welche?
Dazu uĚberlegen wir uns, dass fuĚr alle x â (â1, 1) nach Satz 5.1.15 gilt
â
X
n=1

n

nx = x

â
X

nâ1

nx

n=1

=x

â
X
n=1

â
X
â˛
n
(x ) = x
x .
n â˛

n=1

Die Reihe, die nun ganz rechts steht, ist, bis auf den fehlenden ersten Summanden,
die uns schon bekannte geometrische Reihe. Also bekommen wir fuĚr alle x â
(â1, 1)
â
 1
â˛
X
x
â1
n
nx = x
â1 =x
(â1) =
.
2
1âx
(1 â x)
(1 â x)2
n=0

5.1.3. HoĚhere Ableitungen
Wir beginnen diesen Abschnitt mit einem abschreckenden Beispiel.
Beispiel 5.1.18. Wir betrachten auf I = [0, â) die Funktion
f (x) :=

(

x3/2 sin(1/x),
0,

falls x > 0,
falls x = 0,

vgl. Abbildung 5.1.

205

5. Analysis â Teil II: Differential- und Integralrechnung

Abbildung 5.1.: Der Graph der Funktion f (x) = x3/2 sin(1/x)
Dann ist f in allen x > 0 offensichtlich differenzierbar und es gilt
 1
3â
f â˛ (x) =
x sin(1/x) + x3/2 cos(1/x) â 2
2
x
1
3â
x sin(1/x) â â cos(1/x).
=
2
x
Um f auf Differenzierbarkeit in 0 zu untersuchen, betrachten wir den Differenzenquotienten
lim

xâ0

â
x3/2 sin(1/x)
f (x) â f (0)
= lim
= lim x sin(1/x)
xâ0
xâ0
xâ0
x
â
â¤ lim x = 0.
xâ0

Da der Grenzwert selbst uĚber etwas Positives gebildet wird, muss er auch groĚĂer
oder gleich Null und damit gleich Null sein. Somit ist f auf ganz [0, â) differenzierbar, wobei f â˛ (0) = 0 gilt.
Anhand dieses Beispiels sieht man nun, dass etwas Abstruses passieren kann.
WaĚhrend f in Null differenzierbar ist, ist die Funktion f â˛ : [0, â) â R in
Null nicht einmal mehr stetig. Um das zu sehen, betrachten wir die Folge xn :=
1/(2nĎ), n â Nâ . Dann gilt fuĚr jedes n â Nâ
f â˛ (xn ) =

â
â
â â
3 1
â
sin(2nĎ) â 2nĎ cos(2nĎ) = â 2nĎ Âˇ 1 = â 2Ď n.
2 2nĎ

Also ist die Funktion f â˛ auf dem kompakten Intervall [0, 1] nicht beschraĚnkt, sie
kann also nach Satz 4.7.26 nicht stetig sein.
Definition 5.1.19. Ist f : I â R eine in I differenzierbare Funktion und ist f â˛
auf I stetig, so nennt man f stetig differenzierbar.
Man schreibt C 1 (I) := {f : I â R : f stetig differenzierbar}.

206

5.1. Differenzierbarkeit von Funktionen in einer Variablen
Ist eine Funktion stetig differenzierbar, so koĚnnen wir uns natuĚrlich die Frage
stellen, ob ihre Ableitungsfunktion selbst wieder differenzierbar ist. Das fuĚhrt
auf den Begriff der zweiten und allgemeiner n-ten Ableitung, die wir rekursiv
definieren.
Definition 5.1.20. (a) Es sei f : I â R differenzierbar auf I, x0 â I und
n â N mit n âĽ 2. Dann heiĂt die Funktion f in x0 (bzw. auf I) n mal
differenzierbar, falls sie auf I schon (n â 1) mal differenzierbar ist und die
Funktion f (nâ1) in x0 (bzw. auf I) wieder differenzierbar ist.
In diesem Fall heiĂt f (n) (x0 ) = (f (nâ1) )â˛ (x0 ) die n-te Ableitung von f in
x0 , bzw. x 7â f (n) (x) die n-te Ableitungsfunktion von f auf I.
(b) Ist die n-te Ableitung von f auf I selbst sogar wieder stetig auf I, so sagt
man f sei n-mal stetig differenzierbar auf I. Man schreibt
C n (I) := {f : I â R : f n-mal stetig differenzierbar}.
(c) Ist f â C n (I) fuĚr alle n â N, so nennt man f beliebig oft differenzierbar.
Man verwendet dafuĚr auch die Bezeichnung
\
f â C â (I) :=
C n (I).
nâN

Bemerkung 5.1.21. Es ist oft praktisch die Funktion selbst als ihre nullte Ableitung aufzufassen und entsprechend f (0) := f zu setzen.
Beispiel 5.1.22.

(a) Ist f (x) = sin(x), so gilt
f â˛ (x) = cos(x),

f â˛â˛ (x) = â sin(x),

f â˛â˛â˛ (x) = â cos(x), f (4) (x) = sin(x) = f (x).
(b) Betrachten wir auf R die Funktion
(
f (x) =

x2 , falls x âĽ 0
âx2 , falls x < 0,

so ist f auf ganz R differenzierbar mit f â˛ (x) = 2|x|, x â R (nachrechnen!), aber da die Betragsfunktion in Null nicht differenzierbar ist (vgl.
Beispiel 5.1.3 (b)), ist diese Funktion in Null nicht mehr differenzierbar,
d.h. f ist in x0 = 0 stetig differenzierbar aber nicht zweimal differenzierbar.
P
n
(c) Es sei f (x) = â
n=0 an (x â x0 ) , d.h. f sei durch eine Potenzreihe gegeben, von der wir annehmen wollen, dass der Konvergenzradius r > 0 ist.
207

5. Analysis â Teil II: Differential- und Integralrechnung
In Satz 5.1.15 haben wir gesehen, dass f dann auf I := (x0 â r, x0 + r)
differenzierbar ist mit
â
X

â˛

f (x) =

n=1

an n(x â x0 )nâ1 ,

x â I,

und dass dies wieder eine Potenzreihe mit Konvergenzradius r ist. Also ist
nach nochmaliger Anwendung dieses Satzes f sogar zweimal differenzierbar
auf I mit
â
X
â˛â˛
f (x) =
an n(n â 1)(x â x0 )nâ2 , x â I.
n=2

Durch weitere Iteration dieses Arguments (Formalisten moĚgen eine saubere
Induktion fuĚhren), ist dann f auf I beliebig oft differenzierbar und es gilt
fuĚr jedes k â Nâ
f

(k)

(x) =

â
X
n=k

an n(n â 1) Âˇ Âˇ Âˇ (n â (k â 1))(x â x0 )nâk ,

x â I.

Setzt man speziell x = x0 ein, so erhaĚlt man die fuĚr spaĚtere Betrachtungen
wichtige Beziehung
f (k) (x0 ) = ak k(k â 1) Âˇ Âˇ Âˇ 2 Âˇ 1 = k!ak .
Diese verraĚt uns insbesondere die Gestalt der Koeffizienten ak fuĚr Funktionen, die durch eine Potenzreihe dargestellt werden:
ak =

f (k) (x0 )
.
k!

UĚbungsaufgabe 5.1.23. Bestimmen Sie den Konvergenzradius der Potenzreihe
â
X
n(n â 1)
n=1

2

xn

und geben Sie eine geschlossene Darstellung, der durch diese Potenzreihe gegebenen Funktion an.

5.2. Eigenschaften differenzierbarer Funktionen
Wir sammeln wertvolle Eigenschaften differenzierbarer Funktionen. Den Start
bildet der Mittelwertsatz, den wir hier nicht beweisen wollen.

208

5.2. Eigenschaften differenzierbarer Funktionen
Satz 5.2.1 (Mittelwertsatz der Differenzialrechnung). Es seien a, b â R mit a < b
und f â C([a, b]) sei differenzierbar in (a, b). Dann gibt es ein Îž â (a, b), so dass
f (b) â f (a)
= f â˛ (Îž),
bâa

bzw. gleichbedeutend f (b) â f (a) = f â˛ (Îž)(b â a)

gilt.
Anschaulich bedeutet dieser Satz, dass die Sekantensteigung der Funktion, die
man anhand der beiden Punkte a und b erhaĚlt, irgendwann dazwischen tatsaĚchlich
als Tangentensteigung angenommen wird, vgl. Abbildung 5.2. Man kann sich das
verdeutlichen, indem man versucht, eine differenzierbare Funktion zu zeichnen,
fuĚr die das nicht gilt, was (hoffentlich) nicht klappen wird.
y
f(b)

e

ant

Sek

nte

e
ang

T

f(a)

Îž

a

b

x

Abbildung 5.2.: Der Mittelwertsatz der Differenzialrechnung
Wir wollen nun einige Folgerungen aus dem Mittelwertsatz ziehen.
Satz 5.2.2. (a) (Satz von Rolle) Es seien a, b â R mit a < b und f â
C([a, b]). Ist f auf (a, b) differenzierbar und gilt f (a) = f (b), so gibt es ein
Îž â (a, b) mit f â˛ (Îž) = 0.
(b) Es sei f : I â R auf dem Intervall I differenzierbar. Dann gilt
Ist
Ist
Ist
Ist
Ist

fâ˛
fâ˛
fâ˛
fâ˛
fâ˛

=
>
<
âĽ
â¤

0
0
0
0
0

auf
auf
auf
auf
auf

I,
I,
I,
I,
I,

so
so
so
so
so

ist
ist
ist
ist
ist

f
f
f
f
f

auf
auf
auf
auf
auf

I
I
I
I
I

konstant.
streng monoton wachsend.
streng monoton fallend.
monoton wachsend.
monoton fallend.

(c) Sind f, g : I â R auf I differenzierbare Funktionen und gilt f â˛ = g â˛ auf I,
so gibt es eine Konstante c â R, so dass f (x) = g(x) + c fuĚr alle x â I gilt.
Beweis.

(a) folgt direkt aus dem Mittelwertsatz.

209

5. Analysis â Teil II: Differential- und Integralrechnung
(b) Es seien a, b â I mit a < b. Dann gibt es nach dem Mittelwertsatz ein
Îž â (a, b) mit f (b) â f (a) = (b â a)f â˛ (Îž). Ist die Ableitung von f nun
konstant Null auf I, so muss also f (a) = f (b) gelten. Da a und b in I
beliebig waren, ist f auf I konstant.
Weiter ist der Ausdruck b â a immer positiv, also ergibt sich das Vorzeichen
von f (b) â f (a) direkt aus dem Vorzeichen von f â˛ (Îž). Daraus kann man die
4 restlichen Behauptungen sofort ablesen.
(c) Wir setzen h := f â g. Dann ist hâ˛ = f â˛ â g â˛ = 0 auf I, d.h. h ist konstant
nach (b).
Bemerkung 5.2.3. Mit Hilfe von Satz 5.2.2 (b) koĚnnen wir nun einen schnellen
Beweis des trigonometrischen Pythagoras aus Satz 4.10.8 geben. Wir berechnen
die Ableitung der Funktion f (x) = sin2 (x) + cos2 (x), x â R, zu
f â˛ (x) = 2 sin(x) cos(x) + 2 cos(x)(â sin(x)) = 0,
also ist f konstant. Den Wert der Konstante koĚnnen wir leicht bestimmen, denn
es ist
f (0) = sin2 (0) + cos2 (0) = 02 + 12 = 1.
Beispiel 5.2.4. Wir haben in Beispiel 5.1.16 (b) gesehen, dass tanâ˛ (x) = 1 +
tan2 (x) gilt. Damit ist die Ableitung des Tangens auf seinem gesamten Definitionsbereich positiv, vgl. Abbildung 4.7. Trotzdem ist er nicht auf dem ganzen
Definitionsbereich strikt monoton wachsend, wie ein Blick auf die selbe Abbildung
sofort zeigt.
Dieses Beispiel zeigt, dass Satz 5.2.2 (b) wirklich nur gilt, wenn das Vorzeichen
der Ableitung auf einem Intervall die jeweilige Voraussetzung erfuĚllt.
Beispiel 5.2.5. Der Mittelwertsatz ist auch ein wertvolles Hilfmittel um Differenzen abzuschaĚtzen. Als Beispiel zeigen wir, dass der Arcustangens auf R
Lipschitz-stetig ist. Wir muĚssen ein L âĽ 0 finden, so dass fuĚr alle x, y â R
gilt
arctan(x) â arctan(y) â¤ L|x â y|,
vgl. Definition 4.7.20. Seien also x, y â R mit x < y gegeben. Da der Arcustangens auf [x, y] differenzierbar ist, koĚnnen wir den Mittelwertsatz anwenden und
bekommen ein Îž â (x, y) mit
arctan(x) â arctan(y) = arctanâ˛ (Îž)(x â y) =
Da fuĚr alle Îž â R die AbschaĚtzung
1
â¤1
1 + Îž2

210

1
(x â y).
1 + Îž2

5.2. Eigenschaften differenzierbarer Funktionen
gilt, liefert das
arctan(x) â arctan(y) =

1
|x â y| â¤ 1|x â y|,
1 + Îž2

wir koĚnnen also L = 1 waĚhlen.
Die Differenzialrechnung liefert auch ein starkes Werkzeug zur Bestimmung von
Grenzwerten, naĚmlich den folgenden Satz.
Satz 5.2.6 (Satz von de lâHospital). Es sei (a, b) ein offenes Intervall in R
(dabei ist hier a = ââ oder b = â zugelassen) und f, g : (a, b) â R seien
differenzierbar auf (a, b) mit g â˛ (x) 6= 0 fuĚr alle x â (a, b). Gilt dann
lim g(x) = Âąâ

lim f (x) = lim g(x) = 0 oder

xâa

xâa

xâa

und existiert der Grenzwert

f â˛ (x)
xâa g â˛ (x)
(hierbei ist wieder L = Âąâ zugelassen), dann gilt
L := lim

f (x)
= L.
xâa g(x)
lim

Die Aussage dieses Satzes bleibt richtig, wenn man uĚberall x â a durch x â b
ersetzt.
Warnung 5.2.7. Dieser Satz hat viele Voraussetzungen und diese sind wirklich alle noĚtig! Im Eifer des Gefechts gegen einen hartnaĚckigen Grenzwert wird
hier gerne die eine oder andere vergessen. Besonderer Beliebtheit erfreut es sich,
nicht nachzupruĚfen, ob es sich wirklich um einen sogenannten uneigentlichenâ
â
Grenzwert der Form 0/0 oder Âąâ/ Âą â handelt. Nur solche kann dieser Satz
behandeln!
Beispiel 5.2.8.

(a) Es seien Îą, Î˛ > 0. Wir wollen den Grenzwert
Îąx â Î˛ x
xâ0+
x
lim

untersuchen. Wir betrachten das Intervall (0, 1) und die Funktionen f (x) =
Îąx â Î˛ x und g(x) = x auf (0, 1). Diese sind dort beide differenzierbar und
es gilt g â˛(x) = 1 6= 0 fuĚr alle x â (0, 1). AuĂerdem ist
lim f (x) = lim (Îąx â Î˛ x ) = lim Îąx â lim Î˛ x = 1 â 1 = 0 = lim g(x).

xâ0+

xâ0+

xâ0+

xâ0+

xâ0+

Wir koĚnnen also den Satz von de lâHospital anwenden und erhalten
Îąx ln(a) â Î˛ x ln(Î˛)
Îąx â Î˛ x
= lim
= ln(Îą) â ln(Î˛),
lim
xâ0+
xâ0+
x
1
was wohl nur sehr schwer zu erraten gewesen waĚre.

211

5. Analysis â Teil II: Differential- und Integralrechnung
(b) Ebenso kann man zeigen:
1
ln(x)
x
= lim = 0.
lim
xââ 1
xââ
x

In diesem Fall hat man es mit einem uneigentlichen Grenzwert der Form
â/â zu tun.
(c) Eine kleine Umformung fuĚhrt dazu, dass man mit der Regel von de lâHospital auch Grenzwerte der Form 0 Âˇ â behandeln kann. Das geht exemplarisch
so:
1
ln(x)
lim x ln(x) = lim 1 = lim x1 = lim (âx) = 0.
xâ0+
xâ0+
xâ0+ â 2
xâ0+
x
x

Man beachte, dass hier u.a. wegen limxâ0+ ln(x) = ââ und limxâ0+ 1/x =
â die Anwendung des Satzes gerechtfertigt war.
Dieser Grenzwert ermoĚglicht uns nun zusammen mit der Stetigkeit der Exponentialfunktion noch die Berechnung von
lim xx = lim ex ln(x) = elimxâ0+ x ln(x) = e0 = 1.

xâ0+

xâ0+

Das ist auch eine nachtraĚgliche Rechtfertigung fuĚr die in Definition 4.2.1 (c)
so willkuĚrlich erscheinende Setzung 00 = 1.
Wir haben in Beispiel 5.1.22 (c)
P gesehen, dassn eine Funktion, die auf einem Intervall durch eine Potenzreihe â
n=0 an (x â x0 ) gegeben ist, immer Koeffizienten
an der Form
f (n) (x0 )
an =
fuĚr alle n â N
n!
aufweist. Haben wir umgekehrt eine Funktion aus C â (I) vorliegen, koĚnnen wir fuĚr
ein x0 â I obige Koeffizienten ausrechnen und die dadurch gegebene Potenzreihe
betrachten. Diese bekommt zunaĚchst einen Namen.
Definition 5.2.9. Es sei I â R ein offenes Intervall, x0 â I und f â C â (I).
(a) Die Potenzreihe

â
X
f (n) (x0 )
n=0

n!

(x â x0 )n

heiĂt Taylorreihe von f um x0 .

(b) FuĚr jedes k â N heiĂt das Polynom
Tk,f (x; x0 ) :=

k
X
f (n) (x0 )
n=0

n!

(x â x0 )n

das Taylorpolynom k-ten Grades von f in x0 .

212

5.2. Eigenschaften differenzierbarer Funktionen
Bemerkung 5.2.10. Das Taylorpolynom k-ten Grades von f in x0 ist das eindeutige Polynom, das an der Stelle x0 in der nullten bis k-ten Ableitung mit der
Funktion f uĚbereinstimmt. Anschaulich sollte es also so etwas wie die bestmoĚgliche Approximation an die Funktion f sein, die durch ein Polynom vom Grad k
moĚglich ist.
Es erhebt sich die Frage, ob die Approximation wirklich gut ist, ob sie zum Beispiel mit steigendem Grad immer besser wird und im Grenzwert die Taylorreihe
wirklich die Funktion exakt darstellt, d.h. ob in einer Umgebung von x0 in der
Situation obiger Definition gilt
f (x) =

â
X
f (n) (x0 )
n=0

n!

(x â x0 )n .

Ein solches Verhalten hoĚrt sich plausibel an und waĚre auch sehr wuĚnschenswert, denn es gaĚbe ein Mittel an die Hand, um die Auswertung komplizierter
Funktionen naĚherungsweise auf die Auswertung eines der Funktion angepassten
NaĚherungspolynoms zuruĚckzufuĚhren.
Leider ist die Anwort auf die Frage in obgiger Bemerkung ein entschiedenes
manchmalâ. Wir betrachten dazu das folgende Beispiel.
â
Beispiel 5.2.11. Wir waĚhlen I = R, x0 = 0 und
(
2
eâ1/x , falls x 6= 0,
f (x) :=
0, falls x = 0.
Dann ist f offensichtlich in jedem Punkt x 6= 0 differenzierbar, aber wir sind
ja gerade an x0 = 0 interessiert. Um Differenzierbarkeit in Null zu untersuchen,
muĚssen wir uĚber den Differenzenquotienten gehen. Es ist mit t := 1/x
2

2

eâ1/x
eât
t
f (x) â f (0)
=
=
= t2 .
xâ0
x
1/t
e
Dieser Ausdruck strebt fuĚr x gegen 0, d.h. t gegen Âąâ, gegen Null. Das sieht
man z.B. mittels der Regel von de lâHospital. Also ist f in Null differenzierbar
und es gilt f â˛ (0) = 0. Mittels Induktion kann man nun zeigen, dass f in 0 sogar
beliebig oft differenzierbar ist und f (n) (0) = 0 fuĚr alle n â N gilt. Also ist in
diesem Fall die Taylorreihe
â
X
f (n) (0)
n=0

n!

n

x =

â
X
n=0

0 Âˇ xn = 0 6= f (x) fuĚr alle x 6= 0.

Da die Taylorreihe nach einem vielversprechenden Mittel aussieht, um komplizierte Funktionen zu untersuchen und Ihre Potenzreihenentwicklungen auszurechnen, und da diese ein unverzichtbares Hilfsmittel der Analysis, der Physik, der

213

5. Analysis â Teil II: Differential- und Integralrechnung
Ingenieurwissenschaften und vieler anderer Bereiche sind, haĚtten wir gerne ein
Kriterium, wann die Taylorreihe brav zu ihrer Funktion passt. Diese Frage ist das
Thema des Satzes von Taylor.
Satz 5.2.12 (Satz von Taylor). Es seien I â R ein offenes Intervall, x, x0 â I
und fuĚr ein k â N0 sei f : I â R eine k + 1-mal differenzierbare Funktion. Dann
gibt es ein Îž zwischen x und x0 , so dass gilt
f (x) = Tk,f (x; x0 ) +

f (k+1) (Îž)
(x â x0 )k+1 .
(k + 1)!

Bemerkung 5.2.13. (a) Im Fall k = 0 ist dieser Satz genau der Mittelwertsatz (vgl. Satz 5.2.1).
(b) Der Fehlerterm
Rk,f (x; x0 ) :=

f (k+1) (Îž)
(x â x0 )k+1 ,
(k + 1)!

der die Differenz zwischen f (x) und der NaĚherung durch das Taylorpolynom
k-ten Grades beschreibt, wird auch als Restglied bezeichnet.
(c) Der Wert von Îž haĚngt natuĚrlich jeweils von x, x0 und k ab und ist im
Allgemeinen nicht zu bestimmen. Das waĚre auch sehr erstaunlich, denn
dann waĚre ja die Berechnung von f auf den Schwierigkeitsgrad eines Polynoms zuruĚckgefuĚhrt, was dann fuĚr einfach nur (k + 1)-mal differenzierbare
Funktionen doch ein bisschen zu simpel waĚre. Im Îž steckt sozusagen die
KomplexitaĚt der Funktion f .
Trotzdem ist der Satz Gold wert, denn er uĚbersetzt jede Information uĚber
die (k + 1)-te Ableitung in eine Information daruĚber wie genau das Taylorpolynom k-ten Grades die Funktion f annaĚhert.
Beweis von Satz 5.2.12. Wir betrachten nur den Fall x0 < x (der andere geht
analog) und setzen
Ěş :=
Dann gilt


(k + 1)!
f
(x)
â
T
(x;
x
)
.
k,f
0
(x â x0 )k+1

f (x) â Tk,f (x; x0 ) = Ěş

(x â x0 )k+1
(k + 1)!

und unsere Aufgabe ist es ein Îž â (a, b) zu finden, so dass Ěş = f (k+1) (Îž) ist. Dazu
schreiben wir die letzte Gleichung so um, dass auf der rechten Seite Null steht
und definieren dann die Hilfsfunktion
k
X
(x â t)k+1
f (n) (t)
(x â t)k+1
= f (x) â
(x â t)n â Ěş
g(t) := f (x) â Tk,f (x; t) â Ěş
(k + 1)!
n!
(k + 1)!
n=0

214

5.2. Eigenschaften differenzierbarer Funktionen
fuĚr t â [x0 , x]. Da in g hoĚchstens die k-te Ableitung von f auftaucht ist nach den
Voraussetzungen g noch einmal differenzierbar auf [x0 , x]. AuĂerdem gilt direkt
g(x) = f (x) â f (x) = 0 und so wie wir Ěş gewaĚhlt haben gilt auch g(x0 ) = 0. Nach
dem Satz von Rolle 5.2.2 (a) gibt es also ein Îž â (x0 , x), so dass g â˛(Îž) = 0 gilt.
Andererseits ist (nachrechnen!)
g â˛(t) = Ěş

(x â t)k
(x â t)k
â f (k+1) (t)
,
k!
k!

womit

(x â Îž)k
(x â Îž)k
â f (k+1) (Îž)
k!
k!
(k+1)
und schlieĂlich Ěş = f
(Îž) folgt.
0 = g â˛ (Îž) = Ěş

Beispiel 5.2.14. Wir betrachten die Funktion f : (â1, â) â R mit f (x) =
ln(1 + x) um x0 = 0. Zur Aufstellung der Taylor-Polynome und der Taylor-Reihe
muĚssen wir alle Ableitungen von f bestimmen. Es ist fuĚr x > â1
f â˛ (x) =

1
,
1+x

f â˛â˛ (x) =

â1
,
(1 + x)2

f â˛â˛â˛ (x) =

2
,
(1 + x)3

f (4) (x) =

â2 Âˇ 3
(1 + x)4

und per Induktion allgemein
f (n) (x) =

(â1)nâ1 (n â 1)!
,
(1 + x)n

n â Nâ .

Insbesondere ist also fuĚr alle n â Nâ
f (n) (0)
(â1)nâ1
=
.
n!
n
Da f (0) = 0 gilt, ist damit das k-te Taylorpolynom von f um Null
Tk,f (x; 0) =

k
X
(â1)nâ1
n=1

und die Taylorreihe

â
X
(â1)nâ1
n=1

n

n

xn

xn .

Die spannende Frage ist nun natuĚrlich, ob diese zumindest in der NaĚhe der Null
eine brauchbare NaĚherung des Logarithmus liefern. ZunaĚchst sieht man mit dem
Satz von Hadamard, dass die Taylorreihe Konvergenzradius Eins hat, wir muĚssen
uns also auf jeden Fall auf x â (â1, 1] beschraĚnken und wir werden, um den
Rechenaufwand einzudaĚmmen ab jetzt nur noch x â [0, 1] behandeln.
Die Graphen der ersten Taylorpolynome in Abbildung 5.3 sehen ja schon mal
sehr ermutigend aus, aber das ist natuĚrlich noch kein Beweis.

215

5. Analysis â Teil II: Differential- und Integralrechnung
1.0

0.8

0.6

0.4

0.2

K

0

0.5

0.5

K

1.0

x

0.2

K

0.4

K

0.6

ln(1+x)

x

x-x^2/2

x - x^2/2 + x^3/3

Abbildung 5.3.: Die Funktion x 7â ln(1 + x) mit den Taylorpolynomen 1., 2. und
3. Grades um Null
Wir schlachten nun den Satz von Taylor aus. Dieser besagt, dass es fuĚr jedes
x > â1 ein Îž zwischen Null und x gibt mit
f (k+1) (Îž) k+1
x
(k + 1)!
(â1)k
= Tk,f (x; 0) +
xk+1 .
k+1
(1 + Îž) (k + 1)

ln(1 + x) = f (x) = Tk,f (x; 0) +

Um zu zeigen, dass das Taylorpolynom tatsaĚchlich eine NaĚherung der Funktion
f darstellt, d.h. dass die Taylorreihe von f auf dem Intervall [0, 1] gleich f ist,
muĚssen wir das Restglied
Rk,f (x; 0) =

(â1)k
xk+1
(1 + Îž)k+1(k + 1)

untersuchen und zeigen, dass dieses fuĚr k gegen unendlich gegen Null geht. Im
von uns behandelten Fall x â [0, 1] muss 0 â¤ Îž < x â¤ 1 gelten und wir bekommen
deshalb fuĚr jedes k â Nâ
0 â¤ Rk,f (x; 0) =

xk+1
1
xk+1
â¤
â¤
,
k+1
(1 + Îž) (k + 1)
k+1
k+1

was mit dem Sandwich-Theorem fuĚr jedes x â [0, 1] bedeutet, dass
lim Rk,f (x; 0) = 0

kââ

216

5.2. Eigenschaften differenzierbarer Funktionen
ist.
Damit haben wir nun fuĚr alle x â [0, 1]
â
X
(â1)nâ1

ln(1 + x) =

n

n=1

xn

und es sei der VollstaĚndigkeit halber erwaĚhnt, dass diese Darstellung tatsaĚchlich
fuĚr alle x â (â1, 1] gilt.
Damit haben wir zum Einen die fuĚr uns neue Potenzreihendarstellung des Logarithmus gewonnen, zum Anderen finden wir im Spezialfall x = 1
â
X
(â1)nâ1
n=1

n

= ln(2)

den Reihenwert der alternierenden harmonischen Reihe (vgl. Beispiel 4.5.8).
Um etwaigen MaĚkeleien zuvorzukommen: Wer meint, dass das aber viel Aufwand
fuĚr so einen mickrigen Reihenwert war, hat noch nie selbst versucht einen Reihenwert zu bestimmen.
Beispiel 5.2.15. Als zweites Taylor-Beispiel noch ein klassisches NaĚherungsproblem: Man bestimme den Wert 1, 051,02 mit einer Genauigkeit von mindestens
10â4 .
Wir verwenden die Funktion f : [1, â) â R mit f (x) = x1,02 und betrachten ihr
Taylorpolynom 1. Ordnung mit Entwicklungsstelle 1. Dazu berechnen wir
f â˛ (x) = 1, 02 Âˇ x0,02

f â˛â˛ (x) = 1, 02 Âˇ 0, 02 Âˇ xâ0,98

und f (1) = 1, sowie f â˛ (1) = 1, 02. Damit gilt
T1,f (x; 1) = f (1) + f â˛ (1)(x â 1) = 1 + 1, 02(x â 1).
FuĚr den NaĚherungsfehler R1,f (1, 05; 1), der uns interessiert, erhalten wir
1
R1,f (1, 05; 1) = f â˛â˛ (Îž)(1, 05 â 1)2 = 1, 02 Âˇ 0, 01 Âˇ Îž â0,98 Âˇ 0, 052 = 2, 55 Âˇ 10â5 Âˇ Îž â0,98
2
mit einem Îž zwischen 1 und 1, 05. Nun ist die Funktion g(t) := tâ0,98 auf dem
Intervall (1; 1, 05) monton fallend, denn fuĚr die Ableitung gilt auf diesem Intervall
g â˛ (t) = â0, 98 Âˇ tâ1,98 < 0. Also nimmt g auf dem Intervall (1; 1, 05) maximal den
Wert g(1) = 1 an und wir koĚnnen
|R1,f (1, 05; 1)| = 2, 55 Âˇ 10â5 Âˇ Îž â0,98 â¤ 2, 55 Âˇ 10â5 < 10â4
abschaĚtzen. Damit ist der Wert
T1,f (1, 05; 1) = 1 + 1, 02(1, 05 â 1) = 1 + 1, 02 Âˇ 0, 05 = 1, 051
ein fuĚr die Aufgabenstellung ausreichend exakter NaĚherungswert von 1, 051,02 .

217

5. Analysis â Teil II: Differential- und Integralrechnung

5.3. Extremwerte
Definition 5.3.1. Es sei D â R und f : D â R eine Funktion.
(a) Man sagt, dass f in x0 â D ein globales Maximum (bzw. globales Minimum) hat, falls f (x) â¤ f (x0 ) (bzw. f (x) âĽ f (x0 )) fuĚr alle x â D gilt.
(b) f hat in x0 â D ein relatives Maximum (bzw. relatives Minimum), falls ein
Î´ > 0 existiert, so dass f (x) â¤ f (x0 ) (bzw. f (x) âĽ f (x0 )) fuĚr alle x â D
mit |x â x0 | < Î´ gilt.
(c) Allgemein spricht man von einem globalen bzw. relativen Extremum in x0 ,
wenn f dort ein entsprechendes Maximum oder Minimum hat.
Bemerkung 5.3.2. Statt relativesâ Extremum/Maximum/Minimum ist auch
â
die Bezeichnung lokales Extremum/Maximum/Minimum uĚblich.
Wir fuĚhren noch einen weiteren topologischen Begriff ein.
Definition 5.3.3. Es sei V ein normierter R-Vektorraum und D â V eine
Menge. Ein x0 â D heiĂt innerer Punkt von D, falls es ein Îľ > 0 gibt, so dass
BÎľ (x0 ) â D ist. Man nennt
D âŚ := {x â D : x innerer Punkt von D}
das Innere von D.
Machen Sie sich klar, dass D genau dann offen ist, wenn D = D âŚ gilt.
Satz 5.3.4. Es sei f : I â R differenzierbar in x0 â I. Ist x0 ein innerer Punkt
von I und hat f in x0 ein relatives Extremum, so gilt f â˛ (x0 ) = 0.
Warnung 5.3.5. Da dieser Satz so oft verwendet wird, wird er auch gerne falsch
verwendet. Darum hier (aus vielfach gegebenem Anlass) zwei Warnungen.
(a) Die Voraussetzung x0 ist innerer Punktâ ist wesentlich. Ein einfaches Beiâ
spiel ist die Funktion f (x) = x auf dem Intervall [0, 1]. Diese hat ein relatives
Minimum in x0 = 0, aber f â˛ (0) = 1.
(b) Die Umkehrung gilt nicht! Das sieht man sofort an dem Beispiel f (x) = x3
auf I = R. Dann ist naĚmlich f â˛ (x) = 3x2 , also f â˛ (0) = 0, aber diese Funktion
hat in 0 kein Extremum, denn fuĚr jedes Îľ > 0 finden sich im Intervall (âÎľ, Îľ)
Punkte mit f (x) > 0 = f (0), z.B. x = 1/(2Îľ), und mit f (x) < 0 = f (0),
z.B. x = â1/(2Îľ).

218

5.3. Extremwerte
Beweis von Satz 5.3.4. Wir gehen zunaĚchst davon aus, dass f in x0 ein relatives
Maximum hat. Dann existiert ein Î´ > 0, so dass fuĚr alle x â (x0 â Î´, x0 + Î´)
gleichzeitig x â I und f (x) â¤ f (x0 ) gilt. Die erste Bedingung koĚnnen wir erfuĚllen,
weil x0 innerer Punkt von I ist, die zweite ist genau die Definition des relativen
Maximums. FuĚr alle solchen x auĂer x = x0 ist
(
f (x) â f (x0 ) â¤ 0, falls x > x0 ,
x â x0
âĽ 0, falls x < x0 .
Da f auĂerdem in x0 differenzierbar ist, muss damit gelten
f â˛ (x0 ) = lim

xâx0 +

f (x) â f (x0 )
f (x) â f (x0 )
â¤ 0 und f â˛ (x0 ) = lim
âĽ 0.
xâx
â
x â x0
0
x â x0

Also ist f â˛ (x0 ) = 0.
Wir widmen uns nun dem Fall, dass f ein relatives Minimum in x0 hat. Dann gibt
es ein Î´ > 0, so dass fuĚr alle x â I mit |x â x0 | < Î´ die Ungleichung f (x) âĽ f (x0 )
erfuĚllt ist. Also ist fuĚr alle diese x auch âf (x) â¤ âf (x0 ) und wir sehen, dass die
Funktion âf in x0 ein relatives Maximum hat. Nach dem ersten Teil des Beweises
gilt also f â˛ (x0 ) = â(âf )â˛ (x0 ) = â0 = 0.
MoĚgliche Extremstellen finden wir also an den Nullstellen der Ableitungen, aber
wie koĚnnen wir nun klaĚren, ob an einer solchen kritischen Stelle tatsaĚchlich ein
Extremum vorliegt und wenn das der Fall ist, ob es ein Minimum oder ein Maximum ist?
Wenn unsere Funktion zumindest zweimal stetig differenzierbar ist, hilft dabei
wieder der Satz von Taylor. Wir betrachten also eine Funktion f â C 2 (I) auf
einem Intervall I und nehmen an, dass in einem x0 â I âŚ , also einem inneren
Punkt, f â˛ (x0 ) = 0 gilt. Dann sagt der Satz von Taylor 5.2.12 mit k = 1, dass es
fuĚr jedes x â I ein Îž zwischen x0 und x gibt mit

1
1
f (x) = f (x0 ) + f â˛ (x0 )(x â x0 ) + f â˛â˛ (Îž)(x â x0 )2 = f (x0 ) + f â˛â˛ (Îž)(x â x0 )2 .
2
2
Also ist
1
f (x) â f (x0 ) = f â˛â˛ (Îž)(x â x0 )2 .
2
â˛â˛
Ist nun f (x0 ) > 0, so gilt dieses Vorzeichen dank der Stetigkeit von f â˛â˛ auch in
einer ganzen Umgebung von x0 . FuĚr alle x in dieser Umgebung muss auch das
zugehoĚrige Îž in der Umgebung sein, es gilt also dann f (x) â f (x0 ) > 0, denn
(x â x0 )2 ist ja immer positiv. Also ist dann f (x) > f (x0 ) fuĚr alle x in dieser
Umgebung, was gerade bedeutet, dass in x0 ein lokales Minimum vorliegt.
Mit einer analogen Vorzeichenbetrachtung kann man auch den Fall eines Maximums erledigen.
Ist auch die zweite Ableitung an der kritischen Stelle Null, so kann man, genuĚgende Differenzierbarkeit vorausgesetzt, die Ordnung des betrachteten Taylorpolynoms weiter nach oben treiben und erhaĚlt dann das folgenden Resultat.

219

5. Analysis â Teil II: Differential- und Integralrechnung
Satz 5.3.6. Es sei I â R ein Intervall, x0 â I âŚ und f â C n (I) fuĚr ein n âĽ 2.
Weiter gelte f â˛ (x0 ) = f â˛â˛ (x0 ) = Âˇ Âˇ Âˇ = f (nâ1) (x0 ) = 0, aber f (n) (x0 ) 6= 0. Ist
nun n ungerade, so hat f in x0 kein Extremum, ist n gerade, so liegt in x0 ein
Extremum vor, und zwar falls f (n) (x0 ) > 0 ein Minimum und falls f (n) (x0 ) < 0
ein Maximum.
Beispiel 5.3.7. Wir bestimmen die lokalen und globalen Extrema der Funktion
f : [0, â) â R mit f (x) = xeâx . Es ist f â˛ (x) = eâx â xeâx , x â [0, â), also
haben wir genau dann f â˛ (x0 ) = 0, wenn eâx0 = x0 eâx0 , d.h. x0 = 1 ist. Wegen
f â˛â˛ (x) = âeâx â eâx + xeâx = xeâx â 2eâx , ist f â˛â˛ (x0 ) = 1/e â 2/e = â1/e < 0,
wir haben also nach obigem Satz in x0 = 1 ein lokales Maximum.
Um festzustellen, ob dieses auch ein globales Maximum ist, muĚssen wir noch die
RaĚnder des Definitionsbereiches betrachten. Es gilt f (0) = 0, f (1) = 1/e und
nach der Regel von de lâHospital
x
1
=
lim
= lim eâx = 0.
xââ ex
xââ ex
xââ

lim f (x) = lim xeâx = lim

xââ

xââ

Also liegt in x0 = 1 tatsaĚchlich das globale Maximum vor. Das globale Minimum
findet sich in Null, denn es ist f (x) âĽ 0 fuĚr alle x â [0, â). Dieses ist dann
natuĚrlich auch ein lokales Minimum.

Abbildung 5.4.: Der Graph der Funktion x 7â xeâx auf [0, 10]

5.4. Differenzieren von Funktionen mehrerer
Variablen â Partielle Ableitungen
Wir betrachten nun wieder Funktionen f : G â Rp mit G â Rd und wollen den
Ableitungsbegriff und die Differentialrechnung auf diese erweitern.

220

5.4. Partielle Ableitungen
Die direkte UĚbertragung der Definition uĚber den Differenzenquotienten, vgl. Definition 5.1.1 und Bemerkung 5.1.2,
f (x) â f (x0 )
f (x0 + h) â f (x0 )
f â˛ (x0 ) = lim
= lim
xâx0
hâ0
x â x0
h

ist nicht moĚglich, denn nun sind ja x, x0 â G â Rd und damit Vektoren und durch
Vektoren kann man nicht teilen. Wir haben hier also ein strukturelles Problem.
In diesem und dem naĚchsten Abschnitt werden wir zwei AnsaĚtze kennenlernen,
um dieses Problem zu umgehen, die beide im Wesentlichen zum selben Ergebnis
fuĚhren. Trotzdem sind beide ZugaĚnge wichtig, da der erste ohne den zweiten
keine vernuĚnftige Theorie ergibt und der zweite ohne den ersten zu einer nicht
praktikabel zu berechnenden Ableitung fuĚhrt.
Wir versuchen zunaĚchst den Differenzenquotienten zu retten, indem wir uns darauf beschraĚnken mehrere eindimensionale Ableitungen zu berechnen, die zusammen das Verhalten der Funktion f beschreiben sollen. Dazu berechnen wir die
Ableitung zunaĚchst nur in eine Richtung.
Definition 5.4.1. Es sei G â Rd offen, f : G â Rp eine Funktion, x0 â G und
v â Rd \ {0}. Existiert dann der Grenzwert

f (x0 + hv) â f (x0 )
,
hâ0
h
so heiĂt f in x0 in Richtung v differenzierbar und (âv f )(x0 ) die Richtungsableitung von f in x0 in Richtung v.
(âv f )(x0 ) := lim

Anschaulich bedeutet diese Definition, dass wir uns nur die Funktionswerte von f
entlang der Geraden {x0 +Îťv : Îť â R} in G anschauen und den Schnitt von f entlang dieser Geraden im eindimensionalen Sinne differenzieren. Wir bestimmen die
Steigung am Hang, wenn wir stur in Richtung v laufen. Dies ist in Abbildung 5.5
angedeutet.
UĚbungsaufgabe 5.4.2. Definiert man gv (h) := f (x0 + hv) fuĚr alle h â R, fuĚr
die x0 + hv in G liegt, so gilt
(âv f )(x0 ) = gvâ˛ (0).
Im Normalfall werden wir den Rd mit der Standardbasis ausstatten. Die Ableitungen in Richtung der Standardbasisvektoren bekommen einen eigenen Namen.
Definition 5.4.3. Es seien G â Rd offen, f : G â Rp eine Funktion und
{e1 , e2 , . . . , ed } die Standardbasis des Rd .

(a) Existieren in einem x0 â G die Richtungsableitungen von f in alle Richtungen e1 , e2 , . . . , ed , so heiĂt f in x0 partiell differenzierbar. Man schreibt
dann fuĚr j = 1, 2, . . . , d auch
âf
âj f (x0 ) :=
(x0 ) := fxj (x0 ) := (âej f )(x0 )
âxj
fuĚr die partielle Ableitung von f in x0 nach der j-ten Koordinate.

221

5. Analysis â Teil II: Differential- und Integralrechnung

3

3

2
2
-1.0

y
-0.5

1

1.0

-0.5

-1.0

00.0
0.0
0.5

1

-1.0
y

-0.5
x 0.5

-1
z

1.0
1.0

-1.0
-0.5

0
0.00.0
x
-1 0.5
0.5
z
-2

1.0

-3

-2

Abbildung 5.5.: Eine Funktion von R2 nach R und der Schnitt, der zu einer partiellen Ableitung fuĚhrt
(b) Ist f in allen x0 â G partiell differenzierbar, so sagt man f ist in G partiell
âf
differenzierbar und schreibt âj f = âx
= fxj : G â R fuĚr die partielle
j
Ableitung(sfunktion).
(c) Ist f in G partiell differenzierbar und sind saĚmtliche partiellen Ableitungen
â1 f, â2 f, . . . , âd f : G â R stetig, so nennt man f stetig partiell differenzierbar in G.
Bemerkung 5.4.4. Die Notation ist im Bereich der partiellen Ableitungen leider
ziemlich vielfaĚltig. Alle oben angefuĚhren Bezeichnungen sind synonym und in der
Literatur uĚblich, so dass man sich wohl oder uĚbel an alle gewoĚhnen muss.
Beispiel 5.4.5. Wir betrachten die IdentitaĚt f : R3 â R3 mit f (x) = x. Dann
gilt fuĚr jede Richtung v â R3 \ {0} und jedes x0 â R3
f (x0 + hv) â f (x0 )
x0 + hv â x0
= lim
= v.
hâ0
hâ0
h
h

(âv f )(x0 ) = lim

Damit gilt fuĚr die partiellen Ableitungen
âj f (x0 ) = ej

fuĚr alle j = 1, 2, . . . , d.

Bemerkung 5.4.6. Die praktische Berechnung der partiellen Ableitungen ist
einfach: Will man die j-te partielle Ableitung von f bestimmen, so behandelt
man die anderen Variablen x1 , . . . , xjâ1, xj+1 , . . . , xd als konstante Parameter und
leitet ganz wie gewohnt nach der einen Variablen xj ab. Das sieht man z.B. fuĚr

222

5.4. Partielle Ableitungen
j = 1 an der Rechnung


f (x1 , . . . , xd )T + h(1, 0, . . . , 0)T â f (x1 , . . . , xd )
â1 f (x1 , . . . , xd ) = lim
hâ0
h
f (x1 + h, x2 , . . . , xd ) â f (x1 , x2 , . . . , xd )
= lim
.
hâ0
h
2

Beispiel 5.4.7. FuĚr die Funktion f : R3 â R mit f (x, y, z) = xexz+y gilt nach
obiger Bemerkung damit
2

2

2

â1 f (x, y, z) = exz+y + xexz+y Âˇ z = (1 + xz)exz+y ,
2

2

â2 f (x, y, z) = xexz+y Âˇ 2y = 2xyexz+y ,
2

2

â3 f (x, y, z) = xexz+y Âˇ x = x2 exz+y .

Der Fall einer Funktion mit Vektoren als Werten laĚsst sich wie schon bei der
Stetigkeit auf den Fall p = 1 zuruĚckspielen.
Satz 5.4.8. Ist G â Rd offen, f : G â Rp eine Funktion und x0 â G, so
ist f in x0 genau dann partiell differenzierbar, wenn alle Koordinatenfunktionen
f1 , f2 , . . . , fp : G â R in x0 partiell differenzierbar sind. In diesem Fall gilt
T
âj f (x0 ) = âj f1 (x0 ), âj f2 (x0 ), . . . , âj fp (x0 ) .
Beweis. Die Funktion f ist in x0 genau dann partiell nach der j-ten Koordinaten
differenzierbar, wenn der Grenzwert

f (x0 + hej ) â f (x0 )
hâ0
h
in Rp existiert und der Wert ist dann die partielle Ableitung. Da Konvergenz in
Rp das selbe wie koordinatenweise Konvergenz ist, vgl. die SaĚtze 4.6.5 und 4.8.4,
existiert dieser Grenzwert genau dann, wenn der entsprechende Grenzwert fuĚr
jede Koordinatenfunktion existiert und der Grenzwert ist dann der Vektor der
Grenzwerte.
lim

Bemerkung 5.4.9. Mit Bemerkung 5.4.6 und diesem Satz haben wir das Problem der konkreten Berechnung von partiellen Ableitungen auf den Fall von Funktionen von R nach R zuruĚckgespielt. Wir brauchen also keine neuen Ableitungsregeln, sondern koĚnnen mit unserem bisherigen Wissen alle partiellen Ableitungen
berechnen, sofern diese existieren.
Definition 5.4.10. Es sei G â Rd offen und f : G â Rp in x0 â G partiell
differenzierbar. Die p Ă d-Matrix aller partiellen Ableitungen
ďŁŤ
ďŁś
â1 f1 (x0 ) â2 f1 (x0 ) . . . âd f1 (x0 )
ďŁŹâ1 f2 (x0 ) â2 f2 (x0 ) . . . âd f2 (x0 )ďŁˇ
ďŁŹ
ďŁˇ
Jf (x0 ) := ďŁŹ
ďŁˇ
..
..
..
..
ďŁ­
ďŁ¸
.
.
.
.
â1 fp (x0 ) â2 fp (x0 ) . . . âd fp (x0 )
223

5. Analysis â Teil II: Differential- und Integralrechnung
heiĂt Jacobi-Matrix von f .
Im Spezialfall p = 1 nennt man die 1 Ă d-Matrix, d.h. den Rd -Zeilenvektor

âf (x0 ) := Jf (x0 ) = â1 f (x0 ), â2 f (x0 ), . . . , âd f (x0 )
den Gradient von f .

Bemerkung 5.4.11.

(a) Es gilt in dieser Notation
ďŁŤ
ďŁś
âf1 (x)
ďŁŹâf2 (x)ďŁˇ
ďŁŹ
ďŁˇ
Jf (x) = ďŁŹ .. ďŁˇ .
ďŁ­ . ďŁ¸
âfp (x)

(b) Der Gradient einer Funktion f : G â R mit G â Rd hat auch eine anschauliche Bedeutung. Ist f glatt genug, so gibt der Vektor âf (x0 ) die Richtung
an, in der der Graph von f an der Stelle x0 am staĚrksten ansteigt und seine
LaĚnge entspricht dieser maximalen Steigung. Einen Beweis dieser Aussage
koĚnnen wir erst in Bemerkung 5.5.11 geben.
Auf dieser Eigenschaft beruhen viele numerische Optimierungsverfahren,
die zum Suchen des Optimums in Richtung des Gradienten der zu optimierenden GroĚĂe gehen ( Gradientenmethodenâ), getreu dem Motto: Der
â
schnellste Weg zum Gipfel ist immer in die steilste Richtung den Hang
hinauf und das ist eben die Richtung des Gradienten.
T
2
Wir veranschaulichen uns das am Graph
p der Funktion f : {(x, y) â R :
T
2
2
k(x, y) k2 < 1} â R mit f (x, y) = 1 â x â y , deren Graph die obere
Halbkugelschale mit Radius 1 beschreibt. Der Gradient ist dann gegeben
durch
 

â2x
â2y
âf (x, y) = â1 f (x, y), â2f (x, y) = p
, p
2 1 â x2 â y 2 2 1 â x2 â y 2
1
(x, y).
= âp
1 â x2 â y 2

An den Punkt (x, y) â R2 angeklebt zeigt der Vektor â(x, y)T genau in
Richtung des Ursprungs und das ist auf der KugeloberflaĚche die Richtung
des steilsten Anstiegs. Der Betrag des Gradienten, also die StaĚrke dieses
Anstiegs wird Null, wenn wir uns in den Ursprung bewegen und unendlich
groĂ, wenn wir uns dem Rand des Definitionsbereiches naĚhern.

Da wir jede Richtung v â Rd \{0} durch die Standardbasis darstellen koĚnnen, liegt
die Hoffnung nahe, dass wir durch die Kenntnis aller partiellen Ableitungen jede
Richtungsableitung bestimmen koĚnnen, dass uns also die Jacobi-Matrix ausreicht.
Dass es dabei ein Problem gibt, zeigt das naĚchste Beispiel.

224

5.4. Partielle Ableitungen
Beispiel 5.4.12. Wir betrachten wieder die Funktion f : R2 â R mit
(
xy
, falls (x, y) 6= (0, 0),
x2 +y 2
f (x, y) =
0,
falls (x, y) = (0, 0),
vgl. Beispiel 4.8.6. AuĂerhalb von (0, 0)T ist diese offensichtlich partiell differenzierbar mit
y(x2 + y 2) â 2x Âˇ xy
y 3 â x2 y
=
und
(x2 + y 2)2
(x2 + y 2)2
x(x2 + y 2) â 2y Âˇ xy
x3 â xy 2
â2 f (x, y) =
=
, (x, y) 6= (0, 0).
(x2 + y 2)2
(x2 + y 2)2

â1 f (x, y) =

Die partiellen Ableitungen im Ursprung berechnen sich uĚber den Differenzenquotienten:
f (h, 0) â f (0, 0)
= lim
hâ0
hâ0
h
f (0, h) â f (0, 0)
â2 f (0, 0) = lim
= lim
hâ0
hâ0
h

â1 f (0, 0) = lim

0
h2

â0
= 0,
h
0
â0
h2
= 0.
h

und

Also ist âf (0, 0) = (0, 0).
Versuchen wir nun die Richtungsableitung im Ursprung in eine Richtung v =
(v1 , v2 ) â R2 \ {0} zu bestimmen, so bekommen wir den Grenzwert
f (0 + hv) â f (0)
= lim
lim
hâ0
hâ0
h

hv1 hv2
(hv1 )2 +(hv2 )2

h

v1 v2
.
hâ0 h(v12 + v22 )

= lim

Sobald v1 und v2 beide nicht Null sind, wir uns also nicht in Richtung einer Koordinatenachse bewegen, existiert dieser Grenzwert aber gar nicht. Die einzigen
Richtungen in die hier Richtungsableitungen existieren, sind also gerade die Richtungen der Standardbasis, die zu den partiellen Ableitungen gehoĚren. Betrachten
Sie dazu auch noch einmal den Graphen der Funktion f in Abbildung 4.2
Damit sieht man, dass man aus der Existenz der partiellen Ableitungen alleine
nicht auf irgendwelche anderen Richtungsableitungen schlieĂen kann.
Das Beispiel zeigt daruĚber hinaus auch aus einem anderen Grund, dass alleine
mit dem Begriff der partiellen Differenzierbarkeit kein Staat zu machen ist. Wir
haben hier naĚmlich eine im Ursprung partiell differenzierbare Funktion, die dort
aber noch nicht einmal stetig ist wie wir in Beispiel 4.8.6 gesehen haben.
Beachten Sie, dass f im vorstehenden Beispiel nicht stetig partiell differenzierbar
ist. TatsaĚchlich ist dies ein entscheidendes Detail, denn wenn die partiellen Ableitungen stetig sind, koĚnnen solche Sauereien nicht passieren. Bis wir das exakter
formulieren koĚnnen brauchen wir aber noch einen Stapel Theorie.
Wir definieren zunaĚchst die partiellen Ableitungen hoĚherer Ordnung. Die Definition duĚrfte keine groĂe UĚberraschung sein.

225

5. Analysis â Teil II: Differential- und Integralrechnung
Definition 5.4.13. Es seien G â Rd offen, n â Nâ mit n âĽ 2, x0 â G und
f : G â Rp eine Funktion. Diese nennt man n-mal (stetig) partiell differenzierbar
in x0 , wenn sie schon (nâ1)-mal (stetig) partiell differenzierbar auf G ist und alle
(n â 1)-ten partiellen Ableitungen in x0 wieder (stetig) partiell differerenzierbar
sind.
Notiert werden mehrfache partielle Ableitungen durch Hintereinanderschreiben
der einzelnen Ableitungen, also z.B.
â1 â3 â1 f,

â23 â1 f,

â3f
âx1 âx22

oder fx1 x2 x3 ,

je nach der verwendeten Notation.
Beispiel 5.4.14. Wir betrachten die Funktion f : R2 â R mit f (x, y) = x3 y +
xey . Dann haben wir fuĚr die partiellen Ableitungen erster Ordnung
â1 f (x, y) = 3x2 y + ey

und â2 f (x, y) = x3 + xey .

Die partiellen Ableitungen zweiter Ordnung lauten
â12 f (x, y) = 6xy
â2 â1 f (x, y) = 3x2 + ey

â1 â2 f (x, y) = 3x2 + ey
â22 f (x, y) = xey .

So kann man nun natuĚrlich ewig weitermachen. Hier ist noch die dritte Ordnung:
â13 f (x, y) = 6y
â1 â2 â1 f (x, y) = 6x
â12 â2 f (x, y) = 6x
â2 â12 f (x, y) = 6x

â1 â22 f (x, y) = ey
â22 â1 f (x, y) = ey
â2 â1 â2 f (x, y) = ey
â23 f (x, y) = xey

Betrachtet man die partiellen Ableitungen in obigem Beispiel noch einmal genauer, so stellt man fest, dass das Ergebnis der Ableiterei nicht von der Reihenfolge
der Differenziationen, sondern nur von der Anzahl abzuhaĚngen scheint, wie oft
jeweils nach der ersten bzw. der zweiten Koordinaten differenziert wird. So ist
in obigem Beispiel z.B. â1 â2 f (x, y) = â2 â1 f (x, y). Das ist tatsaĚchlich kein Zufall,
denn es gilt der folgende Satz, den wir hier nicht beweisen wollen.
Satz 5.4.15 (Satz von Schwarz). Ist G â Rd offen und f : G â Rp eine nmal stetig partiell differenzierbare Funktion, so ist die Reihenfolge der partiellen
Ableitungen bis zur Ordnung n vertauschbar.
Kein Satz bleibt ohne die Warnung auf die Voraussetzungen zu achten. Ist f nur
partiell differenzierbar, aber sind die partiellen Ableitungen nicht stetig, so gilt
der Satz von Schwarz nicht, wie das naĚchste Beispiel zeigt.

226

5.5. Totale Differenzierbarkeit
Beispiel 5.4.16. Wir betrachten die Funktion f : R2 â R mit
(
2 ây 2
xy xx2 +y
falls (x, y) 6= (0, 0)
2,
f (x, y) :=
0,
falls (x, y) = (0, 0).
Dann ist f auf ganz R2 partiell differenzierbar mit
( 4 2 2 4
+4x y ây
, falls (x, y) 6= (0, 0)
y x (x
2 +y 2 )2
â1 f (x, y) =
0,
falls (x, y) = (0, 0),
( 4 2 2 4
â4x y ây
x x (x
, falls (x, y) 6= (0, 0)
2 +y 2 )2
â2 f (x, y) =
0,
falls (x, y) = (0, 0).
Damit ist
4

h âh4 â 0
= â1 und
â2 â1 f (0, 0) = lim h
hâ0
h
4
h hh4
= 1.
â1 â2 f (0, 0) = lim
hâ0 h

5.5. Differenzieren von Funktionen mehrerer
Variablen â Totale Differenzierbarkeit
Wir wollen nun die unbefriedigenden Anteile des vorigen Abschnittes aufloĚsen
und das Differenziationsproblem im Rd noch mal ein wenig abstrakter anschauen.
Dazu erinnern wir uns daran, dass die Ableitung uĚber die Tangente eine lineare
Approximation der Funktion darstellt. Dieser Gedanke kam in Satz 5.1.7 schon
einmal vor.
Wir koĚnnen nun natuĚrlich nicht mehr durch eine Gerade approximieren, aber
weiterhin durch eine lineare Abbildung. Schauen wir uns die Umformulierung der
Differenzierbarkeit in dem gerade erwaĚhnten Satz an, so stellen wir tatsaĚchlich
fest, dass dort nicht mehr durch das Argument geteilt wird und wir diese Charakterisierung der Differenzierbarkeit tatsaĚchlich auf mehrere Variablen verallgemeinern koĚnnen.
Definition 5.5.1. Es sei G â Rd offen und x0 â G. Eine Funktion f : G â Rp
heiĂt (total) differenzierbar in x0 , wenn es eine lineare Abbildung ÎŚ : Rd â Rp
gibt, so dass gilt
f (x) = f (x0 ) + ÎŚ(x â x0 ) + r(x),

x â G,

mit einer Funktion r : G â Rp , die
kr(x)k
=0
xâx0 kx â x0 k
lim

227

5. Analysis â Teil II: Differential- und Integralrechnung
erfuĚllt.
Die lineare Abbildung Df (x0 ) := ÎŚ heiĂt dann (totale) Ableitung von f in x0 .
Ist f in allen x0 â G total differenzierbar, so nennt man die Funktion Df : G â
{Î¨ : Rd â Rp : Î¨ linear}, die Ableitung(sfunktion) von f .
Bemerkung 5.5.2. (a) Bei den beiden Normen in obiger Definition muĚsste
man eigentlich dazu sagen, welche hier gemeint sind. Wir haben jedoch
in Satz 4.8.11 (b) gesehen, dass es in Rd fuĚr die Konvergenz einer Folge
unerheblich ist mit welcher Norm man misst. Man kann sich hier also immer
die Norm aussuchen, die gerade am besten passt, bzw. am leichtesten zu
bestimmen ist.
In diesem Sinne werden wir auch in allen weiteren Betrachtungen in Rd
einfach k Âˇ k schreiben, wenn die konkrete Wahl der Norm unerheblich ist.
(b) Eigentlich muĚssten wir obige Definition noch insofern rechtfertigen als nicht
klar ist, dass die Bedingung, wenn sie zutrifft, die lineare Abbildung ÎŚ
eindeutig festlegt. Wir glauben das jedoch einfach und betrachten lieber
ein Beispiel.
Beispiel 5.5.3. Es sei y â Rd fest gewaĚhlt, (Âˇ|Âˇ) das Standardskalarprodukt in
Rd und f : Rd â R gegeben durch f (x) = (y|x). Dann gilt fuĚr jedes x0 â Rd
dank der LinearitaĚt des Skalarprodukts
f (x) = (y|x) = (y|x0 ) + (y|x â x0 ) = f (x0 ) + y T (x â x0 ).
Die Abbildung w 7â y T w, w â Rd ist linear. Man kann y T z.B. als eine 1 Ă dMatrix auffassen. Wir haben also eine Darstellung fuĚr f (x) wie in Definition 5.5.1
gefunden, sogar mit r(x) = 0, x â Rd , womit die Grenzwertbedingung sicher
erfuĚllt ist. Die Ableitung Df (x) ist also die konstante Funktion mit der linearen
Abbildung, die durch die Multiplikation mit y T gegeben ist, als Wert.
Bemerkung 5.5.4. Allgemein gilt, dass die Ableitung einer linearen Abbildung
ÎŚ : Rd â Rp in jedem Punkt die Abbildung ÎŚ selbst ist. Rechnen Sie das doch
mal nach!
UĚbungsaufgabe 5.5.5. Diskutieren Sie, eine Abbildung von wo nach wo die
zweite totale Ableitung D(Df ) ist.
Wir wollen nun zunaĚchst zeigen, dass die totale Differenzierbarkeit Stetigkeit
der Funktion impliziert, im Gegensatz zur partiellen Differenzierbarkeit, vgl. Beispiel 5.4.12.
Satz 5.5.6. Ist G â Rd offen und f : G â Rp in x0 â G total differenzierbar, so
ist f auch stetig in x0 .

228

5.5. Totale Differenzierbarkeit
Beweis. FuĚr alle x â G gilt nach der Definition der totalen Differenzierbarkeit
f (x) = f (x0 ) + Df (x0 )(x â x0 ) + r(x).
Ist nun (an ) eine Folge in G, die gegen x0 konvergiert, so gilt dank UĚbungsaufgabe 4.8.7 auch

lim Df (x0 )(an â x0 ) = Df (x0 ) lim (an â x0 ) = Df (x0 )(0) = 0.
nââ

nââ

Weiter impliziert die totale Differenzierbarkeit von f insbesondere limxâx0 r(x) =
0. Also haben wir

lim f (an ) = lim f (x0 ) + Df (x0 )(an â x0 ) + r(an ) = f (x0 ) + 0 + 0 = f (x0 )
nââ

nââ

und das bedeutet gerade Stetigkeit von f in x0 .

Im Abschnitt zur linearen Algebra haben wir lineare Abbildungen sehr erfolgreich
durch Matrizen beschrieben. Dazu muĚssen wir zunaĚchst in den beiden beteiligten
RaĚumen Basen waĚhlen und die Standardbasis ist die natuĚrliche Wahl. TatsaĚchlich
gilt der folgende Satz, der die BruĚcke zu den Richtungsableitungen und damit
auch zu den partiellen Ableitungen schlaĚgt.
Satz 5.5.7. Es sei G â Rd offen, f : G â Rp eine in x0 â G total differenzierbare
Funktion und v â Rd \ {0}. Dann existiert in x0 die Richtungsableitung von f in
Richtung v und es gilt
(âv f )(x0 ) = Df (x0 )(v).
Beweis. Wir wenden fuĚr h â R\{0} die Definition der totalen Differenzierbarkeit
mit x = x0 + hv an. Das ergibt
f (x0 + hv) = f (x0 ) + Df (x0 )(x0 + hv â x0 ) + r(x)
mit einer Funktion r : G â Rp , fuĚr die
0 = lim

hâ0

kr(x)k
kr(x)k
1
kr(x)k
1
r(x)
= lim
=
lim
=
lim
hâ0
hâ0
hâ0
kx â x0 k
khvk
kvk
|h|
kvk
h

gilt. Also ist limhâ0 r(x)/h = 0 und damit dank der LinearitaĚt von Df (x0 )
Df (x0 )(hv) + r(x)
f (x0 + hv) â f (x0 )
= lim
hâ0
hâ0
h
h
h
r(x) i
= lim Df (x0 )(v) +
= Df (x0 )(v),
hâ0
h
lim

was nach der Definition der Richtungsableitung genau die Behauptung ist.
Damit koĚnnen wir nun den folgenden zentralen Zusammenhang zwischen der
totalen Ableitung und den partiellen Ableitungen beweisen.

229

5. Analysis â Teil II: Differential- und Integralrechnung
Satz 5.5.8. Es sei G â Rd offen, x0 â G und f : G â Rp eine Funktion. Ist
f in x0 total differenzierbar, so ist f in x0 auch partiell differenzierbar und die
Abbildungsmatrix von Df (x0 ) bezuĚglich der Standardbasen von Rd bzw. Rp ist die
Jacobi-Matrix Jf (x0 ).
Beweis. Es sei Bd := {e1 , e2 , . . . , ed } die Standardbasis in Rd und Bp die Standardbasis in Rp . Wir muĚssen nun die Bilder der Basisvektoren e1 , e2 , . . . , ed unter der linearen Abbildung Df (x0 ) bestimmen. Das ergibt nach Satz 5.5.7 fuĚr
j = 1, 2, . . . , d
Df (x0 )(ej ) = (âej f )(x0 ) = âj f (x0 ),
also enthaĚlt die j-te Spalte von MBBpd (Df (x0 )) die Koordinaten der partiellen
Ableitung âj f (x0 ) bezuĚglich Bp . Da dies aber die Standardbasis ist, sind die
Vektoren ihre eigenen Koordinatenvektoren und die j-te Spalte ist âj f (x0 ) genau
wie in der Jacobi-Matrix.
Bemerkung 5.5.9. (a) Man beachte, dass die Umkehrung dieses Satzes falsch
ist. Das folgt aus Beispiel 5.4.12 und Satz 5.5.6.
(b) Im Folgenden werden wir oft Df (x0 ) mit der Jacobi-Matrix identifizieren,
d.h. wir trennen nicht sauber zwischen der linearen Abbildung und der
Abbildungsmatrix. Was in der linearen Algebra noch strikt verboten war,
ist hier opportun, um unnoĚtige Haarspaltereien zu vermeiden. Das geht gut,
weil wir im Folgenden nie von der oben getroffenen Wahl der Standardbasen
abweichen werden.
Unter der Voraussetzung totaler Differenzierbarkeit bekommen wir nun auch den
Zusammenhang wie man aus den partiellen Ableitungen jede Richtungsableitung
bestimmen kann, den wir im letzten Abschnitt noch so vermisst haben.
Korollar 5.5.10. Ist G â Rd offen und f : G â Rp in x0 â G total differenzierbar, so gilt fuĚr jedes v â Rd \ {0}
âv f (x0 ) = Jf (x0 )v.
Bemerkung 5.5.11. Nun koĚnnen wir auch einen Beweis fuĚr die Behauptung
in Bemerkung 5.4.11 (b) geben, dass der Gradient in die Richtung des steilsten
Anstiegs zeigt. Es sei also G â Rd offen, x0 â G und f : G â R in x0 total differenzierbar mit âf (x0 ) 6= 0. Dann gilt fuĚr die Richtungsableitung mit Richtung
v â Rd \ {0} in x0 nach Korollar 5.5.10 und mit Hilfe der Cauchy-Schwarzschen
Ungleichung, vgl. Satz 3.4.9,

âv f (x0 ) = âf (x0 )v = (âf (x0 ))T |v â¤ kâf (x0 )k2 kvk2
und wenn Gleichheit gilt, so muĚssen âf (x0 ) und v linear abhaĚngig sein, d.h. es
gibt ein Îť â R mit âf (x0 )T = Îťv. Nehmen wir an Îť waĚre negativ, so ist
âv f (x0 ) = âf (x0 )v = Îťv Âˇ v = Îťkvk2 < 0

230

5.5. Totale Differenzierbarkeit
und damit ganz sicher nicht maximal. Also muĚssen âf (x0 ) und v die gleiche
Richtung haben, wenn âv f (x0 ) maximal ist.
Wir sind damit schon in einer ziemlich komfortablen Situation. Die totale Differenzierbarkeit verallgemeinert unser Konzept der Differenzierbarkeit in einer Variablen ins mehrdimensionale und wenn wir die Ableitungen konkret ausrechnen
muĚssen, koĚnnen wir uns an die einfach zu berechnenden partiellen Ableitungen
halten, denn die totale Ableitung ergibt sich ja aus der Jacobi-Matrix. Es bleibt
noch eine Frage zu klaĚren: KoĚnnen wir irgendwie an den partiellen Ableitungen
auch schon sehen, ob eine Funktion total differenzierbar ist? Ja, es gibt ein sehr
brauchbares notwendiges Kriterium:
Satz 5.5.12. Ist G â Rd offen und f : G â Rp in x0 â G stetig partiell
differenzierbar, so ist f in x0 sogar total differenzierbar.
Den Beweis schenken wir uns, es lohnt sich aber die verschiedenen Beziehungen
zwischen totaler, partieller und Richtungs-Differenzierbarkeit noch mal zusammenzufassen:
stetig partiell differenzierbar
â
partiell differenzierbar

=â
â=

total differenzierbar
â
alle Richtungsabl. existieren

=â stetig

Wichtig ist noch zu bemerken, dass bei allen Implikationen in diesem Diagramm
die RuĚckrichtung falsch ist.
Auch wenn wir die praktische Berechnung der Ableitungen durch die partiellen
Ableitungen schon auf den eindimensionalen Fall zuruĚckgespielt haben und damit
von dort alle Ableitungsregeln uĚbernehmen koĚnnen, ist es eine gute Idee zumindest die Kettenregel noch einmal in Matrixschreibweise zu formulieren, da man
sie immer wieder gewinnbringend in dieser Form nutzen kann.
Satz 5.5.13 (Kettenregel). Es seien G â Rd und H â Rp offen, sowie g : G â
Rp mit g(G) â H und f : H â Rq Funktionen, so dass g in x0 â G und f in
g(x0 ) total differenzierbar sind. Dann ist auch die Funktion f âŚ g : G â Rq in x0
total differenzierbar und es gilt
D(f âŚ g)(x0 ) = Df (g(x0)) Âˇ Dg(x0 ).
Bemerkung 5.5.14. Man beachte, dass obige Gleichung eine brav definierte
Matrixmultiplikation enthaĚlt, denn D(f âŚ g)(x0 ) â RqĂd , waĚhrend Df (g(x0 )) â
RqĂp und Dg(x0 ) â RpĂd sind.
Beispiel 5.5.15. Wir betrachten f : R2 â R mit f (x, y) = x3 y + xey , vgl.
Beispiel 5.4.14, und interessieren uns fuĚr

d
f (t2 , t3 ) .
dt
231

5. Analysis â Teil II: Differential- und Integralrechnung
Damit ist gemeint, dass wir die ganz normale eindimensionale Ableitung der
Funktion t 7â f (t2 , t3 ) suchen. Wir setzen also g : R â R2 g(t) = (t2 , t3 )T und
berechnen mit der Kettenregel

d
f (t2 , t3 ) = (f âŚ g)â˛ (t) = D(f âŚ g)(t) = Df (g(t)) Âˇ Dg(t) = âf (g(t)) Âˇ Dg(t).
dt

Es ist Dg(t) = (2t, 3t2 )T und in Beispiel 5.4.14 haben wir
âf (x, y) = (3x2 y + ey , x3 + xey )

berechnet. Das liefert zusammen mit (x, y) = g(t) = (t2 , t3 )
 

d
3
3
2t
2 3
4 3
t3 6
2 t3
= 6t8 + 2tet + 3t8 + 3t4 et
f (t , t ) = (3t t + e , t + t e ) Âˇ
2
3t
dt
3

= 9t8 + (2t + 3t4 )et .

Um den Mittelwertsatz, vgl. Satz 5.2.1, auf Funktionen meherer VeraĚnderlicher zu
uĚbertragen, muĚssen wir ein wenig tricksen. So ist z.B. nicht klar was es bedeuten
soll, dass ein Vektor zwischen zwei anderen liegen soll. Dazu definieren wir fuĚr
a, b â Rd die Schreibweise
ab := {a + Îť(b â a) : Îť â [0, 1]}
fuĚr die Verbindungsstrecke von a nach b.
Damit gilt nun der folgende Satz.
Satz 5.5.16 (Mittelwertsatz). Es sei G â Rd offen und f : G â R eine total
differenzierbare Funktion. Sind a, b â G so gewaĚhlt, dass ab â G, so gibt es ein
Îž â ab mit
f (b) â f (a) = âf (Îž)(b â a).
Beweis. Wir definieren g : [0, 1] â G durch g(Îť) = a+Îť(bâa) und F : [0, 1] â R
als f âŚ g. Da g mit Dg(Îť) = b â a stetig partiell differenzierbar und damit auch
total differenzierbar ist, vgl. Satz 5.5.12, ist auch F total differenzierbar und es
gilt nach der Kettenregel in Satz 5.5.13
F â˛ (Îť) = âf (g(Îť)) Âˇ Dg(Îť),

Îť â (0, 1).

Auf diese Funktion koĚnnen wir nun den Mittelwertsatz fuĚr Funktionen in einer
Variablen, Satz 5.2.1, anwenden. Es gibt also ein Ď â (0, 1), fuĚr das
f (b) â f (a) = f (g(1)) â f (g(0)) = F (1) â F (0) = F â˛ (Ď )(1 â 0) = âf (g(Ď ))(b â a)
gilt. Mit Îž := g(Ď ) folgt also die Behauptung.

232

5.5. Totale Differenzierbarkeit
Man beachte die geometrische EinschraĚnkung, dass die Verbindungslinie von a
und b ganz in G liegen muss. Das fuĚhrt uns auf folgenden Begriff.
Definition 5.5.17. Eine Menge M â Rd heiĂt konvex, wenn fuĚr alle a, b â M
auch ab â M gilt.
Satz 5.5.18 (Schrankensatz). Es sei G â Rd offen und konvex, sowie f : G â R
total differenzierbar. Gibt es ein L âĽ 0 mit kâf (x)k2 â¤ L fuĚr alle x â G, so gilt
|f (x) â f (y)| â¤ Lkx â yk2

fuĚr alle x, y â G,

d.h. f ist Lipschitz-stetig auf G.
Beweis. Es seien x, y â G. Dann ist dank der KonvexitaĚt von G die Verbindungsstrecke xy in G enthalten und wir bekommen aus dem Mittelwertsatz 5.5.16 ein
Îž â xy mit
|f (x) â f (y)| = |âf (Îž)(x â y)|

Man beachte, dass das Produkt auf der rechten Seite nun ein Skalarprodukt ist.
Also koĚnnen wir die Cauchy-Schwarzsche Ungleichung, vgl. Satz 3.4.9, anwenden
und erhalten
|f (x) â f (y)| â¤ kâf (Îž)k2kx â yk2 â¤ Lkx â yk2.
Beispiel 5.5.19. Wir betrachten die Abbildung F : R2 â R2 mit
ďŁŤ
ďŁś
arctan(x1 )
2ďŁˇ
ďŁŹ
F (x1 , x2 ) = ďŁ­ (sin(x2 ) + 3) ďŁ¸ .
1 sin(x1 +x2 )/3
e
4
Dann gilt fuĚr die Koordinatenfunktionen F1 und F2

1
â2 arctan(x1 ) cos(x2 ) 
1
âF1 (x1 , x2 ) =
,
(sin(x2 ) + 3)2 1 + x21
(sin(x2 ) + 3)3

und

1  sin(x1 +x2 )/3 1
1
e
cos(x1 + x2 ), esin(x1 +x2 )/3 cos(x1 + x2 )
4
3
3

1 sin(x1 +x2 )/3
cos(x1 + x2 ) 1, 1 .
= e
12

âF2 (x1 , x2 ) =

Nun gilt fuĚr alle (x1 , x2 )T â R2
h
1 i2 h 2 arctan(x1 ) cos(x2 ) i2
1
+
kâF1 (x1 , x2 )k22 =
(sin(x2 ) + 3)2 1 + x21
(sin(x2 ) + 3)3
Ď
h
i
i
 Ď 2
h
2
2Âˇ 2 Âˇ1 2
1
1
5
1
1
â¤
Âˇ
1
+
+
=
,
â¤
=
+
(â1 + 3)2
(â1 + 3)3
16
8
16 4
16
233

5. Analysis â Teil II: Differential- und Integralrechnung
da Ď â¤ 4 gilt. Also ist
kâF1 (x1 , x2 )k2 â¤

â

5
=: L1 .
4

Weiter ist fuĚr alle (x1 , x2 )T â R2
1 sin(x1 +x2 )/3
e
cos(x1 + x2 ) k(1, 1)k2
12
â
1
1
1
â¤ e1/3 Âˇ 1 Âˇ 2 â¤ 81/3 Âˇ 2 = =: L2 .
12
12
3

kâF2 (x1 , x2 )k2 =

Nun ist R2 offen und konvex und F1 und F2 sind wie gerade gesehen total differenzierbar auf R2 . Also koĚnnen wir auf beide den Schrankensatz anwenden und
erhalten so fuĚr alle x = (x1 , x2 ), y = (y1 , y2 ) â R2
2

kF (x) â F (y)k22 = F1 (x) â F1 (y) + F2 (x) â F2 (y)
2
2
5
1
â¤ L1 kx â yk2 + L2 kx â yk2 = kx â yk22 + kx â yk22
16
9
5
9
4
kx â yk22 = kx â yk22
+
â¤
16 16
16

Wir haben also gezeigt, dass F auf R2 Lipschitz-stetig ist mit L = 3/4 < 1. Und
was soll das nun? Damit ist F eine strikte Kontraktion auf R2 , also hat F nach
dem Banachschen Fixpunktsatz 4.6.22 genau einen Fixpunkt in R2 . Wir haben
also gezeigt, dass das Gleichungssystem
2
x1 sin(x2 ) + 3 = arctan(x1 )
4x2 = esin(x1 +x2 )/3

genau eine LoĚsung in R2 hat. HaĚtten Sie das dem Gleichungssystem angesehen?
Wir wollen nun noch den Satz von Taylor in mehreren Variablen betrachten.
Diese Verallgemeinerung ist sehr weitreichend moĚglich, benoĚtigt aber eine Menge
an neuen Notationen. Im Sinne eines Kompromisses zwischen Allgemeinheit und
Darstellbarkeit beschraĚnken wir uns auf den Fall einer Funktion f : G â R mit
G â Rd und auf die Betrachtung des Taylor-Polynoms erster Ordnung.
Dazu definieren wir uns zunaĚchst die Hesse-Matrix der zweiten partiellen Ableitungen, die auch im naĚchsten Abschnitt noch einmal Verwendung finden wird.
Definition 5.5.20. Es sei G â Rd offen und f : G â R in x0 â G zweimal
partiell differenzierbar. Dann heiĂt die Matrix der zweiten partiellen Ableitungen

Hf (x0 ) := âj âk f (x0 ) j,k=1,...,d

Hesse-Matrix von f in x0 .

234

5.6. Extremwertprobleme in mehreren Variablen
Bemerkung 5.5.21. (a) Man beachte, dass die Hesse-Matrix immer ein quadratische Matrix ist. Ist f sogar stetig partiell differenzierbar in x0 , so ist
die Hesse-Matrix nach dem Satz von Schwarz 5.4.15 symmetrisch.
(b) Machen Sie sich klar, dass Hf (x0 ) = Jâf (x0 ) ist.
Satz 5.5.22 (Satz von Taylor). Es sei G â Rd eine offene und konvexe Menge
und f : G â R sei zweimal stetig partiell differenzierbar (und damit auch zweimal
total differenzierbar, vgl. Satz 5.5.12) in G. Zu jeder Wahl von x0 , x â G gibt es
dann ein Îž â x0 x mit
1
f (x) = f (x0 ) + âf (x0 )(x â x0 ) + (x â x0 )T Hf (Îž)(x â x0 ).
2

5.6. Extremwertprobleme in mehreren Variablen
Die Fragestellung ist nun die selbe wie in Abschnitt 5.3: Gegeben eine Funktion f :
G â R mit G â Rd , finde die Stellen x â G, an denen die Werte von f maximal,
bzw. minimal werden. Der einzige Unterschied ist nun, dass f von mehreren
Variablen abhaĚngt. Wir werden aber sehen, dass die LoĚsung des Problems im
Prinzip genauso aussieht wie in einer Variablen.
Noch ein Wort zum betrachteten Zielbereich. Wir betrachten hier nur Funktionen
mit Werten in R, denn Vektoren in Rp sind nicht vergleichbar, die Frage nach
maximalen, bzw. minimalen Werten hat dort also einfach keinen Sinn.
Die Definition von relativen, bzw. globalen Extrema bekommen wir per Copy&Paste:
Definition 5.6.1. Es sei G â Rd und f : G â R eine Funktion.
(a) Man sagt, dass f in x0 â G ein globales Maximum (bzw. globales Minimum) hat, falls f (x) â¤ f (x0 ) (bzw. f (x) âĽ f (x0 )) fuĚr alle x â G gilt.
(b) f hat in x0 â G ein relatives Maximum (bzw. relatives Minimum), falls ein
Î´ > 0 existiert, so dass f (x) â¤ f (x0 ) (bzw. f (x) âĽ f (x0 )) fuĚr alle x â G
mit kx â x0 k < Î´ gilt.
(c) Allgemein spricht man von einem globalen bzw. relativen Extremum in x0 ,
wenn f dort ein entsprechendes Maximum oder Minimum hat.
Wie in einer Dimension gilt das folgende notwendige Kriterium. Die Warnungen
dazu aus Warnung 5.3.5 bleiben alle auch hier guĚltig!
Satz 5.6.2. Es sei G â Rd und x0 ein innerer Punkt von G, sowie f : G â R
total differenzierbar in x0 . Hat f in x0 ein relatives Extremum, so gilt âf (x0 ) = 0.

235

5. Analysis â Teil II: Differential- und Integralrechnung
Beweis. Da x0 ein innerer Punkt von G ist, gibt es ein Îľ > 0, so dass BÎľ (x0 ) â G
ist. FuĚr jedes j â {1, 2, . . . , d} betrachten wir nun die Funktion gj : (âÎľ, Îľ) â R
mit gj (t) = f (x0 + tej ), wobei ej der j-te Basisvektor der Standardbasis ist. Dann
ist nach der Kettenregel jedes gj eine in Null differenzierbare Funktion (in einer
Variablen) und da f ein lokales Extremum in x0 hat, hat gj ein lokales Extremum
in Null.
Nun liefert uns der entsprechende eindimensionale Satz 5.3.4 sofort
0 = gjâ˛ (0) = âf (x0 + 0ej )ej = âj f (x0 ).
Da j beliebig war, liefert das âf (x0 ) = 0.
Ein Beispiel einer Funktion mit einer Nullstelle des Gradienten, die keine Extremalstelle ist, ist der sogenannte Affensattel, der durch die Funktion f : R2 â R
mit f (x, y) = x3 â 3xy 2, vgl. Abbildung 5.6, gegeben ist.

Abbildung 5.6.: Der Graph der Affensattelâ-Funktion f (x, y) = x3 â 3xy 2
â
Um ein hinreichendes Kriterium zu erreichen, koĚnnen wir nun wie im eindimensionalen Fall den Satz von Taylor bemuĚhen. Es sei also nun f : G â R eine
Funktion auf G â Rd , die in einem inneren Punkt x0 â G eine Nullstelle ihres
Gradienten habe. Dann gibt es fuĚr jedes x in einer kleinen Kugel um x0 nach dem
Satz von Taylor 5.5.22 ein Îž â xx0 mit
f (x) = f (x0 ) + 0 + (x â x0 )T Hf (Îž)(x â x0 ).
Damit wir ein relatives Maximum haben, muss also der Ausdruck

(x â x0 )T Hf (Îž)(x â x0 ) = (x â x0 ) Hf (Îž)(x â x0 )
236

5.6. Extremwertprobleme in mehreren Variablen
fuĚr alle Îž nahe bei x0 negativ sein. Nun muĚssen wir tief im GedaĚchtnis kramen
und Definition 3.11.20 finden, in der die Begriffe positiv/negativ definit eingefuĚhrt
wurden. Genau das brauchen wir hier!
Man beachte, dass fuĚr zweimal stetig differenzierbare Funktionen f die HesseMatrix immer symmetrisch ist, es macht also Sinn von positiver Definitheit zu
sprechen.
TatsaĚchlich gilt folgender Satz, vgl. Satz 5.3.6 fuĚr n = 2.
Satz 5.6.3. Es sei G â Rd offen, f : G â R zweimal stetig partiell differenzierbar
und fuĚr x0 â G gelte âf (x0 ) = 0. Ist dann die Hesse-Matrix Hf (x0 )
(a) positiv definit, so hat f in x0 ein relatives Minimum.
(b) negativ definit, so hat f in x0 ein relatives Maximum.
(c) indefinit, so hat f in x0 kein relatives Extremum.
FuĚr Methoden zum Nachweis der (In-)Definitheit sei an Satz 3.11.22 erinnert.
Beispiel 5.6.4. Wir bestimmen die relativen Extrema von f : R2 â R mit
f (x, y) = (x2 + 2y 2)eâx

2 ây 2

,

vgl. Abbildung 5.7.

0.6

0.4

0.2
1.5

1.5

0.5

0.5
0.0
-0.5 x
y
-1.5

-1.5

Abbildung 5.7.: Der Graph der Funktion f (x, y) = (x2 + 2y 2)eâx

2 ây 2

Wir bestimmen die kritischen Punkte mit âf (x, y) = (0, 0). Da
âf (x, y) = 2xeâx
= 2eâx

2 ây 2

2 ây 2

â 2x(x2 + 2y 2 )eâx

2 ây 2

, 4yeâx

2 ây 2


x(1 â x2 â 2y 2 ), y(2 â x2 â 2y 2) ,

â 2y(x2 + 2y 2)eâx

2 ây 2



237

5. Analysis â Teil II: Differential- und Integralrechnung
ist, bekommen wir die kritischen Punkte als die LoĚsungen des Gleichungssystems

x(1 â x2 â 2y 2) = 0
y(2 â x2 â 2y 2) = 0.
Wir betrachten zunaĚchst den Fall x = 0. Dann ist die erste Gleichung erfuĚllt und
die zweite vereinfacht sich zu y(2 â 2y 2) = 0. Diese hat die drei LoĚsungen 0, 1
und â1. Also haben wir bereits die drei Nullstellen des Gradienten (0, 0), (0, 1)
und (0, â1).
Im Fall x 6= 0 koĚnnen wir die erste Gleichung durch x dividieren und verbleiben
mit âx2 â 2y 2 = â1. Setzen wir das in die zweite Gleichung ein, so erhalten
wir 0 = y(2 â 1) = y. In diesem Fall muss also y = 0 sein. Dann ist die zweite
Gleichung auf jeden Fall erfuĚllt und nun vereinfacht sich die erste Gleichung zu
1 â x2 = 0, also muss dann x = 1 oder x = â1 sein.
Insgesamt haben wir fuĚnf kritische Stellen (0, 0), (0, 1), (0, â1), (1, 0) und (â1, 0).
Wir bestimmen nun die Hesse-Matrix zu


2
2
4
2 2
â2xy(3 â x2 â 2y 2)
âx2 ây 2 1 â 9x â 2y + 4x + 8x y
.
Hf (x, y) = 2e
â2xy(3 â x2 â 2y 2)
2 â x2 â 10y 2 + 2x2 y 2 + 4y 4
Damit ist


1 0
Hf (0, 0) = 2
,
Eigenwerte: 2, 4
pos. def.
0 2


2 â1 0
â2 â8
Hf (0, 1) =
, Eigenwerte:
,
neg. def.
e 0 â4
e
e
Hf (0, â1) = Hf (0, 1)


2 â4 0
â8 2
Hf (1, 0) =
,
Eigenwerte:
,
indefinit
e 0 1
e e
Hf (â1, 0) = Hf (1, 0)

Minimum,
Maximum,
Maximum,
kein Extr.,
kein Extr..

Bemerkung 5.6.5. In Rd koĚnnen ein paar Dinge passieren, die im eindimensionalen nicht vorkommen, z.B. kann eine auf ganz R2 definierte Funktion zwei relative Maxima haben, ohne ein relatives Mimimum zu besitzen, vgl. Abbildung 5.8.
HuĚten Sie sich also vor eindimensionalem Denken!

5.7. Integration in R
5.7.1. Definition des bestimmten Integrals
Wir haben nun zunaĚchst unsere Betrachtungen zur Differenziation abgeschlossen und wollen uns einem auf den ersten Blick ganz anderen Problem zuwenden,
der Berechnung von FlaĚcheninhalten von krummlinig begrenzten FlaĚchen. Wir

238

5.7. Integration in R

1.0
0.75
0.5
0.25
0.0
4

2
x

0

-2

2

1

-2
-1
0 y

Abbildung 5.8.: Der Graph der Funktion f (x, y) = eâx

2 ây 2

2 ây 2

+ eâ(xâ3)

werden jedoch feststellen, dass sich uns dabei ein sehr uĚberraschender Zusammenhang zur Differenziation offenbart.
Wir betrachten das Problem der FlaĚchenberechnung unter einem Funktionsgraphen, d.h. fuĚr a, b â R mit a < b und eine gegebene beschraĚnkte Funktion
f : [a, b] â R wollen wir den FlaĚcheninhalt der FlaĚche bestimmen, die von der
x-Achse, den beiden Geraden x = a und x = b und dem Graphen der Funktion
eingeschlossen wird.
Die grundlegende Idee der Integration nach Riemann ist es, die FlaĚche unter dem
Graphen durch die Summation der FlaĚcheninhalte von Rechtecken anzunaĚhern,
die parallel zu den Koordinatenachsen liegen und deren HoĚhe sich nach dem
groĚĂten bzw kleinsten Funktionswert der Funktion im Rechteck richtet. Damit
bekommt man eine AnnaĚherung von oben und von unten, an den wahren FlaĚcheninhalt, die man durch Verfeinerung der Rechtecke (GutmuĚtigkeit der Funktion
sei erst einmal unterstellt) beliebig gut machen kann. Die StaĚrke der analytischen Betrachtung ist, dass wir durch den Grenzwertbegriff diese beliebig gute
â
AnnaĚherungâ mathematisch exakt fassen und formalisieren koĚnnen. Dabei werden wir den oben schon angedeuteten Zusammenhang zur Differentialrechnung
entdecken, und so schlieĂlich tatsaĚchlich in der Lage sein, den FlaĚcheninhalt unter
UmstaĚnden exakt angeben zu koĚnnen.
FuĚr die oben angedeutete Konstruktion brauchen wir ein paar Begriffe.
Definition 5.7.1. Es seien a, b â R mit a < b. Eine endliche Menge Z :=
{x0 , x1 , . . . , xn } â [a, b] heiĂt Zerlegung des Intervalls [a, b], wenn gilt
a = x0 < x1 < x2 < Âˇ Âˇ Âˇ < xnâ1 < xn = b.

239

5. Analysis â Teil II: Differential- und Integralrechnung
FuĚr eine solche Zerlegung und eine gegebene beschraĚnkte Funktion f : [a, b] â R
setzen wir nun fuĚr jedes j = 1, . . . , n
Ij := [xjâ1 , xj ],

|Ij | := xj â xjâ1

mj := inf f (Ij ) Mj := sup f (Ij ).

Mit Hilfe dieser Notationen koĚnnen wir nun unsere Summen uĚber die Rechtecke
definieren.
Definition 5.7.2. Es seien a, b â R mit a < b, Z = {x0 , . . . , xn } eine Zerlegung
von [a, b] und f : [a, b] â R beschraĚnkt. Dann heiĂt der Wert
n
X
mj |Ij | die Untersumme von f zu Z und
sf (Z) :=
j=1

sf (Z) :=

n
X
j=1

Mj |Ij | die Obersumme von f zu Z.

Bemerkung 5.7.3. Offensichtlich ist jeder Summand der Untersumme kleiner
oder gleich dem entsprechenden Summanden der Obersumme, d.h. wir haben
immer sf (Z) â¤ sf (Z).
Definition 5.7.4. Es seien a, b â R mit a < b und f : [a, b] â R sei beschraĚnkt.
Wir nennen
Z b
f (x) dx := sup{sf (Z) : Z Zerlegung von [a, b]}
a

unteres Integral von f auf [a, b] und
Z

b

f (x) dx := inf{sf (Z) : Z Zerlegung von [a, b]}

a

oberes Integral von f auf [a, b].
Weiter heiĂt f auf [a, b] (Riemann-)integrierbar, wenn
Z

b

f (x) dx =

a

Z

b

f (x) dx

a

gilt. In diesem Fall nennen wir
Z

b

f (x) dx :=
a

das (Riemann-)Integral von f auf [a, b].

240

Z

b

f (x) dx
a

5.7. Integration in R
Bemerkung 5.7.5. (a) Auch fuĚr das obere und das untere Integral gilt analog
zu Bemerkung 5.7.3 immer die Ungleichung
Z

b

a

f (x) dx â¤

Z

b

f (x) dx.

a

(b) Das Integral von f auf [a, b] gibt nun also, falls es existiert, den exakten
FlaĚcheninhalt unter dem Graphen an. Dabei wird allerdings das Vorzeichen beachtet: FlaĚchenanteile unter der x-Achse zaĚhlen negativ. Will man
auch diese positiv rechnen, muss man statt uĚber f uĚber die Funktion |f |
integrieren. Machen Sie sich das an einem Bild klar!
Wir betrachten einige einfache Beispiele.
Beispiel 5.7.6. (a) Es sei f : [a, b] â R eine konstante Funktion mit f (x) = c
auf [a, b]. Dann gilt fuĚr jede Zerlegung Z = {x0 , x1 , . . . , xn } von [a, b] und
jedes j â {1, . . . , n} natuĚrlich mj = Mj = c und damit
sf (Z) = sf (Z) =

n
X
j=1

Mj |Ij | = c

n
X
j=1

(xj â xjâ1 ) = c(xn â x0 ) = c(b â a).

Also haben auch das obere und das untere Integral diesen Wert, woraus
Z b
c dx = c(b â a)
a

folgt.
(b) Wir untersuchen auf dem Intervall [0, 1] die etwas gewoĚhnungsbeduĚrftige
Funktion
(
1, falls x â [0, 1] âŠ Q
f (x) =
0, falls x â [0, 1] und x 6â Q,
die sogenannte Dirichletsche Sprungfunktion. Sei also Z eine beliebige Zerlegung von [0, 1]. Dann liegt in jedem Teilintervall je eine irrationale und
eine rationale Zahl, also gilt immer mj = 0 und Mj = 1. Damit ist jede
Untersumme Null und damit auch das untere Integral
Z 1
f (x) dx = 0.
0

Jede Obersumme ist
sf (Z) =

n
X
j=1

1|Ij | =

n
X
j=1

|Ij | = 1 â 0 = 1

241

5. Analysis â Teil II: Differential- und Integralrechnung
und damit gilt fuĚr das obere Integral
Z 1
f (x) dx = 1.
0

Wir haben damit also ein Beispiel einer beschraĚnkten aber nicht Riemannintegrierbaren Funktion.
(c) Auf dem Intervall [0, 1] sei die Funktion
f (x) = x
gegben. Wir betrachten zunaĚchst den Spezialfall einer aĚquidistanten Zerlegung, d.h. wir setzen Zn := {0, 1/n, 2/n, . . . , (n â 1)/n, 1} fuĚr jedes n â Nâ ,
d.h. xj = j/n, j = 0, 1, . . . , n. Dann ist |Ij | = 1/n fuĚr jedes j = 1, . . . , n
und Dank der Monotonie von f gilt
mj = f (xjâ1 ) =
Damit gilt wegen
sf (Zn ) =

n
X
j=1
2

=

Pn

j=1 j

und Mj = f (xj ) =

j
.
n

= n(n + 1)/2, vgl. Beispiel 1.5.5,

mj |Ij | =

n ân
.
2n2

jâ1
n

n
X
jâ11
j=1

n
1 (n â 1)n
1 X
(j â 1) = 2
= 2
n n
n j=1
n
2

und damit limnââ sf (Zn ) = 1/2. FuĚr die Obersummen bekommen wir genauso wegen
sf (Zn ) =

n
X
j1
1 n(n + 1)
n2 + n
= 2
=
nn
n
2
2n2
j=1

den Grenzwert limnââ sf (Zn ) = 1/2.
Das schoĚne daran, dass schon diese beiden Grenzwerte uĚbereinstimmen,
ist nun, dass wir damit gar keine anderen Zerlegungen mehr betrachten
muĚssen, denn nun gilt fuĚr jedes n â Nâ mit Hilfe von Bemerkung 5.7.5 (a)
Z 1
Z 1
x dx â¤ sf (Zn ).
x dx â¤
sf (Zn ) â¤
0

0

Da die Untersumme links und die Obersumme rechts beide gegen 1/2 gehen,
muĚssen das obere und das untere Integral in der Mitte beide den Wert 1/2
haben und es gilt
Z 1
1
x dx = .
2
0

242

5.7. Integration in R
Die FlaĚcheninhalte in den Beispielen (a) und (c) sind natuĚrlich auch mit einfachen geometrischen UĚberlegungen bestimmbar. Die Bestimmung von Integralen
fuĚr kompliziertere Funktionen uĚber das obere und untere Integral ist offensichtlich nicht praktikabel, wir muĚssen also nach weiteren Methoden suchen. FuĚr den
Moment vertagen wir diese Frage noch und sammeln zunaĚchst einige elementare
Rechenregeln fuĚr Integrale.
Satz 5.7.7. Es seien a, b â R mit a < b und integrierbare Funktionen f, g :
[a, b] â R gegeben. Dann gelten die folgenden Aussagen.
(a) (Monotonie) Ist f (x) â¤ g(x) fuĚr alle x â [a, b], so ist auch
Z b
Z b
f (x) dx â¤
g(x) dx.
a

a

(b) (HomogenitaĚt) Ist Îą â R, so ist auch Îąf integrierbar und es gilt
Z b
Z b
Îąf (x) dx = Îą
f (x) dx.
a

a

(c) (AdditivitaĚt) Auch die Funktion f + g ist integrierbar und es gilt
Z b
Z b
Z b

f (x) + g(x) dx =
f (x) dx +
g(x) dx.
a

a

a

(d) (Dreiecksungleichung) Die Funktion |f | ist ebenfalls integrierbar und es gilt
Z b
Z b
f (x) dx â¤
|f (x)| dx.
a

a

(e) Ist c â (a, b), so ist f auch integrierbar auf [a, c] und [c, b] und es gilt
Z b
Z c
Z b
f (x) dx =
f (x) dx +
f (x) dx.
a

a

c

Beweis. Wir fuĚhren nur exemplarisch den Teil (a) aus.
Es sei Z = {x0 , x1 , . . . , xn } eine Zerlegung von [a, b] und Ij , |Ij | und mj fuĚr
die Funktion f wie in den vorherigen Nummern. Weiter setzen wir fuĚr g analog
mĚj := inf g(Ij ), j = 1, 2, . . . , n. Da f (x) â¤ g(x) auf ganz [a, b] gilt, gilt das auch
auf jedem Intervall Ij und wir bekommen mj â¤ mĚj fuĚr alle j = 1, 2, . . . , n. Das
liefert fuĚr die Untersummen
sf (Z) =

n
X
j=1

mj |Ij | â¤

n
X
j=1

mĚj |Ij | = sg (Z).

243

5. Analysis â Teil II: Differential- und Integralrechnung
Also ist dank der Integrierbarkeit von f und g
Z b
Z b
f (x) dx = sup{sf (Z) : Z Zerlegung von [a, b]}
f (x) dx =
a

a

â¤ sup{sg (Z) : Z Zerlegung von [a, b]} =

Z

b

g(x) dx =

a

Z

b

g(x) dx.

a

Die folgende AbschaĚtzung fuĚr den Wert eines Integrals ist zwar aĚuĂerst grob, aber
trotzdem haĚufig hilfreich.
Satz 5.7.8 (StandardabschaĚtzung). Es seien a, b â R mit a < b und f : [a, b] â
R integrierbar. Dann ist
Z b
f (x) dx â¤ (b â a) sup |f (x)| = (b â a)kf kâ .
xâ[a,b]

a

Beweis. ZunaĚchst gilt mit der Dreiecksungleichung in Satz 5.7.7 (d)
Z b
Z b
f (x) dx â¤
|f (x)| dx.
a

a

Die konstante Funktion g : [a, b] â R mit g(x) = supyâ[a,b] |f (y)|, x â [a, b], ist
nach Beispiel 5.7.6 (a) integrierbar mit Integral (b â a) supyâ[a,b] |f (y)|. AuĂerdem
gilt |f (x)| â¤ g(x) fuĚr alle x â [a, b]. Also liefert die Monotonie des Integrals aus
Satz 5.7.7 (a)
Z b
Z b
f (x) dx â¤
g(x) dx = (b â a) sup |f (x)|.
a

xâ[a,b]

a

Definition 5.7.9. Es seien a, b â R mit a < b und f : [a, b] â R sei integrierbar.
Dann setzt man fuĚr jedes c â [a, b]
Z c
Z a
Z b
f (x) dx := 0
und
f (x) dx := â
f (x) dx.
c

b

a

Zum Abschluss dieses Abschnitts wollen wir noch sicherstellen, dass die Menge
der integrierbaren Funktionen groĂ ist. Dazu dient der folgende Satz, den wir mal
wieder nicht beweisen wollen.
Satz 5.7.10. Es seien a, b â R mit a < b. Jede stetige und jede monotone
Funktion f : [a, b] â R ist integrierbar.
Bemerkung 5.7.11. Man beachte, dass sowohl stetige als auch monotone Funktionen f : [a, b] â R automatisch beschraĚnkt sind. FuĚr stetige Funktionen folgt
das aus Satz 4.7.26, fuĚr monotone Funktionen ist einfach einer der beiden Werte
am Rand am groĚĂten bzw. kleinsten.

244

5.7. Integration in R
UĚbungsaufgabe 5.7.12. Es seien a, b â R mit a < b gegeben.
(a) Weisen Sie nach, dass C([a, b]) ein R-Vektorraum ist.
(b) Zeigen Sie, dass durch
(f |g) :=

Z

b

f (x)g(x) dx,
a

f, g â C([a, b]),

auf C([a, b]) ein Skalarprodukt gegeben ist.
(c) BegruĚnden Sie, warum die Abbildung k Âˇ k2 : C([a, b]) â R mit
Z b
1/2
2
kf k2 :=
f (x) dx
,
f â C([a, b]),
a

eine Norm ist.
(d) Ist die Norm kÂˇk2 auf C([a, b]) aĚquivalent zur Supremumsnorm, vgl. UĚbungsaufgabe 4.8.9?

5.7.2. Stammfunktionen und der Hauptsatz
Definition 5.7.13. Es seien a, b â R mit a < b und f, F : [a, b] â R Funktionen.
Man sagt F ist eine Stammfunktion von f , wenn F auf [a, b] differenzierbar ist
und F â˛ = f auf [a, b] gilt.
Bemerkung 5.7.14. Ist F eine Stammfunktion von f , so ist fuĚr jede Konstante
c â R auch F + c eine Stammfunktion, denn es ist auch (F + c)â˛ = F â˛ + 0 = f .
Sind umgekehrt F und G zwei Stammfunktionen von f , so gilt (F â G)â˛ =
F â˛ â Gâ˛ = f â f = 0, also gibt es nach Satz 5.2.2 (c) eine Konstante c â R mit
F âG = c. Zwei Stammfunktionen unterscheiden sich also nur um eine Konstante.
Wir kommen nun zum zentralen Zusammenhang zwischen Integration und Differentiation
Satz 5.7.15 (Hauptsatz der Differenzial- und Integralrechnung).
Es seien a, b, c â R mit a < c < b und eine stetige Funktion f : [a, b] â R
gegeben. Dann gelten die folgenden Aussagen.
Rx
(a) Die Funktion F : [a, b] â R mit F (x) := c f (s) ds, x â I, ist eine
Stammfunktion von f .
(b) Ist ÎŚ : [a, b] â R eine Stammfunktion von f , so gilt
Z x
ÎŚ(x) = ÎŚ(c) +
f (s) ds
fuĚr alle x â [a, b].
c

245

5. Analysis â Teil II: Differential- und Integralrechnung
Beweis. (a) Wir muĚssen zeigen, dass F auf [a, b] differenzierbar ist. Dazu sei
x0 â [a, b] und h â R mit x0 + h â [a, b]. Dann gilt nach Definition von F
Z
Z x0
 1 Z x0 +h
F (x0 + h) â F (x0 )
1  x0 +h
f (s) ds.
f (s) dsâ
f (s) ds =
=
h
h c
h x0
c
R x +h
Weiter gilt f (x0 ) = h1 x00 f (x0 ) ds. Also haben wir mit Hilfe der StandardabschaĚtzung aus Satz 5.7.8
Z
Z
1 x0 +h
1 x0 +h
F (x0 + h) â F (x0 )
f (s) ds â
f (x0 ) ds
â f (x0 ) =
h
h x0
h x0
Z x0 +h

1
=
sup
f (s) â f (x0 ) .
f (s) â f (x0 ) ds â¤
|h| x0
sâ[x0 â|h|,x0 +|h|]

Da f in x0 stetig ist, geht nun dieses Supremum gegen Null, wenn h gegen
Null strebt (warum?). Damit ist gezeigt, dass F in x0 differenzierbar ist mit
F â˛ (x0 ) = f (x0 ).

(b) Sei F wie in (a) mit c = a. Dann gilt mit Hilfe von (a) und der Voraussetzung fuĚr jedes x â [a, b]
(F â ÎŚ)â˛ (x) = F â˛ (x) â ÎŚâ˛ (x) = f (x) â f (x) = 0.
Also gibt es eine Konstante Îą â R mit F (x) = ÎŚ(x) + Îą. Damit erhalten
wir schlieĂlich fuĚr jede Wahl von x aus [a, b]
Z x
Z x
Z c
f (s) ds =
f (s) ds â
f (s) ds = F (x) â F (c)
c

a

a

= ÎŚ(x) â Îą â ÎŚ(c) + Îą = ÎŚ(x) â ÎŚ(c),

woraus durch Umstellen der Gleichung die Behauptung folgt.
Bemerkung 5.7.16. (a) Der Hauptsatz verknuĚpft auf verbluĚffend einfache
Weise die Integral- mit der Differenzialrechung und ermoĚglicht so die explizite Berechnung von vielen Integralen, indem er unsere Erkenntnisse uĚber
die Differentiation zur Integralberechnung nutzbar macht.
Nach Teil (b) des Hauptsatzes koĚnnen wir den Wert eines Integrals uĚber
f leicht bestimmen, sobald wir eine Stammfunktion F finden koĚnnen. Damit ist das Problem der Integration darauf zuruĚckgefuĚhrt den Vorgang der
Differentiation umzukehren.
(b) Ist F eine Stammfunktion von f , so erhaĚlt man sofort
Z b
x=b
f (x) dx = F (a) â F (b) =: F (x) x=a .
a

Letzteres ist dabei eine praktische Schreibweise.

246

5.7. Integration in R
Wir berechnen einige Integrale mit dem Hauptsatz.
Beispiel 5.7.17. (a) FuĚr 0 < a < b betrachten wir die Funktion f (x) = 1/x
auf dem Intervall [a, b]. Diese ist stetig und damit integrierbar. UĚberdies ist
F (x) := ln(x), x â [a, b] eine Stammfunktion von f , denn es gilt F â˛ = f auf
[a, b]. Also bekommen wir mit dem Hauptsatz
Z

a

b

1
dx = ln(x)
x

x=b
x=a

= ln(b) â ln(a).

Ist a < b < 0, so sieht man analog
Z

b
a

1
dx = ln(âx)
x

x=b
x=a

.

(b) Wir betrachten die Funktion f (x) = cos(x), x â [0, Ď]. Diese ist wieder
stetig und damit integrierbar. Eine Stammfunktion ist F (x) := sin(x), also
gilt
Z
Ď

cos(x) dx = sin(x)

0

x=Ď
x=0

= sin(Ď) â sin(0) = 0 â 0 = 0.

Man merkt anhand dieses Beispiels noch einmal, dass nach der Definition des Integrals FaĚchen unter der x-Achse negativ gezaĚhlt werden. Beim
Cosinus heben sich nun der positive und der negative Beitrag genau auf.
Will man, dass auch FlaĚchen unter der x-Achse positiv gerechnet werden,
so integriert man uĚber den Betrag der Funktion.

Definition 5.7.18. Es sei I â R ein Intervall. Besitzt f : I â R auf I eine
Stammfunktion, so schreibt man fuĚr die Menge aller Stammfunktionen auch das
sogenannte unbestimmte Integral
Z
f (x) dx.
R
Man beachte dabei, dass nun das Symbol f (x) dx eine Menge von Funktionen
Rb
bezeichnet, waĚhrend das bestimmte Integral a f fuĚr vorgegebene a, b â R eine
Zahl ist.
Z
Beispiel 5.7.19. (a)
ex dx = ex + c, c â R.
(b)
(c)

Z

Z

sin(x) dx = â cos(x) + c,
xn dx =

1
xn+1 + c,
n+1

c â R.
c â R,

fuĚr jedes n â Z \ {â1} und

247

5. Analysis â Teil II: Differential- und Integralrechnung
(d)

Z

1
dx = ln(|x|) + c,
x

c â R.

Funktionen, die durch konvergente Potenzreihen gegeben sind, sind im Inneren
des Konvergenzbereichs stetig, also auch integrierbar. Freundlicherweise lassen
sie sich sogar summandenweise integrieren:
Pâ
n
Satz 5.7.20. Es sei
eine Potenzreihe in R mit Konvergenzradius
n=0 an x P
an n+1
denselben Konvergenzradius und
groĚĂer Null. Dann hat die Reihe â
n=0 n+1 x
es gilt
Z X
â
â Z
â
X
X
an n+1
n
an x dx =
an xn dx =
x
+c
n
+
1
n=0
n=0
n=0

innerhalb des Konvergenzbereichs.

Beispiel 5.7.21. Mit Hilfe dieses Satzes koĚnnen wir wieder zu einigen Potenzreihen geschlossene Darstellungen fuĚr die durch sie gegebene Funktion bestimmen.
Als Beispiel betrachten wir die Reihe
â
X
xn

n

n=1

.

Diese hat den Konvergenzradius 1 und es gilt im Konvergenzkreis, also fuĚr |x| < 1
nach obigem Satz und mit Hilfe der geometrischen Reihe
â
X
xn
n=1

n

=

â Z
X
n=1

x
nâ1

t

dt =

0

= â ln(1 â t)

Z

0

t=x
t=0

â
xX

nâ1

t

n=1

dt =

Z

0

â
xX
n=0

n

t dt =

Z

x
0

1
dt
1ât

= â ln(1 â x) + ln(1 â 0) = â ln(1 â x).

5.8. Integrationstechniken
Zur Integration von Funktionen ist das Auffinden von Stammfunktionen von
zentraler Bedeutung. Leider gibt es dazu nicht wie bei der Differenziation einen
kompletten Satz von Regeln, mit dessen Hilfe, genug Zeit und Konzentration vorausgesetzt, im Prinzip jede Funktion integriert werden kann. Statt dessen muĚssen
wir uns mit Rechenregeln begnuĚgen, die meist das Problem der Integration einer
Funktion auf das entsprechende Problem fuĚr eine andere Funktion zuruĚckspielen,
die dann hoffentlich einfacher ist.
Das liegt nicht daran, dass uns im Moment noch starke mathematische Hilfsmittel fehlen, sondern ist ein prinzipielles Problem. Es gibt einfache stetige (sogar
beliebig oft differenzierbare) Funktionen, die nach dem Hauptsatz eine Stammfunktion haben, die aber nicht in einer geschlossenen Form angebbar ist.
Zusammengefasst ist dies in dem Spruch:

248

5.8. Integrationstechniken
Differenzieren ist Handwerk, Integrieren ist Kunst.
Wir wollen uns dieser Kunst nun naĚhern, indem wir aus den bekannten Differenziationsregeln, Rechenregeln fuĚr Integrale ableiten. Wir beginnen mit der
Produktregel.
Im gesamten Abschnitt seien wieder a, b â R mit a < b gegeben.
Satz 5.8.1 (Partielle Integration). Es seien f, g : [a, b] â R stetig differenzierbare
Funktionen. Dann gilt
Z

b

x=b

â˛

f (x)g(x) dx = f (x)g(x)

x=a

a

â

Z

b

f (x)g â˛(x) dx.
a

Beweis. ZunaĚchst einmal existieren alle in der Behauptung auftretenden Integrale, denn nach Voraussetzung sind f â˛ g und f g â˛ stetige Funktionen.
Nach der Produktregel gilt nun
(f g)â˛ = f â˛ g + f g â˛ .
Also haben wir mit dem Hauptsatz der Differenzial- und Integralrechnung, Theorem 5.7.15, vgl. Bemerkung 5.7.16 (b),
Z

b
â˛

â˛



f (x)g(x) + f (x)g (x) dx =
a

d.h.

Z

Z

b

(f g)â˛(x) dx = f (x)g(x)

a

b
â˛

f (x)g(x) dx = f (x)g(x)

a

x=b
x=a

â

Z

x=b
x=a

,

b

f (x)g â˛(x) dx.
a

Bemerkung 5.8.2. Dieselbe Regel kann man auch fuĚr unbestimmte Integrale
formulieren. Dann lautet sie fuĚr zwei Funktionen f, g â C 1 ([a, b])
Z
Z
â˛
f (x)g(x) dx = f (x)g(x) â f (x)g â˛ (x) dx.
Beispiel 5.8.3.

(a) Wir betrachten das Integral
Z

1

xex dx,

0

d.h. wir wenden unseren Satz mit g(x) = x und f â˛ (x) = ex auf dem Intervall
[0, 1] an. Dann ist f (x) = ex eine moĚgliche Wahl fuĚr die Funktion f und
wir erhalten mit partieller Integration:
Z 1
Z 1
 x=1 
x=1
x
x
xe dx = xe
â
= e â (e â 1) = 1.
ex dx = e â ex
0

x=0

0

x=0

249

5. Analysis â Teil II: Differential- und Integralrechnung
(b) Die Wahl von f und g kann fuĚr den Erfolg einer Anwendung dieser Regel
sehr entscheidend sein. Wenn wir beispielsweise im Integral aus (a) umgekehrt g(x) = ex und f â˛ (x) = x genommen haĚtten, waĚren wir bei
Z 1
Z 1
1 2 x
1 2 x x=1
x
â
xe dx = x e
x e dx
x=0
2
0 2
0
gelandet. Diese Umformung ist natuĚrlich auch richtig, aber von dem nun
entstandenen Integral weiĂ man erst recht nicht wie man es berechnen soll.
(c) Manchmal muss man sich die zweite Funktion zur partiellen Integration
erst kuĚnstlich schaffen:
Z
Z
Z
1
ln(x) dx = 1 Âˇ ln(x) dx = x ln(x) â x dx
x
= x ln(x) â x + c, c â R,
wobei wir g(x) = ln(x) und f â˛ (x) = 1 gewaĚhlt haben.
(d) Wir wollen
I :=

Z

Ď/2

sin2 (x) dx

0

â˛

bestimmen. Dazu waĚhlen wir f (x) = g(x) = sin(x) und berechnen
Z Ď/2
Z Ď/2
x=Ď/2
â
cos(x)(â cos(x)) dx =
cos2 (x) dx.
I = â cos(x) sin(x)
x=0

0

0

Wenden wir nun mit f â˛ (x) = g(x) = cos(x) noch einmal partielle Integration an, so erhalten wir
Z Ď/2
Z Ď/2
x=Ď/2
â
sin(x)(â sin(x)) dx =
sin2 (x) dx = I
I = sin(x) cos(x)
x=0

0

0

und damit auĂer der Gewissheit, dass wir uns unterwegs nicht verrechnet
haben, nichts neues. Wir muĚssen also einen anderen Weg suchen: Mit dem
Ergebnis unserer ersten partiellen Integration und dem trigonometrischen
Pythagoras sin2 (x) + cos2 (x) = 1, finden wir
Z Ď/2
Z Ď/2
Z Ď/2
Ď
Ď
2
2
I=
cos (x) dx =
(1âsin (x)) dx = â
sin2 (x) dx = âI,
2
2
0
0
0
woraus 2I = Ď/2 und schlieĂlich
Z Ď/2
Ď
I=
sin2 (x) dx =
4
0
folgt.

250

5.8. Integrationstechniken
Die zweite wichtige Integrationsregel ergibt sich aus der Kettenregel der Differenzialrechnung.
Satz 5.8.4 (Substitutionsregel). Es seien [a, b] â R und [c, d] â R kompakte
Intervalle, sowie f â C([a, b]) und g â C 1 ([c, d]) mit g([c, d]) â [a, b]. Dann ist
Z

d
â˛

c

f (g(t)) Âˇ g (t) dt =

Z

g(d)

f (x) dx.

g(c)

Beweis. Als stetige Funktion besitzt f auf [a, b] eine Stammfunktion F . Wir
betrachten die Funktion H := F âŚ g auf [c, d]. Dann gilt fuĚr alle t â [c, d] nach
der Kettenregel
H â˛(t) = F â˛ (g(t)) Âˇ g â˛(t) = f (g(t)) Âˇ g â˛(t).
Also koĚnnen wir mit zweimaliger Anwendung des Hauptsatzes folgern:
Z

d
â˛

c

f (g(t)) Âˇ g (t) dt = H(d) â H(c) = F (g(d)) â F (g(c)) =

Bemerkung 5.8.5.
Fall:

Z

g(d)

f (x) dx.

g(c)

(a) Die Version fuĚr unbestimmte Integrale ist in diesem

Es seien I, J â R Intervalle und f â C(I), sowie g â C 1 (J) seien Funktionen mit g(J) = I. Dann gilt
Z
Z
â˛
f (g(t)) Âˇ g (t) dt = f (x) dx
auf J.
x=g(t)

Die Schreibweise |x=g(t)â auf der rechten Seite bedeutet dabei, dass man
â
zunaĚchst das gesamte Integral auszurechnen hat und dann am Ende uĚberall
fuĚr die Variable x den Wert g(t) einsetzt.
(b) HaĚufig behilft man sich bei der Anwendung der Substitutionsregel einer
intuitiven, aber nicht rigorosen Schreibweise. Diese leitet sich aus der aldy
ternativen Notation dx
(gesprochen dy nach dxâ) statt y â˛ fuĚr eine diffeâ
renzierbare Funktion y ab. Man fasst dann in der Substitutionsregel die
Setzung x = g(t) so auf, als sei x eine Funktion von t und rechnet mit den
Differenzialen dx und dt wie gewohnt:
dx
= g â˛ (t)
dt

â

â dx = g â˛ (t) dt. â

Dabei erhaĚlt man genau die in der Substitionsformel stehende Ersetzung
von dx durch g â˛ (t)dt.
Dieser Formalismus ist sehr uĚbersichtlich und praktisch, es sollte dabei aber
nicht in Vergessenheit geraten, dass das keine saubere Mathematik ist.

251

5. Analysis â Teil II: Differential- und Integralrechnung
Beispiel 5.8.6.

(a) Wir berechnen das Integral
Z

2

e2x + 1
dx
ex

1

mit unserer Schmierrechnungsmethode. Dazu setzen wir x = ln(t), d.h.
wir wenden die Substitutionsregel mit g(t) = ln(t) an. Weiter ist bei der
Anwendung des Satzes c = e und d = e2 , denn dann ist g(c) = 1 und
g(d) = 2. Die natuĚrliche Wahl fuĚr [a, b] ist [1, 2], aber auch [a, b] = [â3, 15]
ist in Ordnung. Nun wenden wir die Substitutionsregel an. Es ist dx
= 1/t,
dt
also dx = dt/tâ. Damit haben wir
â
Z 2 2x
Z e2 2
Z e2 

1  t=e2
e +1
1
t + 1 dt
dx
=
Âˇ
=
dt
=
t
â
1
+
ex
t
t
t2
t t=e
1
e
e
2
â2
â1
= e â e â (e â e ).
(b) Als zweites Beispiel wollen wir das Integral
I :=

Z

0

1

â

1 â x2 dx

bestimmen. Dieses
hat auch eine anschauliche Bedeutung, denn der Graph
â
der Funktion 1 â x2 ist fuĚr x â [0, 1] der Viertelkreisbogen des Kreises mit
Radius 1 um 0 zwischen den Punkten (1, 0) und (0, 1). Wir bestimmen mit
diesem Integral also die FlaĚche dieses Viertelkreises, es sollte also bitteschoĚn
Ď/4 herauskommen.
Wir substituieren x = cos(t). Dann gilt z.B. x = 0 fuĚr t = Ď/2 und x = 1
fuĚr t = 0. Wir waĚhlen also c = Ď/2 und d = 0. Die Schmierrechnung gibt
uns wegen dx
= cosâ˛ (t) = â sin(t) die Ersetzung dx = â sin(t) dt. Nun gilt
dt
fuĚr alle t â [0, Ď/2]
q
p
â
2
2
1 â x = 1 â cos (t) = sin2 (t) = | sin(t)| = sin(t).

Setzen wir das nun alles zusammen, ergibt sich mit Beispiel 5.8.3 (d) tatsaĚchlich
Z 0
Z Ď
2
Ď
sin2 (t) dt = .
I=
sin(t)(â sin(t)) dt =
Ď
4
0
2

(c) SchlieĂlich noch ein Beispiel eines unbestimmten Integrals. FuĚr eine stetig
differenzierbare Funktion Ď : R â R \ {0} betrachten wir
Z â˛
Ď (t)
dt.
Ď(t)
252

5.8. Integrationstechniken
Mit f (x) := 1/x, x â R \ {0}, gilt dann dank der Substitionsregel aus
Bemerkung 5.8.5 (a)
Z â˛
Z
Z
Z
Ď (t)
1
â˛
dt = f (Ď(t)) Âˇ Ď (t) dt = f (x) dx
=
dx
Ď(t)
x=Ď(t)
x
x=Ď(t)


= ln(|x|) + c x=Ď(t) = ln |Ď(t)| + c, c â R.

UĚbungsaufgabe 5.8.7. Zeigen Sie:

(a) Ist f : R â R ungerade, so gilt fuĚr jedes a â R
Z a
f (x) dx = 0.
âa

(b) Ist f : R â R gerade, so gilt fuĚr jedes a â R
Z a
Z a
f (x) dx = 2
f (x) dx.
âa

0

UĚbungsaufgabe 5.8.8. FuĚr alle n, m â N
ďŁą
ďŁ´
Z Ď
ďŁ˛2Ď,
cos(nx) cos(mx) dx = Ď,
ďŁ´
âĎ
ďŁł
0,
Z

Ď

sin(nx) sin(mx) dx =

âĎ

Z

(

Ď,
0,

gilt
falls n = m = 0
falls n = m 6= 0
falls n 6= m

falls n = m 6= 0
falls n =
6 m oder n = m = 0

Ď

sin(nx) cos(mx) dx = 0.

âĎ

Zum Abschluss dieses Abschnitts wollen wir uns mit dem Problem des Differenzierens unter dem Integralzeichen beschaĚftigen. Die Problematik ergibt sich aus
dem folgenden Beispiel: Ist die Funktion
Z 2 xy
e â ey
dy, x â R,
(5.3)
g(x) :=
y
1

differenzierbar auf R und wenn ja, was ist die Ableitung?
Eine befriedigende Antwort gibt der folgende Satz.

Satz 5.8.9 (Differenzieren von Paramter-Integralen). Es sei G â R2 offen mit
[Îą, Î˛] Ă [a, b] â G und f : G â R sei (total) differenzierbar, sowie die partielle
Ableitung â1 f stetig. Dann ist die Funktion
Z b
g(x) :=
f (x, y) dy, x â [Îą, Î˛],
a

253

5. Analysis â Teil II: Differential- und Integralrechnung
differenzierbar und es gilt
Z b
Z b
âf
dg
â˛
(x) =
(x, y) dy,
â1 f (x, y) dy =
g (x) =
dx
a
a âx

x â [Îą, Î˛].

Damit beantworten wir nun obige Frage.
Beispiel 5.8.10. Die Funktion f (x, y) := (exy â ey )/y ist auf R Ă (0, â) total
differenzierbar mit stetigen partiellen Ableitungen, also ist nach obigem Satz die
Funktion g aus (5.3) tatsaĚchlich differenzierbar und ihre Ableitung berechnet sich
zu
Z 2
Z 2 xy
Z 2
Z 2
âf
ye
â h exy â ey i
â˛
g (x) =
dy =
(x, y) =
dy =
exy dy.
âx
âx
y
y
1
1
1
1
Dieses Integral hat nun den Wert 1, wenn x = 0 ist und fuĚr x 6= 0 gilt
g â˛ (x) =

1 xy
e
x

y=2
y=1

=

1 2x
(e â ex ).
x

Beispiel 5.8.11. Was machen wir nun aber, wenn nach der Ableitung folgender
Funktion gesucht ist:
g(x) :=

Z

x2

x+1

exy â ey
dy,
y

Wir definieren G : (0, â)3 â R mit
G(x, u, v) :=

Z

u

v

x â (0, â)?

exy â ey
dy,
y

dann ist g(x) = G(x, x + 1, x2 ) und wir bekommen mit der mehrdimensionalen
Kettenregel, vgl. Satz 5.5.13,
g â˛(x) =

dg
(x) = âG(x, x + 1, x2 ) Âˇ Df (x),
dx

wobei f : (0, â) â R3 durch f (x) := (x, x + 1, x2 )T gegeben ist.
Nun ist nach obigem Beispiel und Teil (a) von Satz 5.7.15
âG(x, u, v) =
=
und wir haben

1

x
1
x

exu â eu exv â ev 
y=v
,
â
,
y=u
u
v
u
xu
xv

e
â
e
e
â ev 
xv
xu

exy
e

âe

,

u

Df (x) = (1, 1, 2x)T ,

254

,

v

,

x â (0, â).

x, u, v â (0, â),

5.9. Uneigentliche Integrale
Zusammen ergibt das
g â˛(x) =

1

x

 e
2
exÂˇx â ex(x+1) ,

x+1

xÂˇx2

x(x+1)

âe
x+1

,

x2 

âe
x2

e

3
2
 ex e â ex2 ex
1 x3
ex â ex
x2 x
= e âe e +
+ 2x
x
x+1
x2
x

e
1
3
2
2
e â ex .
= 3ex â ex (ex + 2) +
x
x+1

ďŁŤ

ďŁś
1
ÂˇďŁ­1ďŁ¸
2x

UĚbungsaufgabe 5.8.12. Es seien f : R2 â R und F : R3 â R gegeben durch
Z Î˛
x 2
f (x, y) := e y und F (y, Îą, Î˛) =
f (x, y) dx.
Îą

Weiter seien Îą(y) = ln(y) und Î˛(y) = 2 ln(y). Bestimmen Sie
âF
(y, Îą(y), Î˛(y)) und
ây

dF
(y, Îą(y), Î˛(y)).
dy

5.9. Uneigentliche Integrale
Bisher koĚnnen wir Integrale nur uĚber kompakte Intervalle und beschraĚnkte Funktionen bilden. Wir wollen unser maĚchtiges Werkzeug des GrenzuĚbergangs jetzt
auch hier verwenden, um etwas allgemeinere Integrale zuzulassen.
In diesem Abschnitt seien stets a, b â R und Îą, Î˛ â R âŞ {ââ, â}.
Definition 5.9.1. Es sei f : [a, Î˛) â R (bzw. f : (Îą, b] â R) integrierbar auf
dem Intervall [a, t] (bzw. [t, b]) fuĚr jedes t â (a, Î˛) (bzw. t â (Îą, b)). Dann heiĂt
f uneigentlich integrierbar auf [a, Î˛) (bzw. (Îą, b]), wenn der Grenzwert
lim

tâÎ˛â

Z


bzw.

t

f (x) dx
a

lim

tâÎą+

Z

t

b


f (x) dx

existiert. In diesem Fall heiĂt das uneigentliche Integral
Z t
Z b
Z b
Z Î˛


f (x) dx
bzw.
f (x) dx := lim
f (x) dx
f (x) dx := lim
a

tâÎ˛â

a

Îą

tâÎą+

t

konvergent.
Beispiel 5.9.2.

(a) Wir betrachten
Z

0

1

â

1
dx.
1 â x2
255

5. Analysis â Teil II: Differential- und Integralrechnung
â
Das ist ein uneigentliches
Integral,
denn
die
Funktion
1/
1 â x2 ist auf
â
2
[0, 1] wegen limxâ1 1/ 1 â x = â nicht beschraĚnkt. FuĚr jedes t â (0, 1)
ist sie aber stetig auf dem Intervall [0, t], also dort auch integrierbar. Wir
haben damit im Sinne der obigen Defintion den Fall a = 0 und Î˛ = 1. Dann
ist fuĚr jedes t â (0, 1)
Z t
x=t
1
â
= arcsin(t)
dx = arcsin(x)
x=0
1 â x2
0
und wegen limtâ1â arcsin(t) = Ď/2 ist das uneigentliche Integral konvergent
und wir haben
Z 1
Z t
1
Ď
1
â
â
dx = lim
dx = lim arcsin(t) = .
tâ1â 0
tâ1â
2
1 â x2
1 â x2
0
(b) WaĚhrend im ersten Beispiel die Funktion unbeschraĚnkt war, schauen wir
uns nun eine Integration uĚber ein unbeschraĚnktes Intervall an:
Z â
1
dx,
1 + x2
0
es ist also a = 0 und Î˛ = â. Es gilt nun
Z t
1
dx = lim arctan(x)
lim
tââ
tââ 0 1 + x2

x=t
x=0

= lim arctan(t) =
tââ

Ď
,
2

also ist auch dieses uneigentliche Integral konvergent und es ist
Z â
1
Ď
dx = .
2
1+x
2
0
Genauso sieht man

Z

0

ââ

Ď
1
= .
2
1+x
2

(c) Es sei s > 0. Wann ist die Funktion 1/xs auf dem Intervall [1, â) uneigentlich integrierbar? FuĚr t â (1, â) gilt fuĚr s = 1
Z t
x=t
1
= ln(t),
dx = ln(x)
x=1
1 x
also ist das uneigentliche Integral in diesem Fall wegen limtââ ln(t) = â
divergent.
FuĚr s 6= 1 ist

Z

1

256

t

1
1 1âs
dx =
x
s
x
1âs

x=t
x=1

=

1
(t1âs â 1).
1âs

5.9. Uneigentliche Integrale
Der Grenzwert dieses Ausdrucks existiert nun genau fuĚr s > 1 und es ist in
diesem Fall
Z â
1
1
1
1
dx = lim
(t1âs â 1) = â
=
.
s
tââ 1 â s
x
1âs
sâ1
1
(d) Genauso wie im vorherigen Beispiel kann man zeigen, dass das uneigentliche
Integral
Z 1
1
dx
s
0 x
genau dann konvergiert, wenn s < 1 ist. In diesem Fall gilt
Z 1
1
1
dx =
.
s
sâ1
0 x

Bisher haben wir nur uneigentliche Integrale betrachtet, die an einer Grenze uneigentlich sind. NatuĚrlich will man auch den Fall behandeln, dass es an beiden
Intervallgrenzen Probleme gibt, man spricht dann oft von einem doppelt uneigentlichen Integral. Dazu muĚssen wir unsere Definition modifizieren.
Definition 5.9.3. Es sei f : (Îą, Î˛) â R integrierbar auf jedem Intervall [Îž, Îˇ] â
(Îą, Î˛). Dann heiĂt f auf (Îą, Î˛) uneigentlich integrierbar, wenn es ein c â (Îą, Î˛)
gibt, so dass die beiden uneigentlichen Integrale
Z c
Z Î˛
f (x) dx und
f (x) dx
Îą

c

im Sinne von Defintion 5.9.1 konvergieren. In diesem Fall heiĂt das uneigentliche
Integral
Z
Z
Z
Î˛

c

f (x) dx :=

Îą

Î˛

f (x) dx +

Îą

f (x) dx

c

konvergent.

NatuĚrlich muss man, damit diese Definition Sinn ergibt, zeigen, dass der so erhaltene Wert fuĚr das uneigentliche Integral nicht von der speziellen Wahl von c
abhaĚngt:
UĚbungsaufgabe 5.9.4. Definition 5.9.3 ist von der Wahl von c â (Îą, Î˛) unabhaĚngig.
Beispiel 5.9.5. (a) Es ist mit Hilfe von Beispiel 5.9.2 (b) das doppelt uneigentliche Integral
Z â
1
dx
2
ââ 1 + x
konvergent und gleich Ď.

257

5. Analysis â Teil II: Differential- und Integralrechnung
(b) Sei s > 0. Kombiniert man (c) und (d) aus Beispiel 5.9.2, so sieht man,
dass das doppelt uneigentliche Integral
Z â
1
dx
xs
0
genau dann konvergiert, wenn s > 1 und s < 1 gilt, d.h. es ist immer
divergent.

5.10. Fourierreihen
Wie speichert man eine Funktion, die man z.B. als Funksignal empfangen oder
uĚber Messwerte abgegriffen hat, effektiv ab? Man kann natuĚrlich einfach alle
Messwerte speichern, bzw. das Funksignal an diversen StuĚtzstellen abtasten und
diese Messwerte speichern, das kann allerdings eine ziemliche Datenmenge werden. In diesem Abschnitt wollen wir ein Werkzeug kennenlernen, dass neben
vielen anderen Anwendungen dazu dienen kann, diese Datenmenge erheblich zu
reduzieren.
Eine erste Idee waĚre die ersten k Koeffizienten einer geeigneten Taylorreihe zu
speichern, das hat allerdigs zwei Nachteile, die diese Methode unbrauchbar machen. Erstens liefert eine Taylorreihe immer nur lokal um den Entwicklungspunkt
eine gute NaĚherung und nicht uĚber die gesamte LaĚnge des Signals. Zweitens, was
noch schwerer wiegt, man braucht dafuĚr die Ableitungen der unbekannten Funktion und an die kommt man nur aus der Kenntnis der Messwerte nicht heran.
Wir werden sehen, dass man statt Polynomen Linearkombinationen von Sinusund Cosinus-Funktionen nehmen kann, um dieses Problem zu loĚsen.
Definition 5.10.1. Seien N â N, Ď > 0 und a0 , a1 , . . . , aN , b1 , . . . , bN â R mit
aN 6= 0 oder bN 6= 0. Dann heiĂt
N

P (x) =


a0 X
+
an cos(nĎx) + bn sin(nĎx)
2
n=1

trigonometrisches Polynom vom Grad N mit Frequenz Ď.
Beispiel 5.10.2. Trigonometrische Polynome sind z.B.
2,

sin(x),

cos(x) + 2 sin(x),

3 + e cos(2Ďx) â 4 sin(5Ďx) + 2 cos(7Ďx).

Bemerkung 5.10.3. (a) Jedes trigonometrische Polynom ist eine auf ganz R
definierte, periodische Funktion mit Periode 2Ď/Ď, denn es gilt
N




i
2Ď  a0 Xh
P x+
=
+
an cos nĎx + 2nĎ + bn sin nĎx + 2nĎ = P (x).
Ď
2 n=1

In den folgenden Nummern bezeichnet immer T :=

258

2Ď
diese Periode.
Ď

5.10. Fourierreihen
(b) Allgemein gilt: Ist f : R â R periodisch mit Periode L, so ist fË : R â R
mit fË(x) = f ( NL x) periodisch mit Periode N, denn dann gilt
L

L

L 
fË(x + N) = f
(x + N) = f
x+L =f
x = fË(x).
N
N
N
Wir werden uns das im Folgenden zu Nutze machen und viele Resultate
nur fuĚr T = 2Ď betrachten. Auf diese Weise wird die Darstellung durch den
Wegfall einiger GroĚĂen uĚbersichtlicher und obige Skalierung erlaubt dann
eine direkte Behandlung des allgemeinen Falls.
Satz 5.10.4. Ist
N


a0 X
+
an cos(nx) + bn sin(nx)
P (x) =
2
n=1

ein trigonometrisches Polynom, so gilt
Z
1 Ď
ak =
P (x) cos(kx) dx, k â N und
Ď âĎ
Z
1 Ď
P (x) sin(kx) dx, k â Nâ .
bk =
Ď âĎ

Beweis. Wir kuĚmmern uns zunaĚchst separat um a0 . Es ist
Z Ď
Z Ď
Z Ď
Z Ď
N h
i
X
a0
P (x) dx =
dx +
an
cos(nx) dx + bn
sin(nx) dx
âĎ
âĎ 2
âĎ
âĎ
n=1
a0 Xh 1
+
an sin(nx)
2
n
n=1
N

= 2Ď

x=Ď
x=âĎ

1
+ bn (â cos(nx))
n

N h
X
i
bn
n
n
= Ďa0 .
(â1) â (â1)
= Ďa0 +
0+
n
n=1

x=Ď
x=âĎ

i

FuĚr die weiteren Koeffizienten verwenden wir die Ergebnisse von UĚbungsaufgabe 5.8.8. Damit bekommen wir fuĚr jedes k â Nâ
Z Ď
Z Ďh
N
i
a0 X
P (x) cos(kx) dx =
+
an cos(nx) + bn sin(nx) cos(kx) dx
âĎ
âĎ 2
n=1
Z
Z Ď
N h Z Ď
i
X
a0 Ď
an cos(nx) cos(kx) dx + bn
cos(kx) dx +
sin(nx) cos(kx) dx
=
2 âĎ
âĎ
âĎ
n=1
{z
}
|
=0 wie oben

= 0+

N
X
n=1

(an Î´nk Ď + bn Âˇ 0) = ak Ď.

Die Rechnung fuĚr die bk , k â Nâ , verlaĚuft analog.

259

5. Analysis â Teil II: Differential- und Integralrechnung
Bemerkung 5.10.5. (a) Das Ergebnis dieses Satzes erklaĚrt auch die bis jetzt
willkuĚrlich seltsame Setzung, dass das a0 in der Defintion eines trigonometrischen Polynoms noch durch zwei geteilt wird. TaĚte man das nicht, traĚte
hier eine Fallunterscheidung auf.
(b) Im Sinne von Bemerkung 5.10.3 (b) bekommen wir fuĚr trigonometrische
Polynome mit beliebiger Periode T das folgende Resultat:
Ist

N


a0 X
+
an cos(nĎx) + bn sin(nĎx)
P (x) =
2
n=1

ein trigonometrisches Polynom, so gilt mit T := 2Ď/Ď
Z
2 T /2
an =
P (x) cos(nĎx) dx, n â N und
T âT /2
Z
2 T /2
P (x) sin(nĎx) dx, n â Nâ .
bn =
T âT /2
Das sieht man so: Das trigonometrische Polynom
PĚ (x) := P

x
Ď

N


a0 X
=
+
an cos(nx) + bn sin(nx)
2
n=1

hat nach ebendieser Bemerkung Periode ĎT = 2Ď. Also gilt
Z
Z
1 Ď
1 Ď x
an =
cos(nx) dx.
PĚ (x) cos(nx) dx =
P
Ď âĎ
Ď âĎ
Ď

Substituieren wir nun y = x/Ď, so erhalten wir
Z
Z
1 Ď/Ď
2 T /2
an =
P (y) cos(nĎy)Ď dx =
P (x) cos(nĎx) dx,
Ď âĎ/Ď
T âT /2

da Ď/Ď = ĎT /(2Ď) = T /2 ist.
Die Rechnung fuĚr die bn , n â Nâ , geht wieder analog.
Bemerkung 5.10.6. Obiges Resultat wollen wir noch einmal von einer abstrakteren Warte aus beleuchten. Nach UĚbungsaufgabe 5.7.12 ist die Bildung
Z Ď
(f |g) =
f (x)g(x) dx,
f, g â C([âĎ, Ď]),
âĎ

ein Skalarprodukt und die Ergebnisse der Integrale aus UĚbungsaufgabe 5.8.8 bedeuten dann gerade, dass die Funktionen
1
â ,
2Ď

260

1
â cos(nx),
Ď

1
â sin(nx),
Ď

n â N,

5.10. Fourierreihen
bezuĚglich dieses Skalarproduktes ein Orthonormalsystem, bilden also eine Menge
von auf LaĚnge Eins normierten Vektoren, die paarweise orthogonal sind.
Weiter sind die trigonometrischen Polynome genau der von diesen Funktionen als
Orthonormalbasis erzeugte Vektorraum. In diesem bekommt man nach Bemerkung 3.4.15 die Koordinaten eines Vektors P gerade, indem man das Skalarprodukt von P mit den Basisfunktionen bildet. Nichts anderes sagt Satz 5.10.4.
Die Idee fuĚr das folgende ist nun so zu beschreiben: Wir wollen eine gegebene
(nicht allzu wilde) Funktion f durch ein bestmoĚgliches trigonometrisches Polynom vom Grad n naĚhern. Wir bekommen wir dieses? Durch die orthogonale
Projektion unserer Funktion f auf den von den entsprechenden Basisfunktionen
aufgespannten Unterraum!
Wir definieren also
Definition 5.10.7. Es sei T > 0 und f : [âT /2, T /2] â R eine Funktion.
(a) Die Funktion f heiĂt stuĚckweise stetig, wenn eine Zerlegung {x0 , x1 , . . . , xn }
von [âT /2, T /2] existiert, so dass f auf jedem der Intervalle (xjâ1 , xj ),
j = 1, 2, . . . , n, stetig ist und jeweils die rechts- und linksseitigen Grenzwerte von f in x0 , x1 , . . . , xn existieren.
Ist die Funktion zusaĚtzlich auf jedem der Intervalle (xjâ1 , xj ) stetig differenzierbar, so nennt man sie stuĚckweise glatt.
(b) Ist f stuĚckweise stetig und setzt man fuĚr Ď = 2Ď/T
Z
2 T /2
an :=
f (x) cos(nĎx) dx, n â N,
T âT /2
Z
2 T /2
f (x) sin(nĎx) dx, n â Nâ ,
bn :=
T âT /2

und

so heiĂt fuĚr N â Nâ
N

FN,f (x) :=


a0 X
+
an cos(nĎx) + bn sin(nĎx)
2
n=1

Fourierpolynom von f vom Grad N.
Die Werte an , n â N und bn , n â Nâ , heiĂen Fourierkoeffizienten von f .
(c) Ist f stuĚckweise stetig mit Fourierkoeffizienten wie in (b), so heiĂt die Reihe
â


a0 X
+
an cos(nĎx) + bn sin(nĎx)
2
n=1
Fourierreihe von f .

261

5. Analysis â Teil II: Differential- und Integralrechnung
Bemerkung 5.10.8. Die Fourierpolynome und auch die Fourierreihe, wenn sie
denn auf ganz R konvergiert, sind jeweils periodische Funktionen mit Periode T .
Satz 5.10.9. Sei f : [âT /2, T /2] â R beschraĚnkt und stuĚckweise stetig. FuĚr die
Fourierkoeffizienten von f gilt dann:
Z
4 T /2
f gerade =â bn = 0 und an =
f (x) cos(nĎx) dx fuĚr alle n â N,
T 0
Z
4 T /2
f (x) sin(nĎx) dx fuĚr alle n â N.
f ungerade =â an = 0 und bn =
T 0
Beweis. Ist f gerade, so ist die Funktion x 7â f (x) sin(nĎx) ungerade, denn
f (âx) sin(nĎ(âx)) = f (x) sin(ânĎx) = âf (x) sin(nĎx),
also ist bn = 0 fuĚr jedes n â N nach UĚbungsaufgabe 5.8.7 (a). Da mit einer analogen Berechnung in diesem Fall x 7â f (x) cos(nĎx) gerade ist, folgt die Formel
fuĚr an aus Teil (b) der selben UĚbungsaufgabe.
Der Beweis im Falle einer ungeraden Funktion f geht analog.
Beispiel 5.10.10. (a) Wir bestimmen alle Fourierpolynome und die Fourierreihe der Funktion f : [âĎ, Ď] â R mit f (x) = x. Dazu beobachten wir,
dass dieses eine ungerade Funktion ist, es gilt also schon mal an = 0 fuĚr
alle n â N. Also berechnen wir mit Hilfe obigen Satzes und mit partieller
Integration fuĚr n â Nâ
Z
Z
1 Ď
2 Ď
bn =
f (x) sin(nx) dx =
x sin(nx) dx
Ď âĎ
Ď 0
Z Ď
i
x=Ď
2h x
â1
=
â cos(nx)
cos(nx) dx
â
Ď n
n
x=0
0
x=Ď i
2
1
2h Ď
= (â1)n+1 .
â cos(nĎ) + 2 sin(nx)
=
x=0
Ď n
n
n
Das Fourierpolynom N-ten Grades ergibt sich also zu
FN,f (x) =

N
X

bn sin(nx) = 2

n=1

N
X
(â1)n+1
n=1

n

sin(nx)

und die Fourierreihe dementsprechend
2

â
X
(â1)n+1
n=1

n

sin(nx).

(b) Was ist die Fourierreihe von f : [âĎ, Ď] â R mit f (x) = cos(x)? Das ist
nicht schwer: cos(x).

262

5.10. Fourierreihen
(c) Wir betrachten noch ein ernsteres
f : [âĎ, Ď] â R mit
ďŁą
ďŁ´
ďŁ˛â1,
f (x) = 0,
ďŁ´
ďŁł
1,

Beispiel, naĚmlich die Signum-Funktion
falls x â [âĎ, 0),
falls x = 0,
falls x â [0, Ď].

Auch diese Funktion ist ungerade, wir haben es also wieder mit einer reinen Sinusreihe zu tun und muĚssen nur die Fourierkoeffizienten bn , n â Nâ ,
ausrechnen. Es gilt
Z
Z 0
Z Ď
i
1h
1 Ď
â
f (x) sin(nx) dx =
sin(nx) dx +
sin(nx) dx
bn =
Ď âĎ
Ď
âĎ
0
i

1h1
1 
1
x=0
x=Ď
=
1 â (â1)n â ((â1)n â 1) .
cos(nx) x=âĎ â cos(nx) x=0 =
Ď n
n
nĎ

Dieser Ausdruch ist 4/(nĎ), falls n ungerade ist und fuĚr gerades n â N ist
er Null. also ist die Fourierreihe dieser Funktion gegeben durch
â
X
k=0


4
sin (2k + 1)x .
(2k + 1)Ď

Die periodische Fortsetzung der Funktion f zusammen mit einigen ihrer
Fourierpolynome ist in Abbildung 5.9 dargestellt. Man sieht, dass sogar
fuĚr solch haĚssliche Funktionen, die trigonometrischen Polynome eine zunehmend bessere NaĚherung zu bieten scheinen.
Definition 5.10.11. Es sei T > 0 und f : [âT /2, T /2] â R eine Funktion. FuĚr
jedes x â R gibt es nun eindeutige Zahlen k(x) â Z und y(x) â (âT /2, T /2], fuĚr
die x = k(x)T + y(x) gilt. Damit definieren wir die periodische Fortsetzung von
f als die Funktion fp : R â R mit fp (x) = f (y(x)).
Der folgende Satz gibt ein Kriterium, wann die Fourierreihe konvergiert und was
ihr Grenzwert ist. Die Aussage kann man sich gut an Abbildung 5.9 verdeutlichen.
Satz 5.10.12 (Konvergenzsatz fuĚr Fourierreihen). Es seien T > 0 sowie eine
stuĚckweise glatte Funktion f : [â T2 , T2 ] â R mit Fourierkoeffizienten an , n â N,
und bn , n â Nâ gegeben. Dann konvergiert die Fourierreihe von f auf ganz R und
es gilt fuĚr alle x â R
â

 limyâx+ fp (y) + limyâxâ fp (y)
a0 X
+
.
an cos(nĎx) + bn sin(nĎx) =
2
2
n=1

Insbesondere konvergiert die Fourierreihe also an allen Punkten x0 â R, an denen
fp stetig ist, gegen fp (x0 ).

263

5. Analysis â Teil II: Differential- und Integralrechnung

1.0

0.5

Kp

Kp

3

Kp

2

p

0

2

x

p

3

p

K

0.5

K

1.0

Abbildung 5.9.: Die periodische Fortsetzung der Signum-Funktion f (rot) zusammen mit ihrem 1. (gruĚn), 5. (gelb) und 17. (blau) Fourierpolynom.
Mit den folgenden drei Nummern tauchen wir ein kleines bisschen in die Theorie
der Fourierreihen ein. Dies soll nur
Einblick sein, der vor allem dazu
Pâein kleiner
2
dient den Reihenwert der Reihe n=1 1/n zu bestimmen.
Definition 5.10.13. Der Vektorraum

â
X

â := (an ) Folge in R :
a2n konvergent
2

n=0

mit dem Skalarprodukt


(an ) (bn ) :=
und der dadurch induzierten Norm
(an )

2

=

q

â
X

an bn

n=0

v
uâ
uX
(an ) (an ) = t
a2n


n=0

heiĂt (klein)-â2 (gesprochen ell-zweiâ).
â
Satz 5.10.14 (Parsevalsches Theorem). Ist f : [âĎ, Ď] â R stuĚckweise stetig mit
Fourierkoeffizienten an , n â N, und bn , n â Nâ , so gilt (an ) â â2 und (bn ) â â2 1
1

Ganz korrekt muĚssen wir dazu zuerst ein b0 definieren, das wir aber einfach Null setzen
koĚnnen.

264

5.10. Fourierreihen
und es ist
1
Ď

Z

â
â
1 2 X 2 X 2
bn .
f (x) dx = a0 +
an +
2
âĎ
n=1
n=1
Ď

2

Beispiel 5.10.15. Wir betrachten wieder die Funktion f : [âĎ, Ď] â R mit
f (x) = x aus Beispiel 5.10.10 (a). Nach dem Parsevalschen Theorem haben wir
1
Ď

Z

Ď
2

f (x) dx =
âĎ

â
X

b2n

mit bn =

n=1

Nun ist zum Einen
Z
Z
1 Ď
1 Ď 2
1 3
2
f (x) dx =
x dx =
x
Ď âĎ
Ď âĎ
3Ď
und zum Anderen

â
X
n=1

Also ist

b2n

2
(â1)n+1 ,
n

x=Ď
x=âĎ

=

n â Nâ .

1 3
2Ď 2
(Ď + Ď 3 ) =
3Ď
3

â
X
4
.
=
n2
n=1

â
X
Ď2
1
=
.
n2
6
n=1

265

6. GewoĚhnliche
Differentialgleichungen
6.1. Problemstellung und Motivation
Viele Probleme aus den Natur- und Ingenieurwissenschaften, aber z.B. auch der
Wirtschaftswissenschaften, fuĚhren auf Fragestellungen, in denen, mathematisch
formuliert, ein Zusammenhang zwischen einer Funktion und ihren Ableitungen
bekannt ist. Wir wollen uns nun mit dem LoĚsen solcher Differentialgleichungenâ
â
beschaĚftigen. Wir starten mit einem Beispiel.
Beispiel 6.1.1. Im einfachst moĚglichen Wachstumsmodell, kann man postulieren, dass der Zuwachs einer Population (von Bakterien, Geld, Menschen, Festplattendefekten,. . . ) proportional dazu ist, wie groĂ die Population schon ist.
Bezeichnen wir mit y(t) die PopulationsgroĚĂe zum Zeitpunkt t âĽ 0, so fuĚhrt
dieses Modell auf die Gleichung
y â˛(t) = Âľy(t),

t âĽ 0,

wobei Âľ die ProportionalitaĚtskonstante, die in diesem Fall Wachstumsrate heiĂt,
ist.
Definition 6.1.2. Es sei n â N, I â R ein Intervall und F : I Ă Rn â R stetig.
Dann heiĂt die Gleichung

y (n) (t) = F t, y(t), y â˛(t), y â˛â˛ (t), . . . , y (nâ1) (t) , t â I,
gewoĚhnliche Differentialgleichung (DGL) der Ordnung n.
HaĚngt die Funktion F nicht von der ersten Variablen t ab, so nennt man die
Differentialgleichung autonom.

Beispiel 6.1.3. Beispiele fuĚr gewoĚhnliche Differentialgleichungen sind neben der
schon in Beispiel 6.1.1 genannten:
(a) y â˛â˛(t) + 2y â˛(t) + y(t) = sin(t).
Hier ist n = 2 und F (t, y(t), y â˛(t)) = sin(t) â 2y â˛(t) + y(t).
(b) y â˛(t) = t2 + 1 mit n = 1 und F (t, y(t)) = t2 + 1.

267

6. GewoĚhnliche Differentialgleichungen
(c) y â˛(t) = earctan[

â

y(t)]


cos y(t)4 â 3 .

Dabei sind die in Beispiel 6.1.1 und in (c) gegebenen Gleichungen autonom.
Bemerkung 6.1.4. Wir werden spaĚter sehen, dass man die Behandlung von Differentialgleichungen der Ordnung n immer auf den Fall von (mehreren) gewoĚhnlichen Differentialgleichungen erster Ordnung zuruĚckspielen kann. Deshalb widmen
wir uns zunaĚchst nur solchen Gleichungen, d.h. wir haben n = 1. In diesem Fall
ist die allgemeine Form einer gewoĚhnlichen Differentialgleichung
y â˛(t) = f (t, y(t)),

t â I,

wobei f : I Ă R â R eine gegebene stetige Funktion ist und y : I â R die
gesuchte Funktion.
Eine autonome Differentialgleichung erster Ordnung schreibt sich also als
y â˛ (t) = f (y(t)),

t â I.

Eine stetig differenzierbare Funktion y : I â R, die die Differentialgleichung
erfuĚllt, nennt man eine LoĚsung der Differentialgleichung.
Beispiel 6.1.5. Wir betrachten noch einmal die Gleichung aus Beispiel 6.1.1.
Da die Ableitung der Exponentialfunktion wieder die Exponentialfunktion ist,
ist es naheliegend eine LoĚsung in diesem Dunstkreis zu suchen. TatsaĚchlich loĚst
die Funktion y(t) = eÂľt die Gleichung, wie man durch einfaches Nachrechnen
verifiziert. Eine weitere offensichtliche LoĚsung ist die konstante Funktion y(t) = 0,
t â R.
TatsaĚchlich sind alle Funktionen der Form y(t) = CeÂľt mit C â R beliebig
LoĚsungen. Sind das nun alle? Ja, denn fuĚr jede LoĚsung y : R â R gilt

d
y(t)eâÂľt = y â˛ (t)eâÂľt â y(t)ÂľeâÂľt = Âľy(t)eâÂľt â y(t)ÂľeâÂľt = 0.
dt

Also gibt es eine Konstante C â R mit y(t)eâÂľt = C fuĚr alle t â R, d.h. es gilt
y(t) = CeâÂľt .
Denken wir an unser Wachstumsmodell, dass durch die Differentialgleichung beschrieben werden soll, so hat die GroĚĂe C auch eine anschauliche Bedeutung: Es
gilt fuĚr jede LoĚsung y(0) = CeÂľÂˇ0 = C, damit ist C also die GroĚĂe der Population
zum (Start-)Zeitpunkt Null.
Bemerkung 6.1.6. Obiges Beispiel ist in dem Sinne typisch, dass Differentialgleichungen im Allgemeinen mehrere LoĚsungen haben und es ist auch meist so
wie oben, dass die Anzahl der frei waĚhlbaren Konstanten gleich der Ordnung der
Gleichung ist. Das macht man sich am besten an den einfachst moĚglichen Differentialgleichungen, wie z.B. (b) aus Beispiel 6.1.3 klar. Hier ist die Aufgabe einfach
eine Stammfunktion der Funktion t 7â t2 + 1 zu finden und dabei faĚngt man eben

268

6.1. Problemstellung und Motivation
eine frei waĚhlbare Konstante ein, da sich alle LoĚsungen zu y(t) = t3 /3 + t + c,
c â R, ergeben.
Betrachtet man z.B. uâ˛â˛ (t) = t2 +1, so ist die allgemeine LoĚsung nach zweimaligem
Integrieren gegeben durch u(t) = t4 /12 + t2 /2 + ct + d, c, d â R, mit zwei frei
waĚhlbaren Konstanten usw.
Beispiel 6.1.7. Das Wachstumsmodell in Beispiel 6.1.1 laĚsst unendliches Wachstum zu, was im Allgemeinen unrealistisch ist. Wir wollen nun davon ausgehen,
dass es eine maximale Grenzpopulation gibt, die wir auf Eins (= 100%) setzen.
Dann ist es naheliegend anzunehmen, dass das Wachstum nun zum Einen weiterhin proportional zur GroĚĂe der schon vorhandenen Population ist, aber zum
Anderen auch zur verbleibenden KapazitaĚt, also dem Abstand 1 â y(t) von der
Grenzpolulation. Das fuĚhrt auf das sogenannte logistische Wachstumsmodell

y â˛(t) = Âľy(t) 1 â y(t) .

Mit f : R â R, gegeben durch f (x) = Âľx(1 â x), lautet unsere Differentialgleichung also y â˛ (t) = f (y(t)).
Eine explizite LoĚsung ist nun nicht mehr durch Draufschauenâ moĚglich, aber
â
auch die Betrachtung der Differentialgleichung kann viele interessante Eigenschaften der LoĚsung verraten, ohne dass man diese explizit kennt.

Abbildung 6.1.: Die Funktion f (x) aus dem logistischen Wachstum fuĚr Âľ = 4
Die Dynamik der Differentialgleichung wird durch die Funktion f bestimmt, die
in Abbildung 6.1 dargestellt ist. Diese gibt wegen y â˛ (t) = f (y(t)) die AĚnderung
y â˛(t) der Population an, wenn die GroĚĂe der Population y(t) eingegeben wird. Ein
positiver Wert f (y) bedeutet so z.B., dass eine Population dieser GroĚĂe waĚchst,
ein negativer bedeutet Schrumpfung der Population.
Starten wir mit einer positiven Population, die echt kleiner als unsere Grenzpopulation ist, also y(0) â (0, 1), so wird die Population also zunehmen. Diese
Zunahme verlangsamt sich aber umso mehr, je naĚher die Population an die KapazitaĚtsgrenze Eins kommt. TatsaĚchlich wird die Population den Wert Eins nicht
in endlicher Zeit erreichen, sondern fuĚr die LoĚsung gilt limtââ y(t) = 1.

269

6. GewoĚhnliche Differentialgleichungen
Startet man umgekehrt mit einer UĚberbevoĚlkerung, also y(0) > 1, so ist f (y(0))
negativ und die Population wird sinken, auch das tut sie wieder umso langsamer je
naĚher man der Grenzpopulation Eins kommt. In diesem Fall wird die LoĚsung also
eine strikt monoton fallende Funktion mit Grenzwert Eins fuĚr t gegen Unendlich
sein.
UĚbungsaufgabe 6.1.8. Es sei f : R â R stetig. Dann ist jede LoĚsung der
autonomen Differentialgleichung y â˛(t) = f (y(t)) entweder monoton fallend oder
monoton wachsend.
Eine wichtige Rolle bei gewoĚhnlichen Differentialgleichungen spielen, wie wir bereits gesehen haben, die Startwerte. Das fuĚhrt auf den folgenden Begriff.
Definition 6.1.9. Es seien n â N, I â R ein Intervall, t0 â I, F : I Ă Rn â R
stetig, sowie y0 , y1 , . . . , ynâ1 â R. Dann heiĂt

 (n)
y (t) = F t, y(t), y â˛(t), . . . , y (nâ1) (t) , t â I,
(AWP)
y (j)(t0 ) = yj ,
j = 0, 1, . . . , n â 1,
ein Anfangswertproblem mit Anfangswerten y0 , y1 , . . . , ynâ1 .
Jede Funktion y : J â R, die
â˘ auf einem offenen Intervall J â I mit t0 â J definiert ist,
â˘ auf J n-mal stetig differenzierbar ist und
â˘ die n Gleichungen in (AWP) erfuĚllt,
heiĂt LoĚsung des Anfangswertproblems.
Ist die LoĚsung sogar auf dem ganzen Intervall I eine LoĚsung der Gleichung, so
nennt man sie eine globale LoĚsung.
Bemerkung 6.1.10. Es kommt immer wieder vor, dass LoĚsungen der Differentialgleichung nicht auf dem ganzen Intervall I, auf dem die Funktion F gegeben
ist, existieren, vgl. Beispiel 6.2.3 (b). Deshalb begnuĚgt man sich in der obigen
Definition mit der Existenz eines Intervalls J.

6.2. Elementare LoĚsungsmethoden
6.2.1. Getrennte VeraĚnderliche
Beispiel 6.2.1. Wir betrachten noch einmal die Differentialgleichung des logistischen Wachstumsmodells aus Beispiel 6.1.7, also

y â˛ (t) = Âľy(t) 1 â y(t) ,
t â [0, â),
270

6.2. Elementare LoĚsungsmethoden
mit einer Konstanten Âľ â R. Wir setzen nun voraus, dass es eine LoĚsung y :
[0, â) â R dieser Gleichung gibt, fuĚr die y(t) â (0, 1) fuĚr alle t âĽ 0 gilt. Es ist
zwar plausibel, dass es eine solche LoĚsung gibt, aber das wissen wir im Moment
natuĚrlich noch nicht, die weitere Rechnung bleibt also zunaĚchst unter diesem
Vorbehalt.
Haben wir aber eine solche LoĚsung, so gilt
y â˛(t)
= Âľ.
y(t)(1 â y(t))
Integrieren wir diese Gleichung von 0 bis t so erhalten wir
Z t
Z t
Z t
y â˛(Ď )
dĎ =
f (y(Ď ))y â˛(Ď ) dĎ,
Âľt =
Âľ dĎ =
0
0
0 y(Ď )(1 â y(Ď ))
wobei wir f (x) := 1/[x(1 â x)] gesetzt haben. Nach der Substitutionsregel aus
Satz 5.8.4 erhalten wir mit der Substitution x = y(Ď )
Z y(t)
Z y(t)
Z y(t) 
1
1 
1
Âľt =
f (x) dx =
dx.
dx =
+
x 1âx
y(0)
y(0) x(1 â x)
y(0)

Die Stammfunktionen von 1/x und 1/(1 â x) sind ln(|x|), bzw. ln(|1 â x|). Da
nach unserer Voraussetzung x = y(Ď ) â (0, 1) liegen wird, koĚnnen wir die BetraĚge
allerdings weglassen. Damit bekommen wir mit y0 := y(0)

 x=y(t)

= ln(y(t)) â ln(y0 ) â ln(1 â y(t)) â ln(1 â y0 )
Âľt = ln(x) â ln(1 â x)
x=y0
 y(t)(1 â y ) 
0
= ln
y0 (1 â y(t))
Dies koĚnnen wir nun nach y(t) aufloĚsen:

eÂľt y0 (1 â y(t)) = y(t)(1 â y0 ) ââ eÂľt y0 = y(t)(1 â y0 + eÂľt y0 )
eÂľt y0
.
ââ y(t) =
1 + (eÂľt â 1)y0
Diese Berechnungsmethode kann stark verallgemeinert werden zur sogenannten
Methode der Trennung der Variablen. Diese sollte man immer dann versuchen,
wenn eine Differentialgleichung y â˛(t) = f (t, y(t)) zu loĚsen ist, bei der die rechte
Seite f von der Form f (t, y) = g(t)h(y) ist, die AbhaĚngigkeit nach den beiden
Variablen t und y also multiplikativ getrennt ist.
Satz 6.2.2 (Trennung der Variablen). Auf einem Intervall I â R sei mit stetigen Funktionen g : I â R und h : R â R, sowie t0 â I und y0 â R das
Anfangswertproblem
 â˛
y (t) = g(t)h(y(t)), t â I,
(6.1)
y(t0 ) = y0
271

6. GewoĚhnliche Differentialgleichungen
gegeben. Ist h(y0 ) 6= 0, so existiert ein offenes Intervall J â I mit t0 â J, auf
dem das Anfangswertproblem (6.1) genau eine LoĚsung besitzt. Diese ist gegeben
durch
Z t
Z y
1
â1
y = H âŚ G mit G(t) :=
g(Ď ) dĎ und H(y) :=
dÎˇ.
t0
y0 h(Îˇ)
Beispiel 6.2.3.

(a) Wir betrachten das Anfangswertproblem
 â˛
y (t) = ty(t), t â R,
y(0) = 1.

Hier ist g(t) = t und h(y) = y, sowie t0 = 0 und y0 = 1. Also ist tatsaĚchlich
h(y0 ) = h(1) = 1 6= 0 und fuĚr die Formel aus obigem Satz berechnen wir
G(t) =
und
H(y) =

Z

y

y0

sowie H

â1

Z

t

g(Ď ) dĎ =
t0

1
dÎˇ =
h(Îˇ)

Z

1

y

Z

t

Ď dĎ =
0

t2
2

1
dÎˇ = ln(y) â ln(1) = ln(y),
Îˇ

x

(x) = e . Damit ist
2 /2

y(t) = H â1 (G(t)) = et

.

Meist rechnet man mit der folgenden Schmierrechnung kuĚrzer:
Z
Z
dy
1
1
t2
= ty =â dy = t dt =â
dy = t dt =â ln(y) = + c.
dt
y
y
2
2

Also ist y(t) = et /2+c und die Konstante stellt man dann uĚber die Anfangsbedingung y(0) = ec = 1 zu c = 0 ein.
Bei dieser Methode ist allerdings wie schon beim Substituieren zu beachten,
dass dieses Herumgeschiebe von dy und dt keine saubere Mathematik ist.
Das Ergebnis ist dann also auf jeden Fall durch eine Probe zu verifizieren!
(b) Nun behandeln wir das Anfangswertproblem
 â˛
y (t) = cos(t)ey(t) , t â R,
y(0) = 2.
Mit obiger Schmierrechnung erhalten wir
dy
= cos(t)ey =â eây dy = cos(t) dt =â
dt

272

Z

ây

e

dy =

Z

cos(t) dt

6.2. Elementare LoĚsungsmethoden
Das liefert âeây = sin(t) + c und damit


ây(t) = ln â sin(t) â c , d.h. y(t) = â ln â sin(t) â c .

Mit dem Anfangswert erhalten wir

2 = y(0) = â ln âc , also eâ2 = âc, d.h. c = âeâ2 .
Zusammen haben wir also


y(t) = â ln â sin(t) + eâ2 .

Nun muĚssen wir eine Probe machen. FuĚr unseren LoĚsungskandiaten gilt
y(0) = â ln(eâ2 ) = â(â2) = 2 und
y â˛ (t) = â
sowie

1
cos(t)
(â cos(t)) =
,
â2
â sin(t) + e
â sin(t) + eâ2
â2 )

cos(t)ey(t) = cos(t)eâ ln(â sin(t)+e

=

also passt alles.

cos(t)
,
â sin(t) + eâ2

Dieses Beispiel zeigt auch den schon in Bemerkung 6.1.10 angesprochenen
Effekt, denn, obwohl die rechte Seite dieser Differentialgleichung fuĚr alle
t â R sinnvoll und beliebig glattâ ist, existiert diese LoĚsung nur solange,
â
wie eâ2 â sin(t) > 0 ist, das ergibt nur ein sehr kleines Existenzintervall J
um Null herum.

6.2.2. Homogene Differentialgleichungen
In einer homogenen Differentialgleichung haĚngt die rechte Seite nur vom Quotienten y/t ab, es gibt also eine Funktion g : R â R, mit der die Gleichung
als
 y(t) 
y â˛ (t) = f (t, y(t)) = g
t
geschrieben werden kann.
Diesen Typ behandeln wir beispielhaft als eine Sorte von Differentialgleichungen,
die durch eine Substitution geloĚst werden koĚnnen. Wir setzen
y(t)
,
t
und schauen, welche Gleichung nun von der Funktion u geloĚst wird, wenn y eine
LoĚsung der Ausgangsgleichung ist. Es gilt nach der Quotientenregel

 1

y â˛ (t) u(t)
1
ty â˛ (t) â y(t)
g
y(t)/t
â
u(t)
=
g(u(t))
â
u(t)
.
=
â
=
uâ˛ (t) =
t2
t
t
t
t
Also erfuĚllt dieses u eine Gleichung, die nach der Methoden der getrennten
VeraĚnderlichen aus Satz 6.2.2 geloĚst werden kann.
u(t) :=

273

6. GewoĚhnliche Differentialgleichungen
Beispiel 6.2.4. Wir betrachten das Anfangswertproblem
(
t2
â y(t)
t â R,
y â˛(t) = y(t)
2,
t
y(1) = 1.
Die obige Substitution u(t) = y(t)/t liefert hier, vgl. die obige Rechnung:

y â˛ (t) u(t)
1
1
1 1
uâ˛ (t) =
u(t) â
â
=
â
u(t)
=â
2
t
t
t
u(t)
t u(t)2
Mit der Methode der getrennten VeraĚnderlichen finden wir
Z
Z
1
1
2
2
u du = â
dt.
u du = â dt, also
t
t
Das liefert nach Integration

p
u3
= â ln(t) + c, d.h. u(t) = 3 â3 ln(t) + 3c,
3
was schlieĂlich zu
p
y(t) = tu(t) = t 3 â3 ln(t) + 3c

fuĚhrt. Mit dem Anfangswert bekommen wir wegen
â
1
3
1 = y(1) = 3c =â 3c = 1 =â c =
3
die LoĚsung
p
y(t) = t 3 1 â 3 ln(t),
die man leicht in einer Probe verifiziert.

6.2.3. Lineare Differentialgleichungen erster Ordnung
Definition 6.2.5. Eine lineare Differentialgleichung erster Ordnung hat die allgemeine Form
y â˛(t) + a(t)y(t) = b(t), t â I,
wobei a, b : I â R stetige Funktionen auf einem Intervall I sind.
Ist b = 0, so nennt man die Gleichung homogen, sonst inhomogen.

Satz 6.2.6 (Superpositionsprinzip). Es seien y1 , y2 : I â R zwei LoĚsungen der
homogenen linearen Gleichung y â˛(t) + a(t)y(t) = 0. Dann ist auch jede Linearkombination y = Îąy1 + Î˛y2 mit Îą, Î˛ â R eine LoĚsung dieser Gleichung.
Beweis. Der Beweis ist simples Nachrechnen:
y â˛(t) + a(t)y(t) = Îąy1â˛ (t) + Î˛y2â˛ (t) + Îąa(t)y1 (t) + Î˛a(t)y2 (t)


= Îą y1â˛ (t) + a(t)y1 (t) + Î˛ y2â˛ (t) + a(t)y2 (t) = Îą Âˇ 0 + Î˛ Âˇ 0 = 0.

274

6.2. Elementare LoĚsungsmethoden
Bemerkung 6.2.7. (a) Eine homogene lineare Differentialgleichung ist von
getrennten VeraĚnderlichen, denn sie lautet y â˛ (t) = âa(t)y(t). Betrachten
wir also das Anfangswertproblem
 â˛
y (t) = âa(t)y(t), t â I,
y(t0 ) = y0 ,
so erhalten wir mit unserer Methode aus Abschnitt 6.2.1
Z
Z
Z
1
1
dy = âa(t) dt, also
dy = âa(t) dt, d.h. ln(|y|) = â a(t) dt
y
y
Wir waĚhlen nun eine Stammfunktion A : I â R von a, indem wir
Z t
A(t) :=
a(s) ds
t0

setzen. Damit ist fuĚr jedes c â R die Funktion
y(t) = ÂąeâA(t)+c = Âąec eâA(t)
eine LoĚsung der Differentialgleichung. Setzt man C = Âąec erhaĚlt man die
LoĚsungen
y(t) = CeâA(t) , C â R.
Man beachte, dass auch C = 0 zugelassen ist, da auch die konstante Nullfunktion eine LoĚsung der Gleichung darstellt.
Als UĚbungsaufgabe verbleibt nun zu zeigen, dass das alle LoĚsungen dieser Gleichung sind. Sie koĚnnen sich dabei von den Betrachtungen in Beispiel 6.1.5 inspirieren lassen.
FuĚr die Einstellung des Anfangswertes berechnet man
y0 = y(t0) = CeâA(t0 ) = Ce0 = C,
also ist
âA(t)

y(t) = y0 e

mit A(t) =

Z

t

a(s) ds.

t0

die eindeutige LoĚsung des obigen Anfangswertproblems.
(b) Auf die LoĚsungen der inhomogenen linearen Differentialgleichung
y â˛(t) + a(t)y(t) = b(t),

t â I,

kommt man nun mit einem frechen Trick, der ein universelles Mittel bei
inhomogenen linearen Differentialgleichungen, auch allgemeinerer Art ist,
der sogenannten Variation der Konstanten.

275

6. GewoĚhnliche Differentialgleichungen
Ja, der Name ist ein Widerspruch in sich, aber er beschreibt was passiert
und das Verfahren geht gut. Wir betrachten den Ansatz
y(t) = c(t)eâA(t) ,
wobei die Funktion A(t) wie im Teil (a) definiert ist.
Setzen wir den Ansatz in die Gleichung ein, erhalten wir
b(t) = y â˛(t) + a(t)y(t) = câ˛ (t)eâA(t) â c(t)Aâ˛ (t)eâA(t) + a(t)c(t)eâA(t)
= câ˛ (t)eâA(t) ,

da Aâ˛ = a gilt. Umgestellt muss fuĚr die Funktion c(t) also câ˛ (t) = b(t)eA(t)
gelten. Diese Gleichung laĚsst sich nun durch Hochintegrieren loĚsen, vgl.
Satz 5.7.15 (b),
Z t
b(s)eA(s) ds.
c(t) = c(t0 ) +
t0

Wegen y(t) = c(t)eâA(t) gilt fuĚr den Anfgangswert
y0 = y(t0) = c(t0 )e0 = c(t0 ),
also ist die LoĚsung insgesamt gegeben durch
âA(t)

âA(t)

y(t) = c(t)e

=e

âA(t)

y0 + e

Z

t

b(s)eA(s) ds.

t0

Wir fassen diese UĚberlegungen zusammen:
Satz 6.2.8 (Variation-der-Konstanten-Formel). Es seien I â R ein Intervall,
a, b â C(I) und t0 â I, sowie y0 â R. Das lineare Anfangswertproblem


y â˛ (t) + a(t)y(t) = b(t), t â I,
y(t0 ) = y0 ,

besitzt genau eine globale LoĚsung, die durch
âA(t)

y(t) = e

âA(t)

y0 + e

Z

t

t0

A(s)

b(s)e

ds

mit A(t) =

Z

t

a(s) ds

t0

gegeben ist.
Diese Formel wird in der Literatur auch manchmal als Duhamelsche Formel bezeichnet.

276

6.3. Systeme von Differentialgleichungen
Beispiel 6.2.9. Wir loĚsen


y â˛(t) = y(t) + t, t â R,
y(0) = 1.

ZunaĚchst betrachtet man die zugehoĚrige homogene Gleichung y â˛(t) = y(t). Diese
hat die LoĚsungen y(t) = cet mit c â R, vgl. Beispiel 6.1.1.
FuĚr die LoĚsung des inhomogenen Problems machen wir den Ansatz der Variation
der Konstanten y(t) = c(t)et . Dann muss gelten
t + c(t)et = t + y(t) = y â˛(t) = câ˛ (t)et + c(t)et ,

also câ˛ (t) = teât .

Damit ist mittels partieller Integration
Z
Z
ât
ât
c(t) = te dt = âte + eât dt = âteât â eât + C,

C â R.

Zusammen haben wir also

y(t) = âteât â eât + C et = ât â 1 + Cet .

Mit Hilfe des Anfangswertes stellen wir nun noch die Konstante ein. Es muss
gelten
1 = y(0) = â0 â 1 + Ce0 , also C = 2.
Die LoĚsung unseres Anfangswertproblems lautet also
y(t) = 2et â t â 1,
was man auch leicht durch eine Probe verifiziert.

6.3. Systeme von Differentialgleichungen
HaĚufig hat man mehrere GroĚĂen, die durch Differentialgleichungen beschrieben
sind, und die sich gegenseitig beeinflussen, z.B.
y1â˛ (t) = 3y1 (t) + y2 (t), t â R,
y2â˛ (t) = y1 (t) + 3y2(t), t â R,
oder die sogenannten Volterra-Lotka-Gleichungen, die ein einfaches RaĚuber-BeuteModell darstellen:

xâ˛ (t) = Îą â Î˛y(t) x(t)

y â˛(t) = â Îł â Î´x(t) y(t),
mit positiven Konstanten Îą, Î˛, Îł, Î´. Dabei ist ersteres ein System von linearen
Differentialgleichungen, das zweite System ist nichtlinear, da die beiden GroĚĂen
x(t) und y(t) auf der rechten Seite als Produkt eingehen. Wir wollen uns hier nur
mit der linearen Variante beschaĚftigen.

277

6. GewoĚhnliche Differentialgleichungen

6.3.1. Lineare Systeme
Definition 6.3.1. Es seien I â R ein Intervall, N â Nâ und fuĚr jede Wahl von
j, k â {1, 2, . . . , N} stetige Funktionen ajk : I â R, sowie bj : I â R gegeben.
(a) Dann heiĂt
ďŁą
ďŁ´
y1â˛ (t) = a11 (t)y1 (t) + a12 (t)y2 (t) + Âˇ Âˇ Âˇ + a1N (t)yN (t)
+ b1 (t)
ďŁ´
ďŁ´
ďŁ˛ y â˛ (t) = a21 (t)y1 (t) + a22 (t)y2 (t) + Âˇ Âˇ Âˇ + a2N (t)yN (t)
+ b2 (t)
2
..
..
..
..
ďŁ´
.
.
.
.
ďŁ´
ďŁ´
ďŁł y â˛ (t) = a (t)y (t) + a (t)y (t) + Âˇ Âˇ Âˇ + a (t)y (t) + b (t),
N1
1
N2
2
NN
N
N
N

t â I, ein System von linearen gewoĚhnlichen Differentialgleichungen erster
Ordnung.

(b) Das dazugehoĚrige Anfangswertproblem ergibt sich, indem fuĚr ein t0 â I
und vorgegebene y1,0 , y2,0, . . . , yN,0 â R noch
y1 (t0 ) = y1,0 ,

y2 (t0 ) = y2,0 ,

...,

yN (t0 ) = yN,0

gefordert wird.
(c) Ist b = 0, so heiĂt das System homogen, sonst inhomogen.
Bemerkung 6.3.2. Wie man an der Schreibweise in obiger Definition schon
sieht, kann man solche Systeme mit einer Matrixnotation viel uĚbersichtlicher
schreiben. Setzt man
ďŁŤ
ďŁś
ďŁŤ
ďŁś
ďŁŤ
ďŁś
y1,0
b1 (t)
y1 (t)
ďŁŹ y2,0 ďŁˇ
ďŁŹ b2 (t) ďŁˇ
ďŁŹ y2 (t) ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
y(t) := ďŁŹ .. ďŁˇ , b(t) := ďŁŹ .. ďŁˇ , y0 := ďŁŹ .. ďŁˇ und
ďŁ­ . ďŁ¸
ďŁ­ . ďŁ¸
ďŁ­ . ďŁ¸
yN,0
bN (t)
yN (t)
ďŁŤ
ďŁś
a11 (t) a12 (t) . . . a1N (t)
ďŁŹ a21 (t) a22 (t) . . . a2N (t) ďŁˇ
ďŁŹ
ďŁˇ
A(t) := ďŁŹ ..
..
.. ďŁˇ ,
.
.
ďŁ­ .
.
.
. ďŁ¸
aN 1 (t) aN 2 (t) . . . aN N (t)
so schreibt sich das Anfangswertproblem aus Definition 6.3.1 als
 â˛
y (t) = A(t)y(t) + b(t), t â I,
y(t0 ) = y0 ,
wobei der Ableitungsstrich komponentenweise zu verstehen ist.

278

6.3. Systeme von Differentialgleichungen
Wir betrachten wie schon im Fall von linearen Gleichungen erster Ordnung, vgl.
Abschnitt 6.2.3, zunaĚchst den Spezialfall von homogenen Gleichungen. Es gelte
also nun b = 0 und wir betrachten das System linearer Differentialgleichungen
y â˛ (t) = A(t)y(t),

t â I,

(6.2)

auf einem Intervall I â R mit einer gegebenen stetigen Funktion A : I â RN ĂN .
Satz 6.3.3. Die Menge L aller LoĚsungen der Gleichung (6.2) ist ein N-dimensionaler Untervektorraum von C 1 (I; RN ).
Beweis. Wir zeigen, dass L die Bedingung (UVR2) aus dem Untervektorraumkriterium, Satz 3.2.3, erfuĚllt. Die Existenz einer LoĚsung und die genaue Dimension
koĚnnen wir erst mit Werkzeugen aus Abschnitt 6.5 nachweisen.
Seien also y1 , y2 â C 1 (I; RN ) LoĚsungen der Gleichung (6.2) und Îą, Î˛ â R. Dann
gilt
(Îąy1 + Î˛y2 )â˛ = Îąy1â˛ + Î˛y2â˛ = ÎąAy + Î˛Ay = A(Îąy1 + Î˛y2 ),
also ist auch Îąy1 + Î˛y2 eine LoĚsung.
Definition 6.3.4. Es sei I â R ein Intervall und A : I â RN ĂN stetig. Jede Basis des LoĚsungsraums aller LoĚsungen von Gleichung (6.2) nennt man ein
Fundamentalsystem dieser Gleichung.
Satz 6.3.5. Es seien y1 , y2 , . . . , yN â C 1 (I; RN ) LoĚsungen der Gleichung (6.2).
Dann sind die folgenden Aussagen aĚquivalent:
(i) y1 , y2 , . . . , yN sind linear unabhaĚngig in C(I; RN ), d.h. {y1 , y2 , . . . , yN } ist
ein Fundamentalsystem der Gleichung.
(ii) FuĚr alle t â I ist die Menge {y1(t), y2 (t), . . . , yN (t)} linear unabhaĚngig in
RN .
(iii) Es gibt ein t â I, fuĚr das die Menge {y1 (t), y2 (t), . . . , yN (t)} linear unabhaĚngig in RN ist.
Betrachten wir nun zusaĚtzlich mit einer stetigen Funktion b : I â RN das inhomogene Problem
y â˛(t) = A(t)y(t) + b(t), t â I,
(6.3)
so erhalten wir das folgende allgemeine Resultat.
Satz 6.3.6. Es seien I â R ein Intervall, sowie A : I â RN ĂN und b : I â RN
stetige Funktionen. Ist yp : I â RN eine LoĚsung der Gleichung (6.3), so ist jede
LoĚsung dieser Gleichung gegeben durch y = yp + yh , wobei yh eine LoĚsung des
zugehoĚrigen homogenen Systems (6.2) ist.

279

6. GewoĚhnliche Differentialgleichungen
Beweis. Ist y = yp + yh mit einer LoĚsung yh des homogenen Systems, so gilt
y â˛ = ypâ˛ + yhâ˛ = Ayp + b + Ayh = A(yp + yh ) + b,
also ist dann y eine LoĚsung des inhomogenen Problems.
Ist umgekehrt y eine LoĚsung von (6.3), so gilt
(y â yp )â˛ = y â˛ â ypâ˛ = Ay + b â Ayp â b = A(y â yp ),
d.h. yh := y â yp ist eine LoĚsung des zugehoĚrigen homogenen Systems (6.2) und
wir haben y = yp + yh .
Bemerkung 6.3.7. (a) Man beachte die Parallele dieses Resultats mit den Resultaten uĚber die LoĚsungen von linearen Gleichungssystemen in Satz 3.8.3.
(b) Ebenso wie in der LoĚsbarkeitstheorie der linearen Gleichungssysteme wird
die LoĚsung yp des inhomogenen Problems als spezielle LoĚsung oder PartikulaĚrloĚsung des inhomogenen Systems bezeichnet.

6.3.2. Lineare Systeme mit konstanten Koeffizienten
Das Problem an Satz 6.3.6 ist, dass wir nicht wissen, wie wir an die spezielle
LoĚsung des inhomogenen Problems kommen sollen. Im Prinzip geht das wieder
mit der Variation-der-Konstanten-Formel, vgl. Satz 6.2.8. Um den Notationsaufwand in Grenzen zu halten, wollen wir dazu noch einmal spezialisieren und
Systeme mit konstanten Koeffizienten anschauen, d.h. wir nehmen von nun an
an, dass die Funktion A in der Differentialgleichung (6.3) konstant durch eine
feste Matrix gegeben ist. Wir betrachten also das Problem
y â˛ (t) = Ay(t) + b(t),

t â I,

(6.4)

auf einem Intervall I â R mit einer stetigen Funktion b : I â RN und einer
Matrix A â RN ĂN .
Der Vorteil von konstanten Koeffizienten ist, dass man dann relativ leicht ein
Fundamentalsystem fuĚr das homogene System angeben kann.
Definition 6.3.8. Es sei A â RN ĂN . Dann heiĂt
A

e :=

â
X
An
n=0

die Matrix-Exponentialfunktion von A.

280

n!

6.3. Systeme von Differentialgleichungen
Bemerkung 6.3.9. Man beachte, dass die Reihe in obiger Definition tatsaĚchlich
fuĚr jede Matrix konvergent ist. Dazu verwendet man, dass fuĚr eine geeignete Norm
auf RN ĂN gilt kAn k â¤ kAkn . Das liefert dann wegen
â
â
â
X
X
kAn k X kAkn
An
=
â¤
= ekAk
n!
n!
n!
n=0
n=0
n=0

die absolute Konvergenz der Reihe.
Satz 6.3.10. Es seien A, B â RN ĂN . Dann gelten die folgenden Aussagen uĚber
die Matrix-Exponentialfunktion:
(a) FuĚr die Nullmatrix O gilt eO = I.
(b) Kommutieren A und B, d.h. gilt AB = BA, so ist eA eB = eA+B .
(c) Die Matrix eA ist invertierbar mit (eA )â1 = eâA .
(d) Ist A eine Diagonalmatrix mit DiagonaleintraĚgen Îť1 , Îť2 , . . . , ÎťN , so ist eA
ebenfalls eine Diagonalmatrix mit den DiagonaleintraĚgen eÎť1 , eÎť2 , . . . , eÎťN .
Beweis.

(a) Es ist
eO =

â
X
On
n=0

n!

=I+

â
X

O = I.

n=1

(b) Der Beweis geht analog zum Beweis der Funktionalgleichung der Exponentialfunktion in C aus Satz 4.5.20 mit Hilfe auf Matrizen verallgemeinerter
Versionen des Cauchy-Produkts und der Binomialformel. Da zum Zusammenfassen der Terme die Reihenfolge der Multiplikation von Matrizen vertauscht werden muss, geht das nur wenn A und B vertauschbar sind. Im
Allgemeinen ist die Formel in (b) schlicht falsch.
(c), (d) UĚbungsaufgabe.
Satz 6.3.11. Es sei I â R ein Intervall und A â RN ĂN . Dann bilden die Spalten
der Matrix etA , t â I, ein Fundamentalsystem der Gleichung y â˛(t) = Ay(t), t â I.
Beweis. Wir bezeichnen fuĚr j â {1, 2, . . . , N} mit ej den j-ten Standardeinheitsvektor in RN . Dann ist y(t) := etA ej die j-te Spalte von etA und wegen
â

â

â

X d tn An ej
d X (tA)n
d X tn An ej
d
ej =
=
y (t) = etA ej =
dt
dt n=0 n!
dt n=0 n!
dt n!
n=0
â˛

=

â
â nâ1 n
â n n
X
X
X
tnâ1 An ej
t A ej
t A ej
n
=
=A
= AetA ej = Ay(t)
n!
(n
â
1)!
n!
n=0
n=0
n=0

281

6. GewoĚhnliche Differentialgleichungen
ist jede solche Spalte eine LoĚsung der untersuchten Gleichung.
AuĂerdem sind die Spalten von etA dank Satz 6.3.10 (c) fuĚr jedes t â I linear
unabhaĚngig, also bilden sie dank Satz 6.3.5 ein Fundamentalsystem von y â˛(t) =
Ay(t).
Bemerkung 6.3.12. Leitet man die gesamte Matrix etA komponentenweise nach
t ab, so bedeutet obiger Satz die eingaĚngige Matrixgleichheit
d tA
e = AetA .
dt
Beispiel 6.3.13. Wir gehen mit dieser Methode das System
y1â˛ (t) = 3y1 (t) + y2 (t), t â R,
y2â˛ (t) = y1 (t) + 3y2(t), t â R,
vom Anfang dieses Abschnitts an. In

3
â˛
y (t) =
1

(6.5)

Matrixform lautet dieses

1
y(t) =: Ay(t).
3

FuĚr die Berechnung von etA erinnern wir uns an die Diagonalisierbarkeit von
symmetrischen Matrizen, vgl. Abschnitt 3.11. Berechnet man die Eigenwerte und
die Eigenvektoren von A, so findet man, dass






1 1 â1
1 1
2 0
â1
â1
und S =
, S=
A = SDS
mit D =
â1 1
0 4
2 1 1
ist. Damit gilt nun
An = (SDS â1)n = SDS â1SDS â1 SD . . . S â1 SDS â1 = SD n S â1
und wir erhalten
tA

e

=

â n n
X
t A
n=0

=S

n!

=

n=0

â n
X
n=0

â n
X
t SD n S â1

t D n  â1
S = SetD S â1 .
n!

Nach Satz 6.3.10 (d) gilt
tD

e
d.h.
tA

e

282

tD

= Se S

â1

1
=
2



n!

â
X
tn D n â1
=
S
S
n!
n=0

1 1
â1 1


 2t
e
0
,
=
0 e4t




  2t

1 e4t + e2t e4t â e2t
1 â1
e
0
=
.
1 1
0 e4t
2 e4t â e2t e4t + e2t

6.3. Systeme von Differentialgleichungen
Also ist


y1 (t), y2 (t) =

  4t

  4t
1 e â e2t
1 e + e2t
,
2 e4t â e2t 2 e4t + e2t

eine Basis des Raums aller LoĚsungen von (6.5).
Diese Basis kann man noch vereinfachen, z.B. ist mit {y1, y2 } auch die Menge
 
o  1

1
2t
4t
,e
y1 + y2 , y1 â y2 = e
â1
1

ein Fundamentalsystem.

Allgemein gilt fuĚr diagonalisierbare Matrizen entsprechend
Satz 6.3.14. Es sei A â RN ĂN diagonalisierbar mit Eigenwerten Îť1 , Îť2 , . . . , ÎťN
und zugehoĚrigen Eigenvektoren v1 , v2 , . . . , vN . Dann ist
 tÎť1
e v1 , etÎť2 v2 , . . . , etÎťN vn

ein Fundamentalsystem der Gleichung y â˛(t) = Ay(t).

Wir wenden uns nun dem inhomogenen Problem (6.4) zu. Nach Satz 6.3.6 fehlt
uns zur Angabe aller LoĚsungen dieses Problems nur noch eine spezielle LoĚsung.
Jede LoĚsung y(t) des zugehoĚrigen homogenen Problems ist nach den obigen Ergebnissen eine Linearkombinationen der Spalten des Fundamentalsystems etA ,
d.h.
y(t) = etA c, mit einem c â RN .

Damit starten wir wieder die Variation-der-Konstanten-Methode, vgl. Bemerkung 6.2.7 (b), d.h. wir setzen yp (t) := etA c(t) an und setzen dieses in das inhomogene Problem ein. Das liefert mit Bemerkung 6.3.12
Ayp (t) + b(t) = ypâ˛ (t) = AetA c(t) + etA câ˛ (t) = Ayp (t) + etA câ˛ (t),
also
câ˛ (t) = eâtA b(t).
Integrieren der Gleichung liefert als ein moĚgliches c die Funktion
Z t
c(t) =
eâsA b(s) ds,
t0

wobei t0 â I beliebig gewaĚhlt werden kann. Hierbei ist die Integration des Vektors
eâsA b(s), genauso wie oben schon die Differentiation von Vektoren, komponentenweise zu verstehen. Wir erhalten also
Z t
tA
eâsA b(s) ds.
yp (t) = e
t0

Diese LoĚsung erweist sich nach einer Probe als richtig und stellt man auch noch
alle Konstanten anhand der Anfangsbedingung richtig ein, so erhaĚlt man folgendes Resultat.

283

6. GewoĚhnliche Differentialgleichungen
Satz 6.3.15 (Variation-der-Konstanten-Formel, bzw. Duhamelsche Formel). Es
seien I â R ein Intervall, A â RN ĂN eine Matrix und b : I â R eine stetige
Funktion, sowie t0 â I und y0 â RN . Dann hat das lineare Anfangswertproblem
erster Ordnung mit konstanten Koeffizienten
 â˛
y (t) = Ay(t) + b(t), t â I,
y(t0 ) = y0
die eindeutige globale LoĚsung
Z t
Z t
âsA
(tât0 )A
(tât0 )A
tA
e(tâs)A b(s) ds.
e b(s) ds = e
y0 +
y(t) = e
y0 + e
t0

t0

Beispiel 6.3.16. Wir versehen unser Problem aus Beispiel 6.3.13 noch mit einer
InhomogenitaĚt und einem Anfangswert und betrachten
ďŁą

 2t 

e
3
1
ďŁ´
â˛
ďŁ´
, t â R,
y(t) +
ďŁ˛ y (t) =
0
1
3
 
1
ďŁ´
ďŁ´
.
ďŁł y(0) =
â1

Dann ist

1
e y0 =
2
tA

und
Z

0

t
(tâs)A

e



e4t + e2t e4t â e2t
e4t â e2t e4t + e2t



1
â1



2t

=e



1
â1





  2s 
e4(tâs) + e2(tâs) e4(tâs) â e2(tâs)
e
b(s) ds =
ds
4(tâs)
2(tâs)
4(tâs)
2(tâs)
0
e
âe
e
+e
0
!

Z 
s=t
1 âe4t eâ2s /2 s=0 + te2t
1 t e4t eâ2s + e2t
ds =
=
s=t
2 0 e4t eâ2s â e2t
2 âe4t eâ2s /2 s=0 â te2t


1 e4t + (2t â 1)e2t
=
.
4 e4t â (2t + 1)e2t
Z

t

1
2

Zusammen ergibt sich also




  
  
 
1 4t 1
1 4t 1
2t + 3
1
2t 2t â 1
2t
2t
=
.
+e
+e
e
e
+
y(t) = e
2t + 1
â2t â 5
1
1
â1
4
4

6.4. Differentialgleichungen hoĚherer Ordnung
Eine gewoĚhnliche Differentialgleichung der Ordnung n mit n âĽ 2 laĚsst sich immer auf ein System von Differentialgleichungen erster Ordnung reduzieren. Dieses
Verfahren soll in diesem Abschnitt vorgestellt werden. Gegeben sei also eine Differentialgleichung der Form

y (n) (t) = F t, y(t), y â˛(t), . . . , y (nâ1)(t) , t â I,
(6.6)
284

6.4. Differentialgleichungen hoĚherer Ordnung
wie in Definition 6.1.2. Hierbei sind n â N mit n âĽ 2, I â R ein Intervall und
F : I Ă Rn â R stetig.
Wir definieren nun die Funktion v : I â Rn mit
v1 = y,

v2 = y â˛,

v3 = y â˛â˛ ,

...,

vn = y (nâ1) .

FuĚr diese gilt dann
ďŁś ďŁŤ â˛
ďŁŤ
ďŁś
ďŁś ďŁŤ
y (t)
v1â˛ (t)
v2 (t)
ďŁŹ v â˛ (t) ďŁˇ ďŁŹ y â˛â˛(t) ďŁˇ ďŁŹ
ďŁˇ
v3 (t)
ďŁˇ ďŁŹ
ďŁŹ 2
ďŁˇ
ďŁˇ ďŁŹ
ďŁŹ v â˛ (t) ďŁˇ ďŁŹ y â˛â˛â˛ (t) ďŁˇ ďŁŹ
ďŁˇ
v
(t)
4
3
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
v â˛ (t) = ďŁŹ .. ďŁˇ = ďŁŹ
ďŁˇ.
ďŁˇ=ďŁŹ
..
..
ďŁŹ . ďŁˇ ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
.
.
ďŁˇ ďŁŹ (nâ1) ďŁˇ ďŁŹ
ďŁŹ â˛
ďŁˇ
ďŁ­vnâ1 (t)ďŁ¸ ďŁ­y
ďŁ¸
vn (t)
(t)ďŁ¸ ďŁ­

â˛
(n)
vn (t)
F t, v1 (t), v2 (t), . . . , vn (t)
y (t)

Diese Gleichung ist ein System von Differentialgleichungen erster Ordnung fuĚr
die Funktionen v1 , v2 , . . . , vn und es gilt der folgende Satz

Satz 6.4.1. Es seien n â N mit n âĽ 2, I â R ein Intervall und F : I Ă
Rn â R eine stetige Funktion. Dann ist y : I â R genau dann eine LoĚsung
der Differentialgleichung in (6.6), wenn v = (y, y â˛, y â˛â˛, . . . , y (nâ1) )T : I â Rn eine
LoĚsung des Systems v â˛ (t) = G(t, v(t)) mit
ďŁś
ďŁŤ
v2 (t)
ďŁˇ
ďŁŹ
v3 (t)
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
.
.
(6.7)
G(t, v(t)) = ďŁŹ
ďŁˇ
.
ďŁˇ
ďŁŹ
ďŁ¸
ďŁ­
vn (t)

F t, v1 (t), v2 (t), . . . , vn (t)
ist.

Beweis. Es sei zunaĚchst y : I â R eine LoĚsung der Differentialgleichung in (6.6).
Dann ist y nach Definition des Begriffs LoĚsung eine n mal stetig differenzierbare Funktion auf I. Also ist v noch einmal stetig differenzierbar und die gerade
durchgefuĚhrte Rechnung zeigt, dass v eine LoĚsung des Systems (6.7) ist.
Ist umgekehrt v eine LoĚsung des Systems (6.7), so ist mit v auch jede Koordinatenfunktion von v stetig differenzierbar. Also koĚnnen wir y := v1 noch einmal
ableiten und erhalten dank der speziellen Form der Funktion G die Beziehung
y â˛ = v1â˛ = v2 . Da v2 ebenfalls stetig differenzierbar ist, gilt selbiges fuĚr y sogar
zweimal und wir finden y â˛â˛ = v2â˛ = v3 , da v eine LoĚsung des Systems (6.7) ist.
Indem wir dieses Argument noch n â 1 Mal wiederholen, sehen wir, dass y sogar
n Mal stetig differenzierbar ist und
y (n) = vnâ˛ = F (t, v1 , v2 , . . . , vn ) = F (t, y, y â˛, y â˛â˛, . . . , y (nâ1) )
gilt. Damit ist y eine LoĚsung der Differentialgleichung n-ter Ordnung in (6.6) und
wir sind fertig.

285

6. GewoĚhnliche Differentialgleichungen
Damit ist (zumindest in der Theorie) das Problem von Differentialgleichungen
hoĚherer Ordnung auf solche von erster Ordnung reduziert. Um den Gewinn dieses Verfahrens zu sehen, wollen wir uns nun eine besonders wichtige Klasse solcher
Gleichungen anschauen, die linearen Gleichungen hoĚherer Ordnung mit konstanten Koeffizienten. Deren allgemeine Form ist
y (n) (t) + anâ1 y (nâ1) (t) + Âˇ Âˇ Âˇ + a1 y â˛ (t) + a0 y(t) = g(t),

t â I,

(6.8)

wobei a0 , a1 , . . . , anâ1 â R die sogenannten Koeffizienten der Gleichung sind und
g : I â R die InhomogenitaĚt.
Zum Umschreiben dieser Gleichung in ein System erster Ordnung definieren wir
also die Funktion v : I â Rn mit

v(t) := y(t), y â˛(t), y â˛â˛(t), . . . , y (nâ1) (t) ,

und finden
ďŁŤ

y â˛ (t)
y â˛â˛(t)
y â˛â˛â˛ (t)
..
.

ďŁś

ďŁŤ

t â I,
ďŁś

v2 (t)
v3 (t)
v4 (t)
..
.

ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
â˛
v (t) = ďŁŹ
ďŁˇ
ďŁˇ=ďŁŹ
ďŁŹ
ďŁˇ
ďŁˇ ďŁŹ
ďŁŹ (nâ1) ďŁˇ ďŁŹ
ďŁˇ
ďŁ­y
ďŁ¸
vn (t)
(t)ďŁ¸ ďŁ­
(n)
âa0 v1 (t) â a1 v2 (t) â Âˇ Âˇ Âˇ â anâ1 vn (t) + g(t)
y (t)
= Av(t) + b(t)

mit
ďŁŤ

ďŁŹ
ďŁŹ
ďŁŹ
A := ďŁŹ
ďŁŹ
ďŁ­

0

1

0

0
..
.

0
..
.

1
..
.

0
0
...
âa0 âa1 âa2

...
..
.
..
.

0
..
.

0
0
1
. . . âanâ1

ďŁś
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁˇ
ďŁ¸

und

ďŁŤ

0
0
..
.

ďŁś

ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
ďŁŹ
ďŁˇ
b(t) = ďŁŹ
ďŁˇ.
ďŁŹ
ďŁˇ
ďŁ­ 0 ďŁ¸
g(t)

(6.9)

Unsere Differentialgleichung (6.8) erweist sich also als aĚquivalent zu einem System von linearen Gleichungen erster Ordnung mit konstanten Koeffizienten, vgl.
Abschnitt 6.3.2. Damit koĚnnen wir auch die gesamte LoĚsbarkeits- und LoĚsungstheorie von dort uĚbersetzen. Das ergibt das folgende Resultat.
Satz 6.4.2. Es seien I â R ein Intervall, a0 , a1 , . . . , anâ1 â R und g : I â R
eine stetige Funktion. Dann gelten die folgenden Aussagen:
(a) Ist g = 0, so ist die Menge aller LoĚsungen der Gleichung (6.8) ein Untervektorraum der Dimension n von C n (I).

286

6.4. Differentialgleichungen hoĚherer Ordnung
(b) Ist yp eine LoĚsung der Gleichung (6.8), so ist jede LoĚsung dieser Gleichung
gegeben durch y = yp +yh , wobei yh eine LoĚsung des zugehoĚrigen homogenen
Systems (d.h. mit g = 0) ist.
Die ZuruĚckfuĚhrung dieser Aussagen auf die entsprechenden Resultate fuĚr Systeme
aus Abschnitt 6.3.2 verbleibt als UĚbungsaufgabe.
Definition 6.4.3. (a) Jede Basis des Raums aller LoĚsungen in Satz 6.4.2 (a)
nennt man ein Fundamentalsystem der homogenen Gleichung.
(b) Die LoĚsung yp der inhomogenen Gleichung in Satz 6.4.2 (b) heiĂt spezielle
LoĚsung, oder auch PartikulaĚrloĚsung der Gleichung (6.8).
Beispiel 6.4.4. Wir betrachten das Anfangswertproblem
ďŁą â˛â˛
ďŁ˛ y (t) + y â˛ (t) â 2y(t) = 0, t â R,
y(0) = 3
ďŁł
y â˛ (0) = 0.

Mit obigen UĚberlegungen gilt fuĚr die Funktion v(t) = (y(t), y â˛(t))T , t â R, die
Differentialgleichung


0 1
â˛
v(t)
v (t) = Av(t) =
2 â1
Die Eigenwerte der Matrix A sind 1 und â2 mit zugehoĚrigen Eigenvektoren
(1, 1)T , bzw. (1, â2)T . Also ist nach Satz 6.3.14 ein Fundamentalsystem dieses
Systems gegeben durch
 
  
1
â2t
t 1
.
,e
e
â2
1
Damit sind alle LoĚsungen gegeben durch
 
 


1
y(t)
â2t
t 1
,
+ c2 e
= c1 e
v(t) =
â2
1
y â˛ (t)

c1 , c2 â R.

Davon interessiert uns allerdings nur die erste Zeile und wir erhalten alle LoĚsungen
der Differentialgleichung unseres Anfangswertproblems zu
y(t) = c1 et + c2 eâ2t ,

c1 , c2 â R.

Es bleiben noch die Anfangswerte einzustellen. Wegen y â˛(t) = c1 et â c2 2eâ2t muss
fuĚr die Konstanten c1 , c2 â R gelten:
3 = y(0) = c1 + c2

und

0 = c1 â 2c2 .

LoĚst man das Gleichungssystem auf, so erhaĚlt man c1 = 2 und c2 = 1, also ist
die LoĚsung unseres Anfgangswertproblems
y(t) = 2et + eâ2t ,

t â R.

287

6. GewoĚhnliche Differentialgleichungen
Dank der speziellen Form der Matrix A in (6.9) lassen sich selbst im allgemeinen
Fall die Eigenwerte und Eigenvektoren und damit die Matrix-Exponentialfunktion
etA bestimmen. So bekommt man das charakteristische Polynom durch Entwickeln nach der letzten Zeile zu
âÎť
det(A â ÎťIn ) =

Definition 6.4.5. Es sei

0
..
.

1
âÎť
..
.

0
...
0
..
..
..
.
.
.
..
.
1
0
0
âÎť
1
. . . âanâ2 âanâ1 â Îť

0
...
âa0 âa1


= (â1)n a0 + a1 Îť + Âˇ Âˇ Âˇ + anâ1 Îťnâ1 + Îťn

y (n) (t) + anâ1 y (nâ1) (t) + Âˇ Âˇ Âˇ + a1 y â˛(t) + a0 y(t) = 0
eine homogene lineare Differentialgleichung der Ordnung n mit konstanten Koeffizienten. Dann heiĂt
n

Îť + anâ1 Îť

nâ1

n

+ Âˇ Âˇ Âˇ + a1 Îť + a0 = Îť +

nâ1
X

ak Îťk

k=0

charakteristisches Polynom der Differentialgleichung.
Satz 6.4.6. Es seien I â R ein Intervall und n âĽ 2. Mit a0 , a1 , . . . , anâ1 â R sei
die Differentialgleichung
y (n) (t) + anâ1 y (nâ1) (t) + Âˇ Âˇ Âˇ + a1 y â˛(t) + a0 y(t) = 0,

t â I,

(6.10)

gegeben und es seien Îť1 , Îť2 , . . . , Îťk die Nullstellen des zugehoĚrigen charkateristischen Polynoms, sowie mj die Vielfachheit der Nullstelle Îťj fuĚr j â {1, 2, . . . , k}.
Dann ist
 Îťj t Îťj t
e , te , . . . , tmj â1 eÎťj t : j = 1, 2, . . . , k
ein Fundamentalsystem von (6.10).

Beispiel 6.4.7. Wir bestimmen ein Fundamentalsystem der Differentialgleichung
y (4) (t) â 2y â˛â˛â˛(t) + 2y â˛(t) â y(t) = 0,

t â I.

Dazu zerlegen wir das zugehoĚrige charakteristische Polynom in Linearfaktoren:
Îť4 â 2Îť3 + 2Îť â 1 = (Îť â 1)3 (Îť + 1).
Also ist 1 eine Nullstelle mit Vielfachheit drei und â1 mit Vielfachheit eins. Nach
Satz 6.4.6 ist also
 t t 2 t ât
e , te , t e , e

ein Fundamentalsystem.

288

6.5. Existenz- und Eindeutigkeitsresultate

6.5. Existenz- und Eindeutigkeitsresultate
In diesem Abschnitt werden kurz die beiden wichtigsten SaĚtze vorgestellt, die die
LoĚsbarkeit, bzw. eindeutige LoĚsbarkeit von Anfangswertproblemen garantieren.
Wir formulieren diese SaĚtze jeweils fuĚr Systeme von Gleichungen erster Ordnung.
Damit ist dann nach den Ergebnissen des vorhergehenden Abschnitts auch der
Fall von Gleichungen hoĚherer Ordnung abgedeckt.
Satz 6.5.1 (Satz von Peano). Es sei I â R ein Intervall und f : I Ă Rn â Rn
stetig. Dann hat fuĚr jedes t0 â I und y0 â Rn das Anfangswertproblem

 â˛
y (t) = f t, y(t) ,
t â I,
y(t0) = y0
eine LoĚsung, d.h. es gibt ein offenes Intervall J â I mit t0 â J und eine Funktion
y â C 1 (J; Rn ), die das Anfangswertproblem auf J loĚst.
Die reine Stetigkeit der Funktion f reicht jedoch nicht aus, um die eindeutige
LoĚsbarkeit des Anfangswertproblems zu garantieren, wie das folgende Beispiel
zeigt.
Beispiel 6.5.2. Wir betrachten das Anfangswertproblem

2/3
y â˛ (t) = y(t) ,
t â R,
y(0) = 0.
Die rechte Seite f (t, y(t)) = |y(t)|2/3 ist stetig, nach dem Satz von Peano ist das
Anfangswertproblem also loĚsbar. Eine LoĚsung ist auch schnell gefunden, denn
offensichtlich loĚst y(t) = 0, t â I. Aber das ist leider nicht die einzige LoĚsung,
denn auch
1
y(t) = t3
27
ist eine, wie man leicht nachrechnet.
Satz 6.5.3 (Satz von Picard-LindeloĚff). Es sei I â R ein kompaktes Intervall, f :
I ĂRn â Rn stetig, t0 â I und y0 â Rn . GenuĚgt dann f einer Lipschitzbedingung,
d.h. existiert ein L > 0 mit
f (t, y1 ) â f (t, y2) â¤ Lky1 â y2 k

fuĚr alle t â I und y1 , y2 â Rn ,

so ist das Anfangswertproblem

 â˛
y (t) = f t, y(t) ,
y(t0) = y0

t â I,

eindeutig loĚsbar.

289

6. GewoĚhnliche Differentialgleichungen
Bemerkung 6.5.4. Damit koĚnnen wir nun den Beweis von Satz 6.3.3 vervollstaĚndigen. Dort ging es um den Untervektorraum L aller LoĚsungen des linearen Systems von Differentialgleichungen y â˛(t) = A(t)y(t), t â I, mit einer
stetigen Funktion A : I â RN ĂN und es blieb noch zu zeigen, dass es eine solche
LoĚsung uĚberhaupt gibt und dass die Dimension des Raums N ist.
Die Existenz einer LoĚsung folgt sofort aus dem Satz von Peano. FuĚr die Dimension
betrachten wir zu einem fest gewaĚhlten t0 â I die Abbildung ÎŚ : L â RN mit
ÎŚ(y) = y(t0) und zeigen, dass diese eine bijektive lineare Abbildung, also ein
Isomorphismus ist. Dann gilt nach Satz 3.6.8 (c) dim(L) = dim(RN ) = N.
FuĚr die LinearitaĚt seien y1 , y2 â L und Îą, Î˛ â R. Dann gilt
ÎŚ(Îąy1 + Î˛y2) = (Îąy1 + Î˛y2 )(t0 ) = Îąy1(t0 ) + Î˛y2 (t0 ) = ÎąÎŚ(y1 ) + Î˛ÎŚ(y2 ).
FuĚr die BijektivitaĚt uĚberlegen wir uns folgendes: Auf jedem kompakten Teilintervall J von I ist die Funktion t 7â A(t) als stetige Funktion beschraĚnkt, vgl.
Satz 4.8.8. Also gilt fuĚr alle y1 , y2 â RN
kA(t)y1 â A(t)y2 k = kA(t)(y1 â y2 )k â¤ kA(t)kky1 â y2 k â¤ Mky1 â y2 k,
wobei M := maxtâJ kA(t)k ist. Das heiĂt, dass die rechte Seite der betrachteten
Differenzialgleichung y â˛(t) = A(t)y(t) die Lipschitzbedingung aus dem Satz von
Picard-LindeloĚff erfuĚllt, d.h. das Anfangswertproblem y â˛(t) = A(t)y(t) mit y(t0 ) =
y0 ist fuĚr jedes y0 â RN eindeutig loĚsbar. Andersherum formuliert bedeutet das,
dass es fuĚr jedes y0 â RN genau eine Funktion y â L gibt mit ÎŚ(y) = y(t0 ) = y0 ,
also ist ÎŚ bijektiv.

290

7. Allgemeine Algebra
7.1. Allgemeine Algebren
Definition 7.1.1. Es sei A eine Menge und n â N. Eine Abbildung
n
f : |A Ă A Ă
{zÂˇ Âˇ Âˇ Ă A} = A â A
n Mal

heiĂt n-stellige Operation auf A.
Die Menge aller n-stelligen Operationen auf A bezeichnen wir mit
Opn (A) := {f | f : An â A}.
Beispiel 7.1.2. (a) Ein Beispiel fuĚr eine zwei-stellige Operation ist die Addition in Z oder allgemein die VerknuĚpfung â in einer Gruppe (G, â), denn
dies ist eine Abbildung â : G Ă G â G.
(b) Eine ein-stellige Operation ist z.B. der Betrag auf R, also | Âˇ | : R â R, oder
die Inversenbildung g 7â g fuĚr g â G in einer Gruppe G.
Bemerkung 7.1.3. In obiger Defintion ist n = 0 zugelassen. Was ist Op0 (A)?
Dazu ist es hilfreich, sich klar zu machen, dass man An als die Menge aller Funktionen von {1, 2, . . . , n} nach A auffassen kann, also

An = g g : {1, 2, . . . , n} â A .
FuĚr n = 0 ist dann A0 die Menge aller Abbildungen g : â â A. Auf der leeren
Menge gibt es nur eine Abbildung, die sogenannte leere Abbildung, die wir hier
mit âŚ bezeichnen wollen. Damit ist A0 = {âŚ} und wir bekommen
Op0 (A) = {f | f : {âŚ} â A}.
Da der Defintionsbereich dieser Funktionen nun einelementig ist, gibt es zu jedem
a â A genau eine Abbildung fa â Op0 (A), naĚmlich die mit fa (âŚ) = a. Damit entspricht Op0 (A) genau der Menge A. Man nennt diese Null-stelligen Operationen
Konstanten und identifiziert fa normalerweise mit a.
Definition 7.1.4. (a) Ein Typ ist ein Paar (F , Ď), wobei F eine Menge und
Ď : F â N eine Abbildung, die sogenannte Stelligkeitsabbildung ist.
Ein Element f von F wird Ď(f )-stelliges Operationssymbol genannt.

291

7. Allgemeine Algebra
(b) Es sei A eine Menge und (F , Ď) ein Typ. Ordnet man jedem f â F eine
Ď(f )-stellige Operation fA â OpĎ(f ) (A) auf A zu, so heiĂt
A = (A, F ) mit F = {fA : f â F }
eine (allgemeine) Algebra vom Typ (F , Ď).
Die Menge A wird Grundmenge der Algebra A genannt und jedes fA â F
nennt man eine Operation auf A.
Bemerkung 7.1.5. (a) Man stelle sich fuĚr den Moment unter eine Algebra
eine Menge A mit einer Familie von Operationen F vor. Die Stelligkeitsabbildung Ď gibt dann zu jeder Operation an, wievielstellig sie ist.
(b) In vielen Anwendungen hat man es nur mit endlich vielen Operationen
f1 , f2 , . . . , fk in F zu tun. Dann verwendet man fuĚr die Algebra A = (A, F )
oft die Kurznotation
A = (A, f1 , f2 , . . . , fk ) vom Typ (Ď1 , Ď2 , . . . , Ďk ),
wobei Ďj = Ď(fj ) fuĚr jedes j â {1, 2, . . . , k} gilt.
Beispiel 7.1.6. Eine Gruppe G ist eine Algebra (G, â,
die die folgenden Axiome erfuĚllt:

, n) vom Typ (2, 1, 0),

(a) âa, b, c â G : (a â b) â c = a â (b â c),
(n) âa â G : n â a = a â n = a,
(i) âa â G : a â a = a â a = n.
Weiter ist die Gruppe abelsch, falls zusaĚtzlich
(k) âa, b â G : a â b = b â a
gilt.
Bemerkung 7.1.7. (a) Man beachte, dass eine strenge
Funktionsschreibweise


fuĚr z.B. (i) so aussaĚhe: âa â G : â a, (a) = â (a), a = n. Durch
so einen Formelsalat will sich natuĚrlich niemand kaĚmpfen, man schreibt
deshalb die Operationen wie oben und damit wie gewohntâ auf.
â
(b) Die Darstellung einer algebraischen Struktur, z.B. Gruppe, als allgemeine
Algebra ist nicht eindeutig. Man kann dabei Informationen von den Operationen in die Axiome verlagern und umgekehrt.
Beispielsweise ist eine Gruppe auch eine Algebra (G, â) vom Typ (2), so
dass die Axiome aus Definition 2.3.1 gelten.

292

7.1. Allgemeine Algebren
Beispiel 7.1.8. (a) Ein Ring ist eine Algebra (R, +, â, 0, Âˇ) vom Typ (2, 1, 0, 2),
fuĚr die (R, +, â, 0) eine abelsche Gruppe ist und die folgenden Gleichungen
fuĚr alle a, b, c â R gelten
â˘ a Âˇ (b Âˇ c) = (a Âˇ b) Âˇ c,

â˘ a Âˇ (b + c) = a Âˇ b + a Âˇ c,
â˘ (a + b) Âˇ c = a Âˇ c + b Âˇ c.

(b) Etwas komplizierter ist es, einen K-Vektorraum V als Algebra aufzufassen,
denn hier haben wir es eigentlich mit zwei Mengen, naĚmlich V und K zu
tun. Man kann nun entweder Algebren uĚber mehreren Mengen, sogenannte
mehrsortige Algebren anschauen, oder nutzen, dass wir unendliche Familien
von Operationen F zugelassen haben:
Ein K-Vektorraum V ist eine Algebra (V, +, â, 0, K) vom Typ (2, 1, 0, (1)ÎąâK ),
wobei K ein KoĚrper und (V, +, â, 0) eine abelsche Gruppe ist, sowie die folgenden Gleichungen fuĚr alle v, w â V und alle Îą, Î˛ â K gelten:
â˘ 1v = v,

â˘ (ÎąÎ˛)v = Îą(Î˛v),

â˘ (Îą + Î˛)v = Îąv + Î˛v,

â˘ Îą(v + w) = Îąv + Îąw.

Dabei haben wir jedes KoĚrperelement Îą als eine einstellige Operation auf V
identifiziert, die der Skalarmultiplikation mit Îą entspricht, d.h. Îą : V â V
mit Îą(v) := Îąv.
Beispiel 7.1.9. Wir fuĚhren noch eine neue algebraische Struktur ein, die z.B. das
Schneiden und Vereinigen von Mengen oder auch die VerknuĚpfung von Aussagen
mit undâ und oderâ algebraisiert.
â
â
Ein Verband ist eine Algebra (L, â§, â¨) vom Typ (2, 2), der folgende Gleichungen
fuĚr alle x, y â L erfuĚllt:
â˘ x â¨ y = y â¨ x und x â§ y = y â§ x

(KommutativitaĚt)

â˘ (x â¨ y) â¨ z = x â¨ (y â¨ z) und (x â§ y) â§ z = x â§ (y â§ z)
â˘ x â¨ x = x und x â§ x = x

(AssoziativitaĚt)

(Idempotenz)

â˘ x â¨ (x â§ y) = x und x â§ (y â¨ x) = x

(Absorption)

Gelten zusaĚtzlich die Distributivgesetze
x â¨ (y â§ z) = (x â¨ y) â§ (x â¨ z) und x â§ (y â¨ z) = (x â§ y) â¨ (x â§ z)
fuĚr alle x, y, z â L, so spricht man von einem distributiven Verband .
Damit ist zum Beispiel fuĚr jede Menge X die Algebra (P(X), âŠ, âŞ) vom Typ
(2, 2) ein distributiver Verband.

293

7. Allgemeine Algebra

7.2. Unteralgebren und Erzeugnis
Definition 7.2.1. Es sei A = (A, F ) eine Algebra vom Typ (F , Ď).
(a) Ist B â A so beschaffen, dass fuĚr jedes Operationssymbol f â F mit der
Setzung n := Ď(f ) gilt
â b1 , b2 , . . . , bn â B : fA (b1 , b2 , . . . , bn ) â B
so heiĂt B := (B, FB ) mit FB := {fA |BĎ(f ) : f â F } eine Unteralgebra von
A.
(b) Die Menge aller Grundmengen von Unteralgebren von A bezeichnen wir mit

Sub(A) := B â A : (B, FB ) Unteralgebra von A .

Satz 7.2.2. Es sei A = (A, F ) eine Algebra vom Typ (F , Ď). Dann gilt
(a) A â Sub(A).

(b) Ist I 6= â eine beliebige Indexmenge und Bj â Sub(A) fuĚr jedes j â I, so
ist auch
\
Bj â Sub(A)
jâI

Beweis. Da A eine Unteralgebra von A ist, haben wir auch A â Sub(A), es ist
also eigentlich nur im Teil (b) etwas zu beweisen.
T
Sei dazu f â F beliebig mit n := Ď(f ). Nun ist zu zeigen, dass B := jâI Bj
unter der Wirkung von fA abgeschlossen ist. Seien dazu b1 , b2 , . . . , bn â B. Dann
gilt nach der Definition von B auch b1 , b2 , . . . , bn â Bj fuĚr jedes j â I. Da nun Bj
die Grundmenge einer Unteralgebra von A ist, muss fA (b1 , b2 , . . . , bn ) â Bj sein,
und zwar wieder fuĚr alle j â I. Das liefert fA (b1 , b2 , . . . , bn ) â B und wir sind
fertig.
Korollar 7.2.3. Ist A = (A, F ) eine Algebra und X â A, so ist auch
\
hXi :=
B â Sub(A),
BâSub(A)
BâX

d.h. (hXi , FhXi ) ist eine Unteralgebra von A.
Beweis. Es ist auf jeden Fall A â Sub(A) mit A â X, also wird der Schnitt
nicht uĚber eine leere Indexmenge gebildet. Damit folgt die Behauptung direkt
aus Satz 7.2.2 (b).
Definition 7.2.4. Es sei A = (A, F ) eine Algebra und X â A. Dann heiĂt
(hXi , FhXi ) mit hXi aus Korollar 7.2.3 die von X erzeugte Unteralgebra oder
auch das Erzeugnis von X in A.

294

7.3. Homomorphismen und Isomorphsimen
Bemerkung 7.2.5. Diese Konstruktion haben wir schon in den SpezialfaĚllen
von Gruppen, vgl. Definition 2.3.10, und VektorraĚumen, vgl. Definition 3.2.7 und
UĚbungsaufgabe 3.2.10, kennengelernt. Wie man nun sieht, ist das ein ganz allgemeines Verfahren, das in jeder allgemeinen Algebra funktioniert.
FuĚr die Vorstellung: Das Erzeugnis ist, wie schon in den oben angefuĚhrten SpezialfaĚllen, die kleinste Unteralgebra von A, die die Menge X ganz enthaĚlt.
UĚbungsaufgabe 7.2.6. Es sei A = (A, F ) eine Algebra. Zeigen Sie, dass fuĚr alle
X, Y â A gilt
(a) X â hXi,
(b) X â Y =â hXi â hY i,
(c) hXi = hhXii.
UĚbungsaufgabe 7.2.7. Es sei A = (A, F ) eine Algebra. Auf der Menge Sub(A)
definieren wir die Operationen â¨ und â§ durch
B â§ C := B âŠ C,
B â¨ C := hB âŞ Ci ,

B, C â Sub(A),
B, C â Sub(A).

Zeigen Sie, dass (Sub(A), â§, â¨) ein Verband ist. Ist der Verband auch distributiv?

7.3. Homomorphismen und Isomorphsimen
Definition 7.3.1. Es seien A mit Grundmenge A und B mit Grundmenge B
zwei Algebren desselben Typs (F , Ď).
(a) Eine Abbildung Ď : A â B heiĂt Homomorphismus von A nach B, falls fuĚr
alle f â F mit der Setzung n := Ď(f ) und fuĚr alle a1 , a2 , . . . , an â A gilt


Ď fA (a1 , a2 , . . . , an ) = fB Ď(a1 ), Ď(a2 ), . . . , Ď(an ) .
Man schreibt dann auch Ď : A â B.

(b) Ist die Abbildung Ď in (a) zusaĚtzlich bijektiv, so nennt man Ď einen Isomorphismus und die Algebren A und B heiĂen dann isomorph.
Beispiel 7.3.2. Wir haben in Beispiel 7.1.6 gesehen, dass wir eine Gruppe als
Algebra (G, â, , n) vom Typ (2, 1, 0) auffassen koĚnnen. Die Homomorphiebedingung aus obiger Defintion bedeutet dann, dass eine Abbildung Ď von einer
Gruppe (G, â, , nG ) in eine andere Gruppe (H, â, b , nH ) ein Homomorphismus
ist, wenn fuĚr jede Wahl von g1 , g2 â G gilt


Ď â(g1 , g2 ) = â Ď(g1 ), Ď(g2) , d.h. Ď(g1 â g2 ) = Ď(g1 ) â Ď(g2 )
295

7. Allgemeine Algebra
sowie

[
Ď(g1 ) = Ď(g
1)

und
Ď(nG ) = nH .
Die erste Bedingung ist genau die aus unserer Definition des Gruppenhomomorphismus 2.3.14 und wie wir in Satz 2.3.17 (b), bzw. (a) gesehen haben, impliziert
diese die anderen beiden, die in diesem Fall also redundant sind.
Satz 7.3.3. Es seien A und B Algebren desselben Typs (F , Ď) mit Grundmengen
A bzw. B, sowie Ď : A â B ein Isomorphismus. Dann ist auch Ďâ1 : B â A ein
Isomorphismus.
Beweis. Die BijektivitaĚt von Ďâ1 folgt sofort aus der BijektivitaĚt von Ď. Es bleibt
zu zeigen, dass Ďâ1 wieder ein Homomorphismus von B nach A ist. Es sei dazu
f â F und n := Ď(f ). WaĚhlen wir beliebige b1 , b2 , . . . , bn â B, dann existiert
dank der BijektivitaĚt von Ď fuĚr jedes j â {1, 2, . . . , n} genau ein aj â A mit
Ď(aj ) = bj . Damit gilt nun dank der Homomorphieeigenschaft von Ď



Ďâ1 fB (b1 , b2 , . . . , bn ) = Ďâ1 fB Ď(a1 ), Ď(a2 ), . . . , Ď(an )


= Ďâ1 Ď fA (a1 , a2 , . . . , an )

= fA (a1 , a2 , . . . , an ) = fA Ďâ1 (b1 ), Ďâ1 (b2 ), . . . , Ďâ1 (bn )
und wir haben die Homomorphiebedingung fuĚr Ďâ1 nachgewiesen.

UĚbungsaufgabe 7.3.4. Es seien A, B und C Algebren desselben Typs, sowie
Ď : A â B und Ď : B â C Homomorphismen. Zeigen Sie, dass dann auch
Ď âŚ Ď : A â C ein Homomorphismus ist.
Satz 7.3.5. Es seien A und B Algebren desselben Typs und Ď : A â B ein
Homomorphismus. Dann gilt
(a) U â Sub(A) =â Ď(U) â Sub(B),
(b) V â Sub(B) =â Ďâ1 (V ) â Sub(A),
(c) X â A =â hĎ(X)i = Ď(hXi).
Beweis. (a) Es seien f â F , n := Ď(f ) und b1 , b2 , . . . , bn â Ď(U). Dann existieren a1 , a2 , . . . , an â U mit Ď(aj ) = bj fuĚr alle j â {1, 2, . . . , n}. Damit
erhalten wir dank der Homomorphieeigenschaft von Ď


fB (b1 , b2 , . . . , bn ) = fB Ď(a1 ), Ď(a2 ), Ď(an ) = Ď fA (a1 , a2 , . . . , an ) .

Nun sind alle a1 , a2 , . . . , an â U und da U nach Voraussetzung die Grundmenge einer Unteralgebra von A ist, muss auch fA (a1 , a2 , . . . , an ) â U sein.
Also ist nach obiger Rechnung fB (b1 , b2 , . . . , bn ) â Ď(U) und wir sind fertig.

296

7.3. Homomorphismen und Isomorphsimen
(b) Es seien f â F , n := Ď(f ) und a1 , a2 , . . . , an â Ďâ1 (V ). Dann sind die Bilder
Ď(a1 ), Ď(a2 ), . . . , Ď(an ) â V und wir bekommen, da V nach Voraussetzung
die Grundmenge einer Unteralgebra von B ist,

fB Ď(a1 ), Ď(a2 ), . . . , Ď(an ) â V.
Weiter ist Ď : A â B ein Homomorphismus. Also gilt


Ď fA (a1 , a2 , . . . , an ) = fB Ď(a1 ), Ď(a2 ), . . . , Ď(an ) â V.

Damit ist fA (a1 , a2 , . . . , an ) â Ďâ1 (V ) und die Behauptung bewiesen.

(c) Ohne Beweis.

297

Tabelle der griechischen Buchstaben
groĂ
A
B
Î
â
E
Z
H
Î
I
K
Î
M
N
Î
O
Î 
P
ÎŁ
T
Y
ÎŚ
X
Î¨
âŚ

klein
Îą
Î˛
Îł
Î´
ÇŤ, Îľ
Îś
Îˇ
Î¸, Ď
Îš
Îş, Îş
Îť
Âľ
Î˝
Îž
o
Ď, Ě
Ď, Ěş
Ď, Ď
Ď
Ď
Ď, Ď
Ď
Ď
Ď

Name
Alpha
Beta
Gamma
Delta
Epsilon
Zeta
Eta
Theta
Iota
Kappa
Lambda
My
Ny
Xi
Omikron
Pi
Rho
Sigma
Tau
Ypsilon
Phi
Chi
Psi
Omega

299

Index
1-Norm, 67
2-Norm, 66, 245
â-Norm, 67
Abbildung, 13
kanonische, 13
lineare, 78
Abbildungsmatrix, 92, 107
abelsche Gruppe, 27, 292
abgeschlossene Menge, 156
abgeschlossenes Intervall, 126
Ableitung, 197
partielle, 222
Richtungs-, 221
totale, 228
Ableitungsfunktion, 197
partielle, 222
totale, 228
absolut konvergente Reihe, 148, 154
Abstand, 68
Additionstheoreme, 191
affiner Raum, 73
aĚhnliche Matrizen, 108
Algebra, allgemeine, 292
erzeugte Unter-, 294
isomorphe, 295
mehrsortige, 293
Typ einer, 291
Unter-, 294
allgemeine Algebra, 292
allgemeine lineare Gruppe, 97
allgemeine Potenz, 188
Allquantor, 4
alternierende harmonische Reihe, 147

Anfangswertproblem, 270, 278
LoĚsung eines, 270
angeordneter KoĚrper, 40, 125
antisymmetrische Relation, 9
aĚquivalente Norm, 180
AĚquivalenz, 5
AĚquivalenzklasse, 11
AĚquivalenzrelation, 9, 11
Arcuscosinus, 193
Arcussinus, 193
Arcustangens, 193
Argument, 194
Aufpunkt, 72
Aussage, 3
Aussageform, 3
Automorphismus
KoĚrper-, 39
Axiomensystem, 52
minimales, 53
widerspruĚchliches, 53
widerspruchsfreies, 53
Babylonisches Wurzelziehen, 139
Banachraum, 159
Banachâscher Fixpunktsatz, 160
Basis, 60
Orthogonal-, 71
Orthonormal-, 71
Standard-, 60
BasisergaĚnzungssatz, 61, 71
Basiswechselmatrix, 105
Behauptung, 15
beschraĚnkte
Folge, 132

301

Funktion, 172
Menge, 125, 154
bestimmt divergente Folge, 137
Betrag
in C, 43, 194
in R, 126
Betragsfunktion, 126
Beweis
direkter, 15
durch Kontraposition, 5, 16
durch Widerspruch, 16
indirekter, 16
per vollstaĚndiger Induktion, 17
bijektiv, 14
Bild einer Funktion, 13
Bildraum, 83
Binomialformel, 129
Binomialkoeffizient, 128
BogenmaĂ, 189
Bolzano, Nullstellensatz von, 172
Bolzano-WeierstraĂ, Satz von, 159
C(D), 168
C(D; W ), 175
C, 42
C 1 (I), 206
C n (I), 207
Cauchy-Folge, 139, 154
Cauchy-Kriterium
fuĚr Folgen, 140
fuĚr Reihen, 147
Cauchy-Produkt, 152, 186
Cauchy-Schwarz-Ungleichung, 69
charakteristisches Polynom, 118, 288
Cosinus, 189
hyperbolicus, 195
Darstellungsmatrix, 92, 107
de lâHospital, Satz von, 211
de Moivre, Formel von, 195
De Morganâsche Regeln, 7
Definitheit (Norm), 66
Definitionsbereich, 13

Determinante, 110
einer linearen Abbildung, 116
Entwickeln, 110, 112
DGL, siehe gewoĚhnliche Differentialgleichung
Diagonalisierbarkeit, 118
Diagonalmatrix, 118
Differentialgleichung, gewoĚhnliche, 267
Anfangswertproblem, 270
autonome, 267
homogene, 273
lineare, 274, 278
LoĚsung, 268
von getrennten VeraĚnderlichen, 271
Differenzierbarkeit, 197
in eine Richtung, 221
n-mal stetige, 207
n-malige, 207
partielle, 221
stetig partielle, 222
stetige, 206
totale, 227
Dimension eines Vektorraums, 61
Dimensionsformel, 84
direkter Beweis, 15
Dirichletsche Sprungfunktion, 241
Disjunktion, 4
distributiver Verband, 293
divergente
Folge, 131, 154
Minorante, 149
Reihe, 145, 154
Division mit Rest, 19
Dreiecksmatrix, 111
Dreiecksungleichung
fuĚr Normen, 66
fuĚr Integrale, 243
in C, 44
in R, 126
umgekehrte, 126
verallgemeinerte, 148
Duhamelsche Formel, 276, 284

e, 136, 146
Ebene, 72
Hyper-, 74
Eigenraum, 120
Eigenvektor
einer linearen Abbildung, 116
einer Matrix, 116
Eigenwert
einer linearen Abbildung, 116
einer Matrix, 116
eindeutig loĚsbares LGS, 98
Einheitsmatrix, 90
EinschraĚnkung einer Funktion, 15
Einselement, 35
Elementarumformungen, 100
endliche Menge, 8
Entwickeln einer Determinante
nach erster Zeile, 110
nach j-ter Spalte, 112
nach k-ter Zeile, 112
Entwicklungspunkt, 183
erweiterte Koeffizientenmatrix, 99
erweiterter Euklidischer Algorithmus,
23
Erzeugnis, 32, 294
erzeugte Untergruppe, 32
erzeugte Unteralgebra, 294
Euklidische Norm, 66, 71
Euklidischer Algorithmus, 23
erweiterter, 23
Eulersche Formel, 191
Eulersche Zahl, 136, 146
Existenzquantor, 4
Exponentialfunktion, 151
Funktionalgleichung, 152
Matrix-, 280
Exponentialreihe, 151
Extremum
globales, 218, 235
lokales, 218
relatives, 218, 235
Faktormenge, 12

Faktorraum, 65
FakultaĚt, 128
Fermat, kleiner Satz von, 24
Fixpunkt, 160
Fixpunktsatz, Banachâscher, 160
Folge, 51, 131
beschraĚnkte, 132
bestimmt divergente, 137
Cauchy-, 139, 154
divergente, 131, 154
komplexe, 131
konvergente, 131, 154
Koordinaten-, 155
monoton fallende, 138
monoton wachsende, 138
monotone, 138
Null-, 131
reelle, 131
rekursiv definierte, 138
Teil-, 140, 159
Folgenraum, 51
Formel von Sarrus, 114
Formel von de Moivre, 195
Fortsetzung, periodische, 263
Fourier-Koeffizienten, 261
Fourierpolynom, 261
Fourierreihe, 261
Frequenz, 258
Fundamentalsatz der Algebra, 45
Fundamentalsystem, 279, 287
Funktion, 13
Ableitungs-, 197
partielle, 222
totale, 228
beliebig oft differenzierbare, 207
beschraĚnkte, 172
bijektive, 14
Cosinus-, 189
differenzierbare, 197
Dirichletsche Sprung-, 241
EinschraĚnkung, 15
Exponential-, 151
Funktionalgleichung, 152

gerade, 190
hyerbolische, 195
injektive, 14
integrierbare, 240
Koordinaten-, 175
lineare, 78
monoton fallende, 170
monoton wachsende, 170
monotone, 170
n mal differenzierbare, 207
periodische, 190
Signum-, 168
Sinus-, 189
Stamm-, 245
stetige, 168, 175
Lipschitz-, 171
streng monoton fallende, 170
streng monoton wachsende, 170
streng monotone, 170
surjektive, 14
Tangens-, 192
Umkehr-, 14
uneigentlich integrierbare, 255, 257
ungerade, 190
Verkettung von, 13
Funktionalgleichung der Exponentialfunktion, 152
Funktionenraum, 49
Funktionsvorschrift, 13
GauĂ-Algorithmus, 100
GauĂâsche Zahlenebene, 43
geometrische Reihe, 145
geometrische Summenformel, 137
Gerade, 72
gerade Funktion, 190
Getrennte VeraĚnderliche, 271
gewoĚhnliche Differentialgleichung, 267
Anfangswertproblem, 270
autonome, 267
homogene, 273
lineare, 274, 278
LoĚsung, 268

von getrennten VeraĚnderlichen, 271
GL(n, K), 97
globales Maximum/Minimum, 218, 235
globlaes Extremum, 218, 235
Grad
Fourierpolynom, 261
Taylor-Polynom, 212
trigonometrisches Polynom, 258
Gradient, 224
Gradientenmethode, 224
Graph einer Funktion, 13
Grenzwert, 131
einer Funktion, 165, 174
linksseitig, 165
rechtsseitig, 165
GrenzwertsaĚtze, 133
fuĚr Funktionen, 166
GroĂ-O, 142
groĚĂter gemeinsamer Teiler, 19
groĚĂtes Element, 10
Grundmenge, einer Algebra, 292
Gruppe, 27, 292
abelsche, 27, 292
allgemeine lineare, 97
erzeugte, 32
isomorphe, 33
orthogonale, 109
Permutations-, 28
Symmetrie-, 28
Unter-, 30
triviale, 30
Gruppenhomomorphismus, 33
Gruppenisomorphismus, 33
Hadamard, Satz von, 181
halboffenes Intervall, 127
harmonische Reihe, 146
alternierende, 147
HaĚufungspunkt einer Menge, 165, 174
HaĚufungswert einer Folge, 140, 159
Hauptsatz d. Diff.- u. Integr.-Rechn.,
245
Hesse-Matrix, 234

Hesse-Normalform, 76
Hilbertraum, 159
homogene Differentialgleichung, 273
homogenes LGS, 98
HomogenitaĚt (Norm), 66
Homomorphiesatz, 84
Homomorphismus, 295
Gruppen-, 33
KoĚrper-, 39
Ring-, 37
Vektorraum-, 78
Hospital, Satz von de lâ, 211
HuĚlle, lineare, 56
hyperbolische Funktionen, 195
Hyperebene, 74
Hesse-Normalform einer, 76

halboffenes, 127
offenes, 126
inverse Matrix, 96
inverses Element, 27
invertierbare Matrix, 96
isomorphe
Algebren, 295
Gruppen, 33
KoĚrper, 39
Ringe, 37
VektorraĚume, 78
Isomorphismus, 295
Gruppen-, 33
KoĚrper-, 39
Ring-, 37
Vektorraum-, 78

i, 42
IdentitaĚt, 13
imaginaĚre Einheit, 42
ImaginaĚrteil, 43
Implikation, 5
indefinite Matrix, 122
Indexshift, 54
indirekter Beweis, 16
Infimum, 10
inhomogenes LGS, 98
injektiv, 14
Inklusion, 6
innerer Punkt, 218
Inneres einer Menge, 218
Integral, 240
oberes, 240
Parameter-, 253
StandardabschaĚtzung, 244
unbestimmtes, 247
uneigentliches, 255, 257
unteres, 240
Integration, partielle, 249
Integrierbarkeit, 240
uneigentliche, 255, 257
Intervall, 126
abgeschlossenes, 126

Jacobi-Matrix, 224
kanonische Abbildung, 13
kartesisches Produkt, 6
Kern
einer linearen Abbildung, 82
einer Matrix, 94
eines Gruppenhom., 34
Kettenregel, 201, 231
Klein-O, 142
kleiner Satz von Fermat, 24
kleinstes Element, 10
kommutativer Ring, 36
kompakte Menge, 158
Komplement einer Menge, 6
komplexe Folge, 131
komplexe Konjugation, 43
komplexe Zahl, 42
konjugiert, 43
komplexe Zahlenebene, 43
komplexer Vektorraum, 47
Konjugation, komplexe, 43
Konjunktion, 4
Konstante, 291
Kontraposition, 5
konvergente

Folge, 131, 154
Majorante, 149
Reihe, 145, 154
Konvergenzradius, 182
konvexe Menge, 233
Koordinaten, 62
Polar-, 194
Koordinatenfolge, 155
Koordinatenfunktion, 175
Koordinatenvektor, 62
KoĚrper, 38
angeordneter, 40, 125
der komplexen Zahlen, 42
isomorphe, 39
KoĚrperautomorphismus, 39
KoĚrperhomomorphismus, 39
KoĚrperisomorphismus, 39
Kreuzprodukt, 78
Kronecker-Delta, 51
Kugel, 156
K-Vektorraum, 47, 293
â2 , 160, 264
Landau-Symbole, 142
leere Menge, 6
Leibniz-Kriterium, 147
LGS, siehe lineares Gleichungssystem
Limes, 131
einer Funktion, 165, 174
linksseitig, 165
rechtsseitig, 165
lineaere HuĚlle, 56
lineare AbhaĚngigkeit, 58
lineare Abbildung, 78
Determinante einer, 116
diagonalisierbare, 118
lineare Differentialgleichung, 274
homogene, 274
inhomogene, 274
System von, 278
lineare UnabhaĚngigkeit, 58
lineares Gleichungssystem, 98
eindeutig loĚsbares, 98

homogenes, 98
inhomogenes, 98
loĚsbares, 98
Matrixform, 98
unloĚsbares, 98
Linearkombination, 56
linker Nullteiler, 38
linksseitiger Grenzwert, 165
Lipschitz-stetig, 171
Logarithmus, natuĚrlicher, 187
Stammfunktion, 250
logistisches Wachstumsmodell, 269
lokales Extremum, 218
lokales Maximum/Minimum, 218
loĚsbares LGS, 98
LoĚsung einer DGL, 268
LoĚsung eines Anfangswertproblems, 270
globale, 270
Majorante, konvergente, 149
Majorantenkriterium, 149
Matrix, 48, 87
Abbildungs-, 92, 107
aĚhnliche, 108
Basiswechsel-, 105
Darstellungs-, 92, 107
Determinante einer, 110
Diagonal-, 118
diagonalisierbare, 118
Dreiecks-, 111
Einheits-, 90
Hesse-, 234
indefinite, 122
inverse, 96
invertierbare, 96
Jacobi-, 224
negativ definite, 122
negativ semidefinite, 122
Null-, 49
orthogonale, 109
positiv definite, 122
positiv semidefinite, 122
quadratische, 89

regulaĚre, 96
singulaĚre, 96
Spur einer, 108
symmetrische, 121
transponierte, 90
Matrix-Exponentialfunktion, 280
Matrixform eines LGS, 98
Matrixprodukt, 87
Maximum einer Funktion, 218, 235
globales, 218, 235
lokales, 218
relatives, 218, 235
Maximum einer Menge, 10
Maximumsnorm, 67
Menge, 5
abgeschlossene, 156
beschraĚnkte, 125, 154
endliche, 8
Faktor-, 12
Grund-, 292
kompakte, 158
konvexe, 233
leere, 6
nach oben beschraĚnkte, 125
nach unten beschraĚnkte, 125
Ober-, 6
offene, 156
partiell geordnete, 9
Potenz-, 8
Teil-, 6
total geordnete, 9
Mengendifferenz, 6
Mengeninklusion, 6
mersortige Algebra, 293
minimales Axiomensystem, 53
Minimum einer Funktion, 218, 235
globales, 218, 235
lokales, 218
relatives, 218, 235
Minimum einer Menge, 10
Minorante, divergente, 149
Minorantenkriterium, 149
Mittelwertsatz, 209, 232

modulare Arithmetik, 20
monoton fallende
Folge, 138
Funktion, 170
monoton wachsende
Folge, 138
Funktion, 170
monotone
Folge, 138
Funktion, 170
Monotonie-Kriterium
fuĚr Folgen, 138
fuĚr Reihen, 147
Multiplikationstafel, 36
N, Nâ , 6
n-te Ableitung, 207
n-te Wurzel, 128
nach oben beschraĚnkte Menge, 125
nach unten beschraĚnkte Menge, 125
natuĚrlicher Logarithmus, 187
Stammfunktion, 250
Negation, 4
negativ definite Matrix, 122
negativ semidefinite Matrix, 122
neutrales Element, 27
Norm, 66
1-, 67
2-, 66, 245
â-, 67
aĚquivalente, 180
Euklidische, 66, 71
Maximums-, 67
Supremums-, 178
Normaleneinheitsvektor, 75
normierter Raum, 66
vollstaĚndiger, 159
n-stellige Operation, 291
Null-stellige Operation, 291
Nullabbildung, 50
Nullelement, 35
Nullfolge, 131
Nullmatrix, 49

Nullstellensatz von Bolzano, 172
Nullteiler, 38
Nullvektor, 47
O(n, R), 109
O-Notation, 142
obere Schranke, 10
oberes Integral, 240
Obermenge, 6
Obersumme, 240
offene Menge, 156
offenes Intervall, 126
Operation, 292
n-stellige, 291
Null-stellige, 291
Operationssymbol, 291
Ordnung einer DGL, 267
Ordnungsrelation, 9
Orthogonalbasis, 71
orthogonale Gruppe, 109
orthogonale Matrix, 109
orthogonale Vektoren, 71
Orthogonalprojektion, 72
Orthonormalbasis, 71
Parameter-Integral, 253
Parsevalsches Theorem, 264
Partialsumme, 145
partiell differenzierbar, 221
stetig, 222
partielle Ableitung, 222
partielle Integration, 249
partielle Ordnung, 9
PartikulaĚrloĚsung, 99, 280, 287
Pascalâsches Dreieck, 130
Peano, Satz von, 289
Periode, 190
periodische Fortsetzung, 263
periodische Funktion, 190
Permutationsgruppe, 28
Picard-LindeloĚff, Satz von, 289
Polarkoordinaten, 194
Polynom

charakteristisches, 118, 288
Fourier-, 261
Taylor-, 212
trigonometrisches, 258
Polynomring, 36
positiv definite Matrix, 122
positiv semidefinite Matrix, 122
Potenz
allgemeine, 188
rationale, 128
Potenzfunktion, 188
Potenzmenge, 8
Potenzreihe, 181, 183
Entwicklungspunkt, 183
Primzahl, 19
Private Key, 26
Produktregel, 200
Public Key, 26
quadratische Matrix, 89
Quotient (Division mit Rest), 20
Quotientenkriterium, 150
Quotientenraum, 65
Quotientenregel, 200
R, 125
Radius, 156
Rang
einer lineren Abbildung, 83
einer Matrix, 94
rationale Potenz, 128
Raum
affiner, 73
Banach-, 159
Bild-, 83
Eigen-, 120
Hilbert-, 159
normierter, 66
Realteil, 43
rechter Nullteiler, 38
rechtsseitiger Grenzwert, 165
reelle Zahlen, 125
reelle Folge, 131

reeller Vektorraum, 47
reflexive Relation, 9
Regeln von De Morgan, 7
regulaĚre Matrix, 96
Reihe, 145
absolut konvergente, 148, 154
divergente, 145, 154
Exponential-, 151
Fourier-, 261
geometrische, 145
harmonische, 146
alternierende, 147
konvergente, 145, 154
Potenz-, 181, 183
Taylor-, 212
Reihenwert, 145, 154
rein imaginaĚre Zahl, 43
rekursiv definierte Folge, 138
Relation, 8
antisymmetrische, 9
AĚquivalenz-, 9, 11
Ordnungs-, 9
reflexive, 9
symmetrische, 9
transitive, 9
relatives Extremum, 218, 235
relatives Maximum/Minimum, 218, 235
Rest, 20
Restglied, 214
Richtungsableitung, 221
Richtungsvektor, 72
Riemann-Integral, 240
Riemann-integrierbar, 240
Ring, 35, 293
der Polynome, 36
isomorpher, 37
kommutativer, 36
mit Eins, 35
Ringhomomorphismus, 37
Ringisomorphismus, 37
Rolle, Satz von, 209
RSA-Algorithmus, 25

Sandwich-Theorem, 133
Sarrus, Formel von, 114
Satz
Banachâscher Fixpunkt-, 160
BasisergaĚnzungs-, 61, 71
Fundamental- der Algebra, 45
Haupt-, 245
Homomorphie-, 84
Mittelwert-, 209, 232
Schranken-, 233
von Bolzano, Nullstellen-, 172
von Bolzano-WeierstraĂ, 159
von de lâHospital, 211
von Fermat, kleiner, 24
von Hadamard, 181
von Peano, 289
von Picard-LindeloĚff, 289
von Rolle, 209
von Schwarz, 226
von Taylor, 214, 235
Zwischenwert-, 172
Schnitt von Mengen, 6
Schrankensatz, 233
Schwarz, Satz von, 226
senkrechte Vektoren, 71
Signum, 168
singulaĚre Matrix, 96
Sinus, 189
hyperbolicus, 195
Skalar, 47
Skalar-Multiplikation, 47
Skalarprodukt, 68
Standard-, 68
Spaltenrang, 94
spezielle LoĚsung, 99, 280, 287
Spur einer Matrix, 108
Stammfunktion, 245
StandardabschaĚtzung fuĚr Integrale, 244
Standardbasis, 60
Standardskalarprodukt, 68
Standardvektorraum, 48
Stelligkeitsabbildung, 291
stetig differenzierbar, 206

n-mal, 207
stetig partiell differenzierbar, 222
Stetigkeit, 168, 175
Lipschitz-, 171
stuĚckweise, 261
streng monoton, 170
streng monoton fallend, 170
streng monoton wachsend, 170
stuĚckweise
glatt, 261
stetig, 261
Substitutionsregel, 251
Summationsindex, 54
Summenschreibweise, 54
Superpositionsprinzip, 274
Supremum, 10
Supremums-Norm, 178
surjektiv, 14
Symmetriegruppe, 28
symmetrische Relation, 9
symmetrische Matrix, 121
Tangens, 192
hyperbolicus, 195
Taylor, Satz von, 214, 235
Taylorpolynom, 212
Taylorreihe, 212
Teilbarkeit, 19
Teilfolge, 140, 159
Teilmenge, 6
Teilsumme, 145
Teleskopsumme, 129
Theorem, Parsevalsches, 264
total differenzierbar, 227
total geordnete Menge, 9
totale Ableitung, 228
Totalordnung, 9
transitive Relation, 9
transponierte Matrix, 90
Transposition, 48, 90
Trennung der Variablen, 271
trigonometrische Funktionen, 189
Additionstheoreme, 191

trigonometrischer Pythagoras, 190
trigonometrisches Polynom, 258
triviale Untergruppen, 30
Typ, 291
umgekehrte Dreiecksungleichung, 126
Umkehrfunktion, 14
unbestimmtes Integral, 247
uneigentlich integrierbar, 255, 257
uneigentliches Integral, 255, 257
unendlichdimensionaler Vektorraum,
61
ungerade Funktion, 190
Ungleichung
Cauchy-Schwarz-, 69
Dreiecks-, 44, 66, 126, 243
verallgemeinerte, 148
umgekehrte Dreiecks-, 126
unloĚsbares LGS, 98
Unteralgebra, 294
erzeugte, 294
untere Dreiecksmatrix, 111
untere Schranke, 10
unteres Integral, 240
Untergruppe, 30
erzeugte, 32
triviale, 30
Untergruppenkriterium, 31
Unterminor, 123
Untersumme, 240
Untervektorraum, 55
Untervektorraumkriterium, 55
Urbild, 13
Variation der Konstanten, 275
Variation-der-Konstanten-Formel, 276,
284
Vektor
Eigen-, 116
Normaleneinheits-, 75
Richtungs-, 72
transponierter, 48
Vektoraddition, 47

Vektorraum, 47, 293
Dimension, 61
isomorpher, 78
komplexer, 47
reeller, 47
Standard-, 48
unendlichdimensionaler, 61
Unter-, 55
Vektorraum-Homomorphismus, 78
Vektorraum-Isomorphismus, 78
Vektorraumbasis, 60
verallgemeinerte Dreiecksungleichung,
148
Verband, 293
distributiver, 293
Verbindungsstrecke, 232
Vereinigung von Mengen, 6
Verkettung von Funktionen, 13
VerknuĚpfung, 27
vollstaĚndige Induktion, 17
vollstaĚndiger normierter Raum, 159
VollstaĚndigkeitsaxiom, 125
Volterra-Lotka-Gleichungen, 277
Voraussetzung, 15
Wachstumsmodell, 267
logistisches, 269
Wachstumsrate, 267
Wahrheitstafel, 5
widerspruĚchliches Axiomensystem, 53
widerspruchsfreies Axiomensystem, 53
Wurzel, 128
Wurzelkriterium, 150
Zahl
Eulersche, 136, 146
komplexe, 42
reelle, 125
rein imaginaĚre, 43
Zerlegung eines Intervalls, 239
Zielbereich, 13
Zn , 13
Zwischenwertsatz, 172

